elastic shard allocation awareness

для начала я расскажу к чему в конечном итоге оно приведет.



пусть у нас 3 физ сервера.
на каждом по 3 виртуалки.

мы указываем на каждой виртуалке rack_id 
в котором прописано на каком сервере крутится виртуалка.

пусть у нас индекс = 5 шардов = 0 1 2 3 4

положим на скажем пусть будет у него 20 реплик.

возникает вопрос как на практике эластик рассует шарды по 
виртуалкам и по серверам в целом. какое праивло какой закон.

так вот эластик недаст засунуть 20 реплик при включенном шард аллокейшн 
аварнесс. максимально что он даст сделать это:


виртуалка 1 = 0 1 2 3 4
виртуалка 2 = 0 1 2 3 4
виртуалка 3 = 0 1 2 3 4

тоесть при актививраонной настройке эластик категорически недаст засунуть
ДВА ОДИНАКОВЫХ шарда\реплики на одну ноду. заметим мы щас подметили правило про ноду а не про сервер. 
также если сказать чуть с другой стороны то одна нода максимально в себе
может нести только один индекс целиком. неболее.


получается если у нас три сервера. то максимально мы сможем засунуть на сервер три копии данных ( столько сколлько виртуалок на сервере),
если серверов три то суммарно будет 3 х 3 = 9 копий данных.
это равно 8 реплик. остальные реплики будут висеть незааллокейчены.
вот так на практике работате шард аллокейшн если он активирован.
главное правило - на виртуалке неможет быть двух одинаковых щардов, или
двух одинаковых реплик, или шарда с репликой. на сервере в целом может.
на виртуалке нет.

дальше. положим у нас на физ сервере три виртуалки ( как и прежде).
мы создаем индекс 10 шардов 0 реплик. возникает вопрос как эластик распределит. он все 10 шардов разместит на одном физ сервере? ну типа мы же как признак ввели rack_id. то есть признак это сервер.
ответ - нет. часть шардов будет сидеть на одном сервере часть на другом
итп. тоесть у эластика нет такой цели разместить индекс как целое 
на одном rack_id

тоетсть логика у эласиика совершенно не совпадает с логикаой которую от него ожидаешь когда elastic shard allocation awareness включен.

если у нас есть одна группа виртуалок на одно физ сервере
и другаая група виртуалок на другом физ сервере то ожидаешь 
что он постарается разместить одну копию индекса на одном физ сервере
а другую копию на другом. а если копия одна то всю на одном.
но это нетак. у него логика совершенно другая. он индекс одну копию 
данных старается равномерно размазать по всему кластеру по всем физ 
серверам. единственное за чем он следит и что он недаст сделать это чтобы два одинаковых  шарда\реплика оказались наодной виртуалке.
логика странная. но так у него работает.
логика походу такая что если погибает физ сервер то погибнет минмальная часть непродублированных данных.

походу логика работает так. 
скжаем есть у нас индекс и 20 реплик

(0) (1) (2) (3)|(0) (1) (2) (3) | (0) (1) (2) (3) | ...

есть у нас три сервера


[1] [2] [3].

эластик берет порцию (0) у индекса и кладет на первый серер

[1] (0)


потом берет следущий кусок данных (1) и кладет на следущий сервер
потому что так надежнее в плане отказа

[1] (0) ,     [2] (1)

берет следущий кусок  (2) и кладет  в следущий сервер ибо так надежнее.
чем больше мест хранения тем надежнее

[1] (0) ,     [2] (1),   [3] (2)


потом берет следущий кусок (3) и опять снова кладет на первый

[1] (0)(3) ,     [2] (1),   [3] (2)

получается индекс целиком размазан максимально по физ серверам. 
а занчит макс надежен от отказа. при отказе 1 физ сервера потеряется
только 1\3 данных. а если разместить индекс на одном сервере то потеряется
сразу 100% данных. (ну чуть несимметрично потому что индекс неделится на три нацело)


и так до конца


[1] (0)(3)(2)... ,     [2] (1)(0)(3)...,   [3] (2)(1)(0)...


теперь вспоминаем что внути каждого сервере крутится три виртуалки.
и эдластик начинает по такому же принципу распределять данные
уже между виртуалками . по принципу что каждый новый кусок надо положить по возможности обязательно в новое место. поэтому будет

[1] (0)(3)(2)(1)(0)(3)(2)(1)(0)(3)(2)(1)(0)(3)(2)(1)....

покажу распределние данных по шагам

шаг 1
{виртуалка 1} (0)
{виртуалка 2} (3)
{виртуалка 3} (2) 

шаг 2

{виртуалка 1} (0)(1)
{виртуалка 2} (3)(0)
{виртуалка 3} (2)(3) 

шаг 3

{виртуалка 1} (0)(1)(2)
{виртуалка 2} (3)(0)(1)
{виртуалка 3} (2)(3)(0)

шаг 4

{виртуалка 1} (0)(1)(2)(3)
{виртуалка 2} (3)(0)(1)(2)
{виртуалка 3} (2)(3)(0)(1)

а вот дальше уже ничего небудет

шаг 5
потому что на этом шаге у нас появляется виртуалка к которой
есть две одинаковые порции данных. а это эластик при данной настройке
делать отказыается.

{виртуалка 1} (0)(1)(2)(3) (0)
{виртуалка 2} (3)(0)(1)(2)
{виртуалка 3} (2)(3)(0)(1) 

поэтому вот так выгляди тлогика расрпдедения порций данных реплик\шардов
при этой опции. минимальная потеря порций данных при падении 
энтити тоесть виртуалки или сервера.
ну и установлено доп ограничение что на виртулке может лежать максмум
полная копия индекса и неболее.

далее. научимся на практике
вычислять какая степень отказоустойчивости 
к потере даныным у нас есть 
при заданном количестве копий данных и числе физ серверов.

пусть  у нас есть индекс который состоит из 1 шарда +0 копий.

тогда понятно что гибель одного сервера убьет данные.

[1] [2] [3](0) [4] [5]

тоесть при падении сервера [3] индекс исчезнет.
видим что у нас 1 копия данных и устойчивость у нее 0 серверов.

возьмем 1 шард + 1 реплика

[1](0) [2] [3](0) [4] [5]

видно что теперь мы 100% выживаем при падении 1 любого сервера.
тут логика такая что у нас два нолика (0) и надо понять сколько ноликов
надо чтоб погибо чтобы погибли данные и оставить один нолик.

(0)  

один нолик. погибать ему нельзя.

(0) (0)
два нолка. один может поиьуннуть один останется.


(0) (0) (0)
три нолика. два может погибунть. один все равн останется.

получается в случае когда индекс состиоит из 1 шарда + n реплик
то копий данных всего (n+1) , погибнуть могут n и останется (n+1)-n =1
значит при конфигурации индекс + n копий устойчивость = n серверов может пгибнуть

например 

индекс + 2 реплики. значит безболезненно погибнуть может два сервера.

а что на случай если индекс состоит из несокльких шардов.

(0)(1)(2)(3)...
тут важно понять что если все нолики (0) погибнут то и остальных шардов 
толку уже нет. так что мы возврашаемся к предыдущей задаче.

ИТОГО: если дан индекс + n реплик.
и мы включили elastic shard allocation awareness
то безболезненно может погиб любые n штук серверов.

 индекс + 0 реплик = 0 сервров
 индекс + 1 реплик = 1 сервер
 индекс + 2 реплик = 2 сервера
 
 понятно что в определеннром случае может и больше серверов погибнет
 и индекс выживет. нам важно оценка снизу то есть 100% гарантия.
 тоесть если  у нас указано что может погибнуть 2 сервера то порой гибель 
 трех сереров неубьет индекс но порой и убьет. а вот ЛЮБЫЕ  два умершие
 серверы никак 100% неубьют индекс.
 
 таким образом откаузоустойвость индекса незавиисит от числа шардов
 а зависит только от числа реплик.
 
 это очень удобно для практических расчетов.
 
 




далее.
как это настроить

во первых как проверить в каком состоянии щас эти настройки

curl -X GET -H 'Content-Type: application/json' http://localhost:9200/_cluster/settings/?pretty

вот пример

~# curl -X GET -H 'Content-Type: application/json' http://localhost:31907/_cluster/settings/?pretty
{
  "persistent" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "awareness" : {
            "attributes" : "rack_id"
          },
          "enable" : "all"
        }
      }
    },


также сразу скажу а как же заресетить настройку.тоесть как бы
как удалить настройку. ответ - надо ей присвоить значение null
пример

curl -XPUT -H 'Content-Type: application/json' localhost:31907/_cluster/settings -d '{
                "persistent" : {
                    "cluster.routing.allocation.awareness.force.zone.values": null
                }

        }'


здесь мы удаляем настройку

"cluster.routing.allocation.awareness.force.zone.values": 

также настройка может иметь тип transient а может persistent.
транзиент типа исчезает если однроверменно погасить все мастер ноды
а персистент она хранится на диске в кишках мастер нод и переживает
ресет всех мастер нод.

тогда вопрос а что если у нас сразу выставлена настройка и в транзиент
и в персистент. что главнее. главнее транзиент.

с этой хернй разобрались. 

наконец ближе к делу двигаем далее к самой сути:

как это настроит в реализации для стейтфуллсета на кубе

нужно в стейтфуллсете в том месте где описыватся под дата нода
раздел ENV добавить кусок


>>>       - name: node.attr.rack_id
>>>         valueFrom:
>>>           fieldRef:
>>>             fieldPath: spec.nodeName

ниже я показываю куда его вставлять

 env:
          - name: cluster.name
            value: es-cl-01
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: "discovery.seed_hosts"
            value: "masters-headless"
          - name: "node.master"
            value: "false"
          - name: "node.voting_only"
            value: "false"
          - name: "node.data"
            value: "true"
          - name: "node.ingest"
            value: "true"
          - name: "node.ml"
            value: "false"
>>>       - name: node.attr.rack_id
>>>         valueFrom:
>>>           fieldRef:
>>>             fieldPath: spec.nodeName
          - name: ES_JAVA_OPTS
            value: "-Xms100m -Xmx100m"

и в для мастер нод добавить кусок

>>>       - name: "cluster.routing.allocation.awareness.attributes"
>>>         value: "rack_id"


 env:
          - name: cluster.name
            value: es-cl-01
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: "cluster.initial_master_nodes"
            value: "es-master-0, es-master-1, es-master-2"
          - name: "discovery.seed_hosts"
            value: "masters-headless"
          - name: "node.master"
            value: "true"
          - name: "node.voting_only"
            value: "false"
          - name: "node.data"
            value: "false"
          - name: "node.ingest"
            value: "false"
          - name: "node.ml"
            value: "false"
>>>       - name: "cluster.routing.allocation.awareness.attributes"
>>>         value: "rack_id"
          - name: ES_JAVA_OPTS
            value: "-Xms100m -Xmx100m"

все. 
	
ссылка
	https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#shard-allocation-awareness
	
есть еще момент.
если  у нас физ сервер падает и виртуалки на нем.
то пронятное дело что все что на нем все реплики
они начнут генерироваться на оставшемся железе.
проблема в том что при этом нужна будет туча дискового места.
плюс конечно в том что у нас остается по прежнуму таже самая реданданси. это хорошо. но на оставщихся серверах может небыть столько места
свободного.

так вот эластику можно скзаать чтобы он этог онеделал.
типа если физ сервер упал то ненадо с него генерировать реплики 
на оставшемся. а то нагенрирукются и все место убьют и записывать 
в кластер будет некуда.

ранее на  мастерах мы указали

cluster.routing.allocation.awareness.attributes: rack_id

теперь на мастерах надо добавить +1 строчку

cluster.routing.allocation.awareness.force.zone.values: server1,server2, server3

в ней надо перечислить все серверы.

так вот походу я эту настройку добавлять в эластик небудут.
потому что с ней мудота. 
нужно будет помнить что при добавлении нового сервера его нужно
внести в эту строку, при убавлении сервера его нужно оттуда убрать
это реально все забудется это мудота.

гораздо проще если сервер вылетел то зайти на эластик и уменьшить число 
реплик на время починки сервера.

это точно незабудется

с другой стороны  в эластика поумолчанию так сделано что если диск заполнен на 80% то новые шарды генерироватся небудут. поэтому 
вот эту настройку можно в принципе и неиспользовать

cluster.routing.allocation.awareness.force.zone.values: server1,server2, server3

так как больше чем на 80% диск новыми шардами при падении сервера 
ненаполнится. тоесть без нее мы будем иметь по крайней мере
частичное восстановление уровня реплиикации а сдругой стороны новые реплики несмогу засрать диски больше чем на 80%



очень плохо что нужно прписывать кажыдый конкретный сервер в списочке.
отстой.
надо чтоб просто она работалп по принципу вкл\выкл без всяких списочков

ссылка
	https://www.elastic.co/guide/en/elasticsearch/reference/7.x/modules-cluster.html#forced-awareness
	
==
