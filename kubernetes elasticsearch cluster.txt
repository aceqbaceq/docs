делал на основе этого = https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes


при установке эластика в куб нужно будет заюзать такую вещь как
headless service. но прежде чем о нем говорить надо узнать что такое
просто service в кубе. поды в кубе появляются и исчезают. у них меняются
dns имена и ip. и если у нас одни поды зависят от других подов
то нам неудобна такая ситуация когда одним подам нужно будет постоянно
узнавать что под до которого он стучался у него нето что ip изменился
у него dns имя изменислось. и вот приходит на помощь service.
service это такой же одьект как и pod для куба. его можно создать.
так вот у него и dns и ip не меняется поэтому его хорошо указывать как
точку входа. сам service имеет прописанный признак label. все поды которые
имеют такой же label автомтом считаются принадлежашим этому service.
ну и видимо serivice на основе этого label всегда в курсе какой ip и dns 
имя у всех подов которые ему "принадлежат". таким образом мы 
в качестве точкти входа на другом поде указываем service а он уже перенаправляет поток на один из подов который ему принадлежит. это уже
задача serivice знать какие ip и dns имена его подопечных подов
и как они меняются. насколько я понял service кидает поток на свои бекенд поды по закону roundrobin.  я думаю мы могли бы в исходном поде
заюзать тот же механизм label и убрать из цепочки service но вот как есть 
так есть. вот этот вот IP который имеет service он виден только внутри
хостов куба и он зоавется Cluster-IP.

переходу к headless service. он бывает с селекторами и без.
селекторы это как раз те самые labels которые мы прописываем в свойствх
service и по которым он ищет pods  которые ему принадлежат
так вот headless service он не имеет Cluster-IP. а его DNS запись этого сервиса как я понял резолвится во все IP подов. условно говоря
если dns имя headless serice = vasya.local то в coredns куба будут 
такие записи типа A

192.168.1.1 A vasya.local
192.168.1.2 A vasya.local
192.168.1.3 A vasya.local

таким образом если мы обратимся к vasya.local мы зарезолвимся в IP адреса
подов.

значит как на уровне yaml задается headless service. он задается только тем что мы  в сервисе указываем что у него нет IP адреса => clusterIP: None

#cat *.yaml

kind: Service
  clusterIP: None
 
и куб понимает что мы хотим именно headless service

вот как целиклм выглядит yaml от headless service

# cat elasticsearch_svc.yaml

kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: kube-logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
    - port: 9200
      name: rest
    - port: 9300
      name: inter-node


здесь указано имя сервиса, в каком куб неймспейсе ( не путать с линукс
неймспесом это соввсем разное) он будет принадлежать, и признаки (селекторы) подов которые принадлежат этому сервису.

насколько я понимаю слектор только один  app: elasticsearch
выполняем kubectl -f apply...
проверяем что сервис  сооздался

# kubectl get svc --namespace=kube-logging
NAME            TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)     
elasticsearch   ClusterIP  None          <none>        9200/TCP,9300/TCP

важно заметить что мы увидим этот сервис толко если сппециально укажем
что ищем в неймспейсе kube-logging

как только мы создали сервис то у него в coredns пояивлось dns запись
elasticsearch.kube-logging.svc.cluster.local

 я сразу же проверил но чтото coredns пищет что такого DNS имени нет.
 ладно..
 
 далее пееходим к следущему компоненту который нам нужен это 
 Stetefull Set.  в чем его фишка. 
 как я понял эта хрень она наблюдает и управляет за группой подов.
 при этом . если под умирает то новый который создается вместо него 
 будет иметь :  тот же pod-id, тот же dns-name, тот же ip, будет поднят
 на том же куб хосте. statefull означает что нам важно identity пода.
 хотя поды и запускаются из одного spec но после рождения они обладают
 индивидуальными чертами. например в под пробрасывается папка из фс хоста
  и под в нее пишет индивидуальные данные (ровно как и есть у дата нод
  эластика например) поэтому нам важно что если под умер упал сломался 
а под это процесс нам важно вместо него поднят процесс\под на том же 
куб хосте со всеми теми индиивдуальными признаками что имел умерший под.
такой под очень сильно напоминает вирт машину и "простой" процесс из systemd на линуксе. если он умер мы его заново запускаем на той же машине
и он получает все теже свойства что умерший процесс. итак
поды которые входятв statefull set они все обладаютт уникальными признаками и они НЕЯВЛЯБТИСЯ взаимозаменяемыми. ровно так и есть в случае
подов для эластика. для противопоставления таким подам в кубе есть другрй
обьект это Deployment который руководит абсолютно равнознаыными подами.
поэтому если под из depliymetn умирает то вместо него запускает под 
который будет запущен нефакт что натом же самом хосте и у этого пода
новые pod-id, новый ip, новоый dns -name. но нам это и неважно. поды
из deployment все абсолюбтно заменяемы. примеров таких подов можно
представить поды  от хапрокси. они все имеют один и тот же конфиг.
ничего в себе нехранят. поэтому если deployment перенаправить поток на любой под с хапрокси реузультат будет один и тот же для нас.
условно говоря еще например веб сервера хорошо подоходят под роль
подов в deplpyment. тот же самый kub-apiserver тоже хорошо подходит 
на роль подов в deployment ибо он читает все данные из etcd а в себе ничего нехранит.

итак еще раз в statefull set поды стартуют из одного spec но после 
рождения получают индивидальные признаки которые для нас важны. 
поэтому если такой под умер надо создать на месте умешего  ровно такой же как под как был -  с таким же ip, dns name, pod-id.
тогда для нашего запроса клиентского это будет выглядет так как бутто
служба к которой мы обращались просто перезагрузилась.

как напписано в доке от куба statefeull сет хорошо подходит для подов
которые чтото хранят на проброшенных внутрь их PV с хоста ФС. 
поэтму нам важно что если под умер чтобы созданный на месте его (и подключенный к PV пода который умер) имел все теже свойства что умерщий
под. 

навскидку непонятно зачем нам headless service если у нас есть statefulll set. хотя может быть это разное как теплое и синее.

навскидку я понимаэ это так. service он работает с уже созданными подами.
это поды создал ктото другой. сервис работает с тем что есть.
statefull set он непосредственно руководит созданием подов. именно 
стетфулл сет заставит куб генерироват поды по определенному правилу.
например что если под умер то новый должен обладать определенными свойствами.


конфиг yaml от statefull set по частям я буду давать



apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: kube-logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch


имя его es-cluster, лежит в неймспейсе kube-logging, что важно что тутже
указано что данные statefullSet принадлежит наверх к сервису elasticsarch 
(serviceName: elasticsearch), значит то что мы прикрепляем sttefull set
к сервису да непросто сервису а headless сервису дает то что каждый под
будет доступен по dns имени 

es-cluster-[0,1,2].elasticsearch.kube-logging.svc.cluster.local

насколько я понимаю вот это поле app: elasticsearch оно указывает  какой
label будет присобачен к подам.

публикую следущий блок statefull set

spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
			
			
	
очевидные поля небуду описывать. только особые
вот этот кусок. 

volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        
он говорит о том что внутрь контейнера будет проброшено из хост ФС папка.

в разделе 

env:

показано как вбить в под опции из классического конфига эластика

мы видим что 

name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"
			
перменная discovery.seed_hosts = "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"

откуда делаем вывод что созданный под должен иметь переменную 
в своей памяти которая обозначает название ноды для эластика и названия 
нод будут  
es-cluster-0.elasticsearch
es-cluster-1.elasticsearch
es-cluster-2.elasticsearch

соотсвтенно теперь надо понять каким макаром мы задаем нзвание ноды 
в смысле ноды эластика, тоесть фишка втом что мы неможем прописать 
в кониге пода константу нужна какая то переменная, вот как это выглядит

name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name


значит если мы отмотаем наверх ища metadata.name

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  
  мы найдем что metadata.name = es-cluster
  в итоге неочень понятно как у нас es-cluster будет превращаться в 
  es-cluster-0
  es-cluster-1
  es-cluster-2
  
  если честно. хотя может быть metadata.name относится не к метадате
  stetefullSet а к метадате самого пода и излекается чтото оттуда. непонятно.
  
  про перменную discovery.seed_hosts . эластик разрещает в этой переменной указывать не IP адреса а DNS имена. 
  как нам узнать будущие DNS имена наших подов. а вот 
  наши поды благодаря тому что 
  они будут принадлежать headless service они гарантированно будут иметь DNS имена вот такие. именно ради зараннее известности их dns имен
  мы и присобачиваем поды к headless service.
  имена будут такие:
  
  es-cluster-[0,1,2].elasticsearch.kube-logging.svc.cluster.local
  
  но в конфиге мы можем указать дажее их укороченную версию, потому что
  куб знает про какой домен идет речь.
  поэтому мы их укаываем в таком виде
  
  es-cluster-[0,1,2].elasticsearch
  
  можно заметить что cluster.initial_master_nodes
  указано другое нетакое же как discovery.seed_hosts
  . сравним
  
		  - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
  


казалось бы почему в cluster.initial_master_nodes неуказать тоже саоме.
а засада в том что в discovery.seed_hosts мы указываем DNS ИМЕНА

а в cluster.initial_master_nodes мы указываем не dns имена а имена 
В СМЫСЛЕ ЭЛАСТИКА

поэтому эти перменные и разные.


публикую следующий кусок yaml кода

. . .
      initContainers:
      - name: fix-permissions
        image: busybox
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
		  
		  

это так называемые Init контейнеры.
они будут запущены при создании пода причем друг за другом, должны отработать причем успешно
  итолько потом будет запущен уже основной контейнер пода.
  
  первый инит контейнера устанаолвает верные права на папку под индексы
  
  command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
  
  что я пока непонял. написано что 1000:1000 это дефолтовый юзер
  непривилигированный под которым эластика в 
  контейнере обычно стремиттся начать работать. так вот - если ты зайдешь 
  в работающий контейнер то там в /etc/passwrd можнро найти созданного
  юзера elasticsearch у котрого uid=1000,guid=1000. все окей.
  но если зайти на хост то в /etc/password небудет пользователя elasticsearch. что я непойму. насколько я вижу user namespace что на хосте   что в поде один и тотже.но при этом получается что /etc/passwd
  у них разные. я еще нечитал про user namespace походу время чтения
  все ближе.
  
  
  
  второй инит контейнер повысит значение переменной max_map_count
    command: ["sysctl", "-w", "vm.max_map_count=262144"]
	
	сейчас я поясню за что отвечает эта переменная и зачем это нам надо
	менять в рамках запуска эластика
	при работа с файлами есть несколько способов. один из них это через
	системный вызов mmap. я пока смутно понимаю но по моему это работает так мы говорим линуксу-ядру что мы хотим получить в нашей памяти
	кусок который имеет прямое соотвествие с куском файла на диске начиная
	 с такого то смещения в файле и размер такйото. я неочень понял ограничен ли размер куска который мы хотим замапить в память.
	 возможно размер этого куска ограничен. скажем мы говорим хотим 
	 в памяти иметь участок отображаенный в файл начиная с 1МБ от старта
	 а его размер скажем ограничен лиуксом в размере 100КБ.
	 далее мы обращаемся к отдельным участкам этого куска а линукс нам
	 по мере нашего обращения подкачивает туда содержимое файла.
	 так вот этот способ получения данных из файла еще раз скажу назвыаетмся mmap. есть и другие способы например через системный вызов read. он рботает по другому. щас мы это все обсудим. возвращаясь к эластику у него можно задать каким способом эластик
	 будет работать с индексом на диске. один из них как раз работает 
	 через mmap. так вот как я примерно понимаю возможно суть вот в чем
	 что эластику нужно постоянно лазить на диск в свои индексы причем
	 чтение происходит с него рандомно и маленькими кусками и одновременно
	 из множества мест. поэтому эластику нужно одновременно читать и писать в индекс на диске в миллион мест мелкими кусками.  и каждая
	 такая операция требует замапить в память кусочек файла с диска.
	 так вот по дефолту линукс ограничивает максимальное количество
	 кусочков памяти которое процесс может создать для целей маппинга.
	 и это число надо увеличить . по умолчанию оно равно 65 000 типа того 
	 мы его увеличиваем до vm.max_map_count=262144. еще раз это не изменение размера одного замапленного куска. это число таких 
	 на данных момент замапленных кусков макс.
	 
	 итак еще раз . эластик работает с файлами индексов на диске. читает 
	 и пишет туда. это можно делать несколько разными способами в линуксе.
	 в эластике можно типа даже выбирать каким способом эластик будем
	 читать и писать в индексы. один из способов использует системный
	 вызов mmap. если этим способом читать и писать на диск то тогда для 
	 этого в памяти нужно выделять участки которые будут иметь отображения
	 кусочков диска. по умолчанию линукс для процесса дает ограничение 
	 что процесс максимально может иметь под задачи маппинга 65000 кусочков памяти для маппинга. как я понял эластику этого мало и мы
	 увеличиваем количество таких кусочков которые эластик может единомоментно иметь созданными до 200 000 штук. как я понимаю что 
	 если оставить = 60000 то на каком то этапе  эластик напишет что мол 
	 у меня нет больше доступной памяти для чтения записи в индекс.
	 как я понимаю перменная vm.max_map_count увеличивает число кусочков
	 под маппинг максимальное нетолько для нашего процесса но для каждого.
	 поэтому когда наш инит контейнер умрет то данное значение останется в силе для уже того основного процесса эластика что будет запущен в итоге. иначе бы эта настройка неимела бы смысла. ибо инит контейнер процесс умер а с ним и его индивидуальная настройка.
	 
	 а щас я малек углублюсь на счет как работает mmap. и его отличие 
	 от другого систмного вызова read. насколько я понимаю когда мы юзаем
	 mmap мы указвыаем смщение начиная мол с какого байта на диске 
	 мы хотм начать отображать файл в память. предположим мы хотим начать
	 отображать файл на диске сначала с нулевого байта. скажем на компе
	 физически 10ГБ памяти а файл размером 100ГБ. возникает вопрос как же
	 тогда мы можем получить доступ к хвосту файла и думаю ответ такой - процесс работает с вирт адресным пространством а не с реальным а 
	 на 64 цпу и 64 ОС оно равно 8ТБ.понятно что это фиктивное пространство и ядро обеспечивает ячейки этого фиктивного пространства
	 памяти по мере того как процесс обращается к его конкртным кускам.
	 далее я оставляю эту тему в этом документе и переношу ее в отдельный текстовый файл kernel linux
	 
	 
	 
	
	 переходим к следущему init контейнеру который выполняет команду
	 
	 command: ["sh", "-c", "ulimit -n 65536"]
	 
	 эта хрень задает сколько процесс может иметь открытых файлов.
	 я ночень понял зачем это надо. скажем я посмотрел сколько 
	 открытых файлов щас на дата ноде боевого эластика у процесса эластика
	 
	 # ls -l /proc/$PID/fd | wc -l
	 
	 оказалось 772 и эта цифра она нескачет она слабо меняется.
	 спрашивается захера при средней цифре 772 ставить ее равной 65000.
	 что за бред. вопрос открытй.
	 
	 я посмотрел сколько  вообще суммарно на ноде  открытых файлов у всех процессов суммарно. получил 145 000
	 
	 # lsof | wc -l
      145837

	 я посмотрел число  открытых файлов для всех процессов работающих под юзером elasticsearch 
	 
	 # lsof -u elasticsearch | wc -l
      1195
     
	 поэтому смысл изменения этой настройки я вобще невижу.
     
     помимо целеесообразности изменения этой настройки осатеся неопнятно
	 а как эта настройка долетит до процесса эластика. это тоже непонятно.
	 
	 дело в том что как Я понимаю механизм: у нас стартует инит контейнер
	 то бишь процесс и он через ulimit типа меняет число . окей. но процесс то потом умирает и к новому процессу эта настройка НЕ БУДЕТ ИМЕТЬ НИКАКОГО ОТНОШЕНИЯ. вот чего я понять немогу.!	 
 	  
	 значит я тут небуду писать портянку про ulimit а 
	 напишу ее в kernel-linux.txt ищи по слову "про ulimit"
	 

    я проверил на практике. создал просто pod который мне покажет
дефолтовое значение ulimit -n для пода.
	
	# cat init-container-pod4.yaml
	
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod4
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && ulimit -n && sleep 3600']


# kubectl logs myapp-pod4
The app is running!
1048576

итак дефолтовое значение = 104 85 76	


далее я создал под с init контейнеров в котором я меняю 
... так я пока оставляю это и двигаю дальше.   я считаю что ulimit небуде
рабоать и точка.


публикую слеующйи кусок statefull set.yaml

. . .
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: do-block-storage
      resources:
        requests:
          storage: 100Gi


в этом куске мы прописываем что мы прописываем как я поимаю чтото 
типа pvc только для statefullset.

name: data = это как я понял название pvc\volumeClaimTemplates (замечу что в helm в конфиге буквы s на конец нет тоесть volumeClaimTemplate)

замечу что data мы указщали выше в спеке контейнера 

        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data

таким образом цепочка такая. спек контейнера говорит что внутри 
контейнера папка /usr/share/elasticsearch/data в нее надо с хоста ФС
пробрсить папку.типа внещняя папк условно обывзается data

затем в pvc мы прописываем что это и есть data.


в этом pvc мы указываем сторадж класс к которому этот pvc будет
обращаться за pv , имя сторадж класса do-block-storage
 
 зачем это я незнаю.
       labels:
        app: elasticsearch


итого вот полнвый текст statefull set

elasticsearch_statefulset.yaml



apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: kube-logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
      initContainers:
      - name: fix-permissions
        image: busybox
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: do-block-storage
      resources:
        requests:
          storage: 100Gi
		  
		  

публиккуем этот стэтфуллсет.

# kubectl apply -f ...yaml

и смотрим статус развертывания

# kubectl rollout status sts/es-cluster --namespace=kube-logging

значит эластик в итоге развернулся.

что я выяснимл что если есть 
сервис с именем vasya
который устанолвен в куб неймспейсе kuku
то его полное dns имя будет
vasya.kuku.svc.cluster.local

в нашем случае сервис имеет имя elasticsearch
устанолвне он в куб неймспейсе kube-logging
значит его полное dns имя будет
elasticsearch.kube-logging.svc.cluster.local

и если запросить какие ip он имеет то 
он покажет ip подов под ним. так как это непросто сервис 
а хэдлесс сервис

Name:   elasticsearch.kube-logging.svc.cluster.local
Address: 10.252.2.42
Name:   elasticsearch.kube-logging.svc.cluster.local
Address: 10.252.2.41
Name:   elasticsearch.kube-logging.svc.cluster.local
Address: 10.252.1.29

а если у нас есть поды входящие состав сервиса то можно к ним 
обратиться и лично перснонально чрез DNS имя

для пода с именем es-cluster-0
его DNS name
es-cluster-0.elasticsearch.kube-logging.svc.cluster.local


убивая поды statefullSet я увидел что в обратно от обещанного новые поды
имебт уже другой ip. 


конифг реально эластика дата нода с железного сервера:
та часть которая представляет интерес


# это простая часть
cluster.name: escluster-2
node.name: data06-222

# это тоже простая часть
http.port: 9200
http.host: [ _local_ ]

# а вот тут надо думать. этот ip нода будет сообщать мастеру а мастер
другим нодам
transport.host: [ "192.168.7.222" ]
transport.port: 9300


# тут мы указыаем мастеров
discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]

надо читать как пробросить наружу эти поды.

вот таким макаром я пробросил сервис на каждом хосте на его
хостовый 9200 порт

# kubectl expose service elasticsearch --name=nodeport --port=9200 --target-port=9200 --type=NodePort --namespace=kube-logging

что  я прочитал в книге что новый под вместо умершего 
который входит в состав statefull set у него гарантированно будет 
такой же самый
1)pod-name
2)network hostname то бишь и DNS name(внутри кубовый) тоже

он будет поднят необязательно на тойже ноде но это неважно 
так как  PV доступен с любой ноды и к нему будет приаттачен ТОТ же PV

а вот IP получается будет может и другой. необязан сохраниться.

я узнал как задать квоту на цпу и память на на уровне 
отделного пода  а сразу на уровне куб неймспейса

# cat quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
   name: kube-logging
spec:
  hard:
    requests.cpu: 3000m
    requests.memory: 1.5Gi
    limits.cpu: 9000m
    limits.memory: 12Gi

# kube apply -f quota.yaml --namespace=kube-logging

ЧТО ВАЖНО ОТМЕТИТЬ! что в yaml не прописывается куб неймспейс
а его указывать нужно в kubect apply

посмотреть статистику потребленных ресурсов:
# kubectl describe quota --namespace=kube-logging

но мало указать квоту. также нужно указать дефолтовые значения 
для пода чтобы их не прописывать в свойства пода. иначе аписервер
непропустит публикацию такого пода

c# cat limits.yaml
apiVersion: v1
kind: LimitRange
metadata:
   name: kube-logging
spec:
  limits:
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1500m
      memory: 1.7Gi
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 10Mi
    default:
      cpu: 200m
      memory: 100Mi
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1000
      memory: 1Gi
    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim
    min:
      storage: 1Gi
    max:
      storage: 7Gi

этот yaml обсудим потом.

limit относится к свойствам одного пода. 
а quota к свойствам всего куб неймспейса

по сути получается в кубе всего несколько структур.

деплоймент и state full set отвечают за генерацию производство подов.
service отвечает за их сетевую обвязку.

вот в общем то и все структуры в кубе.

если у нас поды все аболютно одинаковые и нам насрать что после
гибели пода новый под будет иметь новый hostaname, новый dns имя,
новое куб имя то есть сами поды получаются типа "обезличенные" 
то нам надо в качестве генератора подов выбирать deployment

если нам важно чтобы новый под вместо умершего имел тот жесамый куб имя, тоже dns имя, тоже hostname как был у умерщего , чтобы он присобачивался к тому же PV что был у умершего пода то нам надо выбирать statefull set.

такая хрень как replica set нам и ненужна. мы ее не будем юзать.сейчас
это внутренняя вспомогательная структура под урплавленем deployment.

сервис позвлояет по сети обращатся к группе подов под единым IP.
сервис бывает трех видов

cluster-ip: это когда сервис имеет внутрений IP. мы на него обращется 
и в рандомно порядке он нам преебрасыает на один из подов из на бекенде

load balancer: это когда чтото с облачным продвайдером связано

headless service: это когда у него нет IP а при dns запросе этого 
сервиса нам вернется все IP адреса подов.

вот и все струткртуры куба.

значит что крутого я обнаружил про эластик  , в его конфиге переменная
discovery.seed_hosts в ней указываются IP адреса мастеров.
именно ip адреса нужны в конечном итоге эластику.  
так вот если там указать dns имя то это имя будет зарезовлено в
конечном итоге , так вот круто то что если при резолвинге имя возвращает
несколько IP адресов то эластик  засасывает их все! 
это дает очень крутую штуку. мы можем указать в конфиге эластика всего
одно dns имя

discovery.seed_hosts [ vasya ]

а далее на dns серверер вбить несоклько A записей для vasya
192.168.1.1 A vasya
192.168.1.10 A vasya
...
192.168.1.20 A vasya
и получается что потом при добавлении мастеров нам на конфигах нод
менять ничего ненадо. нам только надо всего навсег менять записи
на DNS сервреу. это мега круто

в кубе такую вещь предоставляет headless service.
его DNS имя резволисят сразу во все IP адреса подов


далее. 

привожу конфиг (его самые важные куски) с рабочего эластика который сидит на обычных виртуалках


дата нода

transport.host: [ "192.168.7.222" ]
transport.port: 9300
http.port: 9200
http.host: [ _local_ ]


discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]

===========================================

мастер нода

cluster.initial_master_nodes: ["master01-234", "master02-224", "master03-223", "master04-221", "master05-220"]

transport.host: [ "192.168.7.234" ]
transport.port: 9300
http.port: 9200
http.host: [ "192.168.0.234", "127.0.0.1" ]


discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]

===============================================

что касается мастер ноды то с ней проблем нет.
мастера по моему заданию сидят все вне куба.
во вторых вот в этой строке 

cluster.initial_master_nodes: ["master01-234", "master02-224", "master03-223", "master04-221", "master05-220"]

здесь задаются имена нод но в DNS смысле а в смысле эластиковом. 
те идентификаторы которые вбитвы в elasticsearch.yml
то есть эти идентификаторы ищутся на нодах 	УЖЕ ПОСЛЕ ТОГО КАК МЕЖДУ 
НОДАМИ УСТАНОВЛЕНА TCP СВЯЗЬ.
поэтому с этими идентификаторы проблем нет.

ну и так как мастера сидят все за кубом. то они без проблем друг 
друга найдут.

в этой строчке указювыатся ip или dns имена мастеров.
так как  мастер ноды лежат вне куба то в их конфигах
мы без прблем эту строчку можем редактировать настраивать.


discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]

вобщем главное то что на мастеро нодах которые лежат вне куба
нет никаких анстроек касающихся дата нод. а это важно 
так как часть дата ноду нас лежит в кубе.

поэтому в целом конфиг масетров изза того что часть дата нод
леит в кубе ника менять ненадо.

это глвное.

едиснтвенное что надо поменять discovery.seed_hosts 
на таку строку

discovery.seed_hosts: [ master.domain.local ]

и на dns сервере вбить все ip мастеров.
и тогда в конфиг мастеров больше ничего менять непридется никогда.

конечно в строке cluster.initial_master_nodes будут не все мастера.
но это нестрашно

переходим к дата нодам которые мы хотим засунуть в куб часть


дата нода

transport.host: [ "192.168.7.222" ]
transport.port: 9300
http.port: 9200
http.host: [ _local_ ]


discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]


строчка discovery.seed_hosts не проблема . так как в ней
лежат ip от мастеров. вобдем эта строка непроблема.

проблема в этих строках

transport.host: [ "192.168.7.222" ]
transport.port: 9300

если ее вобще убрать и весь эластик бы сидел в кубе то как это бы работало - эластик на дата ноде испольует настройку по дефолту.
тогда он берет первую попавшуся сет карту типа того считывает ее ip
и садится на ее порт 9300 а на мастера которые указаны в discovery.seed_hosts: он посылает свой IP и 9300 порт что мол привет 
мастер мне звони на этот IP и 9300 порт. таким макаром мастер узнает 
куда звонить на дата ноды.
когда все дата ноды сидят в кубе то им друг другу позвонить непроблема.
ip адреса видят друг друга без проблем.

а вот если мастер сидит за кубом и другая часть дата нода за кубом
то они по сообщенному ip адресу в куб пролезить не смогут

нстройка transport.host
разбивается на две отделные настройки

transport.bind_host = эта чтобы эластик понял на какую сет карту ему 
сокет слушающий сажать. с этой штукой проблем нет.

transport.publish_host = а вот эта штука сообщает мастеру и другим нодам
через как dns\ip к этому хосту можно достучаться по tcp
и получается внутренний ip пода надо пробрасывать наружу на какотой 
внешний ip. тут проблема...

приходит только одна мысль пока
поставить в transport.publish_host внешний dns в котором пропсать
все ip хостов куба. ноды вне куба будут пихать пакеты они будут прилетать
на произвонльную ноду эластика. и посмотреть может прокатит?
есть еще проблема что вохмдно тут можно укзаывать толко IP а не dns 
имя это будет жопка тоже
судя по описанию более общей настройки network.host оно должно принимать
dns имя а нетолко IP

ЧТО СУПЕРВАЖНОЕ Я ВЫЯСНИЛ:
вот есть у нас PV. уже созданный.
и вот мы создали pvc.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
	  
в котором мы указали StorageClass.
ТАК ВОТ = чтобы этот pvc нашел наш pv СОВСЕМ НЕОБЯЗАТЕЛЬНО чтобы в кубе
сущестововал этот самый storageclass. как я понял pvc вначале ищет среди
уже созданных существующих pv нет ли там такого у которого бы был
а) заданный размер диска
б) прописанный в свойствах pv тот же storageclass что и storageclass
прописанный в совйтсвах pvc.

вот я создал заранее руками pv у которого прописан storageclass:manual

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
	
и после этого я создаю pvc который выше указал.
этот pvc ищет в кубе pv у которого в свойствах storageclass = manual
и он его находит ибо такой pv уже создан. отлично. pvc нашел нужный pv.
при этом я даже не создавал в кубе никакой storageclass. 
а вот если бы pvc ненашел pv удовлетроворяющий его запросам то тогда 
бы pvc начал бы искать storrageclass:manual (manual просто имя никакого
особого значения слово manual ненесет) и будет просить этот сторадж класс
создать нужный pv.

как заставить чтобы поды запускались на конкретной ноде.

вначале надо узнать какой label  у наших нод
label имеет вид ключ=значение

# kubectl get nodes --show-labels

вот скажем какие label имеет одна из нод

test-kub-03   Ready    <none>   35d   v1.19.2   kubernetes.io/hostname=test-kub-03,kubernetes.io/os=linux

здесь мы видим что нода имеет например label 
kubernetes.io/hostname=test-kub-11

при желании можно добавить ноде доп label

# kubectl label nodes <node-name> <label-key>=<label-value>

после этого можно укзаать поду чтобы он публиковался на ноде с 
label kubernetes.io/hostname=test-kub-11

добаиви в под поле

nodeSelector:
    ключ: значение
	
в нашем случае это будет

nodeSelector:
    kubernetes.io/hostname: test-kub-11
	
и  в полном виде

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    kubernetes.io/hostname: test-kub-11
	
	

далее. я хочу сделать pv чтобы он был только на определенном хосте
test-kub-11

помогает опция node affinity

пример

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-host-11
  labels:
    type: local
spec:
  storageClassName: host-11
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - test-kub-11


	
вот этот кусок задает опцию чтобы pv был развернут только на 
хосте у которого Label : kubernetes.io/hostname= test-kub-11

далее надо создать pvc который будет приклеиваться именно к этому
pv. что для этого достаточно в pvc. надо в pvc только указать размер
диска и название сторадж класса который прописан в pv.
таким образом storageclass вобщем то однозначно позволяет 
искать pv. то есть если мы в pv для хоста-11 укажем некий уникальный 
stroageclass то тогда указав в pvc этот уникальый stoage class
мы гаранированно будем попадать именно на этот pv.
(при этом из приятрых мелочей сторадж класс вообще создавтаь в кубе
нет нужды сторадж класс в данном случае это скорее признак селектор
между pvc и pv )

видно что в моем pv storage class = host-11

вот как выглядит pvc тогда

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-host-11
spec:
  storageClassName: host-11
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


таким образом данный pvc приклеиться именно к нашему pv.

теперь приклеим наш под к этому pv

что теперь можно отметить при этом что на поде раз мы хотим его приклеит
к определнному хосту ненужно прописывать в нем самом никаких
 nodeSelector:
    kubernetes.io/hostname: test-kub-11
	
мы теперь вместо этого в поде пропишем наш pv который гарантиированно
приклеился к pv хоста-11 и наш под гаоантированно развернется на 
хосте-11


# cat pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod-11
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: pv-host-11
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage


вот строка где мы приклеили наш под к pvc нашему

persistentVolumeClaim:
        claimName: pv-host-11






так. далее.
 я создал pv 
в коотором прописал нод аффинити чтобы этот pv был создан на 
задданном хосте 

nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - test-kub-11
		  
поэтому он будет создан на test-kub-11 
и прописал в этом pv уникальный сторадж класс 
storageClassName: pv3-host-11  
теперь если мы в pvc или в volumeClaimTemplates: пропишем этот сторадж класс то 100% пориклеимся именно к этому pv

далее я создал новый куб неймпейс. и в нем создал стейтфулл сет 
в котором разоваричавется 1 штука подов с нжинкс 
и в это поде  прописано чтобы он юзал именно тот самый сторадж класс  
storageClassName: pv3-host-11
поэтому под приклиться именно к этому Pv а занчит также будет развернут
именно на хосте test-kub-11

таким образом в итоге под будет развернут именно там где лежит 
наш созданный pv. таким образом мы приклеили нащ под к конкретному pv.

привожу все файлы

# cat nm.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: ss-3


# cat pv3.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv3-host-11
  labels:
    type: local
spec:
  storageClassName: pv3-host-11
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data2"
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - test-kub-11




# cat ss3.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: filewriter
  namespace: ss-3
spec:
  serviceName: ss-3-expose
  replicas: 1
  selector:
    matchLabels:
      app: el-2
  template:
    metadata:
      labels:
        app: el-2
    spec:
      containers:
      - name: nginx-container
        image: nginx
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 80
          name: "http-server"
        volumeMounts:
        - name: data
          mountPath: "/usr/share/nginx/html"
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: pv3-host-11
      resources:
        requests:
          storage: 100M



в итоге при удалении пода он будет автормтом поднят 
на том же сервере где лежит наш pv приклеиться именно  к нему
и будет иметь тот же  куб имя и тот же dns имя.
и также падение пода нам нестрашно стейтфулл сет за этим строго
следит.

это ровно все те условия что надо для пода для эластика.

при этом мы юзаем несетевой сторадж а локлаьный сторадж

теперь надо под пробросить на внешний порт хоста на постоянной 
основе. чтобы мы обращаясь на ip хоста : порт которые являются
неимзенными всегда попадали внутрь этого пода.


значит в statefulll set   я прописал сервис к которму 
этот стейтфулл сет будет прикреплен.
вот эта строка
serviceName: ss-3-expose


создадим теперь этот сервис чтобы заэкспоузить наш под наружу.
тип сервиса будет NodePort



# cat nodeport.yaml

apiVersion: v1
kind: Service
metadata:
  name: ss-3-expose
  namespace: ss-3
  labels:
    app: el-2
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
    nodePort: 30867
  selector:
    app: el-2

что супер важно в этом сервисе чтобы он подхватил нащ под
это вот эти строки


  labels:
    app: el-2


  selector:
    app: el-2


здесь мы прописываем тот label по которому этот сервис
будет искать поды просматриваля их лейбелы. напоминаю что 
label он имеет ключи  и значение . то есть ключ=значение 
в нашем случае ключ=значение равен

app: el-2

потому что если мы посмотрим в наш стейтфулл сет то там прописанло
для пода

selector:
    matchLabels:
      app: el-2
	  
labels:
        app: el-2
		
		

именно благодаря совпдаению лейбелов наш сервис найдет 
наши поды.

после публикации сервиса проверим что он нашел поды


# kubectl describe service/ss-3-expose  --namespace=ss-3
Name:                     ss-3-expose
Namespace:                ss-3
Labels:                   app=el-2
Annotations:              <none>
Selector:                 app=el-2
Type:                     NodePort
IP:                       10.106.252.209
Port:                     http  80/TCP
TargetPort:               80/TCP
NodePort:                 http  30867/TCP
Endpoints:                10.252.3.29:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


проверяся по непустой строке

Endpoints:                10.252.3.29:80

в ней будут указаны все IP подов которые нашел нащ  сервис

убедимя что этот ip это ip именно нашего пода

# kubectl get pods -o wide  --namespace=ss-3
NAME           READY   STATUS     IP            NODE         
filewriter-0   1/1     Running   10.252.3.29   test-kub-11  

видим полное совпдаение


вот как у нас выглядит вся публикация

# kubectl get all  --namespace=ss-3
NAME               READY   STATUS    RESTARTS   AGE
pod/filewriter-0   1/1     Running   0          17m

NAME                  TYPE       CLUSTER-IP       EXTERNAL-IP
service/ss-3-expose   NodePort   10.106.252.209   <none>        80:30867/TCP   18m

NAME                          READY   AGE
statefulset.apps/filewriter   1/1     17m


значит свойства сервиса нам сообщают что 
внутри куба этот сервис доступен через cluster-ip:80
то есть внутри кластера

# curl 10.106.252.209:80

а снаружи кластера сервис доступен через IP любого хоста куба и порт 30867

вот мои хосты куба




3# kubectl get nodes
NAME          STATUS   ROLES    AGE    VERSION
test-kub-01   Ready    master   38d    v1.19.3
test-kub-02   Ready    <none>   23d    v1.19.2
test-kub-03   Ready    <none>   35d    v1.19.2
test-kub-11   Ready    <none>   3d2h   v1.19.3

поскольку все мои ноды внесены в dns то я могу все проверимть 
испольщуя их dns имена




# curl test-kub-01:30867

# Generated by /usr/bin/select-editor
SELECTED_EDITOR="/usr/bin/mcedit"

# curl test-kub-02:30867

# Generated by /usr/bin/select-editor
SELECTED_EDITOR="/usr/bin/mcedit"

# curl test-kub-03:30867

# Generated by /usr/bin/select-editor
SELECTED_EDITOR="/usr/bin/mcedit"

# curl test-kub-11:30867

# Generated by /usr/bin/select-editor
SELECTED_EDITOR="/usr/bin/mcedit"


все рабтает потому что вот этот текст

# Generated by /usr/bin/select-editor
SELECTED_EDITOR="/usr/bin/mcedit"

это файл лежаший на pv в поде который отдает нжинкс пода

ура

при такой схеме можно будетзапустить на кубе дата ноды
эластика и добаавить их к существующему эластику который вращается вне
куба

следуший этап - поднять аналогично не пода нжинкса
а под эластика



первая успешна публикация эластика под будущие цели


4# ls -1
nm.yaml
nodeport.yaml
pv4.yaml
ss-4.yaml


4# cat nm.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: ss-4




4# cat nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: ss-4-expose
  namespace: ss-4
  labels:
    app: es-4
spec:
  type: NodePort
  ports:
  - port: 9300
    targetPort: 9300
    protocol: TCP
    name: http
    nodePort: 30868
  selector:
    app: es-4



-4# cat pv4.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv4-host-11
  labels:
    type: local
spec:
  storageClassName: pv4-host-11
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data4"
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - test-kub-11



ss-4# cat ss-4.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es4-cluster
  namespace: ss-4
spec:
  serviceName: ss-4-expose
  replicas: 1
  selector:
    matchLabels:
      app: es-4
  template:
    metadata:
      labels:
        app: es-4
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "es4-cluster-0.elasticsearch"
          - name: cluster.initial_master_nodes
            value: "es4-cluster-0"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
      initContainers:
      - name: fix-permissions
        image: busybox
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: es-4
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: pv4-host-11
      resources:
        requests:
          storage: 1Gi




ура

СТОЛКНУЛСЯ  с тем что когда на диске где поды осталось 16% то 
куб начал пытаться оттуда поды убрать. типа мало места на ноде

 а вот пример как заэкспоузить сразу несколько портов в одном 
 сервисе
 
 4# cat nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: ss-4-expose
  namespace: ss-4
  labels:
    app: es-4
spec:
  type: NodePort
  ports:
  - port: 9300
    targetPort: 9300
    protocol: TCP
    name: tcp-elastic-transport
    nodePort: 30868
  - port: 9200
    targetPort: 9200
    protocol: TCP
    name: http-elastic-client
    nodePort: 30869
  selector:
    app: es-4


в этом примере порты 9200 и 9300 пода будут доступны
через порты 30868б 30869 на хостах куба

а вот так я опубликовал 9200 порт и чтобы он небыл проброшен
на порты хоста а просто это был сервис внутри куба
потому что 9200 от дата ноды ненужно чтбы было доступно извне
а еще мне нехчеося каждый раз лазить и смотреть изменившийся ip пода.
удобнее лазить через ip севриса. 
опять же этого можно было бы неделать если бы dns хоста был подвязан 
к coredns. тогда бы можно было просто пользоваться неизменным dns
именем сервиса.



# cat svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: ss-4-internal
  namespace: ss-4
  labels:
    app: es-4
spec:
  ports:
  - port: 9200
    targetPort: 9200
    protocol: TCP
    name: http-elastic-client
  selector:
    app: es-4

итак один сервис пробыарасывает 9200 от пода внутрь кластера для внутрикубовго исползоваия. а второй сервис (тип нодпорт) пробрасыет
порт 9300 наружу клкстера чтобы к поду можпно было достучаться извне


вобщем я хотел попробовал чтобы имя сторадж класса в volumetemplate
было не хардкодед а типа перменной. 

volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: es-4
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: (вот здесь пытался вставить переменную)
      resources:
        requests:
          storage: 1Gi

но сходу не получается.
может такое и нельзя. пока оставляем. 
пока полуается на каждую дата ноду надо будет создать свой стейтфулл сет

перехожу к ресурсам подов.
у подов ресурсвы прописывабтися прежде для контейнеров пода.
прикольно то что если вобще енпрописать требованияк ресурсам
то этому кгнтейнеру может вообще ресурсов и не перепасть. 
так что это иделаьно для джобов и нетребоваетоельных бэкграунд подов
они будут жрать только то что осталось.

когда шедулер решет куда пихануть под то он проверяет чтобы на ноде было
свободно миниум столько ресурсов сколько укахано для пода в разделе
реквест. а реквест это типа мимум сколько мы считаем будет жрать под
если на ноде нет миниумма то под туда небудет засунут
получается что реквест занчение в поде это тот гарантированный минимум
который нода должна ему гарантировать.

как я понял щедулер когда смотори куда пихать под то он сравнивает 
сколько на ноде уже подтреблено подами которые там ресурсов суммарно
с точки зрения из суммы реквест настроек их. и если у ноового пода
реквест больше чем там осталось  то под туда небудет засуснуть

пример.

нода имеет 2 цпу и 2ГБ 
на нейуже крутится два пода один за реквестил 1 цпу и 1 гб
второй зареквестил 0.5 цпу и 0.5 гб
суммарно они зареквестили 1.5цпу и 1.5гб.

третий под реквестит 0.5цпу и 1гб. и его шедудер туда незапустит.
приэтом неважно сколько по факту щас текущие поды жрут.
они могут жрать больше могут меньше на шедулер это не повлияет.

еще как я понял в кубе можно сдлеать две настройки.
либо щедулер будет до усрачки набивать одну ноду и только потом
начинать набивать вторую. либо шедулер будет старатся набивать все ноды
равноемерно.

пока я ненашел как это менять

во первых выяснили что реквест размер ресурсов указанный в поде
это типа гарантированный мимум который под обязательно получит на хосте.
но реквест размер влияет нетолко на это. если есть несколько подов 
на хосте и на хосте есть свободные циклы цпу то они будут разделены
между подами в соотношении их реквестов.пример

превый под реквестит 0.2цпу
второй под реквестит 1 цпу.

всео на компе 3 цпу.

положим оба пода шарашат на максиум поэтому они оба стараются сожрать 
все 3 цпу на компе.
так вот куб и выдаст вот какой обьем цпу каждому :
каыэдый получит свой реквест - первый 0.2 цпу, второй 1цпу.
остается 3-1.2=1.8 цпу и оно бует разделено между подами в
соотношениее 1:0.2 тоест ькак я понимаю иттоговая мощность будет 
разделена ровно так как указано в реквестах. 1:0.2
x+y=3
x:y=0.2

получаем 0.2y+y=3, 1.2y=3, y=2.5, тогда x=0.5
тоесть витоге первый под получит 0.5цпу а второй 2.5 цпу.

но это распределние имеет место если поды борются за мощность.
если один из подов стоит а второму очень надо то второй получим всю мощность внезависимости от реквестов своих.

лимит это сколко макимум контейнтер\под может потьребить.
если указано только лимит без реквеста то реквест приравнивается к лимитиу

разрешено чтобы суммарная сумма лиимитов подов превосходида 
сумму ресурсов ноды. хотя если по факту ресрсы ноды исчерпаются то куб
начнет килять поды на ноде.

это разберемс позже.

если под пытется сожрать больше цпу чем екму можно по лимиту то
просто навпросто его троттлят. тут все норм.
если процесс пытатеся соэрать больше памяти чем ему можно по лимиту то 
процесс киляется. тут тоже все понятно. процесс невлез в память.

если проецсс закрашился то куб его перезапускает несоклько раз
ти каждый следующий раз уэе пауза больше между перезапускми
5 раз пытвется перезаупстить под куб  а потом преркращает

при запуске пода ему можно присоавиить класс. 
и тошгда если поды выжрали ресурсы на ноде то куб будет принимать решение
какой под килять на основе классса QOS у пода.

самый дишманский класс это besteffort
саый крутой клсс это guaranteed 

burstable class стоит между ними

это надо будет устанавливать потом на поды.

limitrange применяется для подов и контейнеров внутри пода которые лежат 
внутри того же неймспейса где и limitrange обьект.
таким образом он неможет принмеиться для всего неймспейса суммарно нет.
только индивидщульнаые поды.
соотвесвтенно он задает либо дефолтовые если в поде они не прописаны.
либо если они прописаы в поде то на стадии его создания куб проверит
не превыешают ли они то что прописано в limitrange портянке.

ресурскваота портянка зато может примнться к неймспейсу в целом.
и задает она минималные гарантированный обьем ресурсов который 
этот неймспейс получит.

ха! прикол в том что в портянке (обьекте) ресурс кваота прописываетися нетолько ресурс квотра НО И ЛИМИТ КВОТА но на весь неймспейс!
приеколно если мы опуликовали ресурс квоту то мы несможем создать под
у которого не прописаны ресурсы и лимиты. куб недаст стартануть.
поэтому после создания ресурс квоты надо обьязательно создать limitrange 
потому тчо это нас освоодит от обязанност в каждом поде прописывать 
реквесты и лимиты.

метрик сервер у меня пока неполучилось завести. неработате.
хотя под его работает.

для лимит рэндж:

для пода по цпу от дата ноды эластика  я могу поставить по цпу
реквест(миниум) 1 цпу , лимит (макс) весь сервер 16цпу

для пода по памяти от эластик ноды
по памяти я думаю наверно реквест (минуим)  22 ГБ 
максимум это 29GB

для ресурс квоты на весь неймспейс

95% от всех цпу на сервере.

чтоб было мало мудежа можно кокнертно прописывать для каждого
контейнера.
и потом ресурс квоту на все про все сразу.



дата нода


# cat /etc/elasticsearch/elasticsearch.yml

path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch


cluster.name: escluster-2
node.name: data06-222


transport.bind_host: [ "0.0.0.0" ]
transport.port: [ "9300" ]

transport.publish_host: [ "test-kub-02.mk.local" ]
transport.publish_port: [ "30892" ]



http.port: 9200
http.host: [ _local_ ]



discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]


node.master: false
node.voting_only: false
node.data: true
node.ingest: true
node.ml: false


xpack.monitoring.enabled: true
xpack.monitoring.collection.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.231:9200"]

xpack.security.enabled: false




 env:
          - name: cluster.name
            value: escluster-office
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
		  - name: "transport.bind_host"
            value: "0.0.0.0"
		  - name: "transport.port"
            value: "9300"
		  - name: "transport.publish_host"
            value: "test-kub-02.mk.local"
		  - name: "transport.publish_port"
            value: "30892"
          - name: "discovery.seed_hosts"
            value: "escluster-office-masters.mk.local"
          - name: "node.master"
            value: "false"
           - name: "node.voting_only"
            value: "false"
          - name: "node.data"
            value: "true"
          - name: "node.ingest"
            value: "true"
          - name: "node.ml"
            value: "false"
          - name: ES_JAVA_OPTS
            value: "-Xms1024m -Xmx1024m"
      initContainers:




//  
сейчас по красоте мешает все сделать два фактора
0) перенести эластик на проде на openebs+hostpath
1) эластик дожен цеиком лежать в кубе
1.1) если перенести эластик внутрь то структура деплоя упроститься
очень сильно птому что nodeport нужен будет только интерфейсным
нодам и все. а интерфейсных нод раз два и обчелся
1.5) куб должен лежать на железе голом  а не esxi
1.6) контрол панель долны быть кластеризована как и etcd
2) сететвой сторадж нужен glusterfs например
//

//
что улучшать:
0) на ноовом кластере куба изменить реплики число.
00) надо добавить еще +2 контрол панели. 
000) надо уюбедиться еслт опубликовать эластик и малозначимые поды
        через приоритеты. нагрузит на 100% малозначинмый , наргрузить эластик и убедиться - 1) что эластику пофиг. 2) что эластик работает также бодро. таки м образом убудиться что в отличи от сферы куюб дает возможность более эфектвно юзать мощность сервреа.
3) что куб невыдваливыает никакие поды из хоста.
4) как сделать так чтобы поды куба были доступны через много сетевых карт
и если хост куба умер чтобы это никак не влияло. ставим на каждый хост куба keepalived и создаем столько cluster IP сколько хостов. привызываем
проверку clusterip либо к сервису кубелет либо к сервису куб-прокси.если хост умирает то кластерный ip переезжает а к соседу и значит для внешнего
клиента ничего неменяется. и тогда для клиентов мы указыавем все clustr-ip
а порт единый. и тогда мы мжоем попаст на под через все хосты а если они умерли то коннекшн стринг все равно живет. и при этом даже хапрокси ненужен.




1) bootstrap.memory_lock: true на данный момент в кубе неработает
2) мониторинг metric-server настроить
3) научиться приоритетизировать поды  и проверить что поды не убиваются
когда нода уходит в 100% по цпу.чтоб можно было спокойно сажать на
ноду помимо основных подов еще и воркеры всякие итп. чтобы заставить 
на ноде работать цпу по макс все время.

//

//
вопросы:
1) как увеличиить динамически размер pv для пода
2) надо разобратся как ограничивать лимит для все сверху
чтобы для кубелета оставался цпу
//

//
ближайие шаги 
1) применить на эластике все рекомендации по сайзингу шардирования 
и прочему
2) уменьшить на одной дата ноде jvm чтобы узнать сколько GB 
реальнно эластик жрет


//



выфснилось что квота рботает по дурацки. если суммарный лимит 
лимитов отедбных подов преавышает суммарный лимит то под не публикуется
а это фигня.мне надо чтобы рекест был строго пропиман а лимитов у дата 
нод недолжно быть они дроожны по ммксимуму шарашить и конкруировать.

еще больше проянислось то что отправка статситкии на кибану
имеет однонаправленное направление. от дата ноды на кибану.
с ноды исходящее на кибану входящее. это хорошо.

,,про конфиг прояснилось эластика и про формат yaml
вот есть у нас классичесткая запись в кофиге

xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.231:9200"]

оказыется эквивалентная ей

xpack.monitoring.exporters.id1.type: http
xpack.monitoring.exporters.id1.host: "192.168.0.231:9200"

это очень важно чтобы вбивать это в кубернетес
потому что в yaml в env разделе описания контейнера
нужно вбивать переменные вида ключ=значеение
,,


какая схема была

3 esxi сервера, на каждой по 2 VM  с дата нодой = 6 дата нод по 7 цпу.
каждая VM была загружена макс на 80%. это получается одна нода
жрала 5.6 cpu

сейчас 3 ноды как и преждле работает на esxi.
и 3 ноды работает на кубе.
я посмотрел загрузку. и там и там нода жрет примерно одно 
и тоже обьем в цпу.

Исходя из того что 1хнода может потенциально выжрать 5.5-6 цпу
я делаю вывод что на один сервер куба с 16 цпу можно накатывать
порядка 3 нод. небольше.

витоге наша  инсталляция выливается в 2 физ сервера с кубом
по сравнению с тем что было 3 физ сервера  с esxi.но
разница еще и втом что на тех серверах даже при макс нагрузке
на каждом оставалось по 2цпу ненагруженых. итого 6 цпу пропадало.
плюс когда эластик там на холостом ходу то там ничего нельзя все равно
дополнительно крутить иначе отклик ухудится а здесь на кубе
можно посадить кучу доп подов которые будут немешая эластику подьедать
все что от него остается по цпу недоеденным.
теперь эластик кластер можно установить развернуть за минуты. хоть на физ серверах хоть в облаке стороннем. 
но одно самое главное это повышение цпу загрузки сервера, повышение плотности, чтобы он не простаивал.
у нас было 4 сервера которые работали на 20% все время и пару дней
за месяц на 55%. тоесть сервер просто простаивает. простаивает на 80%!
в новой схеме с кубом сервер тоже будет простаивать на 80% 
на обычной нагрузке. но! это время на нем могут работать другие
поды и наргужать его на 100%. а когда эластику понадобистя мощность
они заткнутся. на схеме из esxi это невозможно. мы неможет пускат на 
esxi нагрузку отличную от еластка потому что у него даже на холостом
ходу понизится отклик а если доп проги начнут грузить проц да еще
эластик начнет грузить проц то отклик будет ужасный.
тепрь весь эластк можно развернуть в секунды. и конфигурацию менять
мгновенно по нодам и вообще. и можно равзернуть хоть в облаке. 


вопрос - куб хост. на нем 3 дата ноды.
или на нем 6 дата нод. как при этом изменится загрузка 
хоста в целом.
входящий поток запросов будет один и тотже. тоесть суммарная
нагрузка по потоку запросов одна и таже.
поэтому по идее без разницы будет их 3 иди 6 или 100 на хосте.

>>>>>>>>>>
по окончании редеплоя проверить полный вывод со всех нод эластика
что уних там за транспорт!

ПРОЛБЛЕМА!!!)!! когда мы пробрасываем порт 9300 поды эластика 
на ноду то нам нужно в конифге эластика указать IP хоста
но так как я для эластика транспорттной сети использую сеть 192.168.7.0
а не дефолтовую сеть 192.168.0.0 то нужно во первхы на каждй
куб хосте завести карту доп в сети 192.168.7.0 и !!! в когфиге
эластика в качестве транспорт IP указыавть карту из 7 сети!!!
а вторых почемуто я прописал elastic-transport=192.168.7.243 хостнейм на 
м9 dns серервереврвре и в поде прописал elastic-trnaposrt.m9.local
но под почемуто пишет что он неомжет зарезолвить это dns имя.
так как я понимаю что жласки на ноде сам резолвит это в IP адрес и отсывалеет людям. нелюди резовлят егло DNS записть а сама этта нода и она
неомэет зарезовдить.!!!пока что япрорписал IP = 192.168.7.243

такеж вылезло то что если укажу dns  вида elastic-transport-clusrterip
и там укжау все 7-ые ip всех куб хостов и если один из них ляжет
то связь будет через жопу с моим подом.

получается что кубовый раунд робин это гавно. надо подумать куда я 
его еще всунул. надо перед кубом ставить кластеризованный хапрокси поулчатся



>>>>>>>>>>>>>





