когда нода стартанула
то эластик коннектит друг с другом членов через жопу.
через два отдельных механизма.

нода ищет соседей используя ip сокеты указанные в
	discovery.seed_hosts: [ip:port , dns_name:port  ]

сокеты которые указаны в этой опции они отвечают за нахождение 
нод между друг дружкой на стадии фаза-1. 
причем невсех нод а масетров только 
фаза-1(первый контакт) это фаза когда нода хочет связаться с соседом один мастер хочет
получить ответ от другого мастера. фишка втом что указанные сокеты они небудут использоваться потом для постоянной связи масетров друг с другом.
мастер свзяыается с друним мастеров по указаннму сокету. далее тот мастер указывает сам по какому сокету сним общатся на постоянной основе.
поэтому на фазе-2 (постояння связь) мастер с мастером неиспользует сокет
указанный в discovery.seed_hosts: он исольузует тот сокет который ему сообдлил его сосед мастер сам. зачем так дебильно сделано непонятно.
потому что по факту витоге все равно настраиваешь мастеров чтобы они собщали для фазы-2 ровно теже сокеты что и указаны в discovery.seed_hosts: на фазе1.

пример 

фаза-1. нода-1 смотрит в discovery.seed_hosts видит там ip-1:9300
и по нему ищет соседа.
сосед нода-2 сообщает что с ним надо связываться для фазы-2 через ip-1:9300
в итоге нода-1 общается с нодой-2 через ip-1:9300

на ноде сокет для фазы-2 указываетс янатсройкой

transport.publish_host
transport.publish_port
либо
transport.profiles.default.publish_host
transport.publish_port

небольшой замечение всторону от темызамечу что такой настройки как 
transport.profiles.default.publish_port
нет


кстатив этой строке указываем только мастеров.

пример

	discovery.seed_hosts: [ "svc-master-01:9300, svc-master-02:9300, test-kub-02.mk.local:30900, test-kub-02.mk.local:30901, test-kub-14.mk.local:30902" ]
	
он при своем старте вначале резолвит эти DNS имена в IP (делает 
он это всего один раз только при старте. и потом если ip поменялись
у данных dns имен то нода об этом понятия неимеет.)

вот зарезволвила нода эти dns имена и имеет список IP:порт, ip:порт
и туда нода начинает долбиться. если порт неуказан то исопльзуется дефолтовый 9300

таким макаром нода теперь знает где ей искать мастеров. но ! привол в том
что дальше все происходит ОСОБЕННО. ака дебильно.
нода делает первое обращение по сокету из списка и получает в ответ 
от той ноды (внимание!) сокет по которому этой ноде нужно вдальнейшем
к той обрашаться.дада! обана! таким образом та нода может сообщить совершенно другой сокет и теперь наша нода будет обрашаться к той ноде 
по новому сокету.
таким образом сокеты в discovery.seed_hosts это всего навсего фаза1, начальные сокеты по которым нода ищет соседей-мастеров. и если они откликаются то они ей (непонятно зачем) сообщают новые сокеты по которым наша ноды к тем нодам должна обращаться.
ваУ! эйнштейн отдыхает.

нарисую схему как надо настрить нашу и ту ноду если их разделяет 
проброс портов.

пусть у нас 
	нода-1 вращается в поде на хосте-А
	нода-2 вращается в поде на хосте-Б

хост-А имеет ip-A
хост-Б имеет ip-B

нода-1 пусть имеет транспортный порт 9300,
с помощью сервиса мы пробрасыаем 9300 из пода наружу ноды на порт 39000

таким образом если мы обратимся на ip-A:39000 то мы попадем внутрь пода на порт 9300

ip-A:39000 ---> под (нода-1 порт 9300)

таким образом мы сидя в сети можем обраться на ip-A хоста и в итоге
попасть на под.

тоже самое для ноды-2 на хосте-Б

ip-Б:39002 ---> под (нода-2 порт 9300)


тогда общая схема как под с подом может связаться друг с другом.
фишка в том что хост-А и хост-Б они сидят в разных кубернетесах
и между ними нет прозрачной оверлейной связи


под(нода-1 порт 9300)  ---> хост(ip-A:39000)  --> сеть <-- хост(ip-Б:39002) <--- (нода-2 порт 9300)


конфиг (нода-1)
	 - name: "discovery.seed_hosts"
       value: "ip-Б:30902"  (внешний ip того хоста)

     - name: "transport.profiles.default.publish_host"
       value: "ip-А"  (внешний ip этого хоста)
     - name: "transport.publish_port"
       value: "30900"  (внешний порт этого хоста)



	
конфиг (нода-2)
	- name: "discovery.seed_hosts"
      value: "ip-A:30900"  (внешний ip того хоста)

     - name: "transport.profiles.default.publish_host"
       value: "ip-Б"  (внешний ip этого хоста)
     - name: "transport.publish_port"
       value: "30902"  (внешний порт этого хоста)


еще раз как это будет работать
нода-1  на стадии "фаза-1" смотрит в конфиг в discovery.seed_hosts
и стучиться на ip-Б:30902 получает ответ от той ноды-2 которая ей еще раз 
для "фазы-2" говорит чтобы она к ней обращалась через ip-Б:30902

вот такой дебилизм.

ЧТО ЕЩЕ СУПЕР ВАЖНО ОТМЕТИТЬ:
	как видно в конфиге я использовал такую штуку как профили.
	так вот ОЧЕНЬ ВАЖНО ОТМЕТИТЬ  что параметр 
	
	что это за хрень профили. ооооооо... это отдельная песня.
	
	
	port: The port to which to bind.
	bind_host: The host to which to bind.
	publish_host:

	
!! transport publish_port в рамках профилей неработает хотя они завялвляют
что работает.

!! из куба из пода обраиться на внешний порт хоста своего = работатет . 

это неработает
   - name: "transport.profiles.default.publish_port"
            value: "9400"

в профилях опция publish_port НЕРАБОТАЕТ

это работает
   - name: "transport.publish_port"
            value: "9400"



 1)написать про фазы
 2)про сеть профили
 3)про логи что в логах указн компонент который в лог пишет
 4)
 
 {"type": "server", "timestamp": "2020-11-25T23:26:20,886Z", "level": "WARN", "component": "o.e.d.HandshakingTransportAddressConnector", "cluster.name": "es-cl-03", "node.name": "master-03-0", "message": "[connectToRemoteMasterNode[172.16.102.31:30900]] completed handshake with [{master-01-0}{1c7F8xZETEyTDgFSE51OKw}{hKHiG0-zQ02ozD8dcegmPg}{svc-master-01}{10.101.253.96:9300}{mr}{xpack.installed=true, transform.node=false}] but followup connection failed",
"stacktrace": ["org.elasticsearch.transport.ConnectTransportException: [master-01-0][10.101.253.96:9300] connect_timeout[30s]",
"at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.onTimeout(TcpTransport.java:1004) ~[elasticsearch-7.7.1.jar:7.7.1]",
"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) ~[elasticsearch-7.7.1.jar:7.7.1]",
"at java.util.concurrent.ThreadPoo

из чего я делаю вывод что если на ноде несколько профилей
когда к ней орашается сосед то она для связи отдает именно дефлотовый профиль.

получатес янгесколько профилей заводить можно но толкеу от них ноль.
они работают только на фазе1. 
на фазе 2 все равно соседполучит инфо чтобы к соседу он обращался по сокету от дефолтового профиля

непонятно какую роль играет cluster iunital nodes.
скажем там 7 мастеров. указано.
потом мы динамически расштряем до 10 масетров. вопрос
сколько мастеров нужно клкстеру для кворума? 4 или 6.

удадение pvc из под sts и перепулбикация пода ничего недадут.
Pvc невосстаналивется. поможет только вот что. уменщшить число реплик
в sts а потом обратно увелчичтть. тогда sts пересоздать pvc.
но опять же первой удаляетс прослоедняя реплика. поэтому если мы уадили
pvc первого пода то нам придется уменьшать число рпелик аж до нуля.


 
 
 
