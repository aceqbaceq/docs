vmware 
esxi 
etherchannel
lacp

вначале я раскажу про то как сфера распределяет трафик из виртуалок
через реальные физ карточки воткнутые в хост.
как проверить какой способ раотает прям щас на хосте.
тыкаем на хост - configuration - networking - выбираем любой vswitch - properties - vswitch - edit - nic teaming
и там указан текущий способ которым сфера берет из виртуалки трафик
и распределяет по физ картам в сеть.

первый способ - "route based on the originating virtual port ID"
этот способ нетребует ничего от физического свича куда ведут физ карты.
тоест свич в физ сети никак специально настраивать ненадо. 
положим в наш вирт свич приаттачено три сетевые физ карты.
тогда в какую физ карту сфера направит трафик от конкретнрой виртуалки зависит от виртуального порта в который воткнута виртуалка на виртуальном свиче. проще говоря сфера поделит виртуалки на три группы и будет слать от каждой трети виртуалок трафик через одну из трех карт.
причем сфера необрабатывает каждый пакетик от виртуалки нет. так как port id виртуалки неменяется то считай что тако способ отправки трафика потрбляет ноль ресурсов хоста. дело в том что отправку трафика от виртуалки через ядро сферы от каждой виртуалки на реальную физ карту обслуживает спец процесс сферы служебный который тоже жрет процессор.
так вот этот способ он жрет супер мало потому что ненужно принимать решение по каждому пакету от виртуалки. по карйней мере так пишут.

второй способ - буквально почти такой же самый что и первый. я невижу особой разницы. притом что этот способ уже будет напрягать сферу гораздо сильнее. тут уже обрабатыавеся каждый пакет от виртуалки. то в какую физ карту попадет трафик уже зависит от src mac адреса пакета. а мудоты больше. положим у виртуалки две сет карты ну значит у нее и два port id поэтому чем этот способ лучше первого непонятно . при существенно больших накаланых расходах. едиснвтенное что приходит это если на виртуалке одна сет карта  а мы ей внуттри ос даем несколько IP но мак адрес то остается один для обоих IP так что опять же смысла никакого нет.
вобщем зачем этот способо непонятно. при этом способе настройки на стороне реального физ свича никакие ненужны.

третий способ - ip hash. решенеи в какую карту трафик засунет сфера принимается на основе src ip и dst ip пакета. понятно что этот способ самый накладорасходный. зато трафик от виртуалки и туда и обратно к разным серверам может ходить через разные физ карты. что круто. 
при этом способе нужно делать спец настройки на стороне физ свича тоже.

есть там и еще способы которые нам неинтересны. см тут
	https://blogs.virtualmaestro.in/2020/05/26/uncovering-virtual-networking-part-8-load-balancing-algorithms/comment-page-1/?unapproved=2372&moderation-hash=0caca0bcdcdf6a48e366b4e9d7ebdb75#comment-2372


значит вот у нас есть на хосте несколько сетевых карт.
обычно мудеж состоит в том что надо определиться как мы будет разбивать
сетевой поток по этим физ картам.
например выделить 1 карту отдельно под vmotion, вторую карту выделить под
iscsi трафик. это архитектурная проблема как разбивать поток.

есть второй момент. положим есть виртуалка к которой обращаются другие виртуалки и каждая из них старается эту виртуалку по сети нагрузить по полной. получается что сетевая карта становится узким местом.
понятно что внутри хоста виртуалки имеет типа карту 10Gb но по факту поток 
идет по одной физ карте на самом хосте которая обычно 1Gb.

так вот можно физ карты обьединить в единый L2  группу комплекс механизм пулемет. тогда что будет. когда к нашей виртуалке будут обращаться друие виртуалки то поток от каждой той будет идти в нашу через отдельную сетевую карту. пример

виртуалка-А сидит на хосте-1 у котрого 
	vmnic0
	vmnic1
	vmnic2
	
мы обьеидиняем эти vmnic* в единый L2 штук и тогда 
поток от 
	виртуалки-Б в виртуалку-А подйет через vmnic0
	виртуалки-W в виртуалку-А подйет через vmnic1
	виртуалки-E в виртуалку-А подйет через vmnic2
	
самой виртуалке-А будет казаться внутри что к ней потоки идут через одну карту 10Гб естественно.

еще фишка такой архитектуры что типа нам неприедтся больше забоатиться 
какой сетевой поток через какую карту пускать. хотя мы при этом теряем возможность иметь выделенную карту для какого особого трафика.


дальше разберемся в терминах . потому что с ними в интернете бардак.

обтединение несколькиз сетевых карточек в одну группу называется
	агрегирование
открытый бесплатный стандарт который это описывает называтеся 
	802.3ad
закрытый пропприетарный стандарт про тоже самое от циско назыается
	etherchannel

один стандарт открытй второй закрытый

оба стандарта поддерживают два режима работы
	статический
	динамический
	
статический кодга мы руками указываем какие карты входят в агрегированный канал
динамический когда оно само находит карточки

динамический режим в открытом стандарте назыается = LACP
динамический режим в закрытом стандарте назыается = pAGP

насколько я понял LACP является частью 802.3ad
pagp является частью etherchannel


аггрегирование карточек по народному может называться
	nic teaming
	nic bonding
	транкинг портов

это все народные названия того же самого

всю вышеописанную инфо  я прочитал тут
	https://en.wikipedia.org/wiki/EtherChannel

итак еще раз - etherchannel это чисто цисковская штука аггрегирования каналов , то есть она работает только между железаками циско-циско.
открый вариант аггрегирования карточек назыавется 802.3ad
LACP это часть 802.3ad которая отвечает за динамическое конфигурирование 802.3ad
pagp это часть etherchannel которое цисковское

переходим к сфере.
насколько я понял из доки от сферы то они непаряться про тонкости 
терминологии и считают что термин etherchannel является прямым синонимом линк аггрегации тоесть по их мнению термин etherchannel это не цисковская линк агрегация а вообще полный синоним линк агрегации тоесть и 802.3ad тоже. об этом они пишут вот здесь
	https://kb.vmware.com/s/article/1001938
хотя я насколько понимаю это вранье. etherchannel это не линг агрегация вообще это именно ее фирменная реализация от циско на основе цисковского проприеатарного протокола.
так что повсеместно в инете наблюдается вот такая мешанина. 
в народе как я понял так и принято -  статическую агрегацию на основе 802.3ad называть etherchannel(что неверно) а динамическую агрегацию 802.3ad называть LACP (что верно ибо lacp это часть 802.3ad отвкчавющая за динамическую реализацию)
 
агрегация карточек называется link agregation


чтоб заработало надо с одной стороны это дело настроить на стороне сферы
а с другой стороны настроить на стороне свича.

переходим к поддержку агрегации со стороны сферы

!!!-- здесь повествование прерывается ибо нет времени больше все описывать

как я понял динамический lacp работает на сфере только если свич распределенный.
у меня обычный свич поэтому на нем динамический lacp со стороны сферы типа неподдерживается.

все тогда скатываемся до обычного статического lacp

дальше. что важно. сфера поддерживает вот такие виды тиминга:
	source MAC
	IP hash


source MAC  - в зависимости от src мак адреса выбирается карточка.
получается как я понимаю что если мы с одной виртуалки будем лезть на три разных то карточка для исходяшего потока будет выбираться одна и таже так как src mac один и тот же. тоесть если к нас стучаться с разных машин то карточкти будут разные а если мы стучимся на разные машины то карточка будет одна. по мне это неочень желательно чтобы и если мы стучимся на разные машины и если к нашей виртулаке стучаться с разных машин чтобы всегда карточки выбиралисть разные. 

IP hash - насоклько я понял карточка выбирается на основе того что смотриится src IP и dst IP и для такой связки выбирывается отдельная карта. по мне именно в этом случае будут разные карты выбраны если мы стучимся на разные машины и если к нашей виртулаке стучаться с разных машин - то что нужно. в итоге я выбрал этот вид тиминга.

наверно если их сравнивать по нагрузке на систему то первый менее напряжный потому что надо анализировать всего ли L2 мак адреса 
а во втором надо поток наализровать на уровне L2 IP адресов

как на сторне сферы это настраивается. идем в свойства хоста в сети
выбираем вирт свич - нажимаем свойства . далее там октрывается список
порт групп на этом свиче и в самом верху там опять всречается наш 
свич в виде строки
	vSwitch 120 ports
вот на него тыкаем нажыаем - edit - и открываем закладку Nic Teaming
и там в Load Balancing выбираем "Route based on IP hash"
там же добавляем карточки сетевые. так оно настраивается в сфере 
но пока этого делать ненужно.

вначале все карты которые мы планируем включить в состав агрегированного канала убираем из вирт свичей  в которые они может быть были воткнуты.
чтобы они были свободны. хотя одну карту нам все таки приелтся оставить
это та карта через которую мы связываемся с хостом. да можно и ее убрать
а зайти на хост через ilo и там делать настройки через F2 текстовую консоль но нафик.
итак все карты от свичей виртуальных отсоединили. оставили одну карту.
на этой стадии сферу хост оставляем и идем конфигурировать свич.


судя по докам от циско на ней тиминг начинаем настраивать с того что 
на свиче гасят все порты которые мы будем включат в аггрегированный канал. настраивают их. а потом включают. на сфере в доке тоже пишут что 
вначале надо делать настройки на свиче. и только потом на сфере.
	https://kb.vmware.com/s/article/1001938
когда используем ip hash то на сфере (забега вперед) неисопльзуем beacon probing - есь такая опция в закладке nic teaming
в доках сферы ( как я понял) они назыавают статический lacp как etherchannel bond. а динамический lacp как lacp. вот такая жопа.




также встречается pagp - это как lacp только фирменный от циско.


минус режима IP hash. насколько я понял из статей от сферы то что если карточка одна из отвалится и сфера и свич будут все равно в нее пытаться направлять поток. и будет потеря пакетов получается. в режиме же MAC src этого не происходит. если карточка одна из отвалилась то связка сфера-свич это понимают и нестараются на нее пускать пакеты.

нормальные источники:
	https://advanxer.com/blog/2013/08/etherchannel-vs-lacp-vs-pagp/
	
