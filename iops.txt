наконец сюда я решил 
заносить все результаты тестов на iops
========================
чек лист по контроллерам:
	- надо убедиться что контроллер подкючился к дискам на их родной скорости
		проверить можно через ssacli ... pd all show detail
		дело втом что бывает что контроллер подлкючается к дискам на скрости ниже
		чем заявлено он умеет и чем поддерживает диск
	- если на контроллере будет сидеть несоклько ssd массивов
		значит надо понимать что он мощность свою будет делит между ними
		вопрос хватит ли у него мощности чтобы полностью выкачиваеть все iops-ы
		которые могут диски всех масивов
	- нужно четкое соответсвие между числом линий в контроллере, числом линий
		в бекплейне и числом дырок под диски в бекплейне. чтобы было соотвемсвтие
		1:1 между число линий контроллера ,числом линий беекплейна , и числом дисков
		чтоб в конечном итоге линия от контролллера единолично доставалась одному 
		и неболее диску.
	- если в массиве 6 и меньше дисков то контроллер надо покупать с кэшем и батарейкой
		так как smart path даст херовый результат нетолько на запись но ина чтение
		


==================
вначале про контроллеры:
про скорость одной линии:
P410 поддерживает SATA 3Gb\s
P440ar поддерживает SATA 6Gb\s
P840 поддерживает SATA 12Gb\s

фактически это означает диск какой специификации можно подключать 
к контроллеру
==========================
P410
его характеристики

6Gb/s SAS [(600MB/s bandwidth per physical link)]
x8 6Gb/s SAS physical links (compatible with 3Gb/s SATA )
256 MB 40-bit wide DDR2-800MHz cache upgradeable to 512 MB 72-bit wide DDR2-800MHz
battery-backed write cache or 512 MB / 1 G 72-bitwide DDR2 flash backed 
write cache provides up to4.2 GB/s maximum bandwidth
x8 PCI Express gen 2 host interface provides maximum bandwidth (4GB/s)
Gen 2.0
Read ahead caching
Write-back caching (with BBWC/FBWC)

========
P410
выводы о нем.

как заявлено о нем в в HP статье у него 
мощность всего 50,000 iops 4K. (это 195МБ\с если переводить какую ширину канала
этот поток отьест, учитывая что на этом контроллере sata каналы это sata300
 то получается что 50,000 iops можно выжать даже из одного диска а не из рейд массива.
 если мы поставим даже один мощный диск то sata300 хватит чтобы все 50,000
 снять с одного диска). 

но проблема в том что 50,000 iops это сейчас очень маленькая мощность 
для контроллера
если на него ставить сейчас любой современный SATA600 SSD диск 
хотя бы даже всего один на весь контроллер, то  учитывая что этот диск может выдать
98,000 iops то мы за счет медленного контроллера теряем половину скорости диска
в иопсах.  тоесть мы потеряем 50,000 иопсов с одного диска.
а если скажем соббрать массив raid10 из 4 дисков то мы условно говоря 
вместо 400,000 иопсов возьмем с массива теже самые 50,000 а 350,000 мы выкинем
в мусор. на четырех дисках мы теряем 75% купленной мощности.

поэтому в плане иопсов на данный контроллер надо ставить какието супер древние
ssd диски так чтобы их суммарная мощность не превышала 50,000.

вот это надо помнить.

что касается мощности контроллера по срупуту. 
значит у контроллера 8 линий sata300 это дает 2400MB\s
контроллер сидит на pci-e шине на которой  у него скорость 4GB\s
так что pci-e  в плане срупута неявляется узким место. ее хватает.
но! в плане срупута контроллер выжмет все с дисков sata300 которые
он максимально поддерживает.
но сейчас то продаются сплошняком диски sata600. поэтому 
каждый диск который мы вставим в этот контроллер мы на нем потеряем половину 
срупута.

также на практике я столкнулся с таким моментом. несмоттря на то что контроллер
поддерживает sata300 но вставленный в него диск sata600 подключился к контроллеру
на скорости всего sata150 ! поэтому даже с одного такого диска мы теряем 75% 
его срупута в этом случае.

таким образом в целом про p410 можно сказать что его для SSDвобще ненадо использовать,
либо можно использовать если нам посрать что мы теряем мощность кулпенных дисков
в диких обьемах.

тут еще такой тонкий момент. вот эти вот супер 100,000 iops мы их можем снять 
с ssd только когда у нас скажем 16 воркеров и q=8 к примеру 
а если у нас w=1 и q=1 то современные диски выдают от 5,000 iops до 10,000 iops

тоесть при такой нагрузке конечно и P410 отлично это из себя выжмет.
другое дело что этот паттерн нагрузки он лабораторный. 
тоесть скажем согласно iomter паттерну условная рабочая станция
имеет паттерн нагрузки w=4 q=64 и тут конечно уже все упрется в 50,000 границу.



========
P410 
как обновить прошивку.

качаем с сайта файл. он есть для vmware а есть в форме rpm пакета.
если под esxi утилита говорит что неможет найти контроллер 
тогда закачичваем утилиту в форме rpm пакета
далее грузимся с centos 6 live cd

далее через dmesg смотрим какой порт ообслуживается каким драйвером.
таким образом поимаем какой порт нужно настраивать.

далее через ifconfig можно настроить ip + тэгированный vlan

активируем поддуржку тэгированных виланов
# modprobe 8021q
назначаем vlan=11 тэгированный для eth0
#vconfig add eth0 11
#ifconfig eth0.11 10.0.0.104 netmask 255.255.255.0 up
#route add default gw 10.0.0.1

запускаем ssh
# service ssh start
при этом центос автомтом создаст rsa ключи
добавлякм пароль админу
# passwd

теперь можно через winscp закачать на сервер rpm с утилитой

hp-firmware-smartarray-14ef73e580-6.64-2.x86_64.rpm

распаковываем ее

# rpm2cpio hp-firmware-smartarray-14ef73e580-6.64-2.x86_64.rpm | cpio -id
спускаемся в распакованный архив и запускаем
# ./hpsetup

утилита должна заработать и найти контроллер и перепрошить его


===========================

4k 100% read 100%random
20 workerov x q=1
SmartArray P840 Raid1  2xSSD Samsung 840 EVO 
smartpath
61 000 iops
0.32 latency


4k 100% read 100%random 
20 workerov x q=1
через FC (данные читаются с этого же контроллера P840 c этого же
массива 2xSSD Samsung 840 EVO но через FC)(сервер Fc=esos, клиент=встроенный
в ESXI клиент FC)
41 000 iops
0.47 latency


4k 100% read 100%random 
20 workerov x q=1
Raid10 Smart Array P440ar  4xSSD KINGSTON SEDC500 
82 000 iops
0.24 latency


4k 100% read 100%random 
20 workerov x q=1
контроллера нет, одиночный диск nvme SSD Intel 3700 372GB
143 000 iops
0.14 latency



режимы:
w=1,q=1
w=20,q=1
smartpath\controller cache
10\90 controller cache, 0/100 controller cache, 100\0 controller cache
random read, random write
seq read, seq write
buffer cache enable\disable
iops 1\1000
мало дисков массиве много дисков в массиве
lsi\hp






Samsung SSD 850 x 1disk
тест на рандом чтение в 1 поток

Smart Array P420i
512MB cache size

драйвер esxi позволяет нагружать DQLEN=1000 потоков одновременно

тип доступа к lun: controller cache
сотношение контроллер кэша 0%R/100%W 

Drive Write Cache: Enabled

размер тестового файла 32GB


4k, 100% read, 100% random
workers=1, q=1
5500 iops
0.18ms latency

изменение сотношение контроллер кэша 25%R/75%W дало прежний результат. 
что ожидаемо. кэш маленький тестовый файл большой.


меняю доступ к луну на smartpath
тип доступа к lun: smartpath
и смотрим скорость того же режима
4k, 100% read, 100% random
workers=1, q=1
5600 iops
0.18ms latency

вывод: smartpath никак неповлиял.

я попробовал менять доступ к луну (controller cache\smartpath) еще на одном
контроллере P840. пробовал и 1 воркер х1 поток и 20 воркеров х1 поток. 
резултат один и тотже. вывод: смартпаф на режим рандом чтения никакой роли неиграет.
типа чо за фигня. далее я нашел в инете тест  в котором видно что смарт паф работает 
заметно ХУЖЕ!( я думаю иза за того что паттерн в том тесте включал и запись. а записть 
в реэиме смартпафточно уходит в жопу) если рейд меньше 18 дисков.
так что теперь стало понятно что надо хрена дисков чтобы этот смарт паф дал какойто 
выхлоп. а если их меньше то он вобще только хуже делает.

вывод- нахер этот смарт паф.

что такое вобще этот смартпаф.
как написано здесь - https://support.hpe.com/hpesc/public/docDisplay?docId=mmr_kc-0125362
что типа смартпаф это фича драйвера. который решает и както там как японял дает сигнал
контроллеру что маленькие запросы на чтение для любого уровня рейда и маленькие запросмы
на запись для raid0 их якобы ненадо пропускать через микрокод который обрабатывает 
входящие запросы на цпу контроллера (я так понял) а типа их сразу надо кидать 
на массив (звучит конечно както мутно). значит там написана и такая мудота что если
запрос на чтение через смартпаф возвращается с ошибкой то тогда запрос еще раз 
кидается на контроллер и контроллер его обрабатвыает обычным путем. у меня вопрос
а как это запрос через смартпаф может вернуться с ошибкой если он кидается на массив 
и захера его обрабатывать после этго обычным способом что это даст.
и в каком случае и почему обычный способ ( запрос обрабатвыается на цпу на микрокоде 
контроллера) может вернуть уже неошибку. также получаетс что если смартпаф активирован
то запросы на запись на raid10 все равно идут обычным путем. но что значит обычным
путем. разве это значит что запрос будет записан в кэш контроллера. помоему нет.
так какой тогда смысл от обычного пути. и что это заобычный путь если запрос 
игнорирует кэш контроллера. а факт такой есть точно что при смартпаф запись на массив 
конкретно проседает.

что асболютно точно что при куче ssd в массиве чтение может и вырастет 
а вот запись точно будет в ж. так как запись идет не накэш контроллера 
а на диски сразу. а если там еще drive cache выключен то вобще хана.


хотя согласно вот этому видео - https://www.youtube.com/watch?v=H0nUnvj5xgU&t=74s
там тест проводили на raid0 массиве эффект увеличения randon чтения iops
для raid10 массива по моим предсказаниям должен быть виден на массиве из
6 дисков raid10 и еще имеет значение сколько у нас дырок на контроллере.


так что общий вывод: если дисков мало меньше 6 в массиве ,
то смартпаф идет нахер. про смарт паф можно забыть.
и да здравтсввуйет controller cache.
если дисков 6 или больше то надо тестировать и сравнивать практический
результат. причем помимо резултатов на чтение надо смотреть а что там с
записью . если drive cache деактивировано то про смартпаф мжоно забыть.
а если активированы то смотреть чо там по результатам.
 а если мы будем юзать кэш контроллера то нужно  к нему батарейку покупать.
прикол втом что кэш на контроллерер для шпиндельных дисков юзали 
для сохранности данных а на ssd получается для скорости. смешно.


причем если активирован смарт паф то чтобы переключить лун 
на caching надо вначале дезактивировать смарт паф и только потом драйвер
даст активировать примененеие кэша на луне.

пример

# ./hpssacli ctrl slot=0   array B  modify ssdsmartpath=disable

# ./hpssacli ctrl slot=0   ld 1 modify caching=enable

причем опят же дебилизм. смартпаф активруется на array а caching на ld

---
что я проверил на практике.
что многопоточный random read дает прирост на raid1 по сравнению
с одиночным диском.
 
 диски samsung 850 1TB
 контроллер HP 420i
 
 нагрузка 20 воркеров х 1 поток
 4k random read
 
один диск
46 000 iops
0.43 latency

raid1
62 000 iops
0.32 latency

что интересно у samsung 850 EVO 1TB (одиночный диск) заявлено 
производилем 
4KB Random Read (QD1): Max. 10,000 IOPS
4KB Random Read (QD32):Max. 98,000 IOPS (1TB)

QD это outstanding i\o в терминах iometer.

на storagereview есть тест этого диска https://www.storagereview.com/review/samsung-ssd-850-evo-ssd-review
и они выжали на (QD1) 9 431 iops нутоесть чуть меньше 10,000.

у меня же на p420i контроллере получилось выжать для QD1 ~ 5 500 iops
а для QD32 ~ 50 000 iops

тоесть в 2 раза меньше чем диски  могут выжимать из себя.

окей ну можно предположить что p420 слабый контроллер.

но на P840 контроллере , там у меня нет одиночного диска доступного
но есть raid1 из 840evo+850evo
на нем (QD1) дает все теже 5,500 iops
можно предположить что это показатель скажем 840evo а не 850evo
но на стораджревью есть и обзор 840evo и он на (QD1) выжимает 10,023 iops https://www.storagereview.com/review/samsung-840-evo-ssd-review
тоесть чтото нето с контроллерами чтоли.

хотя на контроллере от другого производителя lsi 
AVAGO MegaRAID SAS 9361-4i
теже самые диски samsung 840evo 1TB 
на (QD1) он выжимает 6,300 IOPS.
нутоесть тоже явно не 10,000.

на P440ar 
на других новых и быстрых дисках  KINGSTON DC500R 2TB
или kingston dc500M 2TB
на (QD1) я выжимаю попрежнему всего теже самые 5,000 iops
по ним я ненашел тест который бы показал iops на (QD1)
но думаю что он тоже долэжен быть нехуже 10,000 iops

получается на всех диска и на всех контроллерах 
почемуто только половина iops выжимается

=====
также эксперимнт показал что пофиг как запускать 
20 воркеров по 1 или 1 воркер по 20
или скажем 16 воркеров по 8.
одно и тоже.

====
при (qd1) пофиг у нас одиночный ssd диски или массив из дисков.
скосротб одна и та же. по крайней мерер для randon read 4k

==
я на домашнем буке
на диске 

WD PC SN520 477GB

выжал на 4k rand read 
1 воркер и (QD1) = 6,200 IOPS
8 воркеров и (QD64) = 98,000 IOPS 

про его 4k rand read max iops я нашел что оно заявлено до 270,000 iops
замечу что когда я выжал 98,000 iops (что конешно не 270,000 и близко) то 
загрузка цпу была 60%. учитывая что HT активирован то это згачит что цпу был на 100% загружен.
тоесть походу узкое место это цпу поскоьку диск nvme.


=

4k rand read 
16 воркеров по 8qd
массив raid 10 из 10x sasmung 840 evo 1TB

iops 126,000
latency 1ms

контроллер AVAGO MegaRAID SAS 9361-4i
вот здесь гнашел что этот контроллер может тянуть 600k IOPS - https://www.truesystem.ru/solutions/khranenie_danny/361566/

тоесть 126,000 iops Это не его предела. тогда почему он так 
мало выдает на 10 дисках ssd ??!!

скорей всего потому что 10 дисков идут в контроллер по 1 дырке в беклейне 
или чтото типа того. 
получается вобще конешно хрень какаято.


==
что получилось из экспримента

про рейд массив из ssd дисков и как идет рост iops с увеличением количествао дисков
диски теже саый sasung 840\840 evo 1TB

4k rand read

3 воркер , qd32
r0  1 disk 50,000 iops
r1  2 disk 108,000 iops
r10 4 disk 160,000 iops

получается r1 дает двукратный прирост по сравнениб с базовой скоростю одного диска
а r10 дает трехкратный прирост отгосительно скорости базового диска.
но как видно базовый диск из себя выжимает только половину потенциала. вместо 100k всего 50k


==
также согласно графику из сайта Hp походу без смартпаф у p421 200,000 iops это потолок.
причем внезависимости сколько дисков туда напихать.
а чтобы смартпаф дал дальнейщий рост это надо 6 и более дисков.

при этом все равно непонятно почему всего 50k вместо 100k 
на одном диске.

получается если даже  1 диск выдаст 100,000 хотя дело не вдиске он уже такое может 
выдавать тем не менее сам контроллер его цпу неможет выжать более 200,000 iops без смартпаф.
но смарт паф даст прирост свыше 200,000 только от 6 дисков и выше.


===

для w1, q1 rand read, массив из ssd дисков ничего недает.
скорость будет равна такую которую выдает один диск.

===

в целом шас получается что массив из одного диска выжимает на rand read 50,000 iops
а должен 100,000 контрроллер hp

массив из 2-х дисков r1 выжимает 88,000 iops,контроллер hp
из 4-ч дисков r10 выжимает 160,000 iops, контрролллер hp
из 10-и дисков r01 контролер lsi выжимает 120,000-140,000 


===

про sas\sata
порт контроллера имеет несколько линий.
одна линия в итоге идет к одному диску.
обычно порт поддерживает 4 линии.
в зависимости от версии sas\sata которую поеддероживает 
клонтроллер скорость линии может быть от 3gb\s до 12Gb\s

контроллер smart array p420i
он имеет 2 дырки mini-sas  каждая из которых дает 4 линии sata.
причем sata той версии который 3Gb\s (300MB\s полезной инфо).
300MB\s = 76,000 iops по 4kB - можно снять с одного sata порта
получается на этом контроллере из за старой версии sata никак не снять 98,000 iops
с диска evo 840\850 из за самого контроллера.

получается в теории с одного мини-сас порта от sata 300 дисков можно 
снять 76,000 х 4 = 304,000 iops
а с двух мини-сас портов теоретически можно снять 600,000 iops

полуается два мини сам порта в теории могут дать 600,000 iops для sata 3Gb/s дисков

что касается срупута то это 300Mb*8 = 2,400 MB\s = 2,4GB\s
нашел тест на практике человек с 4 дисков r0 выжао 2,0 GB\s на seq read
непонятно как это можно получить если скорость порта с диска 300MB\s тоесть 
выше 1.2GB в сек скорость недолжна быть. склорей всего это из за read ahead 
при seq read. и эта скорость получается из за предчтения и то что мы как клиент
читаем через pcie из кэша контроллера который имеет скорость 
в завимости от вида от примерно 6GB\s то 10,6Gb\s
важно понимать что при seq read размер иопса явно не 4kb поэтому
на 2Gb\s seq read число иопс будет невелико на самом деле.



значит контроллер сидит на pciE v3 x 8 = 1GB\s * 8 = 8GB\s
тоесть срупут неупирается в скорсть шины.
как известно из видео от hp этот контроллер без смарт паф из себя выжимает 
максимум 200,000 iops
получается из за того что он понимает только sata 300 то в теории 
с диска который может дать 98,000 iops он может макс снять в теории 74,000 iops
 на форуме hp нашел инфо со ссылкой яобы на доку от hp что p420i
 выжимает из себя максимум 200,000iops получается что если мы исопльзуем диск который 
 дает 74,000 iops то потолк контроллера будет достигнут на 4 дисках r10.
 а p410 максимум 50,000 iops




контроллер smart array p440ar
ar ксати означает что он вставлен в спец слот - Flexible Smart Array and Smart Host Bus Adapter slot
у него тоже 2 дырки минисас. но он уже поддерживает sata 3 который 6Gb\s на дырку
это 600МБ\с полезнрой нагрузки. если в iops то это 150,000 iops
он поддерживает 8 линий. по 4 линии на дырку.


получается в теоррии он может через эти два мнини сас порта 
засосать 1200,000 iops через sata 600 диски
у него теже pcieV3x8 шина = 8GB\s
срупут через 8 sata 600 дисков составит 4,8GB\s тоесть вшину pcie мы неуираемся

что инетресно что ест p440 без ar тоесть в обычный порт pci-e и у него 
всего 1 физ порт но он тоже поддерживает 8 линий.  как это возможно? ведь 
обычно sas порт поддерживает только 4 линии. просто скорость линии может 
быть от 3Gb\s до 12gb\s но на число дисков поддезиваемых это не влиет
их 4 штуки макс на порт. а тут 1 порт а контроллер имеет 8 линий.
разгадка в том что на контроллере используется толи особоый то ли самый
современный порт sff-8654 , на контролллере он имеет тип double-wide 
поэтому этот порт поддерживает 8 линий. как и контроллер.
на другой стороне провода от порта может быть 8654 single-wide на 4 линии,
тогда их будет два. или там будет такой же sff-8654 double-wide.
все зависит какие разбемы имеет беклейн proliant сервера.
вобщем разгадка как это sas порт контроллера может поддерживаеть 8 линий 
в том что испольщутся спец порт sff-8654 doubdle wide.





контроллер P840 
у него 2 физ порта. 
но поддеживает он 16 устройства а не 8 , потому что у него обпятьже порт 
особоый sff-8654 double wide.

sata диски он понимает которые sata 600
в отличие от  p440контроллер он поддежривает не 8 линий а 16. тоесть
столько дисков напрямую можнок нему подключить.
непонятно какой смысл от скорости его кэша 14GB\s если скорость шины piev3x8
все равно максимум 8GB\s я так понимаю эти 14GB\s имеют токазначение 
между передачей данных между массивом и кэшем. но не между кэшем и пользователем.


получается когда мы смотрим на контроллер над смотеть 
сколько линий sas\sata у него внутри тоетсь сколько дтиско напрямую
к нему можно подключить в теории. какая скорость линии для sas и sata.
и также надо смотреть а как эти линии входят в бекплейн. сколько в бекплейне дырок
и сколько линий дает бекплейн на сколько дырок.
условно говоря в бекпдейне 16 дисков. и две дырки. и каждая дырка имеет 4 линии.
значит гавно. потому что 8 дисков сидят на 4 линиях. значит походршему
только 8 линий\8 дисков мы можем использовать. 


что интересно ввот в этот документе - https://impuls-it.ru/upload/iblock/e25/4AA5-4526ENW.pdf

указано что p420i может на 4k rand read выжать 400,000 iops
добились они этого на 8 sas ssd дисках на raid0. + smart path
у него 8 линий
он поддерживает SAS 6Gb/s (600MB\s, 150,000 iops 4k) на линию.
ну что в теории 8 линии sas конечно позволяют ему выжимать 400,000iops 
разница sas и sata линий на этом контроллере в том что сата 
линия в 2 раза более узкая. так как ssd диски обычно могут из себя выжать 100k
непонятно почему он 800,000 iops не выжал. ведь смарт паф типа полностью 
или почти полностью неюзает цпу на контроллере котоырй узкое место.
ну наверно так как сата в 2 раза медленее то на sata ssd все таки 
даже на 8 дисков + смарт паф этот контроллер более 200,000 iops невыжмет.

а про P440ar на 8 дисков sas ssd raid0 + смарт паф они выжали 930,000 iops
на 4k rand read.


интерсно 420i на четырех дискха показал бы 400,000 или только на восьми дисках 
он это мжоет сделать

также интересно 440-ой он на sata дисках может показат 800,000iops или только
на sas дисках.



===
получтся какой бы контроллер ни был
4 диска на один его порт можно вешать свободно. у порта точно должно быть 
минимум 4 линии. тоесть порт небудет являтся узким местом.
узким место будет цпу контрроллера.
ну а чтобы активироавт смарт паф надо 8 дисков миниум, тоесть два порта задействовать
контроллера

ну или если контроллер поддерживает 8 дисков на порт то хватит и одного порта
контролллера
===

чтобы снять с диска всю заложенную в него скорость это зависит от :
 - модели контроллера
- модели бекплейна на сервере
- как диски распределены по бекплейну
- числа дисков в рейде
- вид рейда

===
еще раз прикол в том что для нагрузки воркер=1 q=1 массив из ssd бесполезен
ничего недает. скорьст будет ровнго такая же как с одного диска 5000=10000 iops
==

получается если есть сервер и  у него сколько дырок под диски.
то надо уточнить а сколько портов сзади есть на беклейне. потому что надо 
чтобы к каждому диску с бекплейна подходила своя линия.
если мы неберем диковинные порты такие как double-wide sff-8654
то обычно один порт (разьем) имеет 4 линии. поэтому на каждые 4 дырки под 
диски на бекплейне сервера должны быть дырка.
на g9 там корзинки вставляются внутрь сервера на 8 дисков. и на эту корзинку
идет на жопе бекпелен с двумя разьемами. тут все хорошо. 8 дисков на два раьзема
это 4 диск на разьем. один разьем как раз имеет 4 линии. все хорошо.
если число дырок под диски соотвестввует числу портов на бекплейне сервера
то дальше надо чтобы уже на контроллере был соотвствубщее число портов
ну или линий. тогда унас в связке диск-порт бекплейна-порт контроллера
устанавлияется правильная связь 1 к 1.
далее если у нас мы в рейд массив хотим собрать 6 дисков и меньше то тогда
смарт паф отключен и поток iops идет через цпу контроллера значит 
этот цпу дролжен иметь достаточную скорость чтобы тянут все это число ioips
которые могут из себя диски выжать. например 6 дисков в raid0 могут дать 540,000iops
значит цпу контроллера должен уметь их держать.
а если дисков более 6 то дальше цпу уже несправляется и нужно включать смартпаф(правда
приэтом скрость записи падает) чтобы выбрать все эти iops которые могут изсебя выжать диски.

тонкий момент в том что если в сервер много дырок под диски а на его бекплейне 
портов мало то хрень ты из этого набора дисков выжмешь много iops
и на контроллере тоже должно быт нужное число портов. и скорость цпу тоже.

также прикол втом что у hp смартпаф активен он ил нет можно проверимть легко
а у lsi активен ли fastpath или нет хрен проверишь. отстой.

====

OCZ-VERTEX3 240GB (SATA 6Gb/s) raid1  2disk
тест на рандом чтение в 1 поток

Smart Array P410i 
512MB cache size
прошивка 6.64

iometer vm на esxi 6.5

внимание контроллер почемуто подключил диски не на скорости 3Gb\s как он
это поддерживает   а на 1.5Gb\s 

драйвер esxi позволяет нагружать DQLEN=1024 потоков одновременно

тип доступа к lun: controller cache
сотношение контроллер кэша 0%R/100%W 

Drive Write Cache: Disabled

размер тестового файла 32GB

тест идет на виртуалке на vsphere

4k, 100% read, 100% random
workers=1, q=1
2,700 iops
0,37ms  latency

а судя по интернетам (https://www.storagereview.com/review/ocz-vertex-3-max-iops-ssd-review-240gb)
должно быть  в районе 6,800 iops

почемуто P410 эти диски подключает как 1.5Gbps хотя P410 поддерживает 
по своей специйaикации SATA 3Gbps а сами диски вообще то по спецификации вообще имеют 6Gbps

я обновил прошивку с 6.60 до 6.64 однако все попрежнему. в инете 
нашел что такая проблема бывает у людей. есть предположение что это из за 
дебильного control plane на конкретном сервере.
тоесть дело не в прошивке


512KB, 100% read, sequential read
чтение выровнено по размеру читаемого блока
workers=1, q=1
213MB\s  truput



1MB, 100% read, sequential read
чтение выровнено по размеру читаемого блока
workers=1, q=1
223\s  truput



2MB, 100% read, sequential read
чтение выровнено по блоку 2MB
workers=1, q=1
236MB\s  truput


значит что еще примечательно. сами диски поддерживают стандарт sata600, карточка
поддерживает SATA300 но при этом по факту карта работает с дисками на стандарте SATA150.
почему непонятно. окей. тогда на этом режиме максимальная скорость срупута должна 
быть 150МБ\с однако как мы видим она 236MB\s.

как же так может получится. а тут в дело уже вступает esxi. дело в том что 
VM iometer пуляет запрос на чтение размером 2MB а (как я понимаю ) esxi 6.5 максимально
может позволить VM читать блоками размером 1MB. таким макаром esxi делит исходный 
запрос размером 2MB на 2 запроса x по 1MB. получается в вм запрос выглядит как
bs=2M, q=1. а esxi на контроллер бросает bs=1M q=2. и оно так и видно 
в VM iops=113 а в esxtop iops=226 и ACTV=2. получается так как q=2
в силу уже вступает что у нас рейд а неодиночный диск и чтение идет с обоих дисков.
половтина с одного половина с другого. и тогда понятно почему получаем число 
больше чем 150МБ\с. получатеся 120МБ\с читает содного и 120МБ\с читает с другого. 
и 120<150 таким образом лимит ширины линии sata150 ненарушен.

для сравнения  я померил на esxi 5.5. там максимальный размер блока который может подавать
esxi на контроллер это 0.5MB. таким образом изначальный запрос 2МБ esxi 5.5 поделить 
на 4 щтуки по 0.5МБ. и будет q=4 bs=0.5MB 

таким образом поведенгие esxi 6.5 отличется от esxi 5.5

вот сколко моментов выяснилось.

также я выяснил что smart array он неимеет настроек для read ahead. едиснтвенное что у 
него есть это если мы кэш порежем не 0\100 как наверху а 25\75 то никакйого выигрыша 
мы опять же не получим. потому что рид ахед работает так что мы читаем блоки и они 
также сохрнаяются в кэше и если повторно к этимж же блокамт обращаться то они из 
кэша считаются. вот какой рид ахед на смарт эррее. ну и нашему режиму это пофиг.
у нас нет повтороного обрашения к данным. для сравннеия у lsi там есть
настройки рид ахед для массива. и выглдяит это так. если мы для масива активировали
рид ахед то если толи два толитри блока подряд мы прочитали то контроллер понимает 
что мы читаем линейно и он третий блок прочитает уже заранее сам. то есть при активированном
рид ахед для масива он неутупо постояно читает наперед а только если по факту он видит
что последние два блока которые были считаны были рядом. тогда он начинает чиатть рид 
ахед. а потом прекращает. и эту фичу можно и выключать и включать.
таким образом на смартаэррейд рид ахед сделано вобще дерьмово. его нельзя ни включить
ни выключить. все регулируется распределнем кэша %начтение\% на запись. и рид ахед 
работает только для повторого обрашения к ссчитанным блокам при условии что они сохранились
в кэше.


таким образом мы подлучили что срупут на чтение с 1 диска = 0.5 * 236МБ\с = 118 МБ\с
а вот здесь в тесте (https://www.storagereview.com/review/ocz-vertex-3-review-240gb)
люди получили с 1 диска (диск подключен через SATA300) = 254 МБ\с
а если этот же диск подключен через SATA600 то = 492МБ\с







=======================

что важно осознать.
кошда мы смотрим на свойства физ диска на smart array контроллере

 Array B

      physicaldrive 2I:2:3
         Port: 2I
         Box: 2
         Bay: 3
         Status: OK
         Drive Type: Data Drive
         Interface Type: Solid State SATA
         Size: 1 TB
         Drive exposed to OS: False
         Logical/Physical Block Size: 512/512
         Firmware Revision: EXT0CB6Q
         Serial Number: S1D9NSAF732593J
         WWID: 3001438038D5C4B2
         Model: ATA     Samsung SSD 840
         SATA NCQ Capable: True
         SATA NCQ Enabled: True
         Current Temperature (C): 30
         SSD Smart Trip Wearout: Not Supported
         PHY Count: 1
         PHY Transfer Rate: 6.0Gbps
         Drive Authentication Status: OK
         Carrier Application Version: 11
         Carrier Bootloader Version: 6
         Sanitize Erase Supported: False
         Shingled Magnetic Recording Support: None


то вот эта вот строка
PHY Transfer Rate: 6.0Gbps

она показвыает не какой версии SATA интерфейс на диске
а какой SATA версии интерфейс идет к диску от контроллера.

тоесть в данном случае это контроллер подключен к диску на скорости 6Gbps
а сам диск может быть имееет 12Gbps. 
тоесть это показан не характиеристика диска а характеристика канал контрроллера
на самом деле.

диск может иметь более быстрый порт.
это надо всегда тоже отслеживать.

=====================
я понял почему  в smart array скорость чтения\записи  в кэш память
значительно превосходит скорост чттения\записи не только на один sas\sata ssd диск
но и на весь массив дисков которые он может поддержать
например контроллер p440ar имеет 8 линий. значит он может держать на максимаьной
скорости либо 8 sata дисков либо 4 sas диска (два порта на диск)

один 12Gb\s SAS диск имеет скрость чтения 2100MB\s через два порта = 2.1GB\s
4 таких диска потянут на 8.5GB\s

скорость  кэш памяти P440AR
2GB 72-bit wide DDR3-1866 provides up to 14.9 GB/s maximum cache bandwidth
это при том что 

как мы видим 14.9GB\s намного больше чем 8.5GB\s

спрашивается какой смысл и зачем.

кстати pci-e разьем карты

PCI Express Gen3 x 8 link width

потянет 8GB\s

получается что мы как клиенты не можем получать данные из кэша на скорости 14GB/s
потому что нам мешает ширина полосы у разьема. и на диски из кэша немогут данные
писаться также на скрости 14GB\s потому что массив из дисков тоже нетакой быстрый.


получается такая схема

pci-e разьем(8GB\s) <--> кэш память (14GB\s) <--> массив рейд дисков (8GB\s)

спрашивается какой смысл в такой скрости кэша. 
тем более незабываем что 8Gb\s мы получим только на дорогущих ssd sas дисках.
да еще в raid0 массиве который никто небудет юзать никогда.
тоесть скорость массива обычно будет заметно меньше.
как я понимаю скорость нужна такая большая у кэша 
чтобы процессор контроллера мог отдавать виртуалке (клиенту) данные 
из кэша (тоесть операция чтения данных из кэша) на полной скорости pci-e разьема.
тоесть клиент должен иметь возможность получать данные из кэша на полной максимальной
скорсти 8GB\s pci-e разьема  и одновременно цпу карточки должен иметь 
возможность писать данные из кэша на диски на полной максимальной скорости 
массива 8GB\s. соотвесвтенно если представить картину условную что одновременно 
из кэша читают данные из клиент через pci-e разьем и дисковый массив то 
их сумманая скорось 16GB\s что чуть меньше чем 14GB\s.
вот эта одновременность и дает требование что скорость чтения из кэша 
должна быть суммой этих двух цифр.

я думаю както примерно так. в чем смысл такой заведомо большой цифры работы скрости
кэша контроллера

 

также например если мы читаем из кэша и данные там есть то 
они долетят до VM с заметно быстрой скоростью чем это будет непосредственно с массива.
запись данных в кэш будет идти заметно быстрее чем в sata массив из 8 дисков-4GB\s) 
если можно получить буст при записи в массив через кэш почему бы это неделать
раз pci-e ширина шины позволяет. 
 
 ответ на вопрос зачем кэш работает на скрости 14 а не 8 как шина по мне заклчается
 еще раз в том что это позволяет чиатть данные из кэша клиенту и записывать данные 
 из кэша на массив без потери скорости и для клиента и для массива.

====================
стоит проксмокс на голом железе.
на нем VM win2012 с iometer

тестируем скорость диска внутри вин VM 

promox
diks vertex4-240GB ssd
HP P410
scsi controler=virtio
в vm intel82371SB
hdd тип ide (qemu harrddisk ata)
4k rand read w=1 q=32
iops = 1,854
late=17 ms

4k rand read w=1 q=1
iops = 1,700
late=0,57 ms

hdd тип virtio (redhat virtio scsi disk)
4k rand read w=1 q=32
iops = 19,400
late= 1,65 ms

4k rand read w=1 q=1
iops = 2,189
late=0,46 ms

видно что virtio диск катастрофически быстрее чем ide диск


====
про DCUI

DCUI - это доступ нетолько к командной строке из ilo когда нет доступа по сети
это вообще доступ к минимальным настройкам esxi из ilo.

может так случится что какойто мудак отключил DCUI.
далее чтото нето стало с сетью и вуаля на сервер уже никак не зайти.

что делать - надо обратно активировать DCUI.

как сделать.

надо снят флэшку с сервера. далее подмонтировать ее в линуксе.

за DCUI отвечает

/etc/init.d/DCUI

как заставить служббу стартануть при загрузке esxi

по идее это моно юыло бы изменить командной chkconfig
но к ней мы неимеем доступа.
в линуксе она что делает она добавляет /etc/rc.d sub-directories.
но в esxi этого нет.

поэтому что надо делать 

надо добавить в файл 

/etc/rc.local.d/local.sh

  /etc/init.d/DCUI start
  
 
по идее должно сработать.


============

что еще интересно.
если мы зашли на работающий esxi 
поменяли конфиги то надо чтобы эти настройки из рамдиска
перелетели на флэшку.

для этого

/sbin/auto-backup.sh

или

/sbin/backup.sh 1

==========

прикольно что на esxi есть cron

которым можно делать очень даже полузные штуки

вот как его настрривать

https://kakpedia.org/esxi-bash-script-%D0%B4%D0%BB%D1%8F-%D0%B0%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B3%D0%BE-%D0%B2%D0%BA%D0%BB%D1%8E%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B2%D0%B8/

===============
приколно вынлядит inittab

/tmp # cat /etc/inittab
::sysinit:/usr/lib/vmware/init/bin/create-busybox-symlinks.sh
::sysinit:/usr/lib/vmware/rp/bin/configRP init
#
::sysinit:/bin/init-launcher
::wait:/bin/services.sh start
::wait:/bin/apply-host-profiles
::wait:/usr/lib/vmware/vmksummary/log-bootstop.sh boot
::wait:/bin/vmdumper -g 'Boot Successful'
::wait:/bin/sh ++min=0,group=host/vim/vimuser/terminal/shell /etc/rc.local
::wait:/bin/esxcfg-init --set-boot-progress done
::wait:/bin/vmware-autostart.sh start
tty1::respawn:/bin/initterm.sh tty1 /bin/techsupport.sh
tty2::respawn:-/bin/initterm.sh tty2 /bin/dcuiweasel
::restart:/bin/init
::shutdown:/usr/lib/vmware/vmksummary/log-bootstop.sh stop
::shutdown:/bin/shutdown.sh
/tmp #
======
