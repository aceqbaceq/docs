наконец сюда я решил 
заносить все результаты тестов на iops




4k 100% read 100%random
20 workerov x q=1
SmartArray P840 Raid1  2xSSD Samsung 840 EVO 
smartpath
61 000 iops
0.32 latency


4k 100% read 100%random 
20 workerov x q=1
через FC (данные читаются с этого же контроллера P840 c этого же
массива 2xSSD Samsung 840 EVO но через FC)(сервер Fc=esos, клиент=встроенный
в ESXI клиент FC)
41 000 iops
0.47 latency


4k 100% read 100%random 
20 workerov x q=1
Raid10 Smart Array P440ar  4xSSD KINGSTON SEDC500 
82 000 iops
0.24 latency


4k 100% read 100%random 
20 workerov x q=1
контроллера нет, одиночный диск nvme SSD Intel 3700 372GB
143 000 iops
0.14 latency



режимы:
w=1,q=1
w=20,q=1
smartpath\controller cache
10\90 controller cache, 0/100 controller cache, 100\0 controller cache
random read, random write
seq read, seq write
buffer cache enable\disable
iops 1\1000
мало дисков массиве много дисков в массиве
lsi\hp






Samsung SSD 850 x 1disk
тест на рандом чтение в 1 поток

Smart Array P420i
512MB cache size

драйвер esxi позволяет нагружать DQLEN=1000 потоков одновременно

тип доступа к lun: controller cache
сотношение контроллер кэша 0%R/100%W 

Drive Write Cache: Enabled

размер тестового файла 32GB


4k, 100% read, 100% random
workers=1, q=1
5500 iops
0.18ms latency

изменение сотношение контроллер кэша 25%R/75%W дало прежний результат. 
что ожидаемо. кэш маленький тестовый файл большой.


меняю доступ к луну на smartpath
тип доступа к lun: smartpath
и смотрим скорость того же режима
4k, 100% read, 100% random
workers=1, q=1
5600 iops
0.18ms latency

вывод: smartpath никак неповлиял.

я попробовал менять доступ к луну (controller cache\smartpath) еще на одном
контроллере P840. пробовал и 1 воркер х1 поток и 20 воркеров х1 поток. 
резултат один и тотже. вывод: смартпаф на режим рандом чтения никакой роли неиграет.
типа чо за фигня. далее я нашел в инете тест  в котором видно что смарт паф работает 
заметно ХУЖЕ!( я думаю иза за того что паттерн в том тесте включал и запись. а записть 
в реэиме смартпафточно уходит в жопу) если рейд меньше 18 дисков.
так что теперь стало понятно что надо хрена дисков чтобы этот смарт паф дал какойто 
выхлоп. а если их меньше то он вобще только хуже делает.

вывод- нахер этот смарт паф.

что такое вобще этот смартпаф.
как написано здесь - https://support.hpe.com/hpesc/public/docDisplay?docId=mmr_kc-0125362
что типа смартпаф это фича драйвера. который решает и както там как японял дает сигнал
контроллеру что маленькие запросы на чтение для любого уровня рейда и маленькие запросмы
на запись для raid0 их якобы ненадо пропускать через микрокод который обрабатывает 
входящие запросы на цпу контроллера (я так понял) а типа их сразу надо кидать 
на массив (звучит конечно както мутно). значит там написана и такая мудота что если
запрос на чтение через смартпаф возвращается с ошибкой то тогда запрос еще раз 
кидается на контроллер и контроллер его обрабатвыает обычным путем. у меня вопрос
а как это запрос через смартпаф может вернуться с ошибкой если он кидается на массив 
и захера его обрабатывать после этго обычным способом что это даст.
и в каком случае и почему обычный способ ( запрос обрабатвыается на цпу на микрокоде 
контроллера) может вернуть уже неошибку. также получаетс что если смартпаф активирован
то запросы на запись на raid10 все равно идут обычным путем. но что значит обычным
путем. разве это значит что запрос будет записан в кэш контроллера. помоему нет.
так какой тогда смысл от обычного пути. и что это заобычный путь если запрос 
игнорирует кэш контроллера. а факт такой есть точно что при смартпаф запись на массив 
конкретно проседает.

что асболютно точно что при куче ssd в массиве чтение может и вырастет 
а вот запись точно будет в ж. так как запись идет не накэш контроллера 
а на диски сразу. а если там еще drive cache выключен то вобще хана.


хотя согласно вот этому видео - https://www.youtube.com/watch?v=H0nUnvj5xgU&t=74s
там тест проводили на raid0 массиве эффект увеличения randon чтения iops
для raid10 массива по моим предсказаниям должен быть виден на массиве из
6 дисков raid10 и еще имеет значение сколько у нас дырок на контроллере.


так что общий вывод: если дисков мало меньше 6 в массиве ,
то смартпаф идет нахер. про смарт паф можно забыть.
и да здравтсввуйет controller cache.
если дисков 6 или больше то надо тестировать и сравнивать практический
результат. причем помимо резултатов на чтение надо смотреть а что там с
записью . если drive cache деактивировано то про смартпаф мжоно забыть.
а если активированы то смотреть чо там по результатам.



причем если активирован смарт паф то чтобы переключить лун 
на caching надо вначале дезактивировать смарт паф и только потом драйвер
даст активировать примененеие кэша на луне.

пример

# ./hpssacli ctrl slot=0   array B  modify ssdsmartpath=disable

# ./hpssacli ctrl slot=0   ld 1 modify caching=enable

причем опят же дебилизм. смартпаф активруется на array а caching на ld

---
что я проверил на практике.
что многопоточный random read дает прирост на raid1 по сравнению
с одиночным диском.
 
 диски samsung 850 1TB
 контроллер HP 420i
 
 нагрузка 20 воркеров х 1 поток
 4k random read
 
один диск
46 000 iops
0.43 latency

raid1
62 000 iops
0.32 latency

что интересно у samsung 850 EVO 1TB (одиночный диск) заявлено 
производилем 
4KB Random Read (QD1): Max. 10,000 IOPS
4KB Random Read (QD32):Max. 98,000 IOPS (1TB)

QD это outstanding i\o в терминах iometer.

на storagereview есть тест этого диска https://www.storagereview.com/review/samsung-ssd-850-evo-ssd-review
и они выжали на (QD1) 9 431 iops нутоесть чуть меньше 10,000.

у меня же на p420i контроллере получилось выжать для QD1 ~ 5 500 iops
а для QD32 ~ 50 000 iops

тоесть в 2 раза меньше чем диски  могут выжимать из себя.

окей ну можно предположить что p420 слабый контроллер.

но на P840 контроллере , там у меня нет одиночного диска доступного
но есть raid1 из 840evo+850evo
на нем (QD1) дает все теже 5,500 iops
можно предположить что это показатель скажем 840evo а не 850evo
но на стораджревью есть и обзор 840evo и он на (QD1) выжимает 10,023 iops https://www.storagereview.com/review/samsung-840-evo-ssd-review
тоесть чтото нето с контроллерами чтоли.

хотя на контроллере от другого производителя lsi 
AVAGO MegaRAID SAS 9361-4i
теже самые диски samsung 840evo 1TB 
на (QD1) он выжимает 6,300 IOPS.
нутоесть тоже явно не 10,000.

на P440ar 
на других новых и быстрых дисках  KINGSTON DC500R 2TB
или kingston dc500M 2TB
на (QD1) я выжимаю попрежнему всего теже самые 5,000 iops
по ним я ненашел тест который бы показал iops на (QD1)
но думаю что он тоже долэжен быть нехуже 10,000 iops

получается на всех диска и на всех контроллерах 
почемуто только половина iops выжимается

=====
также эксперимнт показал что пофиг как запускать 
20 воркеров по 1 или 1 воркер по 20
или скажем 16 воркеров по 8.
одно и тоже.

====
при (qd1) пофиг у нас одиночный ssd диски или массив из дисков.
скосротб одна и та же. по крайней мерер для randon read 4k

==
я на домашнем буке
на диске 

WD PC SN520 477GB

выжал на 4k rand read 
1 воркер и (QD1) = 6,200 IOPS
8 воркеров и (QD64) = 98,000 IOPS 

про его 4k rand read max iops я нашел что оно заявлено до 270,000 iops
замечу что когда я выжал 98,000 iops (что конешно не 270,000 и близко) то 
загрузка цпу была 60%. учитывая что HT активирован то это згачит что цпу был на 100% загружен.
тоесть походу узкое место это цпу поскоьку диск nvme.


=

4k rand read 
16 воркеров по 8qd
массив raid 10 из 10x sasmung 840 evo 1TB

iops 126,000
latency 1ms

контроллер AVAGO MegaRAID SAS 9361-4i


==
что получилось из экспримента

про рейд массив из ssd дисков и как идет рост iops с увеличением количествао дисков
диски теже саый sasung 840\840 evo 1TB

4k rand read

3 воркер , qd32
r0  1 disk 50,000 iops
r1  2 disk 108,000 iops
r10 4 disk 160,000 iops

получается r1 дает двукратный прирост по сравнениб с базовой скоростю одного диска
а r10 дает трехкратный прирост отгосительно скорости базового диска.
но как видно базовый диск из себя выжимает только половину потенциала. вместо 100k всего 50k



===

линейная скорость чтиения  при 0\100 и 25\75



