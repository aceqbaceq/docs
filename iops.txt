наконец сюда я решил 
заносить все результаты тестов на iops




4k 100% read 100%random
20 workerov x q=1
SmartArray P840 Raid1  2xSSD Samsung 840 EVO 
smartpath
61 000 iops
0.32 latency


4k 100% read 100%random 
20 workerov x q=1
через FC (данные читаются с этого же контроллера P840 c этого же
массива 2xSSD Samsung 840 EVO но через FC)(сервер Fc=esos, клиент=встроенный
в ESXI клиент FC)
41 000 iops
0.47 latency


4k 100% read 100%random 
20 workerov x q=1
Raid10 Smart Array P440ar  4xSSD KINGSTON SEDC500 
82 000 iops
0.24 latency


4k 100% read 100%random 
20 workerov x q=1
контроллера нет, одиночный диск nvme SSD Intel 3700 372GB
143 000 iops
0.14 latency



режимы:
w=1,q=1
w=20,q=1
smartpath\controller cache
10\90 controller cache, 0/100 controller cache, 100\0 controller cache
random read, random write
seq read, seq write
buffer cache enable\disable
iops 1\1000
мало дисков массиве много дисков в массиве
lsi\hp






Samsung SSD 850 x 1disk
тест на рандом чтение в 1 поток

Smart Array P420i
512MB cache size

драйвер esxi позволяет нагружать DQLEN=1000 потоков одновременно

тип доступа к lun: controller cache
сотношение контроллер кэша 0%R/100%W 

Drive Write Cache: Enabled

размер тестового файла 32GB


4k, 100% read, 100% random
workers=1, q=1
5500 iops
0.18ms latency

изменение сотношение контроллер кэша 25%R/75%W дало прежний результат. 
что ожидаемо. кэш маленький тестовый файл большой.


меняю доступ к луну на smartpath
тип доступа к lun: smartpath
и смотрим скорость того же режима
4k, 100% read, 100% random
workers=1, q=1
5600 iops
0.18ms latency

вывод: smartpath никак неповлиял.

я попробовал менять доступ к луну (controller cache\smartpath) еще на одном
контроллере P840. пробовал и 1 воркер х1 поток и 20 воркеров х1 поток. 
резултат один и тотже. вывод: смартпаф на режим рандом чтения никакой роли неиграет.
типа чо за фигня. далее я нашел в инете тест  в котором видно что смарт паф работает 
заметно ХУЖЕ!( я думаю иза за того что паттерн в том тесте включал и запись. а записть 
в реэиме смартпафточно уходит в жопу) если рейд меньше 18 дисков.
так что теперь стало понятно что надо хрена дисков чтобы этот смарт паф дал какойто 
выхлоп. а если их меньше то он вобще только хуже делает.

вывод- нахер этот смарт паф.

что такое вобще этот смартпаф.
как написано здесь - https://support.hpe.com/hpesc/public/docDisplay?docId=mmr_kc-0125362
что типа смартпаф это фича драйвера. который решает и както там как японял дает сигнал
контроллеру что маленькие запросы на чтение для любого уровня рейда и маленькие запросмы
на запись для raid0 их якобы ненадо пропускать через микрокод который обрабатывает 
входящие запросы на цпу контроллера (я так понял) а типа их сразу надо кидать 
на массив (звучит конечно както мутно). значит там написана и такая мудота что если
запрос на чтение через смартпаф возвращается с ошибкой то тогда запрос еще раз 
кидается на контроллер и контроллер его обрабатвыает обычным путем. у меня вопрос
а как это запрос через смартпаф может вернуться с ошибкой если он кидается на массив 
и захера его обрабатывать после этго обычным способом что это даст.
и в каком случае и почему обычный способ ( запрос обрабатвыается на цпу на микрокоде 
контроллера) может вернуть уже неошибку. также получаетс что если смартпаф активирован
то запросы на запись на raid10 все равно идут обычным путем. но что значит обычным
путем. разве это значит что запрос будет записан в кэш контроллера. помоему нет.
так какой тогда смысл от обычного пути. и что это заобычный путь если запрос 
игнорирует кэш контроллера. а факт такой есть точно что при смартпаф запись на массив 
конкретно проседает.

что асболютно точно что при куче ssd в массиве чтение может и вырастет 
а вот запись точно будет в ж. так как запись идет не накэш контроллера 
а на диски сразу. а если там еще drive cache выключен то вобще хана.


хотя согласно вот этому видео - https://www.youtube.com/watch?v=H0nUnvj5xgU&t=74s
там тест проводили на raid0 массиве эффект увеличения randon чтения iops
для raid10 массива по моим предсказаниям должен быть виден на массиве из
6 дисков raid10 и еще имеет значение сколько у нас дырок на контроллере.


так что общий вывод: если дисков мало меньше 6 в массиве ,
то смартпаф идет нахер. про смарт паф можно забыть.
и да здравтсввуйет controller cache.
если дисков 6 или больше то надо тестировать и сравнивать практический
результат. причем помимо резултатов на чтение надо смотреть а что там с
записью . если drive cache деактивировано то про смартпаф мжоно забыть.
а если активированы то смотреть чо там по результатам.
 а если мы будем юзать кэш контроллера то нужно  к нему батарейку покупать.
прикол втом что кэш на контроллерер для шпиндельных дисков юзали 
для сохранности данных а на ssd получается для скорости. смешно.


причем если активирован смарт паф то чтобы переключить лун 
на caching надо вначале дезактивировать смарт паф и только потом драйвер
даст активировать примененеие кэша на луне.

пример

# ./hpssacli ctrl slot=0   array B  modify ssdsmartpath=disable

# ./hpssacli ctrl slot=0   ld 1 modify caching=enable

причем опят же дебилизм. смартпаф активруется на array а caching на ld

---
что я проверил на практике.
что многопоточный random read дает прирост на raid1 по сравнению
с одиночным диском.
 
 диски samsung 850 1TB
 контроллер HP 420i
 
 нагрузка 20 воркеров х 1 поток
 4k random read
 
один диск
46 000 iops
0.43 latency

raid1
62 000 iops
0.32 latency

что интересно у samsung 850 EVO 1TB (одиночный диск) заявлено 
производилем 
4KB Random Read (QD1): Max. 10,000 IOPS
4KB Random Read (QD32):Max. 98,000 IOPS (1TB)

QD это outstanding i\o в терминах iometer.

на storagereview есть тест этого диска https://www.storagereview.com/review/samsung-ssd-850-evo-ssd-review
и они выжали на (QD1) 9 431 iops нутоесть чуть меньше 10,000.

у меня же на p420i контроллере получилось выжать для QD1 ~ 5 500 iops
а для QD32 ~ 50 000 iops

тоесть в 2 раза меньше чем диски  могут выжимать из себя.

окей ну можно предположить что p420 слабый контроллер.

но на P840 контроллере , там у меня нет одиночного диска доступного
но есть raid1 из 840evo+850evo
на нем (QD1) дает все теже 5,500 iops
можно предположить что это показатель скажем 840evo а не 850evo
но на стораджревью есть и обзор 840evo и он на (QD1) выжимает 10,023 iops https://www.storagereview.com/review/samsung-840-evo-ssd-review
тоесть чтото нето с контроллерами чтоли.

хотя на контроллере от другого производителя lsi 
AVAGO MegaRAID SAS 9361-4i
теже самые диски samsung 840evo 1TB 
на (QD1) он выжимает 6,300 IOPS.
нутоесть тоже явно не 10,000.

на P440ar 
на других новых и быстрых дисках  KINGSTON DC500R 2TB
или kingston dc500M 2TB
на (QD1) я выжимаю попрежнему всего теже самые 5,000 iops
по ним я ненашел тест который бы показал iops на (QD1)
но думаю что он тоже долэжен быть нехуже 10,000 iops

получается на всех диска и на всех контроллерах 
почемуто только половина iops выжимается

=====
также эксперимнт показал что пофиг как запускать 
20 воркеров по 1 или 1 воркер по 20
или скажем 16 воркеров по 8.
одно и тоже.

====
при (qd1) пофиг у нас одиночный ssd диски или массив из дисков.
скосротб одна и та же. по крайней мерер для randon read 4k

==
я на домашнем буке
на диске 

WD PC SN520 477GB

выжал на 4k rand read 
1 воркер и (QD1) = 6,200 IOPS
8 воркеров и (QD64) = 98,000 IOPS 

про его 4k rand read max iops я нашел что оно заявлено до 270,000 iops
замечу что когда я выжал 98,000 iops (что конешно не 270,000 и близко) то 
загрузка цпу была 60%. учитывая что HT активирован то это згачит что цпу был на 100% загружен.
тоесть походу узкое место это цпу поскоьку диск nvme.


=

4k rand read 
16 воркеров по 8qd
массив raid 10 из 10x sasmung 840 evo 1TB

iops 126,000
latency 1ms

контроллер AVAGO MegaRAID SAS 9361-4i
вот здесь гнашел что этот контроллер может тянуть 600k IOPS - https://www.truesystem.ru/solutions/khranenie_danny/361566/

тоесть 126,000 iops Это не его предела. тогда почему он так 
мало выдает на 10 дисках ssd ??!!

скорей всего потому что 10 дисков идут в контроллер по 1 дырке в беклейне 
или чтото типа того. 
получается вобще конешно хрень какаято.


==
что получилось из экспримента

про рейд массив из ssd дисков и как идет рост iops с увеличением количествао дисков
диски теже саый sasung 840\840 evo 1TB

4k rand read

3 воркер , qd32
r0  1 disk 50,000 iops
r1  2 disk 108,000 iops
r10 4 disk 160,000 iops

получается r1 дает двукратный прирост по сравнениб с базовой скоростю одного диска
а r10 дает трехкратный прирост отгосительно скорости базового диска.
но как видно базовый диск из себя выжимает только половину потенциала. вместо 100k всего 50k


==
также согласно графику из сайта Hp походу без смартпаф у p421 200,000 iops это потолок.
причем внезависимости сколько дисков туда напихать.
а чтобы смартпаф дал дальнейщий рост это надо 6 и более дисков.

при этом все равно непонятно почему всего 50k вместо 100k 
на одном диске.

получается если даже  1 диск выдаст 100,000 хотя дело не вдиске он уже такое может 
выдавать тем не менее сам контроллер его цпу неможет выжать более 200,000 iops без смартпаф.
но смарт паф даст прирост свыше 200,000 только от 6 дисков и выше.


===

для w1, q1 rand read, массив из ssd дисков ничего недает.
скорость будет равна такую которую выдает один диск.

===

в целом шас получается что массив из одного диска выжимает на rand read 50,000 iops
а должен 100,000 контрроллер hp

массив из 2-х дисков r1 выжимает 88,000 iops,контроллер hp
из 4-ч дисков r10 выжимает 160,000 iops, контрролллер hp
из 10-и дисков r01 контролер lsi выжимает 120,000-140,000 


===

про sas\sata
порт контроллера имеет несколько линий.
одна линия в итоге идет к одному диску.
обычно порт поддерживает 4 линии.
в зависимости от версии sas\sata которую поеддероживает 
клонтроллер скорость линии может быть от 3gb\s до 12Gb\s

контроллер smart array p420i
он имеет 2 дырки mini-sas  каждая из которых дает 4 линии sata.
причем sata той версии который 3Gb\s (300MB\s полезной инфо).
300MB\s = 76,000 iops по 4kB - можно снять с одного sata порта
получается на этом контроллере из за старой версии sata никак не снять 98,000 iops
с диска evo 840\850 из за самого контроллера.

получается в теории с одного мини-сас порта от sata 300 дисков можно 
снять 76,000 х 4 = 304,000 iops
а с двух мини-сас портов теоретически можно снять 600,000 iops

полуается два мини сам порта в теории могут дать 600,000 iops для sata 3Gb/s дисков

что касается срупута то это 300Mb*8 = 2,400 MB\s = 2,4GB\s
нашел тест на практике человек с 4 дисков r0 выжао 2,0 GB\s на seq read
непонятно как это можно получить если скорость порта с диска 300MB\s тоесть 
выше 1.2GB в сек скорость недолжна быть. склорей всего это из за read ahead 
при seq read. и эта скорость получается из за предчтения и то что мы как клиент
читаем через pcie из кэша контроллера который имеет скорость 
в завимости от вида от примерно 6GB\s то 10,6Gb\s
важно понимать что при seq read размер иопса явно не 4kb поэтому
на 2Gb\s seq read число иопс будет невелико на самом деле.



значит контроллер сидит на pciE v3 x 8 = 1GB\s * 8 = 8GB\s
тоесть срупут неупирается в скорсть шины.
как известно из видео от hp этот контроллер без смарт паф из себя выжимает 
максимум 200,000 iops
получается из за того что он понимает только sata 300 то в теории 
с диска который может дать 98,000 iops он может макс снять в теории 74,000 iops
 на форуме hp нашел инфо со ссылкой яобы на доку от hp что p420i
 выжимает из себя максимум 200,000iops получается что если мы исопльзуем диск который 
 дает 74,000 iops то потолк контроллера будет достигнут на 4 дисках r10.
 а p410 максимум 50,000 iops




контроллер smart array p440ar
ar ксати означает что он вставлен в спец слот - Flexible Smart Array and Smart Host Bus Adapter slot
у него тоже 2 дырки минисас. но он уже поддерживает sata 3 который 6Gb\s на дырку
это 600МБ\с полезнрой нагрузки. если в iops то это 150,000 iops
он поддерживает 8 линий. по 4 линии на дырку.


получается в теоррии он может через эти два мнини сас порта 
засосать 1200,000 iops через sata 600 диски
у него теже pcieV3x8 шина = 8GB\s
срупут через 8 sata 600 дисков составит 4,8GB\s тоесть вшину pcie мы неуираемся

что инетресно что ест p440 без ar тоесть в обычный порт pci-e и у него 
всего 1 физ порт но он тоже поддерживает 8 линий.  как это возможно? ведь 
обычно sas порт поддерживает только 4 линии. просто скорость линии может 
быть от 3Gb\s до 12gb\s но на число дисков поддезиваемых это не влиет
их 4 штуки макс на порт. а тут 1 порт а контроллер имеет 8 линий.
разгадка в том что на контроллере используется толи особоый то ли самый
современный порт sff-8654 , на контролллере он имеет тип double-wide 
поэтому этот порт поддерживает 8 линий. как и контроллер.
на другой стороне провода от порта может быть 8654 single-wide на 4 линии,
тогда их будет два. или там будет такой же sff-8654 double-wide.
все зависит какие разбемы имеет беклейн proliant сервера.
вобщем разгадка как это sas порт контроллера может поддерживаеть 8 линий 
в том что испольщутся спец порт sff-8654 doubdle wide.





контроллер P840 
у него 2 физ порта. 
но поддеживает он 16 устройства а не 8 , потому что у него обпятьже порт 
особоый sff-8654 double wide.

sata диски он понимает которые sata 600
в отличие от  p440контроллер он поддежривает не 8 линий а 16. тоесть
столько дисков напрямую можнок нему подключить.
непонятно какой смысл от скорости его кэша 14GB\s если скорость шины piev3x8
все равно максимум 8GB\s я так понимаю эти 14GB\s имеют токазначение 
между передачей данных между массивом и кэшем. но не между кэшем и пользователем.


получается когда мы смотрим на контроллер над смотеть 
сколько линий sas\sata у него внутри тоетсь сколько дтиско напрямую
к нему можно подключить в теории. какая скорость линии для sas и sata.
и также надо смотреть а как эти линии входят в бекплейн. сколько в бекплейне дырок
и сколько линий дает бекплейн на сколько дырок.
условно говоря в бекпдейне 16 дисков. и две дырки. и каждая дырка имеет 4 линии.
значит гавно. потому что 8 дисков сидят на 4 линиях. значит походршему
только 8 линий\8 дисков мы можем использовать. 


что интересно ввот в этот документе - https://impuls-it.ru/upload/iblock/e25/4AA5-4526ENW.pdf

указано что p420i может на 4k rand read выжать 400,000 iops
добились они этого на 8 sas ssd дисках на raid0. + smart path
у него 8 линий
он поддерживает SAS 6Gb/s (600MB\s, 150,000 iops 4k) на линию.
ну что в теории 8 линии sas конечно позволяют ему выжимать 400,000iops 
разница sas и sata линий на этом контроллере в том что сата 
линия в 2 раза более узкая. так как ssd диски обычно могут из себя выжать 100k
непонятно почему он 800,000 iops не выжал. ведь смарт паф типа полностью 
или почти полностью неюзает цпу на контроллере котоырй узкое место.
ну наверно так как сата в 2 раза медленее то на sata ssd все таки 
даже на 8 дисков + смарт паф этот контроллер более 200,000 iops невыжмет.

а про P440ar на 8 дисков sas ssd raid0 + смарт паф они выжали 930,000 iops
на 4k rand read.


интерсно 420i на четырех дискха показал бы 400,000 или только на восьми дисках 
он это мжоет сделать

также интересно 440-ой он на sata дисках может показат 800,000iops или только
на sas дисках.



===
получтся какой бы контроллер ни был
4 диска на один его порт можно вешать свободно. у порта точно должно быть 
минимум 4 линии. тоесть порт небудет являтся узким местом.
узким место будет цпу контрроллера.
ну а чтобы активироавт смарт паф надо 8 дисков миниум, тоесть два порта задействовать
контроллера

ну или если контроллер поддерживает 8 дисков на порт то хватит и одного порта
контролллера
===

чтобы снять с диска всю заложенную в него скорость это зависит от :
 - модели контроллера
- модели бекплейна на сервере
- как диски распределены по бекплейну
- числа дисков в рейде
- вид рейда

===
еще раз прикол в том что для нагрузки воркер=1 q=1 массив из ssd бесполезен
ничего недает. скорьст будет ровнго такая же как с одного диска 5000=10000 iops
==

получается если есть сервер и  у него сколько дырок под диски.
то надо уточнить а сколько портов сзади есть на беклейне. потому что надо 
чтобы к каждому диску с бекплейна подходила своя линия.
если мы неберем диковинные порты такие как double-wide sff-8654
то обычно один порт (разьем) имеет 4 линии. поэтому на каждые 4 дырки под 
диски на бекплейне сервера должны быть дырка.
на g9 там корзинки вставляются внутрь сервера на 8 дисков. и на эту корзинку
идет на жопе бекпелен с двумя разьемами. тут все хорошо. 8 дисков на два раьзема
это 4 диск на разьем. один разьем как раз имеет 4 линии. все хорошо.
если число дырок под диски соотвестввует числу портов на бекплейне сервера
то дальше надо чтобы уже на контроллере был соотвствубщее число портов
ну или линий. тогда унас в связке диск-порт бекплейна-порт контроллера
устанавлияется правильная связь 1 к 1.
далее если у нас мы в рейд массив хотим собрать 6 дисков и меньше то тогда
смарт паф отключен и поток iops идет через цпу контроллера значит 
этот цпу дролжен иметь достаточную скорость чтобы тянут все это число ioips
которые могут из себя диски выжать. например 6 дисков в raid0 могут дать 540,000iops
значит цпу контроллера должен уметь их держать.
а если дисков более 6 то дальше цпу уже несправляется и нужно включать смартпаф(правда
приэтом скрость записи падает) чтобы выбрать все эти iops которые могут изсебя выжать диски.

тонкий момент в том что если в сервер много дырок под диски а на его бекплейне 
портов мало то хрень ты из этого набора дисков выжмешь много iops
и на контроллере тоже должно быт нужное число портов. и скорость цпу тоже.

также прикол втом что у hp смартпаф активен он ил нет можно проверимть легко
а у lsi активен ли fastpath или нет хрен проверишь. отстой.

====

