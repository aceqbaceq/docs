здесь будет нетолько про фланнель но также и про сети куба вообще.


--
 
 
 

  BRIDGE
  
 


прежде всего надо понять как работает сеть передача данных.


вначале у нас есть два компа. в каждо сетевая карта.
мы их соебиняем проводом.
таким макаром они могу общаться друг с другом.
но как они это делают с точки зрения компа у которого порт.
по дефолту порт компа имеет MAC адрес. и порт компа по дефолту приимает только 
фреймы предназначенные его MAC адресу. а которые не ему предназначены комп дискардит.
также по дфеолту порт компа отпрлавляет фреймы только от своего имени тоесть src mac 
только от mac порта. таким образом комп отправляет фреймы только от своего имени
и принимает фреймы предназначены только для своего mac.



потом мы хотими содеиниить три компа. и тут нужно уже доп устройтво - это хаб.
хаб это рипитер. у него нет мозгов. он раобтает на уровне L1. тоесть он принимает 
сигнал (сигал а не фрейм) и повторяет его на всех портах кроме того на котором получил
сигнал. все порты хаба образуют collision domain. это значит что если два компа
одновременно посылают фрейм то в хабе это все складывается и на его портах сложенный
сигнал выглядит в форме мусора. вобщем золотое правило - все порты хаба входят в один
колижн домен. все порты свича, бриджа, рутера образуют каждый свой отдельный колижн домен.


еще раз как работает хаб - он работает ниже L2 он енработает с фреймомами.он незнает
что такое mac адреса. он работает чисто на уровне электричества. 
тут важно заметить что порты на  хабе они они прозрачные. у них нет ни мак адресов нихера.
они  просто  как провода.

далее переходим к свичам и бриджам. это одно и тоже. просто свичи имеют больше портов
и ообрабатывают поток через спец asic а бридж имеет мало портов и обрабатывает поток
через мнее мощную хренотень. бридж\свич работает на уоровне L2 тоесть уже с mac адресами.
тут также важно отметиь что порты брижда\свича неимеют своиъ собственных мак адресов
и они принимают аобсолютно все фреймы со всеми маками. и отсылают они фреймы со всеми 
маками. таким макаром порты брилжа свича абсолютно ннвидимы с точки зрения L2. 
тоесть с компа нельзя никак обратиться на порт свича. раз у него нет адреса mac то к нему
никак необаратиться в сети. тоесть порты по сути это как провода не более того.
это я все расписывал непросто так. если мы влинуксе создаем виртуальный бридж который по мне 
лучше было бы назвать вирутальнй свич (ибо слово бридж вызывает только недоумение что за хрень брииждж)
то все порты которые мы включаем в состав бриджа сразу претерпервают огромное изменение - 
1)  порт теряет свой мак. он вообще перестает иметь свой мак. 
2) порт начинает работать в promiscious режиме тоесть порт прнимает абсолютно все фреймы с любыми маками.

в отличие от хаба виртуальный свич\бридж получив с  порта некоторый фрейм выплюнет его только в тот 
порт за которым тот мак находится ( при условии что он уже есть  в таблице), а вот если 
мака нет в таблице тогда вирт свич\бридж выплюнет его во все свои порты. 


Отсюда вытаекают супер важные свойства портов входящих в состав виртуальнго свича\бриджа в линуксе
и его работы с фреймами(повторюсь):
1) если мы добавляем порт в состав бриджа то этот порт нетолко теряет свой IP но и теряет свой мак.
больше этот порт нельзя найти в сети обратиттся в сети к этому порту по МАК. ибо ни IP ни мака у этого 
порта больше нет. ты скажешь подожди! у линукс вирт свича можно
порту назначть IP адрес. например известный порт docker0 который
входит в состав свича. он иммееть IP! об этом моменте пойдет
речь дальше. разгадка будет дана.
2) порт переходит в промискууус мооде. тоесть принимает фреймы со всеми маками. также порт отправляет в сеть
фреймы с любыми маками

3) если в вирт свич влетел фрейм и его мака нет в таблице то свич\бридж его выплюнет через все порты
 а вот если мак есть то он его вылюпнет только через тот порт за которым этот мак сидит. (то есть поведение обыкноченного свича)

4) то что бридж назвали бриджом по мне это плохо потому что никто непомнит что такое бридж и вччем разница
его со свичом. поэтому было бы гораздо прозрачнее было бы назвать вирутальный свич !!! 


интерсесно что хаб рабоатет на уровне L1 и неообаратывает поток на уровне mac и  у его портов нет 
своих мак адресов и это выгядит логично. свич и бридж обраывают поток на уровне мак адресов уровне L2 
но их порты тоже неимеют мак адресов!!! и это выгляди супер нелогично!!!  роутер обратбывает поток на уровне L3 ip адресов и его порты имеют и мак и IP. и это выглядит супер логично! 
вопрос как назвать устройство которое имеет порты котоыре имеют mac адреса. это точно не роутер 
но и не свич\бридж. так что же эт за устроство ?? еще раз свич и бридж своих мак адресов неимеют то есть 
всети они остуствуют невидимы однако обрабатыват поток на уровне mac адресов. разве это не чудо?

что реально важно сразу понять что порты которые мы видим в составе виртуального свича\бриджа в линуксе
это то что это порты без адреса они неимеют ни ip ни mac как это мы привыкли. в отличие от станадртных
портов к котоырм мы привыклив компе. до них ельзя достучаться. 

вот пример

$ brctl show docker0

bridge name                  interfaces

docker0                     veth11d3aa5
                            veth65db7f9
                            veth67fd6bd
                            veth769bafa
                            veth84fbf45
                            vethaaa708f
                            vethb535220



тоесть порты veth* это чисто порты без адресов. пустышки. до них недосутчаться. у них нет адресов.
ни мак ни IP. поэтому на них нам настрать. это число дырки на свчие. и поэтмоу на самом деле 
нам надо узнать какие реальные порты другие воткнуты в эти дырки. порты с адресами

тоесть сами порты бриджа для нас неимеют значеия. они пустышки. имеют згачение тоько порты 
которые к этим поключюены тоесть


veth11d3aa5 <--------> eth5
veth65db7f9 <--------> eth6

таким образом eth5 и eth6 имеют связь через бриджервые порты.




свич получается нужен чтобы в него воткнуть сет карты  от контенеров котоыре сидят в своих 
неймспеййсах отрезанные от внешнего мира. бридж сводит эти порты вместе в один неймспейс.


как выглдяит вся эта хрень с вирт свичем. например это docker0
так вот docker0 это не порт на свиче, это порт на компе который
воткнут в порт на свиче (который в свою очередь не имеет ни мак
ни IP во как!)



                         
комп  порт docker0  <--> порт вирт свич без названия
      (L3 172.17.0.1)    порт вирт свич veth1<----->eth2
                         порт вирт свич veth3<----->eth3 (172.17.0.6, )
                         порт вирт свич veth2<----->eth1 (172.17.0.10, 02:42:ac:11:00:0a)


значком <----> обозначен виртуальный провод UTP

когда мы с контейнера с eth1 пингуем внешний мир 8.8.8.8 то фрейм
влетает в вирт свич через veth2 дырку(порт)
но фрейм этого не чуствует. потом фрейм пролетает через свич в порт
свича безвания оттуда по проводу в порт  docker0 а из него во 
внешний мир, хотя если мы пингуем сам порт docker0 то пакет
влетает в порт docker0 и в ядро. и там его путь заканчивается.
вот этот свич это для ядра выглядит как абсолютно внешний свич.
тоесть как бутто у компа есть реальная физ карта docker0 она
смотрит в физ сеть. точнее провод из порта docker0 воткнуть в некий
физ свич. абсолютно рельный внешний физ свич. 
далее вот эксперимент,  
имеем два контейнера с их мак и ip 

# docker container exec -it bb5 ip a sh dev eth0
51: eth0@if52:  
    link/ether 02:42:ac:11:00:03 
    inet 172.17.0.3/16 brd 172.17.255.255 

# docker container exec -it bb1 ip a sh dev eth0
14: eth0@if15:  
    link/ether 02:42:ac:11:00:02 
    inet 172.17.0.2/16 brd 172.17.255.255 

они тоже воткнуты в этот физ свич. контейнеры это как бы внешние
компы. через провод они воткнуты в этот физ свич. в порты которые
не имеют ни мак ни ip (veth поррты)

имеем порт docker0, именно в этот порт нашего компа влетают 
фреймы от контейнеров через свич,

# ip -c a sh dev docker0
3: docker0:  
    link/ether 02:42:dd:d0:1a:24 
    inet 172.17.0.1/16 brd 172.17.255.255 


далее я пингую с конетейнеров 172.17.0.1 тоесть наш порт docker0,
тоесть фрейм вылетает с контейнера имеет мак и IP карточки контейнера,
влетает в свич. свич никоим образом никогда не трогает оболочку фрейма
это значит что src mac и dst мак осатется прежним. далее свич
плюет этот фрейм в тот порт за которым сидит порт docker0
и наш порт docker0 принимает этот фрейм. тоесть эта схема 
когда два компа в сети пингуют друг друга через свич.
что при этом СУПЕР важно понять что из за этого в порт docker0
фрейм от контейнера влетает без модификаций и изменеий. поэтому 
далее он идет уже в ядро. и это значит что наш комп "видит" мак 
адрес сет карточки контейнера. показываю
 
мак таблица на нашем хосте

# arp -ne | grep  -E "Address|172.17.0"
Address                  HWtype  HWaddress           Flags Mask            Iface
172.17.0.2               ether   02:42:ac:11:00:02   C                     docker0
172.17.0.3               ether   02:42:ac:11:00:03   C                     docker0


как видно мак адреса полностью совпдадают с мак адресами карточек
внутри контенеров.
гениально.

что еще на мой взгляд надо понять это то что порт docker0 который
называют свичевым на мой взгляд это не так. на мой взгляд сам
свич на невиден никак. порт docker0 это доп сет карта которую 
ядро нам автоматом добалвяет на комп. она L3. а уже за ней
где то там во тьме ядра сидит невидимый свич. на котором нет ниаких
L3 портов. там все порты L2. тоест я говорю ситацияи такая у нас
физ карта L3 docker0. а от нее по проводу далее в физ сети вне
компа висит на стене L2 свич. к нашему порту docker0 он не имеет
отношения. он висит на стене а порт docker0 воткнут в наш комп. 
и только они соединенеы через провод. а нам пропаривают что порт 
docker0 находится где то там именно внутри свича. по мне это 
брехня полная.

далее я в iptables увидел вот такие интересные правила

# iptables-save | grep POSTROUTING 
-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

где 172.17.0.0\24 и 172.18.0.0\24 это сети двух докер сетей бриджей
за которыми сидят контенеры, 
так вот эти правила iptables говорят о том что если мы с контейнера
пингуем какую нибудь карту хоста то приложение которое забиндено
на эту карту хоста будет видеть истинный ip адрес контейнера. а если
контейнер пингует чтото находящееся за пределами карточек хоста 
то наш комп выплевывая сет пакет исходящий из контейнера во внешнюю
сеть сделает маскарадинг а имено комп заменим src_IP контйенера
на IP сетевой карты хоста через которую он этот пакет плюет дальше.

пример. есть сет карта контейнера eth10 котоаря имеет IP=171.17.0.2
она идет в вирт свич. далее вирт свич связан с хостом через порт
docker0 IP=172.17.0.1 , если мы с контейнера пингуем 172.17.0.1 
то tcpdump -i docker0 покажет что src_IP=172.17.0.2 мы видим IP нашего
контейнера.
далее на хосте есть еще одна карточка fa1=192.168.0.1 , мы с контейнера
пингуем ее. путь пакета выглядит так

[ eth10        ]---------[(свич)]------------[ docker0 ------fa1  ]
[сет стек  конт]         [ядро  ]            [ сетевой стек хоста ]
 
соотвесвтенно вот это правило на хосте компа
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
не выполняется нигде. поэтому в fa1 пакет прилетает с неизменным
src_IP. значит tcpdump -i fa1 покажет что src_IP=172.17.0.2
тоесть мы там увидим честный ip адрес контейнера. 

тоесть пингуя с контейнера любую сет карту сетевого стека хоста
наш src_IP будет оставаться нетронутым. пакеты от контейнера
до любой сет карты хоста будут плавать неизмеенными в плане IP


а вот если мы заходим попингвать из контйенера интернет скажем 8.8.8.8
(положим что интернет находится за карточкой fa1) то на вылете из
fa1 iptales заменит src_IP контейнера на IP(fa1) потому что у нас
выходная картчока будет -o fa1 что подпадает под правило

-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

понятно!

и тут возникает следущий вопрос - а как  будет если из контйенера
который сидит за одним свичом мы пингуем контйенер который сидит
за другим свичем? имеется ввиду когда пакет долетит до конечного
контенера у него заменится src_IP или нет? ответ - ЗАМЕНИТСЯ конечно.
показываю.  но прежде я хочу сказать очень важную вещь:

по дефолту связь между контенерами сидящими в разных докер 
бриж сетях запрещена. она не работает.
когда
мы создаем докер сеть типа бридж то докер сразу в сет стеке хоста
добавляет правила в iptables, одно из этих правил запрещает 
прохождение пакетов из одной докер бридж сети в другую докер 
бридж сеть. поэтому если мы хотим это дело разрешить то
нам нужно удалить вот такие вот правила 

-A DOCKER-ISOLATION-STAGE-2 -o br-81e0d9ff7558 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP

если их неудалить доступ с контенера сидящего в обной бридж сети
в другую бридж сеть работать не будет!

опять же надо понимать что удаление правил работат ровно до
следущего перезапуска докер демона. и эти правила сразу будут 
восстановлены.
итак окей мы руками эти правила удалили. теерь связь должна 
работать,
показываю как она будет работать:


у нас есть два свича. с сет стека хоста туда ведет 
две карточки docker0 и  br-81e0d9ff7558

 
22: br-81e0d9ff7558:  
    link/ether 02:42:78:27:39:f1 
    inet 172.18.0.1/16 


3: docker0:  
    link/ether 02:42:dd:d0:1a:24 
    inet 172.17.0.1/16 


тоесть за кажодй из этих карточек сидит вирт свич. за которым 
сидят контейнеры. docker0 и br-* это сет карточки сет стека хоста.
на сет стеке хоста определны правила iptables

-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

словами можно эти правила так описать - если покет стартанул
начал свою жизнь откуда то изнутри с той стороны свича который
прикрывает сеть 172.18.0.0\16 и этот пакет щас вылетает из любой
карты сетевого стека хоста кроме той которая смотрит непосред
ственно в этот свич то заменить IP
аналогично для 172.17.0.0\16
такие правила в частности дают то что если пакет вылетел из за
свича с сетью 172.18.0.0

кстати тут мне пришла очень хитрая мысль на счет вот этой опции -o
в iptables. -o означает что пакет вылетает из карты куда то там 
наружу. а что значит он вылетает наружу? а я вам отвечу. это значит
что он вылетает в какойто там другой сетевой неймспейс. ведь
схема на самом деле вот такая

  [ процесс --- ядро ----eth1]----[ eth10       ]
                       |-eth2]
                       |-fa1 ]
                       |-fa2 ]

  [ текущий сетевой стек     ]----[ другой стек ]


тоесть процессу в его сет неймспейсе через ядро доступны
такие то сетевые карточки(eth1, eth2,fa1,fa2)
если пакет влетает в карточки eth1,eth2,fa1,fa2 влетает из 
другого неймспейса то это отвечают опции -i в правилах iptables
если же пакет из текущего неймспейса вылетает в другой 
неймспейс через карточку то это ответчает в iptables
правилу -o
казалось бы а что непонятно? а я щас покажу. вот такую картину
рассмотрим пример когда
пакет летит из контейнера сидящим за одним свичом
в контейнер который сидит за другим свичем,
порты docker0 и br-81 лежат оба в сет стеке хоста. 

         -<-docker0]--<---(свич1)--<---[ контейнер1 eth1  ]
         |
         ->-br-81  ]--<---(свич2)--<---[ контейнер2 eth1  ]


здесь еще раз важно подчеркнуть что и порт docker0 и порт br-81
лежат в одном и томже сет стеке хоста. 
и здесь приходит очень важный вопрос после того как пакет влетел
в порт docker0 и попал в ядро и потом ядро его сует в порт br-81
какие в этом случае применяются правила iptables ?
тут я еще чуть отойду в сторону - есть еще важный вопрос , если я
отслеживаю поток через tcpdump то вопрос эта программа рисует 
нам поток до того как к нему будет примененен netfilter или после?
это пиздец. я нашел ответ (https://superuser.com/questions/925286/does-tcpdump-bypass-iptables)

Wire -> NIC -> tcpdump -> netfilter/iptables

netfilter/iptables -> tcpdump -> NIC -> Wire

тоесть если поток влетает в карту из провода то tcpdump видит поток
до того как его обработает netfilter, а если мы говорим о трафике
который вылетает из сет карты то tcpdump видит только тот поток
который уже отфильтрован netfilter. это пиздец. это очень
пригодится далее.

итак я возвращаюсь к вопросу - какие правила iptables применяются
к пакету который влетел в порт docker0 и потом вылетает через 
порт br-81. (картинка нетфильтер тут https://www.frozentux.net/iptables-tutorial/chunkyhtml/c962.html)
тут важно сказать что очень важно пакет был порожден
ли на самом хосте и он вылетает куда то. либо он влетает снаружи
из провода в карту и предназначен самому хосту либо это пакет
который влетел снаружи из провода и непредназначен хосту и 
будет вылетать из другой какойто карты. так вот в нашем конкретном
случае пакет влетел снаружи из провода в порт docker0
далее этот пакет непредназначен для хоста он транзитный и его 
нужно выплюнуть из другой карты наружу в провод. так вот 
когда пакет только влетел в docker0 то пакет начинает обрабатываться
в таблице MANGLE цепочка PREROUTING , далее в таблице NAT цепочка
PREROUTING. на этом этапе есть только понятие "порт из которого
пакет прилетел" тоесть правила могут содержать только параметр -i
и не может содержать параметр -o потому что в данных цепочках 
ни о какой "выходной" карте сетевой и речи. после прохождения
этих цепочек комп понимает что пакет транзитный , ядро принимает
решение через какой порт он выплюнет карту наружу тоесть в этот 
момент комп уже знает чему равно -o NIC, ядро знает какая выходная карта,
далее ядро направляет пакет в цепочки таблицы MANGLE и FILTER 
в цепочки FORWARD. и вот в правилах этих цепочек у нас уже могут
присуствоввать правила для -o карты. потому что она уже известна компу,
он  знает через какую карту он будет плевать пакет.  далее
почеуто то опять комп начинает ссмотреть в таблицу маршрутизации
и еще раз принимать рещение через какой порт он будет высирать 
пакет. дело в том что таблица FILTER цепочка FORWARD там 
src_IP и dst_IP никак не меняются. может это нетак для таблицы
MANGLE цепочки FORWARD? незнаю.
далее пакет идет в таблицы MANGLE и NAT цепочка POSTROUTING.
на этом этапе комп уже 100% знает через какой порт он будет высирать
пакет и в этих цепочках можно применять опцию -o потому что выходная
карта известна. 
теперь я возвращаюсь к нашему случаю еще более конкретно.
итак пакет влетел в порт docker0 и вылетать из порта br-81
и  у нас в iptables в сет стеке хоста заданы правила

-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

значит так как наш пакет влетел со стороны порта docker0
значит src_IP=172.17.0.0\16 значит вот это правило уже точно
неработает
-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE

тогда смотрим на это правило
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

вот это условие выполняется -s 172.17.0.0/16
а вот это условие " ! -o docker0 " ? да оно выполняется. 
потому что наш пакет вылетает в провод (или говоря по другому в
другой сетевой неймспейс ) через порт br-81, и
    br-81 = ! -o docker0
поэтому пакет когда вылетит из порт br-81 то его src_IP будет
ИЗМЕНЕН и будет равен IP(br-81)

а вот еще рассмотрим сразу же такой случай. мы сидим на процессе
который сидит в сетевом неймспейсе хоста. и мы с этого процесса 
пингуем контейнер сидящий за свичем сети 172.17.0.0\16,
на картинке (картинка нетфильтер тут https://www.frozentux.net/iptables-tutorial/chunkyhtml/c962.html)
это начинается с "local process" далее цепочки OUTPUT и 
все в итоге опять заканчиается на цепочках POSTROUTING
и на правиле 
  -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
совершенно понятно что когда мы будем пиновать контейнер с
dst_IP=172.17.0.4 то конечно процесс пихнет этот пакет через карту 
docker0 и src_IP=172.17.0.1 , это будет выходная карта -o docker0
ты спросишь а какая при этом -i карта? ответ никакая. когда 
процесс сидящий в сет неймспесе хоста чтото куда то шлет то 
никакой -i карты нет. итак мы имеем
      src_IP=172.17.0.1
и     -o docker0

значит вывод что правило
  -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
оно не выполняется так как условия не выполняются. 

так а еще пример. положим мы с контейнра 172.17.0.4 который сидит
за свичем пингуем IP=172.18.0.1 который сидит на порту br-81
мы имеем картинку

         -<-docker0]--<---(свич1)--<---[ контейнер1 eth1  ]
         |
         ->-br-81  ]

возникает вопрос если я хочу запретить ping на порт br-81 (IP=172.18.0.1)
при такой ситауции. это можно сделать вот таким правилом
            -A INPUT -d 172.18.0.1/32 -j DROP
и действиельно это помогло. и статистика iptables это 
подтвердила. (молчу что и пинг перестал работать)

# iptables -t filter -L -v -n
Chain INPUT (policy ACCEPT 16 packets, 1428 bytes)
 pkts bytes target     prot opt in     out    source      destination         
    3   252 DROP       all  --  *      *      0.0.0.0/0   172.18.0.1          


спрашивается как рассуждал комп когда пакет влетел в порт docker0
и попал в ядро из провода? я откидываю таблицу MANGLE.  у меня нет
цепочек в ней. тогда у нас пакет идет в обработку NAT(PREROUTING) там 
у меня пусто, потом пакет идет в Routing Decision , там ядро понимает
что этот пакет предназначен для сетевого стека этого же хоста
тоесть это не транзитный пакет, это пакет предназначенный для 
процесса который привязан к сетевому неймспейсу этого хоста,
кстати ты спросишь а на пинги какой процесс ответчает? отвечаю - отве
чает само ядро. никакого процесса юзерского для этого нет. отвечает
само ядро. и вот далее пакет идет в цепочку FILTER(INPUT)
      -A INPUT -d 172.18.0.1/32 -j DROP
и тут этот пакет грохается.
вопрос а есть ли какой то -o  порт в случае если пакет предназначен
для LOCAL? ответ - нет. выходной карты никакой нет. у нас пакет 
предназначен для того чтобы остаться в ядре. нет никакой выходной карты.
далее.положим что у нас нет запрещающего правила
            -A INPUT -d 172.18.0.1/32 -j DROP
и мы не пинги делаем а делаем какойто tcp запрос на этот 
же IP=172.18.0.1  и порт 46 . и так пакет вошел в ядро из порта docker0
попал в ядро и ядро поняло что пакет предназначен для сет карты
br-81 для ее порта 46. и тогда как я понимаю некий локальный процесс
имеет у себя открытый файловый дескриптор /pric/$PID/fd/12 который 
ведет в ядро на сокет 172.18.0.1:46  
            /proc/$PID/fd/12 ---> socket:[1231312]
где socket:[1231312] это и есть ядерный сетевой сокет который
отвечает за 172.18.0.1:46 
и вот ядро кидает этот пакет в socket:[1231312] и тогда процесс
считывает это из /proc/$PID/fd/12 БАМ! локаьный процесс получил
в себя внутрь пакет из ядра который его получил из сети из порта 
docker0.

я все так распинался чтобы внести ясность о том когда можно
говорить о так называемой -i сетевой карте а когда о -o 
сетевой карте. как их определить. если пакет влетел в одну из с
сетевых карт стека а предназначен другой карте это что какойто
форвард между ними происходит? типа вот пакет влетел в карту 
docker0 и он является -i картой. а потом он как бы из этой карты
вылетает становится -o картой а br-81 становится -i картой ?
оказалось совсем нет. так как карты docker0 и br-81 сидят в одном
сетевом стеке то все работает нетак. снаружи с другого сетевого
стека (по проводу) пакет влетает в порт docker0 и этот порт 
является -i портом. и попадает в ядро. далее ядро понимает что 
пакет предназначен этому же сетевому стеку только для другой карты.
отлично. больше никаких изменений в плане -i или -o карт 
не будет. у нас у пакета есть -i docker0 а -o ее просто нет. потому
что -o карта по определению это такая карта из которой наш пакет
должен быть выплюнут в другой сетевой стек. а у нас этого нет.
у нас пакет предназначен для этого же сетвого стека. далее
ядро просто ищет у себя в своей памяти тот буфер который ведет
к файловому дескриптору который забинден для этой сетевой карты
br-81 и тому порту что указан в пакете. и ядро сует туда этот пакет,
а процесс черз файлоый дескприор считывает этот пакет. все.




возвращаюсь к вопросу пинг из контейнера1 направлен в контейнер2
что происходит в районе портов docker0 и br-81
    ||               ||
    ||               ||
    ||   -<-docker0]-||-<---(свич1)--<---[ контейнер1 eth1  ]
    ||   |           ||
    ||   ->-br-81  ]-||->---(свич2)--<---[ контейнер2 eth1  ]
    ||               ||
    ||               ||

я двойной палкой отделил карточки которые находятся в сетевом
стеке хоста. тоесть docker0 и br-81 оба находятся в сетевом
стеке хоста. остальные карточки и прочие хрени лежат вне
сетевого стека хоста.

значит iptables меняет src_IP dst_IP только в цепочках POSTROUTING
и PREROUTING. также в этих строка содержаться обязательно слова
SNAT, DNAT, MASQUERADE
так вот я посмотрел какие правила в итоге докер прописывает
в таблице. и это по факту всего два правила

-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE

c контейнера 172.17.0.2 пинг летит в контейнер  172.18.0.2
пролетает через свич и влетает в порт docker0 и в ядро.
Далее если мы натравим на docker0 программу tcpdump то (мы уже знаем
почему) она покажет входящий поток в порт до того как он попадет
в netfilter. поэтому мы увидим src_IP=172.17.0.2 dst_IP=172.18.0.2
так будет написано для echo-request.
проверяю:
      # tcpdump icmp -n -p -i docker0
      IP 172.17.0.2 > 172.18.0.2: ICMP echo request
тоесть да все подтвердилось

значит пакет влетел в ядро в сет стек хоста через порт docker0,
далее ядро понимает что будет выплевывать пакет через порт br-81,
перед выплевыванием сработает правило
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
потому что src_IP=172.17.0.2 и -o=br-81 = ! -o docker0
значит на выходе из карты пакет будет иметь
src_IP=IP(br-81)=172.18.0.1   dst_IP=172.18.0.2
если мы натравим tcpdump на порт br-81 то ( и мы знаем почему теперь)
он покажет поток как раз уже после того как его обработает netfilter
тоесть мы должны увидеть для echo-request (тоесть пакет летящий туда,
тоесть от первого контейнера1 до контейнера2 ) параметра пакета будут
src_IP=IP(br-81)=172.18.0.1   dst_IP=172.18.0.2
проверяю
      # tcpdump icmp -n -p -i br-81e0d9ff7558
      IP 172.18.0.1 > 172.18.0.2: ICMP echo request
совпало. практика совпала с теорией.!
получается в ядро через порт docker0 влетает пакет
      src_IP=172.17.0.2   dst_IP=172.18.0.2
далее ядро делает подмену 
и уже из порта br-81 выходит пакет с подменой
      src_IP=172.18.0.1   dst_IP=172.18.0.2

тоесть с пакетом в ядре происходит трансформация
src_IP=172.17.0.2-->(MASQUERADE)-->172.18.0.1  dst_IP=172.18.0.2

и получается внутрь контейнера прилетает пакет вида
src_IP=172.18.0.1  dst_IP=172.18.0.2
почему это важно. щас покажу. значит сетевой стек ядро в контейнере
формирует обратный пакет меняя местами src и dst и посылает
обратно ответный пакет вида

src_IP=172.18.0.2  dst_IP=172.18.0.1

А ДАЛЕЕ ПРОИСХОДИТ НЕЧТО ОЧЕНЬ ВАЖНОЕ. ОБ ЭТОМ Я НЕ ЗНАЛ РАНЬШЕ:
насколько я понял дело обостоит так - вот у нас летит "туда" первый
пакет. он на соотвествующей карточке проходит обработку PREROUTING
и на соотвествующей другой карточке проходит POSTROUTING обработку.
следущий пакеты которые летят "туда" они уже в эти цепочки не суются,
они уже обрабатываются согласно тем правилам NAT которые были примене
ны к первому пакету который летел "туда". это экономит цпу наверное
в том плане что ненужно для каждого пакета анализировать какому
конкретному правилу PREROUTING и POSTROUTING оно относится. делается
это силами модуля conntrack который запоминает это "соединение".
это все хорошо но пока нам ничего недает нового. а дальше самое
интересное. совершенно достоверно известно что когда летит обратный 
пакет то мы для него в iptables уже в цепочках PREROUTING и POSTROUTING
доп правила не пишем. обратная трансляция NAT работает "автоматом".
тоесть если мы добавили в iptables правило -SNAT которое меняет
src_IP то обратно прилетевший пакет у него автоматом будет заменен
его dst_IP на lan_IP. так вот важно тут то что обратно прилетевшие 
пакеты оно все доединого непопадают в обрабоку PREROUTING и POSTROUTING
цепочек которые мы написали. iptables их обрабатывает автоматом
делая обратное преобразование автоматом относительно того преобра
зования которое он делает для пакетов летящих туда.
итак обратные пакеты они не попадают в обработку цепочек POST-
ROUTING и PREROUTING таблица iptables. вместо этого iptabes автоматом
делает обратное преобразование с этим пакетами относительно того
преобразования которое он делает с пакетами летящими "туда".
это приводит к одному поразительному эфффекту для пакетов 
летящих "обратно"" если всего этого незнать.


итак пакет уже летит в обратном направлении (echo-reply)
от контейнера2 к контейнеру1

    ||               ||
    ||               ||
    ||   -<-docker0]-||->---(свич1)--<---[ контейнер1 eth1  ]
    ||   |           ||
    ||   ->-br-81  ]-||-<---(свич2)--<---[ контейнер2 eth1  ]
    ||               ||
    ||               ||


обратный пакет вылетает из контейнера2 с  src_IP=172.18.0.2  
и dst_IP=172.18.0.1 
обрати внимание! обратный пакет нацелен 
на IP свича2 а не на 172.17.0.2 IP адрес контейнера1,
действительно когда пакет летящий туда вылетел из br-81  у него
был заменен src_IP на IP(br-81) поэтому обратный пакет от 
контейнера2 нацелен на IP(br-81)! это очень важный момент!
пакет влетает в сет стек хоста в порт br-81, он влетает в ядро
и так как это не просто пакет от нового соединения а это 
"обратный" пакет от соединения которое уже запомнило ядро когда
пакет пролетал "туда",  то iptables это понимает и iptables
помнит что именно при выходе из этой карты br-81 пакеты которые летят 
туда у них подменяется src_IP с 172.17.0.2 на 172.18.0.1
тогда для "обратного" пакета iptables ни в коем случае не применяет
для этого пакета правила PREROUTING из таблицы правил, а вместо
этого iptables тутже меняет обратно в dst_IP адрес в обратном 
порядке как оно это делает для src_IP для пакетов летящих туда. тоесть
 если для пакета туда он делает src_IP 172.17.0.2 --> 172.18.0.1
 то для пакета оттуда он делает dst_IP 172.18.0.1 --> 172.17.0.2
тоесть вмето того PREROUTING который прописан в таблицах и цепочках
он применяет вот такой свой PREROUTING. далее обработка пакета про
ходит по всем остальным таблицам и цепочкам в штатном режиме. 
iptables понимает что этот пакет надо выплюнуть через порт docker0,
и опять же при этом iptables не прогоняет данный пакет через POSTR-
OUTING цепочки. нет! поэтому вот эти правила 

-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE

обратный пакет не касаются, не затронут.
iptables еще раз скажу просто напросто пропускает 
обработку обратных пакетов в цепочках PREROUTING , POSTROUTING
она просто их туда не сует. 
именно поэтому вот этот маскарадинг не происходит

-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE

и поэтому когда пакет покидает порт docker0 у него
src_IP остается 172.18.0.2 а не маскируется в 172.17.0.1

в чем можно убедитья через tcpdump
значит tcpdump 
натравленный на порт br-81 покажет нам поток до того как он будет
отработан netfilter. значит tcpdump должен показать нам
echo-reply src_IP=172.18.0.2  dst_IP=172.18.0.1
проверяю
      # tcpdump icmp -n -p -i br-81e0d9ff7558
      IP 172.18.0.2 > 172.17.0.2: ICMP echo reply
не совпало!
у меня предположение что как бутто tcpdump показал пакет 
уже после его обработки со стороны netfilter! из за этого.

далее пакет обрабатывается  ядром по всем остальным цепочкам 
как положено и ядро
понимает что его надо выплюнуть через  docker0, 
штатные цепочки POSTROUTING игнориуются.

-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE

поэтому маскирования src_IP не произойдет! далее так как
пакет который летел "туда" никакого PREROUTING на порту docker0
не претерпевает значит и "автоматического обратного" преобразования
для обратного пакета небудет никакого. в итоге пакет из порта
docker0 вылетает без преобразований src_IP и dst_IP
что мы и видим из практики 

      # tcpdump icmp -n -p -i docker0
      IP 172.18.0.2 > 172.17.0.2: ICMP echo reply

полное совпадение практики с теорией!

а еще я вот такое обнаружил
      # iptables -t nat -L -n -v
Chain POSTROUTING (policy ACCEPT 3 packets, 714 bytes)
 pkts bytes target     prot opt in   out               source         destination         
    0     0 MASQUERADE  all  --  *   !br-81e0d9ff7558  172.18.0.0/16   0.0.0.0/0           
    9   756 MASQUERADE  all  --  *   !docker0          172.17.0.0/16   0.0.0.0/0          

откуда видно что действительно правило 
-A POSTROUTING -s 172.18.0.0/16 ! -o br-81e0d9ff7558 -j MASQUERADE
несрабатывает. почему я уже описал выше. это удивительно.

в итоге получается очень интересная картина кооторую показывает
tcpdump для потоков туда и обратно на обоих портах


# tcpdump icmp -n  -i docker0
172.17.0.2 > 172.18.0.2: ICMP echo request
172.18.0.2 > 172.17.0.2: ICMP echo reply


# tcpdump icmp -n -i br-81e0d9ff7558
172.18.0.1 > 172.18.0.2: ICMP echo request
172.18.0.2 > 172.17.0.2: ICMP echo reply     <== ??????

во первых это доказывает что для обратного потока таблица
NAT пропускается ядром. но остается под вопросом как работает
tcpdump там где я показал "??????" , надо собрать тестовый 
стенд чтобы понять tcpdump входящий поток показывает до 
обработки его netfilter или после.

я проверил другой пример связи. с контейнера 
я пингую 8.8.8.8

в таблице есть правило

-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

с конейнера поток идем в docker0 на нем конечно ничего не меняется.
далее поток вылетает из enp5s0 на нем то это правило и срабатывает.
и вот что я вижу через tcpdump

# tcpdump icmp -n -i enp5s0
10.113.151.220 > 8.8.8.8: ICMP echo request
8.8.8.8 > 10.113.151.220: ICMP echo reply

тоесть itables меняет src_IP=172.17.0.2-->10.113.151.220
и tcpdump показывает пакет уже после обработки netfilter
обратный пакет netfilter меняет 
dst_IP=10.113.151.220 --> 172.17.0.2 
но tcpdump показывает пакет до обработки netfilter

а вот tcpdump показыает как обратный пакет вылетает из docker0
это значит что он его показыавет уже после обработки netfiter
# tcpdump icmp -n  -i docker0
8.8.8.8 > 172.17.0.2: ICMP echo reply

поскольку на интерфейсе enp5s0 дамп показан до обработки 
нетфильтр а на интерфейсе docker0 дамп показан уже после обрабоки
нетфильтр то 100% невозможно понять в какой момент нетфильр меняет
у обратного пакета dst_IP на первой карте как только он влетел
или на второй перед ее вылетом.
я считаб что подмена в обратном пакете должна проходит сразу на первой
карте. иначе если пакет обратный влетит в ядро в виде

 8.8.8.8 > 10.113.151.220

то это будет выглядеть для ядра что пакет предназначен самому 
хосту. а это неверно. 



надо будет взять сделать бридж. руками. создать другой сет
неймспейс. там карту. и надо будет настроить правило что если 
снаружи в бридж летит пакет то его надо маскировать. 

-A POSTROUTING -o bridge_0 -j MASQUERADE


и попинговать с хоста  тот неймспейс что за бриджем. и посмтреть
что покажет tcpdump. если дамп будет красивый то значит надо
более внимательно просмотреть правила docker в iptables. значит
там какоето хитрое правило. 
чтобы все четко дешифровать надо:
  - четко понимать как работает NAT в iptables
         (как он работает для обратного потока)
  - надо понимать как снимает дамп tcpdump
         (когда он показыает пакет до обработки netfilter
         а когда он показывает пакет уже после обработки netfilter)
  - надо четко понимать что значит -i NIC и -o NIC у iptables
   (что -i это когда трафик втекает в наш сет неймспейс 
    из другого неймспейса,а -o это когда мы плюем трафиком в 
    чужой другой неймспейс )

иначе легко навыдумывать всякой лживой фигни. легко ошибиться.
<==== закончил тут 




==> рассмотереть еще важный вопрос это когда у нас 1 свич
в него воткнуто 2 контейнера из 1 IP сети. они друг друга пингуют
и при этом у нас трафик ловится на порту docker0 . что за хрень?














полет пакета от одного контейнар до другого выгляди так

[eth0]--[свич1]------[docker0---------br-81]---[свич2]-----[eth0]

[конт]               [ сет стек хоста      ]               [конт] 

пакет из сет стека контейнера  летит в свич1 , свич это ядро, 
далее пакет влетает в порт docker0 это сет стек хоста. далее он 
перебрасывается в порт br-81 это сет стек хоста далее пакет летит
в свич2  это ядро далее пакет летит в сет стек контейнера.

так вот в тот момент когда пакет выплевывается из порта br-81 
в свич2 срабатывает правило

-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

действиелно потому что у нас -o br-81 а это = ! -o docker0
и у нас src_IP контейнар заменяется на IP(br-81)
таким образом когда пакет влетает в сет неймспейс конечного контйенера
то его src_IP=IP(br-81) поэтому вунтри конечного контйенера если 
мы там запустим tcpump будет видеться как бутто контейнер прилетел
с порта br-81 а он для второго контейнера является гейтвеем. поэтому
второму контейнеру будет казаться что пакеты летят с гейтвея.
и так и есть. показываю

я вошел на первый контенер сидящий в сети 172.18.* и имеющий
IP=172.18.0.2  
и запустил пинги во второй контейнер сидящий в сети 172.17.*  и 
имеющий IP=172.17.0.4 

(первый контенер IP=172.18.0.2) # ping -c 3 172.17.0.4


(второй контенйнер IP=172.17.0.4)
# tcpdump icmp -n -i eth0
IP 172.17.0.1 > 172.17.0.4: ICMP echo request
IP 172.17.0.4 > 172.17.0.1: ICMP echo reply

IP 172.17.0.1 > 172.17.0.4: ICMP echo request
IP 172.17.0.4 > 172.17.0.1: ICMP echo reply

IP 172.17.0.1 > 172.17.0.4: ICMP echo request
IP 172.17.0.4 > 172.17.0.1: ICMP echo reply

тоесть четко видно что нашему второму контйенеру кажется видится
что пакеты летят с гейтвея 172.17.0.1


ровно также если я сделаю наоброт. со второго конйенера буду
пиновать первй контейнер. первому контейнру будет видеться что 
пинги летят с его гейтвейя 172.18.0.1

тоесть происхдоит подмена src_IP. еще раз в каком случе она 
происходит (согласно iptables правилам которые туда сует докер сервер).

если мы пингуем из контйенера (сидящего за бриждем) любую карту
принадлежащую сетевому стека хоста то подмена НЕ происходит. 
и приложение видит SRC_IP самого контейнера.  если же пакет от 
контенера хочет летать кудато дальше за пределы сетвеого стека 
хоста то ПОДМЕНА ПРОИСХОДИТ. и IP подменяется на IP сетевой карты 
хоста!
показываю на картинке
                             (             -----  eth1     )
                             (            |                )
(контейнер) ----- (свич) --- (docker0 ----|-----  fa1      )
                             (            ------  eth10    )

                             [  сетевой стек хоста         ]


если пакет из контейнера предназначен для docker0 или любой
другой карточки сетевого стека хоста то IP не подменяется!
тоесть если пакет от конейнера летит на docker0 или eth1
или fa1 или eth10 то его src не подменяется не маскируется.


а вот если пакет от контейнера предназначен лететь куда то дальше
чем сет стек хоста то его src_IP будет подменен! (согласно праивлам
iptabes пропсианыым для сет стека хоста)



                                    (            |-------eth1-----)------------Internet
                                    (            |                )
(контейнер eth1) ----- (свич) ------(docker0 ----|-------fa1------)---(свич)---(контейнер)
                                    (            |-------eth10----)

[сет стек конт] ----- [ядро] ------ [  сетевой стек хоста         ]------------[другой сет стек]


тоесть если пакет от конейнера летит в интернет через eth1 
то при вылете из eth1 его src_IP будет подменен на IP(eth1)
или если паке предназначен для другого контйенера и летит через fa1
то при вылете через fa1 его src_IP будет подмненен на IP(fa1)

тоесть еще раз пакет от каонтенера прилетая в сет стек хоста свой
IP неменяет. тут измеений нет. но если этот же пакет после этого
покидает сет стек хоста вот тут то происходит подмена обязательно!
если сет пакет от контейнера представить как рыбку а сет стек
контейнера, сет стек компа, сет стек вне компа как аквариумы 
то наглядно это можно представить так когда рыбка из аквариума-контйенер
приплывает в аквариум-сет стек компа то ее цвет ее хвоста не 
меняется. а если после этого эта же рыбка покаидает аквариум-сет.стек.хоста
и плывет в следущий аквариуем то цвет ее хвоста обязатеьно меняется!

и здесь я еще раз хочу подчеркнуть еще важный момент - когда
мы создаем докер сеть типа бридж то докер сразу в сет стеке хоста
добавляет правила в iptables, одно из этих правил запрещает 
прохождение пакетов из одной докер бридж сети в другую докер 
бридж сеть. поэтому если мы хотим это дело разрешить то
нам нужно удалить вот такие вот правила 

-A DOCKER-ISOLATION-STAGE-2 -o br-81e0d9ff7558 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP

если их неудалить доступ с контенера сидящего в обной бридж сети
в другую бридж сеть работать не будет!

опять же надо понимать что удаление правил работат ровно до
следущего перезапуска докер демона. и эти правила сразу будут 
восстановлены.

ИТАК я считаю с внутренней магией как работает связь между контейне
ром который сидит за бриджем и сет стеком хоста или даже с 
сет стеком который находится за пределами сет хоста например 
за вторым свичем и контенейром за ним я выяснил.!

--->
ЗАМЕТКА
я узнал как через команду ip посмтреть все сет карточки
которые имеют тип бридж. тоесть за которыми сидит вирт свич

# ip -c a sh type bridge

тоесть помогает ключ type

если есть пакет bridge-utils
то конечно это же самое можно посмотреть и вот так

# brctl show
<---


далее. я обнаружил странную хрень. 
если я пингую с 172.17.0.10 на 172.17.0.6 то поток ловится
нетоллько на veth2 и veth3 тисипидамапом но и на docker0 . 
вопрос какого  хрена поток проходит через порт докер0 ????
если поток идет через свич то пакет влетевший в свич
в порт veth2 должен согласно таблице мак адресов
лететь только в порт veth3 и конечно пакет не должен 
лететь в порт docker0





                        |   |<veth2-------------eth1 (172.17.0.10, 02:42:ac:11:00:0a)
                        |   |
                        |   |
                        |   |
      docker0 ----------|   |<veth1-------------eth2
                        |   | 
                        |   |
                        |   |<veth3-------------eth3 (172.17.0.6, )

   [сет стек хоста]----[свич]------------------[ сет стек контейнера                  ]


тоесть все порты veth  (потоки от них) все входят в порт docker0  
и проходят сквозь него. и далее софт 
решает куда этот пакет двигать дальше - на какойто конкретный veth 
или наружу на комп или на все порты.


а может такая схема veth порты сидят на свиче. в свиче есть некая 
общая шина. а эта шина по типу хаба соединена
с портом docker0 поэтому все что течет по шине также видит 
поорт docker0





                   | Ш    |<veth2-------------eth1 (172.17.0.10, 02:42:ac:11:00:0a)
                   | И    |
                   | Н    |
                   | А    |
docker0 --хаб------|      |--<veth1-----------eth2
                   | Ш    |
                   | И    |
                   | Н    |<veth3-------------eth3 (172.17.0.6, )
                   | А    |

на шине сидит цпу который принимает весь поток и анализирует и направляет
а docker0 порт через хаб соединен с этой шиной.
поэтому когда пакеты от 0.10 летат на 0.6 то они видны через tcpdump на docker0
по крйней мере я так вижу на практике













все что изложено ниже про бридж это зачастю неверная старая информация.
но кое что полезнго там тоже есть



прежде всего напонять что такое бридж.

для начала вспомним как работает свич.

у нас есть комп А с одним сетевым портом А
у нас есть комп Б с сет портом Б

порт А имеет мак адрес и он шлет в сеть
фрейм у которого | src МАК-А | dest МАК-Б

фрейм прилетает в порт свича.
и тут очень важно скзаать что порт свича неимеет свой собственный мак
адрес. а если и имеет это в процессе передачи этого фрейма неучаствует 
вообще! разве это не шок? мы имеем сетевой порт на железке но 
этот порт не имеет или по крайней мере вообще неучаствует в процессе
передачи фрейма.
клмпьютер а ничего незнает о свиче и его портах никак это неисопльзует
при подгтовке фрейма при отправке и тому подобное.

фрейм засосался внуьрь свича. если свич уже знает за каким портом 
сидит комп Б то он этот фрейм выплюнет без изменений ровно как он 
и пришел от компа-А в этот порт. причем выходной порт опять же необязан
иметь никакой свой МАК адрес. разве это не шок? 
если свич незнает за каким портом сидит комп-Б то он выплюнет этот фрейм
во все порты свои.

важно тут увидеть что порты свича хоть и участвуют в процессе передачи фрейма
но они совершенно обезличены ибо не имеют или необязаны иметь свой
мак адрес.

это шок.

ведь мы привыкли что если порт эзернет то он обязан иметь мак адрес
чтобы участвоват в процессе передачи информации.

шок.

теперь таже предстваим что в какойто порт-1 свича-1 воткнут не конечный компьюер
а другой свич-2. это значит что в порт-1 свича-1 постоянно влетает 
не фреймы от одного компа у которого один src MAC а постоянно влетают
фреймы с кучей разных src MAC. 
о чем это говорит. это говорит о том что порт свича работает в режиме 
promiscious mode. он принимает фреймы с любыми src MAC. засасывает в себя все.

это тоже шок.
потому что это тоже совершенно отичается от работы эзернет порта на компе.

итак еще посмотрим на различия как работает эзернет порт на компе и свиче.

на компе порт обязан иметь МАК адрес.
на свиче это необязан

на компе порт принимает только фреймы преднезначенные для его мак адреса
на свиче порт принимает все фреймы с любыми мак адресами
(броадкастный трафик понятно что принимают и комп тоже)

на компе комп отправляет в сеть фреймы только с одним src MAC - своей карты
на свиче порт плюет в сеть фреймы с совершенно разными src mac адресами.

вот такая огромная разница. хотя и там и там эзернет порт.


получается если мы имеем на компе два эзернет порта 
и хотим заставить комп работать как свич то надо 
чтобы софт на компе умел переключать порты и использовать их 
в другом режиме в режиме свича а не врежиме компа.

то есть порт на компе должен начать принимать весь трафик (promiscou mode)
порт на компе должен начать отправлять в сеть фреймы с разными src MAC 
и плевать какой мак имеет сам порт. он вообще неважен и ненужен.
потому что если комп А отделен от компа Б свичом или свичами то они никак
неменяют src\dest MAC так что фрейм летит от компа А до компа Б через все 
свичи безизмененнно.

таким образом сразу видно что чтобы комп смог работать в режиме свича
это надо заствить его сетевые порты работать в совершенно
другом режиме чем сетевые порты обычного компа.

вот это очень важно было хотя просто осознать
придать этому занчение.е
обратить на это внимание.

какя понимаю бридж это свич на 2-4 порта.
и внутри он попроще устроен.
просто витоге маленький простой свич. это бридж.


значит если мы говорим о софтовом бридже.
то появляется некое непонятная хрень которой нет в физ бридже.

а именно. в линуксе среди реальных физ портов появляется 
+1 вирт эзернет порт.

захрена это сделано непонятно.
ибо  в нем нет необходимости.

в реальном бридже скажем есть только два порта один 
в одну сеть второй во вторуб сеть и никаких доп портов специального
назначения в физ бридже нет.
поэтому захрена было в линуксе лепить этот порт непонятно.
в нем нет реального физ смысла.   


далее опять же обращу внимание что когда мы в линуксе 
обявляем тот или иной физ порт частью бриджа то этот порт необязан 
иметь ни мак адрес ни IP адрес. они также как на свиче 
абсолютно этим не будут пользоваться. это чисто физ безликие 
порты без всяких опознавательых знаков. в моем понимании.

слово добавить порт в бридж опять же я считаю сверх неудачным.
я бы сказал обьявить порт переклчит порт в режиме бриджа. вот так.
так вот в линуксе когда мы переключаем порт в режим бриджа и если порт
имеет назначенный IP адрес то либо будет ошибка либо автоматом у порта будет
стерт ip адрес. так вот я считаю что кроме этого надо автоматом удалять и
мак адрес на порту. тогда будет все по фэншуй.  если уж делать все 
как надо то доконца. но мак адрес на порту остается хотя смысла в нем 
никакого нет. порты на свичах не имеют своих индивиудальных 
мак адресов. едиснвтенное насколько я понял если мы активиурем STP
то он требует вот только я непонял толи чтобы весь свич имел один мак адрес
по которрому его можно идентифициаровать то ли чтобы порт каждый имел 
свой мак адрес. если же мы неиспольщуем STP то порты на свиче 
мак адреса иметь необязаны.
точка.

далее следущий важный момент для понимания.
как контейнерные сетевые карты прикрепляются к бриджу.


контейенер имеет сетевую карту уровня L3,
у нее есть IP и mac адрес.
эта шарманка сидит в своем сетевом неймспейсе.

[  сетевой неймспейс 1 ( сетевая карта контейнера MAC+IP ) ] 

эта сетевая карта связана с veth портом который сидит в другом сетевом 
неймспейсе , нашем обычном сетевом неймспейсе init. veth порт имеет мак 
хотя по мне он ему ненужен. как я понял все фреймы что он получает он 
неотбрасывает и неразбирает он их сразу посылает на карту контейнера.

так вот этот veth докер  переключает в режиме бриджа. то есть все что влетает 
в порт он пропускает через себя в обе стороны. то есть то что карта контейнера
из себя плюет и оно прилетает на veth он через себя пропускает дальше.
и то что снаружи прилетает в veth он через себя пропускает и плюет в карту.

за veth сидит ядро. то есть цепочка такая

ядро <-> veth <-> сет карта контейра (IP+MAC)

просто veth одном неймспейсе а карта контейнера в другом сет неймспейсе 
поэтому процесс контейнера видит только свою карту и не видит veth карту
а процесс init видит veth и невидит карту контейнера.

что происходит когда у нас несколько veth и может быть несколько 
реальных физ портов переключены в режим бриджа (или как они говорят 
подключены к софт бриджу).  как это выглядит

		|=
		|<-> eth0
ядро<->	|<-> veth1 - сет карта контейнера 1
		|<-> veth2 - сет карта контейнера 2
		|<-> eth1
		|=

что мы тогда имеем. 
1 каждая карта работает в promiscious mode и она всасывает в себя
абсолютно все фреймы что к ней долетают
2 всосав фрейм карта его тут же передает дальше через себя в ядро.

если в ядре есть уже понимание за какой картой сидит dest MAC 
фрейма то этот фрейм направляет конкретно в тот один порт.
а если такой информации нет то (внимание) фрейм отправляется НА ВСЕ ПОРТЫ
РАБОТАЮЩИЕ В РЕЖИМЕ БРИДЖА. другими словами фрейм флудится на все порты
бриджа. таким образом например если контейнер 1 направляет фрейм на 
контейнер 2 и ядро еще незнает что  сет карта контейнера 2 сидит 
за за veth2 портом то этот фрейм (внимание) полетит в том числе и на eth0
и на eth1 и те его выплюнут в реальные сети.
также (важно) броадкаст фрейм влетевший в один порт бриджа будет тутже 
зафлужен на все порты бриджа.

не все карты позволяют переключатся в режиме бриджинга.
например вай фай карты этого сделат ненадут. например они недают отправлять
в сеть фреймы с мак адресом отличным от мак адреса карты.

внутри линукса можно создать несколько бриджев.
таким образом одна группа портов в режиме бриджинга будет назависима
от другой группы портов в режиме бриджинга. 

при создании бриджа на линуксе создается порт brctl L2 
с мак аресом. еще раз какой смысл физический в нем - по моему его нет.
на фреймы пересылаемые он никак не влияет. 
его мак адрес тоже никому не всрался.
единственное что может быть это делается если мы будет использовать в этом
бридже еще и протокол STP. которым я не будут пользоваться.
опять же этому порту можно даже IP назначить. 
захера тоже непонятно.

важно конечно понимать что бридж это нетолько порты которые все пропускают
снаружи вовнутрь и изнутри наружу. это еще и софт который обрабатывет 
эти фреймы когда они влетели из порта внутрь ядра и этот софт решает
куда эти фреймы отправлять дальше.также как в физ коробке это есть.
то есть фрейм влетел из порта внутрь коробки а дальше там же коробка смотрит
есть у  нее информация за каким портом находистя dst mac и если есть то бридж
плюет фрейм невовсе порты свои а только в один.

порт <-> внутренности коробки <-> порт(ы)


вот как для меня сейчас выглядит бридж  линукса


eth0 ----  |
eth1 ----  |
veth0 ---- | ядро (софт) | ---- brctl0
veth ----  |

и получается что этот brctl0 он ни к селу ни к городу. как бы в него ниоткуда 
немогут никакие фреймы влететь потому что он ни к чему не подключен 
ни вылететь. ну окей влететь могут из ядра.но что дальше. лететь то некуда.

или что там другая схемп что фреймы от всех портов влетают в ядро через brctl0? так что ли?

eth0  <--->|
eth1  <--->| <-> brctl0 <-> | ядро
veth0 <--->|
veth1 <--->|


но это бред какойто. это представить что есть порты на свиче
и от них фреймы летят несразу внутрь железки а на какойто внутренний
невидимый снаружи но такойже по факту ethernet порт а уже из него внутрь 
железки. ну и получается что 10х1ГБ портов идут в 1 порт 1ГБ.
дебилизм какойто.

дальше.
представим мы имеем комп. на нем две физ карты

комп   
eth0 |---------> порт 1 свича  
eth1 |---------> порт 2 свича 
                 порт 3 свича <---------- другой комп
				 

мы их воткнули в обычный свич.
что будет если в свич в порт 3 от другого компа прилетает броадкастный фрейм. 
свич этот фрейм выплюнет во все порты (кроме 3). то есть этот 
фрейм полетит в eth0 и в eth1.

комп засосет эти фреймы и от eth0 и от eth1.
ну и поскрипит софтом и на этом все закончится.

другое дело если мы на компе включили eth0 и eth1 в наш
софт бридж.
когда фрейм влетит в eth0 то софт свича направит его во все остальные порты
свича. то есть он его выплюнет в eth1 обратно в сеть.

 также когда фрейм влеит в eth1 то он его засосет 
 и софт выплюнет пришедший фрейм во все остальные порты бриджа
 то есть в eth0. 
 
 когда обратно в физ свич прилетит броадкастный фрейм в порт1. то он его 
 выплюынет во все порты кроме порта1.
 тоже самое касается когда в физ свич прилетит обратно фрейм в порт2.
 он его выплюнет во все порты кроме порта2.
 таким образом видно что начнется адский ад. на всех портах
 ибо в отличие от IP пакета эзернет фреймы неимеют TTL.
 
 
 вернемся обратно к контейнерам.
 важно понимать что карты контейнеров неявляются портами бриджа.
 это абсолютный мисконепшн. портами бриджа являются veth порты.
 а уже они соединены с портами контейнеров.
 таким образом контейнерные порты имеют уровень L3 с IP
 а veth имеют уровень L2
 
 veth как быявляются теми физ портами на реальных свичах
 куда по проводам подключаются порты компов.
 
 порты компов это порты контейнерров. порты veth это физ порты свичей.
 
 порты veth неимеют IP адресов могут и мак неиметь. 
 порты контейров имеют MAC+IP
 
 |			  veth0|<--------->|if1 карта контейнера1	
 |софт бридж 	   | 
 |			  veth1|<--------->|if2 карта контейнера1


и тут мне кажется важно четко понимать разницу между портами 
которые составляют сам свич и портами которые воткнуты в эти порты.

между ними огромадная разница.


если ты добавляешь какойто порт в свич то ему это недает ничего.
он просто ставноится транзитным портом через который все просто пролетает туда и сюда. 

а вот если ты какойто порт подключаешь к порту бриджа то это уже дает.
дает то что наш порт теперь может связываться с другим портами подключенными 
к бриджу через L2 на уровне фреймов.


походу я понял что дает brctl. на уровне L2 когда у него только мак 
он недает ничего и никак. а вот когда ему присваивают IP адрес. 
то это позволяет наш свич превратить в рутер. вот походу его смысл.

картинка

комп if1|<----->|veth1				  |      
комп if2|<----->|veth2  бридж   brctl0|<----->интернет 
комп if3|<----->|veth3 				  |

 наличие brctl0 порта уровня L3 
 дает то что свич превращается врутер.
 вот походу смысл этого порта.
 
 через brctl0 бридж может выводить компы на схеме в инет если 
 они сидят в одной IP сети.
 
 или если компы сидят в разных IP сетях то через brctl0 они 
 могут общатся друг с другом на основе рутинга на бридже.
 
 то есть еесли комп1 имеет ip = 192.168.1.10
 если комп2 имеет ip = 192.168.2.10
 
 а на свиче мы имеем что 
 brctl0 
 ip1=192.168.1.1
 ip2=192.168.2.1
 
 ну тоесть что brctl0 имеет два ip.
 то тогда комп1 и комп2 смогут пинговать друг друга.
 
 тоесть без l3 brctl0 бридж дает связность компами на уровне фреймов
 уровне l2
 при наличии L3 brctl0 ну я уже описал свич превращается в рутер.
 
 собственно ровно это мы и имеем в домашних рутерах.
 ряд свичевых портов. плюс виртуальный то есть без реального 
 физ интерфейса внутренний L3 lan порт ну и плюс 1 L3 внешний порт
 к провайдеру.
 
 вобщем brctl0 нужен только для одного - превраттить бридж в рутер.
 вот походу его смысл.
 
 это совсем неовидно изначально. и нигде необьяснено
 
 
 возвращается к контейнерам.
 изначальная задача поместить контейнеры каждый в свой независмый L2 
 сетевой стек. это мы сделлаи с помощью индивидуалных сетевых неймспейсов.
 каждому контейнеру по своей независимой сет карте.
 
 далее мы хотим соединить ряд контейнеров друг с другом на уровне L2.
 при этом мы нехотим переделыать их сетевые неймспейсы. пусть они так и сидят
 в своих неймспейсах. с незавиимыми карточками.
 значит их нужно воткнуть в свич виртуальный. вирт свич должен имет какието
 свои свичевые обезличенные порты. 
 такие придумали = veth
 
 veth мы делаем портами свича = получили свич с портами.
 также veth по дизайну соединяются с портами контенеров.
 
 таким образом мы обьеидинили карты контейнеров через свич.
 
 на уровне l2 они могуо друг с другом контактировать.
 
 возникает вопрос вот мы обединли карты контейнеров 
 другс другом через veth-ы через свич софт.
 
 это связь по L2.
 
 а есть ли связь между картами контейнеров и 
 физ картой сервера через которую он выходит в сеть на уровне L2.
 
 то есть понятно что вот эти veth-ы они образуют некий виртуальный свич
 внутри линукса. вопрос в том что есть ли аплинк между этим вирт свичем
 и физ картой сервера
 
 
 eth0(192.168.1.10)----????-----|(аплинк) вирт бридж  veth1|<----->|контейнер1
													  veth2|<----->|контейнер2
													  veth3|<----->|контейнер3
													  

есть ли внутри линукса аплинк L2 между реальной физ картой
и софт бриджом по дизайну?

думаю все таки что его нет. потому что мак адреса которые генерирует 
линукс для вирт карт контейнеров они могут совпать с мак адресами
реальных физ устройство в сети за компом. 

поэтому надо заставить чтобы если мы выпускаем пакеты из контейнера в реальную
сеть за компом чтобы происходил рутинг. то есть чтобы внутри линукса сдирался
фрейм контейнера , одевался фрейм данного компе его карты eth0 (пакет Ip от контейнера при этом остается без изменений конечно) и только тогда выпускался 
в сеть. так что я думаю что на уровне фреймов есть связь только между veth.
 а послать фрейм от контейнера через eth0 во внешнююю сеть нельзя.
 
 я думаю что если контейнеру дать ip сети 192.168.1.0 и сделать 192.168.1.10
 гейтвеем то я думаю что такой пакет успешно покинет комп ( при условии
 что на компе разрешен форвардинг). 
 
 но при такой ситуации когда мы дали контейнеру IP из сети хоста это нам приенсет кучу мудежа только.
 если мы захотим достучаться до компа 192.168.1.100 который лежит несреди
 наших контейнеров а где то в сети хоста. то нужно будет укзаать 
 на контейнере статический маршрут что мол если тебе нужен 1.100 то ищи его
 не среди соседей контейнеров ибо тогда ненужен гейтвей а ищи его за гейтвеем
 
 route add 192.168.1.100 через 192.168.1.1
 
 а на 192.168.1.100 надо тоже будет прописыать обратный стат маршрут 
 что мол если ты ищешь 1.х то посылай на 1.10
 
 тоже самое касается выйти в интернет. надо будет на интернет шлюзе прописываь
 стат маршурт к 1.х через 1.10
 
 причем такие стат маршруты на интернет шлюзе или соседях
 сервера нужно будет на каждом из них прописывать индидуалный маршрут 
 для каждого контейнера. вобщем ужас.
 
 окей. итого понятно что ip сеть под контейнеру надо выбрать совсем другую
 чем сеть хоста. например 192.168.2.0
 
  теперь контейнеры имеют свою сеть а сервер и его соседи по физ сети
 имеют другую сеть.
 
 теперь достаточно на гейтвее физ сети прописать ровно один маршрут 
 для всех контейнеров что мол сеть 192.168.2.0 надо переправлять 
 на 1.10 и все. то есть из физ сети понятно где контейнеры искать.
 
 теперь остается контейнеры выпоустить в сеть. и тут нам на помощь 
 приходит brctl0 L3
 
 
 физ сеть| <---> |eth0 компа (сеть 192.168.1.10) <----> ядро <---->| brctl0(192.168.2.1) вирт свич veth1|<---->|контейнер if1 (192.168.2.10)
 
 получается brctl0 это порт свича L2 причем с L3 фукцией.
 
 мы назначаем конейтнерам гейтвей 192.168.2.1
 
 и у нас пакеты теперь ходят из физ сети к контейнерам и обратно.
 (на компе мы активировали форвардинг и на рутере физ сети прописали 
 стат марушрут что 2.0 надо искать на 1.10)
 
 так вот docker0 это brctl0 порт.
 
 заметим что пока NAT даже непахнет. и это хорошо ибо нат это cpu затратная
 процедура
 
 
 если мы при прохождении eth0 зададим NAT правило. то тогда мы можем подменять 192.168.2.0 через 192.168.1.10 и тогда на рутере физ сети
 прописывать стат маршрут непридется.
 
 
 но отмотаем обратно. итак чтоже нам дает вирт бридж.итоги
 самое главный вопрос  - что будет если я какойто  порт включаю в состав бриджа 
 
 # brctl docker0 addif eth0
 
 docker0 название бриджа
 eth0 порт который мы включаем в состав бриджа
 
 1. eth0 ПОРТ СТАНОВИТСЯ ПОЛНОСТЬЮ ТРАНЗИТНЫМИ
 2. ТО ЧТО ВЛЕТАЕТ В ОДИН ТАКОЙ ПОРТ ПЕРЕНАПРАВЛЯЕТСЯ СРАЗУ ВО ВСЕ ДРУГИЕ
 3. НЕ ПОРТЫ П.1 А ТЕ ПОРТЫ КОТОРЫЕ ПОДКЛЮЧЕНЫ К ПОРТАМ П.1 ПОЛУЧАЮТ 
 СВЯЗЬ ДРУГ С ДРУГОМ. 
 4. СВЯЗЬ УРОВНЯ L2
 5. еще раз важно различать порты которые являются непосредственно частью
 бриджа. в данном случае это eth0. и порты которые неявляются частью бриджа
 но которые соединены с портами бриджа. в нашем случае это порты контейров.
 порты контейнеров неявляются частью самого бриджа. но они соединены с портами бриджа. порты самого бриджа являются транзитными и безликими.
 порты которые подключаются к портами бриджа уже являются полноценными
 и имеют мак+IP. мы настраиваем бридж чтобы через безликие порты создать 
 связь между полноценными L2+L3 портами.
 
 
 что дает бридж разобрались.
 
 и вот теперь мы переходим к задаче что нам надо что контейнеры
 могли друг до друга достучаться между отдельными физ серверами.
 
 есть такой еще момент. если brctl0 это гейтвей для всех карточек контейнеров
 то тогда их срупут упирается в срупут brctl0. получается что скорость каждой карточки условно говоря 1ГБ их 100 штук и все они выходят наружу
 через 1ГБ. это как то пахнет усзким местом. но пока непонятно 
 так как карточки и контейнеров и brctl0 они виртуальные.
 
 без проблема можно создать карточку сетевую с таким вот именем
 
  ip link add petya.1.1  type dummy
  
 # ip -c link show
 
 15: petya.1.1: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000

отсюда приходит понимание что интерфейс фланнеля с именем flannel.1
это не какой субинтерфейс (точка один) а просто такое дурацкое
название этого интерфейса.
 
3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default


 прикольно что карточка в контейнера сетевая она ничто иное как по своей
 сути тоже veth интерфейс. то есть veth он спокойно может быть L3
 
 
 veth напраник = down
 но в нашем неймспейсе мы все равно пингуем veth1
 
 veth link = down. но все равно пингуется
 поразщительно
 
 что dummy что veth оним когда down то при этом саойокной 
 пингуются.
 если сосдем veth = down то его другой сосдет все равно пингутеся.
 
 еще прикол. если бридж не имеет ниодного порта то его brctl интерфейс
 будет оставаться в состоянии down. 
 
 # brctl create docker2
 и он будет оставться в down и его будет неподнять.
 
 ip route get IP
 
 
 если настоящий лупбек погасить то он непигуется
 
 # ip route get  172.16.0.1
local 172.16.0.1 dev lo  src 172.16.0.1
    cache <local>

очень странный маршрут

также если мы от процесса пигуем lo панпример непонятно какой он 
ставит тгда source IP в пакет.наверно того же самого лупбека. 

то есть у нас же должна быть source карта  и source Ip. 
процесс обращается к какрте и с нее пингует ее же.


процесс если он пингует другую карту он обязан иметь исходную карту
когда мы пингуем лупюбек интервейс какая карта является исходной?

если я создам dummy который не лупбэек.
и есл я его погашу то он все равно пингуется.

значит у менять есть lo = 127.0.0.1 UP
есть dummy = 172.16.0.1  UP

я пингую 172.16.0.1 он пингуется
 но самое интересное какой маршрут выбирет линукс
 
 # ip route get  172.16.0.1
local 172.16.0.1 dev lo  src 172.16.0.1
    cache <local>

ия в шоке. строка означает что пинг до 172.16.0.1 пойдет 
через loopback интерфнйс при этом src ip = 172/16/0/1
как бы што? посчему через лупбек?
если мы погаим дамми то все равно 172 16 0 1 пингкуется
а вот если мы погасим лупбек то нифига не будет пинговаться

возможно лупбек интерфейс и дамми интерфейсы сидят на одной "шине" в линуксе.
но тогда получктся вопрос как тогда пакеты из физ интерфейса могут дойти 
до лупюбек интерфейса

пакеты швыряются исключетльно исходя из таблицы маршутизации
прежде всего  а потом все остальвное

если мы veth назначим ip то рут к нему будет проходить через lo интерфейс.

 япрописал маршрут
 
 172.15.0.0/24 dev veth0  scope link  src 172.15.0.2

veth0 неимеет ip

ping 172.15.0.1

цепочка процесс шлет чрез veth0(l2 без ip) на veth1 в другой неймспейс
а оттуда до 172.15.0.1 все дошел отуда оброатно он прилетет в 
veth0 и вопрос что с ппакетом происходит дальше. он что сразу отдается 
процессу ? стоп. он же недошел до 172.15.0.2 
а как до него дойти ? он же недошел до карты назначения.

и причем пинг доходит хотя dummy с ip = 172.15.0.2 находистя down

яя проерил через tcpdump когда мы пингуем veth то на сам veth вробще
ничего не прилетает. вообще.
все летит на lo и на этом все заканчивается

 ip route get  172.14.0.1
local 172.14.0.1 dev lo  src 172.14.0.1
    cache <local>

то есть летит как напписано в таблице марщутизации. 
а на veth вобще не прилетает. бред
не понимаю пакет недоставлен до карты. а он говорит чт доставил.


цепочка. есть карта дамми vasya с 172.15.0.1

есть veth0 который вдетв в другой спейс на другом конце карта с 172.15.0.10

я задал в роутинге

172.15.0.0/24 dev veth0  scope link  src 172.15.0.1

и пинги пошли.

они вообще неисходили из васи и не входили в него. и не входили в лупбек.
они только проходили через veth0 и все.
и как бутто отуда сразу в процесс

ощущение что все дамми они по факту являются алиасами лупбека.
я убрал ручной маршрут 172.15.0.0/24 dev veth0  scope link  src 172.15.0.1

и остался дефолтовый маршрут

172.15.0.0/24 dev vasya  proto kernel  scope link  src 172.15.0.1

через dummy .

и пинги не проходят.

получатеся что пакет испускается из дамми ииии ????? улетает в ядро
и ничео не происходит.

хотя веф это вирт инт как и дамми но они несоидениеы.

 я провел эксперимент все локальные ip адреса достигаются 
 только через lo интерфейс
 
 ~# ip -c route get 172.14.0.1
local 172.14.0.1 dev lo  src 172.14.0.1
    cache <local>
root@test-kub-05:~# ip -c route get 172.16.102.35
local 172.16.102.35 dev lo  src 172.16.102.35
    cache <local>

~#  ip -c route get 10.5.98.1
local 10.5.98.1 dev lo  src 10.5.98.1
    cache <local>


поразительно! неважно пингуешь ты реальную физ карту
или виртвальную - при пинге своих IP доставка идет всегда вседа
всегда через lo интерфейс. и если его погасить то ты несможешь
пинговать свои собстыенные карочки и неважно она физическая или 
вирталнаяю!


шок!

ну понятно что в физ карту ты неможешь сам засунуть пакети снаружи
но вирт карты то ....

вобщем механизм связи с собсвтенными карточками супер странный нефизичный.
значит когда пингуешь свои IP то пингуешь лупбек интерфейс по факту.

когда я пингую свой внутрений docker0 интерфейс то по факту пакеты ходят 
толькон на lo интерфейсе а на docker0 tcpdump неловит ни-че-го

полусется пактеы летят в lo интерфейс и летят непонятно куда но 
до программы долетают. рисует такая картин

процесс  - lo - ядро. ядро говорит все окей и типа ядро обратно в lo
пихает пакет с обратным порядком IP.
то есть типа на другой стороне провода от лупбек адаптера сидит ядро
и принимает пакеты как сетевая карта безадреса. получает пакеты
формирует обратный пакет и посылает его обратно в lo

как lo L3 имеет на другом конце L2 порт который уходит прямиком в ядро.
и ядро типа через lo принимает пакеты на все IP которые есть на компе на
всех картах.

lo это L3 порт который ведет в другой порт L2 который уже воткнут прям 
в ядро.

еще можно по другому представить. LO это L3 порт который
через провод ведет в L3 порт который имеет все IP карт на компе.


а что если представить что линукс это рутер с кучей L3 портов.
LO это тоже L3 порт входящий в состав рутера.
фишка его в том что процессы которые крутятся внутри рутера-линукса
им тоже типа надо дать возможность как то посылать пакеты на L3 порты
через некоторый api. как бы сами порты сидят в ядре. и вот надо 
юзерспейсу дать возможность блядь посылать пакеты к своим портам 
которые снаружи.

цепочка

получается что lo это как бы порт на невидимый на рутере и сидязий внутри корпуса линукса. зато так как он внутри к нему могут подключаться внутренние
процесыы.

цепочка 

процесс -> LO -> ядро рутера линукс. -> внещние порты в том числе как 
это ни смешно и вирт порты линукса кроме lo.

представим мы шлем пинг на IP физ порта.
имеем цепочку

процесс шлет пакет через -> LO -> он попадает в ядро ведь конечная цель
чтобы пакет попал в ядро. и ядро пакет для физ карты уже получило. его 
отправлять на физ карту на его драйвер неимеет смысла ибо он оттуда попал бы в ядро а он уже в ядре. ядро формирует ответ и отправляет обратно через lo.
и мы поэтому с одной стороны видим пинги а сдругой стороны мы ничего невидим
в тисипидамп на физ порту.
тоесть порт это устройство чтобы пакет попал в ядро. если он уже в ядре 
зачем его направлят в порт.

насколько я понимаю когда пакет влетает в порт "снаружи" то влетев в порт
он попадает в ядро. а из ядра в юзерский процесс

пакет -> порт -> ядро -> процесс

почему нерабтате когда мы пигуес IP карты компа 
но эта ккрта лежит вдруо неймспейсе.
потому что пает идет через lo а на том конце там толко IP 
карт которые в этом неймспейсе.

 в итоге 



возможно почему мы невидим в tcpdump чтобы н


поток идет через brctl0 хотя он для него даже не предназначен.

если  дать lo ip = 10.5.98.5/24 
то будет успешно пинговаться ЛЮБОЙ АДРЕС ИЗ СЕТИ 10.5.98.0\24
это пищлец

дале у меня на компе карта veth1 = 10.5.98.3/24
есть маршрут

10.5.98.0/24 dev veth1  proto kernel  scope link  src 10.5.98.3

однако если ввести команду

ip route get 10.5.98.9 
то она выдааст совсес нето что в таблице маршрутизации а

local 10.5.98.10 dev lo  src 10.5.98.5
    cache <local>


то есть игнор таблицы мраутизации и поход через lo.

это пиздец.

я предлагаю на lo невещать ничего кроме 127.0.01
иначе заебешься вычичлять приколы на ровном месте.

 у меня lo во втором неймспейсе имел ip = 10.5.98.9
 и veth1 во втором нейиспейсе имел ip = 10.5.98.10
 
 я из первого неймспейса из veth0 пытался до пинговться
  до 10.5.98.10 и никак не мог! погасил lo непомогает
  удалил ip с lo и только тогда заработало. 
  какой то п..ц с этим lo.
  
  ужас. как он работает логика непонятина.
  
  лучше lo вообще нетрогать и про него забытьб
  
  lo дебилнейщий интерфейс и ни коммутация ни рутинг с ним неопнятне
  тидет не поправилам. лучше его вообще нетрогать. и никакие ip адреса 
  ему недавать кроме дефолтовго
  
  к соажалениею  brctl0 это не просто L3 порт бриджа 
  это еще и транзитный порт через который как я понял протекает транзитно буквально каждый пакет всего бриджа. так что если его погасить ниодлтин порт
  бриджа не сможет связаться с другим портом бриджа. это дебилизм. полнейший.
  я считаю что brctl0 должен быт влкючен только если мы хотим его L3 функционал 
  заюзать. а так это узкое место .
поэтлому brctl погаситл и весь бридж остаовился

ответ на мой изначальный вопрос
взяли пару вефов. 
один оставил а другой зауснули в другой неймспейс и дали ему IP
вопрос что нам надо сделать чтобы тот IP пинговать.

ответ - этому вефу назначить IP из той же сети и пинги пошли.
 аесли у нас два 10 контейнеров как тогда их пинговать. 
удобнее всего тогда наши вефы этого спейса сделать частью бриджа.
и самоу бриджу дать L3 ip. тогда можно будет их всех достать для пинга.

так будет минималная заморочка с этим.
потому что чтобы пиновать без гейтвея надо иметь порт L3 из тожй е сети 
что и порты те. и чтобы наш L3 порт имел связт с каждым тем портом на уровне L2. 

без свича нужно каждому концу нашего веф назначать ip индивдуальный.
ипрописывать инд маршрут к тому концу. 10 контейнеров это 10 IP с нашей стороны назначь + 10 маршрутов пропши до их ip. ужас.
поэтом свич это выход. если бы на самом свиче нельзя был назначать один порт 
 с IP то тогда нужно было бы создать +1 пару веф. оба оствит внашем неймспейсе
 один вотнуть в свич а второй невтыкать и второму дать IP. 
 
 
 
 ЕДИНСТВЕННОЕ ИЗЗА ЧЕГО НЕРАБОТАЛА СВЯЗЬ МЕЖДУ НОДАМИ ДЛЯ ПОДОВ
 ЭТО ЧТО НА ДАТАТ НОДАХ БЫЛ ВЫКЛЮЧЕН IP FORWARDING.!!
 
 ОСТАЕТСЯ деталеьно выяснить как комбтнация фланеля и iptables
 дает связь
 
 я нашел в инете инфо что лупбэк в линуксе и лупбэк в циске 
 работают совсем неодинаково. опа! подстава!
 я нашел что если лупбек имеет IP то для этих IP трафик должен ходить 
 только внутри машины. поясненеие если линукс увидит трафик вроде бы
 который влетает снаружи и при этом имеет IP из сети lo то линукс
 его будет дропаь. пример
 
 lo = 192.168.1.1/24 - есл итак назнваить 
 то любой входящий в комп пакет с src ip = 192.168.1.0/24 комп будет его дропать.
 а мол если мы хотим lo такой же как у циски можно заюзать dummy 
 интерфейсы. жуть
 
 # ip route
192.168.12.0/24 dev eth0  proto kernel  scope link  src 192.168.12.101
default via 192.168.12.1 dev eth0

proto kernel означает что маршрут был создан линуксом
автоматом


посдтава1 
в iprpute2 нет алиасов
едиснтвенное что работает это 


auto ens160
iface ens160 inet static
        address 172.16.102.19
        netmask 255.255.255.0
        network 172.16.102.0
        broadcast 172.16.102.255
        gateway 172.16.102.1
        scope link
        # dns-* options are implemented by the resolvconf package, if installed
        dns-nameservers 172.16.101.2 172.16.101.7
        dns-search mk.local

iface ens160 inet static
        address 172.16.102.20
        netmask 255.255.255.0

второй момент про scope.
нельщя дать два IP адреса с одной подсети и с одним scope



 

про фланель
 
 видно что заведен бридж cni0 который никак не связан 
 с докер нетворкс командой
 
 
  root@test-kub-02:~# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.5efab67e726a       no              veth6b8aeca2
                                                        vethde173048
docker0         8000.0242f0b9d9c1       no              veth45c266d

root@test-kub-02:~# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
1b537e52e590        bridge              bridge              local
bbffd9dc86b7        host                host                local
ce3ae03bbaa1        none                null                local

--
что заметил инеерсно
как и следовало ожидать veth 
интерфейсы работабт в режиме promiscuity

 vethc8d37526@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 62:80:ae:f3:64:cb brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 1
    veth
    bridge_slave state forwarding priority 32 cost 2 hairpin on guard off root_block off fastleave off learning on flood on
    inet6 fe80::6080:aeff:fef3:64cb/64 scope link
       valid_lft forever preferred_lft forever


параметр promiscuity 1

---



VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN 
VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN 
VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN 
VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN 
VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN VXLAN 


что выяснилось в целом.

берем чисто докер.

его все вот эти команды работы с сетью

docker netwrork ls \create

они не делаюи никакой магии. они испольщуют на бекенде

ip link add ....

напрмиер docker0 это бридж.

тоесть бекенд сетей докера опирсатеся на понятный прозрачный функционал.


docker0  это бридж порт


фланнель создает в системе интерфейс который тоже создатся через 
ip link add vxlan1 type vxlan id 1 remote 172.31.0.107 dstport 4789 dev eth0

вот это сейчас более детально изучим

ближайшая тема vxlan в линукс

--
интересно интфрейс vxlan ненужно обьединять скажем через бридж
с выходным интерфейсом

пример
на компе физ интерфейс
ens160

vxlan интерейс
vxlan.50

каждый из них самостоятельно болтется на компе. 
они никак не обьеидинены на уровне L2 

однако фрейм входя в vxlan.50 далее транзитрруется на ens160 
ив сеть. ну я думаю по сути понятно как оно работает

   
   приложение(сокет) ---> vxlan.50  ---- ядро ---> ens60 ---> сеть


это такой же принцип как работают впн проги.

хотя по мне надо было создать пару veth.
ксати при этом на коме форвардинг неактивироан.
возникет вопрос как фреймы пакеты перелетаюти с vslan.50 на ens160

vxlan интерфейс это как бы NAT, при проходжении пакета через vxlanx 
интрфейс у него подменяется dst IP адрес. ну типа того.
обыччный nat подменяет src ip. 
а тут одевается сверху новая оболочка и подменеяется src ip dest ip

еще раз у нас есть L2 интерфейс 
vxlan.50

у нас естть маршрут

10.12.0.0/24  proto kernel  scope link  src 10.12.0.1 dev vxlan.50

который говорит пакеты на 10.12.0.0 плюй в сет через vxlan.50 карту.
и используй впакетах src ip = 10.12.0.1

трафик для 10.12.0.10 согласно маршруту  направляется на vxlan.50 
с параметрами 
src ip = 10.12.0.1
dst ip = 10.12.0.10

входит в  vxlan.50 , на пакет одевается новая оболочка
и dst ip заменятеся на 172.16.102.20 ибо так прописано в насйтроках vxpan.50

 vxlan.50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 
    vxlan id 50 remote 172.16.102.20 
    
	
	далее видимо он ищет на компе с какого реавльного физ карты (точнее мы же это 
    указали когда создавли vxlan порт )
	он может выплюнуть dst ip 172.16.102.20 нахоит ens160 с его 172.16.102.19
	и вперед использует ens160 и src ip = 172.16.102.19
	
	
	---
	создадим vxlan интерфейс
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.19 remote 172.16.102.20 dstport 4789
	
	
	вот так он выглядит
	
	13: vxlan.50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450
     vxlan id 50 remote 172.16.102.20 local 172.10.0.10 dev ens160 dstport 4789 
    
	
	значит что он дает. весь поток который будет в него направлен 
	будет обернут снаружи новым ip пакетом в котороом
	dst_ip=172.16.102.20 (адрес внещнего порта того компа)
	src_ip=172.10.0.10   (адрес ??? нашего компа)
	dev ens160 = указана физ карточка через которую "преобразованный пакет"
	выплевывать в физ сеть
	
	то есть
	remote
	local
	dev
	это параметры пакета в который оборачивается исходный ip пакет.
	
	
	по сути vxaln тоннельный интерфейс.
	
	прикольно что vxlan никак не увязывается через бридж с ens160.
	сточеи зрения линукса мы никак не увязываем vxlan интерфейс и ens 
	интрефейс. поток если попадает в vxlan то каким то неведомым
	ненастроенным образом попадает в esn160. даже без бриджевой
	связм между ними или как у veth пары.
	
	
	преобразование настроено.
	остается прописать какой поток направлят на vxlan.50
	это делается через маршрут в таблице марршрутизации
	
	10.12.0.0/24  proto kernel  scope link  src 10.12.0.10 dev vxlan.50
	
	марушрут - куда, через что, какой src ip.
	
	оно говорит трафик c dst_ip=10.12.0.0/24 шли через карту vxlan.50
	при этом используй src_ip=10.12.0.10
	
	как организовать src_ip=10.12.0.10
	можно либо его присвоить самому vxlan.50
	а можно vxlan.50 оставит как L2 интерфейс и вставить его в бридж.
	а уже какому то другому порту в бридже прсвоить L3 = 10.12.0.10
	
	если мы вставим vxlan.50 в бридж то в маршруте  для vxlan.50
	тогда можно src ip и не указывать
	
	пример
	
	vxlan.50 ---> brctl0 ----> veth0-->|другой неймспейс---> if0(L3)=10.12.0.20
	
	виртуалка когда будет слать пакеты то он автоматом будет прописывать 
	src_ip = 10.12.0.20
	
	кодга vxlan.50 всталвен в бридж тода по идее и маршрут ненужен
	пакеты попадают  в vxan не через маршуртизацию 
	а через бриджизацию.
	
	--
	
	схема
	
	
	другой комп --vxlan.50(10.12.0.2) -- ens160(172.16.102.20) -- (LAN) -- мой комп -- ens160( 172.16.102.19) --vxlan.50 -- brctl0- veth0 -- veth10(10.12.0.100)
	
	
	мой комп 
	настройки
	
	14: vxlan.50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 
    link/ether 8e:78:77:73:5c:d3 brd ff:ff:ff:ff:ff:ff promiscuity 1
    vxlan id 50 remote 172.16.102.20 local 172.16.102.19 dev ens160 srcport 0 0 dstport 4789 ageing 300
    bridge_slave state forwarding priority 32 cost 100 hairpin off guard off root_block off fastleave off learning on flood on
    inet6 fe80::8c78:77ff:fe73:5cd3/64 scope link
       valid_lft forever preferred_lft forever

	как видно vxlan.50 не имеет IP
	и вставлен в бридж
	
	18: veth10@veth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 
    link/ether 22:e3:ec:3d:49:76 brd ff:ff:ff:ff:ff:ff promiscuity 0
    veth
    inet 10.12.0.100/24 scope global veth10
       
	
	и единственный маршрут
	
	10.12.0.0/24 dev veth10  proto kernel  scope link  src 10.12.0.100
	
	как видно когда vxlan.50 вставлен в бридж 
	то на него специально маршрут персонально заворачивающий на него трафик нернужен то ест ьненужен маршрут с параметром  dev vxlan.50 мол такой то
	трафик пускай через карту vxlan.50
	вместо этоого достаточно просто того маршрута что я указал.
	
	в итоге будет работать пинг  между 10.12.0.100 и 10.12.0.2 (адрес vxlan сети другого компа)
	
	# ping 10.12.0.2 
	
	как видно самому интерфейсу вообще по барабану внутренность ip
	пакета который в него придлетает. он просто преобразует все что в него
	прилетает - одевает сверху новый ip пакет и вставляем фиксирванный
	dst_ip=172.16.102.20 и src_ip=172.16.102.19
	
	остатется просто направтиь трафик на этот интерфейс.
	через маршрут в таблице маршрутизации
	либо через бридж
	
	так постпенно мы подходми к понимаю как работает фланнель
	
	
	получается кстати такой простецкий впн условно говоря.
	но штука в том что  в интернет его направялть нельзя
	только внутри площадки 
	так как он незашифрован.
	
	надо понять вот этот вот сервиф фланнеля который висит в кубернетесе
	он както сам поток обрабатывает или он просто нужен как автоматизатор.
	
	линукс почемуто теперь позволяет присваивать дурацкий ip
	интерфейсам. например ip = 10.5.98.0/32 какой смысл такого ip?
	он же вообще некорректен. да он некоректен но для линукса это типа норм ип. линукс не парится.
    для него это типа обычный юникаст адрес
	
	прикольно у нас получается.
	если  у нас в свич воткнуто два компа.
	один комп имеет 192.168.1.1
	вторйо комп имеет 192.168.2.1
	но походу пьесы связь между ними должна работаь на уровне ip
	так как на уровне mac они могут общаться без проблем.
	на первом компе типа надо написать
	ip route add 192.168.2.0/24 via 192.168.2.1 dev ens160 onlink
	и нахер
	
	// линукс bond из нескольких бриджей есть ли смысл//
	
	
	
	раскрыта тайна как сделать чтобы можно было связать через vxlan 
	несолкьо сервреов через юникаст.
	
	оказыется параметр remote в команде
	ip link vxlan
	это параметр для самого проестецкого случая. когда у нас два сервера.
	по факту это параметр вносит данные в другое место
	в forwarding таблицу для vxlan.
	так вот через ip link связь на есколько серверов никак не нстроить
	по юникасту.
	это делатеся  через другу команду которая позвяолеся напрмяму вносит
	данные в таблицу fdb для vxlan
	
	bridge fdb append to 00:17:42:8a:b4:05 dst 192.19.0.2 dev vxlan0
	
	или 
	
	bridge fdb append to 00:00:00:00:00:00 dst 192.19.0.2 dev vxlan0
	bridge fdb append to 00:00:00:00:00:00 dst 192.19.0.3 dev vxlan0
	
	эти две команды создадут два endpointa для vxlan.
	
	как  я понимаю ровно это иделает flannel.
	
	теперь vxlan знает что у него две точки . куда надо все слать
	
	вместо 00:00:00: можно прописать концено конкретные mac-и.
	нгасколь я понимаю если просиать нули то каждвый раз информация 
	будет шарашить сразу на два сервера  засирая сеть
	
	теперь походу понятно как работает фланнель.
	он накажом хосте просто правильно прописывает 
	flannel.1
	и
	bridge fdb add to 00:00:00:00:00:00 dst 192.19.0.2 dev vxlan0
	
	позволятет это елать прозарвачно ватоматично вместро руками работы
	только и всего.
	
	документацтия  у фланнеля на уровне ноль
	
	--
	я прочитал в статье что на каждом сервере ееный vtep клиент в процессе
	того что через него пролетают пакеты с фреймами изучает инфо и 
	записывает в fdb таблицу информацию вида 
	mac, vtep IP, vtep mac, vxlan id.
	
	как понимаю когда vtep незнает за каким vtep соседом находится вирталка
	с MAc он шлет пакет ко всем соседям. а вот когда знает то только тому
	за которым сидит виртуалка с MAc ( в общем такая же логика как и у свича обычного)
	
	----
	----
	!!! теперь надо для тех нод руками настроить vxlan
	
	имею три хоста с IP
	172.16.102.19
	172.16.102.20
	172.16.102.21
	
	создадим на них vtep клиенты L2 
	и назначим им IP.
	172.20.0.19/24
	172.20.0.20/24
	172.20.0.21/24
	
	
	
	host-(test-linux-01)
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.20  dstport 4789
	
	ens160 и 172.16.102.20 это карта через которую пакеты будут транзитться
	в физ сеть и какой src_ip будет у обертки от исходного пакета
	
	
	как видно я не использовал параметр remote потому что сейчас у нас будет
	ни оди сервер сосед а два соседа. прараметр remote здесь никак не поможет
	
	
	удаленные vtep соседи прописываются вот именно так
	
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.19 dev vxlan.50
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.21 dev vxlan.50
	
	append это опция добавить строчку для мак адреса который уже есть.
	работает только для vxlan и ряда других случаев.
	
	неважно какой на vxlan.50 mac у соседа неважно знаем мы его 
	заранее или нет мы всегда указыаем (непонятно почему) именно 00:00:00:00:00:00
	а dst ip указывается именно внешней карточки сервера
	
	
	fdb таблица. когда сквозь свич пролетает мак
	то свич смотрит его src mac и заносит в таблицу fdb
	мол из такого то порта прилетел  такой то mac
	теперь свич знает что фрейм с таким то маком надо слать 
	на такойто порт
	
	так вот по приколу в fdb вносится нетолько информация вида
	мак-порт
	в эту же таблицу вносится информация по vxlan настройкам.
	если в fdb внести мак=00-00-00-00-00-00 vxlan-id dst IP
	то линукс будет эту инфо использовать для доступа к VTEP соседу.
	и в отличие от remote в команде ip link таких соседов можно указать неодин
	а много.
	
	также я прочитал вот здесь https://vincent.bernat.ch/en/blog/2017-vxlan-linux
	что даже если я знаю заранее мак адреса от карточек vxlan
	соседей то все равно в fdb маки должны быть в виде 00-00-00-00-00-00
	
		для меня это еще вопрос почему мы неиспользуем mac от ens160
	
	для простоты добавим IP прям на vxlan.50
	
	# ip addr add 172.20.0.20/24 dev vxlan.50
	# ip link set vxlan.50 up
	
	
	
	host-(test-ansible)
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.19  dstport 4789
	
	
    # bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.20 dev vxlan.50
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.21 dev vxlan.50
	
	
	# ip addr add 172.20.0.19/24 dev vxlan.50
	# ip link set vxlan.50 up
	
	картчока vxlan это ничто иное как vtep клиент в терминологии 
	из доков.
	
	
	host-(test-linux-02)
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.21  dstport 4789
	
	
    # bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.19 dev vxlan.50
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.20 dev vxlan.50
	
	
	# ip addr add 172.20.0.21/24 dev vxlan.50
	# ip link set vxlan.50 up
	
	
	пинги между компами работают
	
	# ping 172.20.0.{19,20,21}
	
	----
	замечу что на дефолтовых линуксах чтобы раотал vpx lan
	ненужно актиививровать ip форвардинг.
	а вот на хостах с кубернеретесом нужно.
	
	--
	еще что необьяснимо
	если vpxlan это карта неважно l2 или l3 уровня то как 
	трафик влетая в эту карту потом магическим образом транзитируется в 
	ens160. если они несвязаны друг с другом по L2. 
	тогда надо хотя бы чтобы какойто veth тоннель был между ними или что.
	или маршрут. а так получается хрень. в одну влетел а потом оказался
	вдругой и вылетел. а между ens160 и vpxlan нет связи как это есть 
	у vethh пар.
	это херовая архитерктура
	
	---
	
	далее надо понять как  вкубренретенс фланнель настроек
	
	например вот этот странный ip у карты фланнеля
	
	10.252.0.0/32
	что за чушь
	
	4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 
    link/ether 0e:d8:dc:0e:7c:cb brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 1 local 172.16.102.31 dev ens160 srcport 0 0 dstport 8472 nolearning ageing 300
    inet 10.252.0.0/32 brd 10.252.0.0 scope global flannel.1
      
	  
	 также флаг nolearning - почему?
	 
	 также почему то vte клиенты вбиты 
	 
	 ~# bridge fdb show | grep flannel
22:1b:9f:bc:ef:45 dev flannel.1 dst 172.16.102.32 self permanent
12:f3:4d:91:f4:05 dev flannel.1 dst 172.16.102.33 self permanent
16:74:4c:0f:6f:6e dev flannel.1 dst 172.16.102.32 self permanent
d2:f7:5f:37:16:2b dev flannel.1 dst 172.16.102.33 self permanent

не в виде 00:00:00:00:00

	---
	
	про brctl
	
	как я понял в чем его минус.
	транзит пакетов между его портами идет не некоей общей шине а
	непосредственно через 1 порт brctl0 поэтому сумматрная проепускная способоость бриджа равна всего авсего скорости 1 порта brctl0
	
	про фланнель
	ка я понял точка в названии карты ничего незначтт это не алиас и ничего такого это ничо незначтт просто символ в имени поэтому 
	имя карты
	
	flannel.1 это просто имя карты.
	
	как я понял теерь в линуксе интрефейсу можно прсиваимтиь бессмысленный
	ip адрес не юникаст. например  
	
	ip=192.168.0.0\24 
	
	правда непонятно что с таким адресом линукс может делать
	 а так понка соображение что это просто адрес для справки
	 но именно такой дурацкий адрес имеет фланнель
	 
	 когда мы приаттачиваем контейнерв  в докере через docker networkds
	 то мы всего навсего меняем у контейнера сетевой неймспейс.
	 
	 а docker netowrk create bridge это просто напросто создатеся
	 brctl бридж в линуксе
	 
	 ----
	 
	надо будет вспомнить почему дофолтовыу куб не смог 
	успешно завести фланнель сеть что не было пингов
	ответ - на нодах был выключен айпи форвардинг
	
	
--
 
 
 

  FLANNEl.
  
 
начнем с фланнеля + докер. 
а потом уже расмотрим куб+фланнель+докер.

начнем просто с фланнеля.
написано что своим бекендом он может использовать VxLAN.
это ровно то что мы будем использовать для начала.

VxLAN использует как и по аналогии с VLAN неокторый идентификатор
VNI который отличает один Vx от другого.

Vx использует UDP. 
для UDP нужно указать dest IP. причем  я не понял если наш хост
сидит в одном VNI то ему нужно указыавть все хосты со всеми VNI
или только хосты с VNI на нашем хосте.

но наш случай самый простой - все хосты будут иметь один VNI.
ну и все равно возникает вопрос. вот у нас три хоста. один 
из хостов шлет пакет он что каждый раз слать на оба оставшихся хоста?.

как я понял нет не каждый раз.

каждый раз когда на хост прилетает vxalan хрень то комп запоминает 
mac адрес удаленного компа, его VNI , ну и может быть его внешний IP.
таки образом когда мы захотим отправить vxlan с таким же маком и таким
же VNI то мы пошлем сразу на тот один сервер.

пока на этом детали vxlan оставляем. 
на стороне свичей железных ничего настраивать ненужно.
для них полностью прозрачная технология.

нам только нужно будет указать все сервера кластера


root@test-kub-02:~#  nsenter --net=/proc/3148/ns/net ip -c -f  inet address show eth0
3: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default  link-netnsid 0
    inet 10.252.1.4/24 brd 10.252.1.255 scope global eth0
       valid_lft forever preferred_lft forever


возможно поды невиын на другх хостах
изза неактивации форвардинга.
а также надо в свойстваох iptables  в  правильных местах
поставить ACCEPT

вопрос - если кубернетес создает контейнеры через докер
то как он умудряетмся подключать контейенеры к сети друогго бриджа (не докер0 ) cni0 
и при этом cni0 его нет в docker networks

список какие порты сидят на бриждами

test-kub-02:~# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.5efab67e726a       no              veth6b8aeca2
                                                        vethde173048
docker0         8000.0242f0b9d9c1       no              veth45c266d


также в свойствах линка указано за каким бридждлм он сидит


7: vethde173048@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 46:98:0c:b8:67:d6 brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::4498:cff:feb8:67d6/64 scope link

master cni0

       valid_lft forever preferred_lft forever
9: veth45c266d@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default
    link/ether fa:24:30:ee:84:a1 brd ff:ff:ff:ff:ff:ff link-netnsid 2
    inet6 fe80::f824:30ff:feee:84a1/64 scope link

master docker0

containerd-shim  = это какой то спец процесс к каждому контейнеру что бы чтото там удобнее работало
на уровне процессов. надо уточнять


kubectl exec -ti kube-system etcd-test-kub-01 -- ps aux


для начала научиться чтоб связка работала докер+ фланнель
и тока потом пытться понять связку кубер+фланнель+докер

стаивм etcd
пототом фланнель



фланнель+докер заработал(прозрачная связь между подами между серверами 
работате)


надо записать всю установку и осознать как это работает от и до.
----------
чтобы vpxlan заработал надо такие вещи
соззать vpxlan интерфейс
в нем указать local ip . это адрес внешней карточки сервера.
он будет служить как src_ip когда исходные пакеты будут оборачиваться в
новый ip пакет.
надо указать адреса соседей которые тоже имеют vpxlan интерфейс.
указываются они либо через параметр remote в ip lab но это работает тлоько
если у нас vpxaln сосед один (их еще зовут vtep сосед)
если их несколько то надо указать сосейде через 

bridge fdb appnd 00:00:00...:00 ip-адрес-соседа

и нужно каким то образом заставить трафик втекать в vpxlan интерфейс.
например с помощью маршрута в табблице маршрутизации.например

10.252.2.0/24  dev vpxlan-if 

поток будет входит в vpxlan-if и оттуда прозрачно транзитироваться в выходную
физ карту ens160 и вылетатть в сеть преобразованный

настривать какую то L2 связь между vpxlan интерфейсом и физ интерфейсом ненужно


насколько я понимаю процесс выгдлядит так 
скажем до входа в vpxlan интерфейс пакет выглядит как

dst_ip=10.252.2.10 src_ip=10.252.2.1

когда поток попадает в vpxlan на него сверху одвется новый ip пакет с
dst_ip=(внешний ip адрес соседа)=192.168.1.20
src_ip=(внешний ip адрес нашего сервера)=192.168.1.10
внутри этого нвоого пакета сидит UDP с dest_port=4872


потом этот пакет просто всовывается во внешнюю физ карту которая с ним 
ничего неделает кроме как тупо пропускает через себя и выплевывает 
наружу в физ сеть.


когда пакет долетает до соседского компьютера он входит в физ карту.
комп вскрывает ip пакет видит внутри UDP с портом 4872 и он ничего неделает
с этим пакетом кроме как он тупо его транзитирует передает на внутреннюю
vpxlan карту.

внутреняя карта вскрывает внешний пакет и видит внутри пакет с
dst_ip=10.252.2.10 src_ip=10.252.2.1 
и она передает этот пакет уже внуьрь кишков компа

если мы направляеи поток на vpxlan карту и хотим указать 
гейтвей на стороне соседа то гейтвеемм можеть быть только походу
ip адрес vpxlan карты соседа.
хоть vpxlan карат и делает преобразование ip адресов когда поток проходит 
сквозь нее но она в тоже время об этом сама незнает. поэтому не может работать  в плане маршрутизаии с внешними ip сетями компа а толко внутренними
сетями компа. как то думаю так

если у нас поток вставляется в vpxlan карту с помощью свича то 
маршрут который бы вставлял поток в vpxlan карту писать ненужно. и 
тогда сама vpxlan карта необязана иметь IP адрес. пример


veth0(172.16.0.10)->veth1 L2-> bridge brctl0-> vpxlan -> физ карта(192.168.1.10) -> сеть->
->физ картадруого компа(192.168.1.2) -> vpxlan ->bridge->veth L2-> veth(172.16.0.2) 

чтоюы попасть с 172.16.10 на 172.16.0.2

нам достаточно иметь на исходном компе маршрут

172.16.0.0\24 dev veth0 src 172.16.0.10

как видно персонализированного маршрута вида 
IP dev vpxlan
который специально вставляет поток в карту vpxlan ненужно
получается поток в vpxlan не маршрутизируется а коммутируется.

посмотрим другой вариант.

в рамках свичинга в рамках L2 домена в компе есть такой тракт
контейнер veth0(172.16.0.10)---> veth1---> бридж docker0 (172.16.0.1) 

и отдельно в компе болтается вот такое
vpxlan (172.20.0.1) который ни с кем несвязан L2 доменом
отдельно также
физ карта ens160 (192.168.0.10) -> сеть -> физ карт другго компа ->vpxlan(172.20.0.10)

как мы уже знаем vpxlan несвязан L2 доменом или маршрутом с ens160
но каким то магическим образом пакеты входящие в vpxlan транзитируются
на ens160 и он их транзитиррует в сеть

как я понимаю если на компе ip форвардинг выключен то тогда 
пакеты с src_ip=172.16.0.10 dst=172.20.0.10 и гейтвей = 172.16.0.1
они дойдут до гейтвея и все . потому что дальше надо 
перебросить пакет с интерфейса docker0 на vpxlan 
а это есть форвардинг а он выключен

форвардинг это когда ядро перебрасывает пакет с карточки на карточку
которая не входит в dest сеть пакета

\\разобраться как работает форвардоинг на компе из 2 карт.
ест ли особая стрка маршрута связывающая две карты друг с другом

\\ расмотеть случай когда две карты каждя в одной сети
192ю168ю1ю10
192ю168ю1ю100
явяется бриджом то есть по обе стороны сеть 192ю168ю1ю0
и посмтреть при каком условии может комп с одной стороны пингвать 
комп с другойс стороны
посмотреть при прохождении компа его МАК фрейм адреса менаяются или нет

значит я сделал комп1 имеет две карты 
которые принадлежат одной IP сети

комп1 
ens160 = 172.16.102.20  
ens192 = 172.16.102.22

при этом ens160 воткнут в свич LAN
а ens190 воткнут в другой свич

в другой свич также воткнут комп2
ens160 = 172.16.102.21


lan switch - ens160 (комп1)  ens 192 -  another swicth - ens160(комп2)

карта второго компа такжепринадлежит той же IP сети.

что я делаю.

1) с компы 2 я пингую 172.16.102.1
при этом на компе 1 на ens192 я фиксирую arp запросы 
мол какой mac имеет комп 172.16.102.1
при этом на компе 1 на ens160 я этого запроса невижу.
тоесть по дефолту линукс нефорвардит фреймы с карточки на карточку.

и я понима сейчас почему.
по дефолту в линуксе порты имеют роль L3.
то есть это порты рутера. и неважно что два порта принадлежат
одной IP сети и неважно что два порта принадлежат одлному вилану.
главное что порты уровня L3 как у рутера. а порты L3 у рутера 
всегда работают вот как - в порт влетает фрейм. 
он сдирает обертку с фрейма и остается голый L3 пакет.

и прикол в том что данный пакет будет направлен в другой порт рутера
только если есть маршрут для этого пакета направляющий его в друго порт.

поэтому ARP влетая в порт нифига больше никуда невылетает.
развернуто все проанализируем. почему ARP хрень непередается на другой 
порт линукса.
что такое локальная сеть. 
что такое рутер. 
что такое порт такого то уровня
что такое протокол такого то уровня

локальная сеть - есть сетевые порты(сетевые карточки). каждая карточка 
имеет адрес. адрес именно карточки. его называют MAC адрес.условно говорят она его получает на 
заводе и его нельзя сменить. да щас это можно суть главного тут в том что
адрес адресует именно карточку устройство. далее мы берем группу таких 
портов и обьединяем их через некоторую проводящую среду. и карточки могут
друг  с другом связываться адресовываться на основе вот этих вот своих 
адресов карточных. это и есть локальная сеть.
получается если есть один порт с адресом и другой порт с адресом
и они немогут друг с другом связаться через свои адреса - значит 
они сидят уже в разных локальных сетях.
когда порт может связываться с другим портом используя свой карточный адрес
это назыавется связь на уровне L2 osi. и порты имеют уровень L2.
также это значит что софт который сидит который обеспечивает работу портов
он умеет обрабатывать поток на уровне L2, это значит что софт воспринимает 
поток как кусочки - фреймы. в которых обязан присуствовать как я понимаю
по крайней мере один MAC. (а чаще и два отправителя и получателя).
и вот порты они оперируют этими фреймами. они их получают, отправляют 
и переправляют с порта на порт. каждый фрейм идентифицируется наличием
MAC адреса. соотсвтеенно фрейм имеет формат вида MAC адрес часть еще 
какойто служебной информации и полезная нагрузка. софт который умеет
рабоать с такого рода фреймами - получать отправлять перенаправлять назщыается софт уровня L2. 
когда мы говорим про поток на уровне L2. это значит что поток состоит
из фреймов. каждый фрейм был испущен каким то портом и предназначен 
какому то порту. когда фрейм влетает в порт уровня L2 значит софт порта
понимает поток на уровне фреймов и задача софта  принять этот фрейм либо
игнорировать либо принять и переправить в другой порт.  это и есть основная работа и задача порта уровня L2 и его софта.
порт уровня L2 это сама железная часть и ее софт.

протокол уровня L2 это значит такой софт который помогает настроить 
передачу данных в локальной сети между портами. то есть задача софта уровня
L2 чтобы локальная сеть работала. он ей в этом помогает. главные тут слова локальная и работала. протокол уровня L2 также понимает что поток разбит на  фреймы то есть на  кусочки идентифицируемые наличием MAС адреса и полезных данных. 

протокол ARP относится к протолку уровня L2. он помогает найти в локальной 
сети найти адрес то есть MAC некоего порта который обладает специфицическими
характеристиками. мы называем ARP протоколу характеристику что мы ищем
а он нам возвращает MAC адрес такого порта. в локальной сети. 

порт уровня L3 автоматом также является портом L2.
потому что L3 использует функционал L2 как бекенд.

когда в порт уровня L3 влетает поток то софт порта вначале распознает в нем 
отдельные фреймы (уровень L2) берет этот фрейм. вскрывает его  и передает
софту уровня L3. софт L3 нахоит в переданной информации информацию уровня L3.
для софт уровня L2 эта информация просто набор бессмысленный нулей и единиц
как для русского англиская речь это просто набор звуков.

протокол уровня L3 позволяет вкладывать внутрь фреймов уровня L2 некую
полезную информацию которая позволяет обмениваться данными портам 
между локальными сетями. ключевое слово между.

если у нас коробка в которой два порта уровня L3 то 
формально у нас за каждым портом лежит своя локальная сеть 
а эта коробка нам должна позволить связать эти две локальные сети.

рутер это корробка у которой есть минимум два порта уровня L3.
и цель этой коробки обеспечить связь локальных сетей лежащих за 
этими портами. потому что напрямую то есть через MAC адрес порта в одной
сети никак не связать с портом который имеет MAC2 адрес в другой локальной
сети. рутер получается оновременно принаддлежит нескольким локальным сетям.

фишка софта L3 и протоколов уровня L3 что они позволяют назначить портам
адреса уровня L3 которые позволяют адресовать эти порты несмотря на то что
они лежат в разных L2 сетях. с точки зрения L3 при этом порты например
могут принадлежать одной L3 сети. на этом фишки уровня L3 конечно
не заканчиыаются 
 
если мы берем коробку и она имеет сетевые порты и эти порты типа заявлено
что они уровня L2 это значит что 
1) данные порты предназначены для связи с другими устройствами в рамках одной
локальной сети
2) данные порты могут обрабатывать поток на уровне фреймов основываясь 
на MAC адресах. основываясь на них они посылают поступающие фреймы в тот 
или иной порт или во все порты
основная задача коробки рассылать входящий поток в порты в те или иные
порты

в чем еще прикол рутера и его L3 портов.
когда фрейм из одной сети попадает внутрь рутера.
он вытаскивает данные из фрейма а служебную част фрейма выкидыает в мусор.
в том числе и MAC информацию содержащуюся во фрейме так как за его портами
лежат независимые локальные сети и MAC информация фрейма из одной сети
непригодна ни вкаком виде для адресации фрейма в другой сети.
далее рутер упаковывает извлеченную информацию в новый фрейм с новым MAC 
актуальным для другой сети и выплевывает новый фрейм с исходным payload
из старого фрейма в другую локальную сеть из другого своего порта. 
также рутер имеет понимание предназначалась ли информция из payload
для другой сети. если нет то он данные из этого фрейма никуда пересылать
небудет.

возвращаюсь к ARP и линуксу.
линукс обладает портами L3.
это значит что за каждой карточки формально сидит своя независимая 
L2 сеть. которые он обьединяет находясь одновременно в обоих. принадлежа обоим. в него через ens192 влетает фрейм внутри которого находится ARP payload. ARP как я уже сказал предназначен для поиска некоей  заданной 
сетевой карты в рамках локальной сети. поэтому ARP запрос не имеет 
никакого отношения ко второй локальной сети лежащей за ens160 за втрой картой.
поэтому ARP запросы по дефолту линуксом не перекидываются из ens192 на ens160.
они к сети ens160 не имеют никаого отношения.

напомню конфиг что выше

комп1 
ens160 = 172.16.102.20  
ens192 = 172.16.102.22

при этом ens160 воткнут в свич LAN
а ens190 воткнут в другой свич

в другой свич также воткнут комп2
ens160 = 172.16.102.21


lan switch - ens160 (комп1)  ens 192 -  another swicth - ens160(комп2)

у нас ens160 компа1 и ens160 компа2 хотя принадлежат одной IP сети ( сети уровня 3) но разным локальным сетям уровня L2 поэтому 
по дефолту связь между этими портами неработает.

чтобы она заработала нужно либо на компе 1 два порта уровня L3 
превратит в два порта уровня только L2 тогда это будет означать
что сети за этими портами являются одной L2 сетью. и тогда ARP запрос
от компа 2 пройдет сквозь комп1.
ибо я с компа2 пытаюсь пинговать комп2 условно говоря который сидит за компом1.

еще вариант нужно на компе2 и компе 3 прописать маршрут.
вопрос что делает линукс когда получает пакет у котрого ip 
принадлежит сети которая у этого линукса лежит не за 
одним портом а за несколькими.
также понятно что нужно будет активиать форвардинг на линуксе.
то есть передачу пакетов между портами линукса которые не предназначены
самому этому хосту линукса. я проверил этот вариант работает.



\\важные моменты за L3 портами лежат по орпделению разные L2 сети.
поэтому фреймы внутри которых лежат L2 протоколы не форвардятся не 
транзитятся между L3 портами
L2 протокол это протокол обеспечивающий или помогающий работаь передавать
данные между портами внутри локальной сети. фреймы с запросами в payload от протокола L2 одной локальной сети нельзя выпускать в другую локальную сеть так как они к ней неимеют никакого отношения. фреймы с запроами в payload от протокола L2 имеют отношение только к данной локальной сети. вне ее они не имеюи никакого смысла.типа никакого права там оказаться\\

 в целом это рельная плохая практика иметь два и более
 L3 интерфейсов на рутере из одной IP сети.
 потому что например когда пакет влетел в рутер и его надо выплюнуть
 в сеть от которой у нас на рутере два порта то непонятно из какого 
 порта выплевывать. скорее всего линукс возьмет первый в списке из двух.
 и через него плюнет. выяснят из какого он будет плевать я не буду
 ибо как сказал это неправильная конфтигурация рутера когда 
 два L3 порта сидят в одной ip сети. должно быть каждый L3 интерфейс
 в другой уникальнрой ip сети.
 а если надо несколько портов засунуть в одну L2 сеть к примеруто порты надо 
 обьединять в бридж бондинг или типа того.
 опять же это совсем другая ситацяи когда у нас 1 порт имеет несколько 
 ip из одной сети. потому что в этом случае нет неоднозначности. а есть 
 полная однозначность что за таким то портом  такая то сеть. 
 а за другим портом другая сеть.
 
 тогда по мне по хорошему если пакет влетел в vxlan интерфейс то 
 для того чтобы он после этого вылетел из ens160 надо чтобы на 
 линуксе был во первых маршрут для этого во вторых активирован
 форвардинг. потому что перебрасывание транзитно пакетов с интерфейса
 на интерфейс это точно форвардинг. а в лиунксе это работает "просто так"
 как кастомное правило. исключеие.
 
 
 








 



---------------



----------------------------
фланнель + кубернетес.

для начала я пытаюсь понять откуда фланнель
читает настройки из кубернетеса

в тупорылой документации от фланнеля они пишут
если фланнель запущен с параметром --kube-subnet-mgr то фланнель читает 
конфиг из своего локального файла /etc/kube-flannel/net-conf.json

если этот параметр при запуске демона фланнеля незадан то фланнель
читает параметры из etcd из его ключа /coreos.com/network/config
опять же если при старте фланнеля указать ключ --etcd-prefix то он будет 
читать из etcd из другого пути.

смотрим как кубернетес запустил фланнель

3983 это pid контейнера фланнеля на мастер ноде

# nsenter --mount=/proc/3983/ns/mnt ps aux
PID   USER     TIME  COMMAND
    1 root      3:32 /opt/bin/flanneld --ip-masq --kube-subnet-mgr

итак фланнель запущен с ключем --kube-subnet-mgr
значит он читает свой конфиг из файла /etc/kube-flannel/net-conf.json
смотрим что в этом файле

# nsenter --mount=/proc/3983/ns/mnt cat /etc/kube-flannel/net-conf.json
{
  "Network": "10.244.0.0/16",
  "Backend": {
    "Type": "vxlan"
  }
}

щас я не буду подробно описыать но этот файл понятное дело проброшен
с файловой системы хоста

теперь посмотрим в итоге какие параметры использует фланнель по факту 
в работе

/# nsenter --mount=/proc/3983/ns/mnt cat /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.252.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true

и тут я вижу совершенно непонятную херню.
что хотя ему задано чтобы он как сеть  использовал 10.244.0.0/16
он в качестве подсетей для хостов использует совершенно другую подсеть
10.252.0.1/24

совершенно ясно что 10.252.0.1/24 не является подсетью.
так что за херня? фланнелю указали какую сеть использовать 
а он откуда то с воздуха взял другую сеть и начал ее использовать.

кстати еще раз замечу что если ключ --kube-subnet-mgr неуказан 
то фланнель ищет конфиг на etcd. а если указан то на своей локальной
ФС. так вот у нас не указан значит он конфиг свой ищет на ФС.
так вот я сделал свою мануальную инсталляцию фланнеля на отдельной 
системе и там фланнель использует etcd для считывания конфига 
так вот при этом фланнель нетолько читает из etcd но и пишет туда данные.
вот какая фланнелю была задана сеть изначлаьно там
~# etcdctl get /coreos.com/network/config
{ "Network": "10.5.0.0/16", "Backend": {"Type": "vxlan"}}

а вот что фланнель потом уже сам дописал
# etcdctl ls /coreos.com/network/subnets
/coreos.com/network/subnets/10.5.29.0-24
/coreos.com/network/subnets/10.5.98.0-24

тоесть фланнель дописал какие подсети этой сети он будет использовать.

при этом еще замечу что фланнель написал что будет использовать две 
подсети но почему то тут он указывает только одну из них

~# cat /var/run/flannel/subnet.env
FLANNEL_NETWORK=10.5.0.0/16
FLANNEL_SUBNET=10.5.29.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false

фланнельо однозначно имеет дебильную неполную документацию. козлы
получается что sybnet.env неотображает все подсети которые фланнель
использует а только одну.


в случае же кубернетеса еще раз скажу фланнель запущен так 
чтобы он читал данные слокальной фс а не из etcd

это еще можно доказать и так что в кубернетесовом etcd 
нет никакого ключа по пути /coreos.com/network/config
и фланнель в контенере запущен без ключом --etcd-prefix который бы мог 
переопределить путь по которому читать конфиг из etcd кубернетеса.

витоге остается загадкой вот как так фланнель считал конфиг с сетью
но по факту использует другую подсеть
# nsenter --mount=/proc/3983/ns/mnt cat /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.252.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
 

откуда вцелом взялась сеть  10.252.0.1/24 понятно. я ее указал 
при создании мастер ноды 

# kubeadm init  --pod-network-cidr=10.252.0.0/16 --apiserver-advertise-address=172.16.102.31

но опять же я задал маску \16 а в фланнель написано \24
все указывает на то что фланнель использует FLANNEL_NETWORK=10.252.0.0/16
но абсолютно непонятно как из kubeadm параметр попал во фланнель

далее я полез в etcd кубернетеса 
и стал там искать эту сеть в какой переменной она прописана

прежде надо сказать что данные в etcd кубернетес 
хранит в некоем хитрожопом формате и нужно поставить прогу которая
позволит их декодировать.

# mkdir /tmp/1
# git clone https://github.com/jpbetz/auger
# cd auger
# make release
# cp ./release/build/auger /usr/local/bin
все декодировщик поставили

далее уже можно читать из etcd  на мастере

# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /registry/clusterrolebindings/flannel --prefix -w simple | auger decode


получить список всех ключей

# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /  --prefix --keys-only

далее. у меня не получилось с помощью auger декодировать сразу 
весь список переменных. 
получается только запрашивать конктерный ключ и смотреть его значение

далее. методом тыка я нашел где в etcd хранится 10.244.0.0/16 которые
фланнель читает но неиспользует

/registry/configmaps/kube-system/kube-flannel-cfg


# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /registry/configmaps/kube-system/kube-flannel-cfg  --prefix | auger decode

apiVersion: v1
data:
...
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
....

ну и вобщем 10.252.0.0/16 мы вбиваем в kubeadm.
я нашел где в кубадм хранится эта цифра

/# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /registry/configmaps/kube-system/kubeadm-config --prefix | auger decode

...
podSubnet: 10.252.0.0/16
...


в общем в итоге соверщенно непонятно откуда фланнель получает данные
чтоб пользовать сеть 10.252.0.0\16

но по факту на серверах в итоге именно фланнель создаеть
 интерфейс flannel.1
вот такой

на мастере
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP>
       inet 10.252.0.0/32 brd 10.252.0.0 scope global flannel.1

на дата ноде
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP>
       inet 10.252.1.0/32 brd 10.252.1.0 scope global flannel.1


а далее (это уже нефланнель а сам кубернетес) создается 
еще интерфейс cni0

на мастере
5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> 
    inet 10.252.0.1/24 brd 10.252.0.255 scope global cni0


на дата ноде
5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP>
    inet 10.252.1.1/24 brd 10.252.1.255 scope global cni0

это бридж интерфейс.
и именно к нему приконнекчены veth-ы от контейнеров.

таким образом на мастер ноде контейнеры созданные кубернетесом 
имеют ip из сети 10.252.0.0\24

контейнера созданные кубернетосом на дата ноде имеют ip из
сети  10.252.1.0\24

(почему я особо подчеркивают контейнера созданные кубернетесом потому что 
если на мастер ноде или дата ноде создать контейнер с помощью докера
то его контейнеры будут сидеть в другой сети за бриджм docker0 и 
будут иметь еще раз как я сказал другие ip из другой сети. но об этом потом)

так вот как работает проход пакета из контейнера на дата ноде 
к контейнеру на мастер ноде ( вы скажете на мастер ноде неожет быть контейнеров - нет это брехня там неможет быть пользовательских контейнеров
их туда куб недеплоит но на мастере прекрасно сидят системные контейнеры)

так вот как работает проход пакета с дата ноды в контенер на мастер ноде
например надата ноде сидит контейнер с 
ip(дата) =  10.252.1.8
а на мастере есть контейнер с ip
ip(мастер) =  10.252.0.21


сетевая карта контейнера if(дата) -> veth -> cni0  (это все идет
коммутация) дальше пакет попадаетв ядро.

и в ядре ядро смотритв таблицу маршрутизации а
там видит маршрут

10.252.0.0/24 via 10.252.0.0 dev flannel.1 onlink

который говорит трафик идущий к 10.252.0.0\24 пихай в карточку dev flannel.1

NAT в меконтейнрорной связи неучастует ! этого требует 
техзадание от куба

для передачи пингов от нашего контейнера до контейнера
в соседнем хосте на flannel.1 нужен IP  то есть нам недостаточно
L2 flannel.1 нам нужен именно L3 flannel.1 с IP.

щас разбереем почему

но прикол в том что на flannel.1 дебильный IP

почему он дебильный потому что он = 10.252.1.0/32

щас мы это тоже обсьудим

значит что нужно обязательно чтобы была связь
1) на flannel.1 должен быть IP
2) должен быть правильный маршрут
3) на компе должен быть прописан mac адрес от flannel.1 от 
соседнего сервера куда будем пинговать
4) еще раз отмечу что NAT никак ненужен никак неучаствует в процессе. ура!


если хотя бы один пункт отстуствует то связи между подами неработает

пример
интерфейс с дебильным IP
# ip -c a s dev flannel.1
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP>
    link/ether 62:33:33:4a:27:86 brd ff:ff:ff:ff:ff:ff
    inet 10.252.1.0/32 brd 10.252.1.0 scope global flannel.1

маршрут    
# ip r l dev flannel.1
10.252.0.0/24 via 10.252.0.0 onlink
гейтвеем указан дебильный ip адрес flanneля соседнего сервера.
мало того что тут указан дебильный ip адрес.
так еще важно то что 
у одного соседа flannel.1=10.252.0.0/32
у второго соседа flannel.1=10.252.2.0/32

комп а
контейнер ip = A

комп б
контейнер ip = б

контейнер б  видит пинги от контейнара а как src_ip= A
это понятно птому что котейнер А имеет одну карту и он посылает пинг с нее
поэтому когда пакет долетает до falnnel.1 то у пактеа у же есть src_ip
поэтму линуксу ненужно решать как src ip посдавтиь в пакет.
 если же мы пингуем контейнер на сосденем сревререерер просто из консоли 
баша на сервререе то линуксу надо решить а как src ip подставить. так вот 
линукс пощствляелт src_ip=адрес flannel.1 нащшего сервера.
поэтому контейнер б увидит пинг с консоли баш с src_ip=10.252.1.0
.. да уж тот самый деьильный ip адрес.

 
 вопрос вот в чем енпонятно как работает связь со вторым сервером 
 сосдеом
 
 у второго соседа flannel.1=10.252.2.0/32
 
 у нас интерфей фланнеля невставлен в бридж поэтому в него поток 
 нужно засоывать чреез маршрут но это еще пол дела 
 нужно в маршруте указывать гейтвей. потому что 
 наодном хосте поды сидят в 10.252.0.0\24
 на втором хосте поды сидят в 10.252.1.0\24
 на третьртье хосте поды содтят в 10.252.2.0\24
 поэтому чтобы пакеты ходили между этими подами где то на пути 
 нужна маршрутизация. поэтому в маршрутек flannel.1 нужно указать гейтвей.
 
 тепреь вопрос .... аааа я понял
 
 для кажого соседа свой маршрут
 
 к одному соседу
 10.252.0.0/24 via 10.252.0.0 onlink

ко второйму сосдеу
10.252.2.0/24 via 10.252.2.0 dev flannel.1 onlink 
 
 сотвтетсвенно получается что гейтвеем как я уже сказал указан 
 ip от flannel.1 на соседе.
 и надо еще прописать для кждого такого гейтвея статистческий MAC
 
 
 вот так одбдодлвятеся стат MAc адрес
 
 # ip neighbor add 10.252.2.0 dev flannel.1 lladdr 4e:4e:0c:ee:ca:3b
 
 а чрез arp команду так нифига несделать
 
 
 итак сетевой тракт от одного пода на первом мерверере до дроугог
 пода на другом сереререр выгляждит так
 
 iptables в этом неучаствует никак и это отлично
 
 еще замечу что vxplan - vxplan между среверами канал 
 это канал уровня L2 в целом поэтому весь L2 трафик L2 прротоколы запросы
 от одного сервера прилетят на другой сервер. типа vpxlan это вирт свитч
 между серверами. 
 но конкретно в нашем случае kuberntes  делает vpxlan канал как L3.
 это значит что между серверами пролетает только L3 трафик но не L2.
 тоесть только содержимое IP пакета долетает от сервера до сервреа.
 L2 трафик непролетает. это птому что flannel.1 (vpxlan интерфейс 
 на серверах невсталвен нив какой свич. поэтому трафик на них нужно рутить
 а не коммутировать)
 
 значит что я выяснил.
 
 если я удалил ip с flannel.1 то удаляется и маршрут и 
 стат мак адрес для соседа.
 
 так вот как нужно это воостатанавливать.
 
 первое = надо назначить ip для flannel.1
 второе  = нужно прописатат стат ip адрес от flannel.1 соседа
 третье = прописать маршрут  к flannel.1 соседа
 
 что существенно если поменять пункты три и два
 то после доавбдлениямаршррута кога будешь прбовать происать стат MAC 
 система выдаст ошибку.
 
 поэтому важно вначале пррописать стат MAC адрес. и толко потом 
 прописывтаь маршрут.
 
 также я проверил если прописать маршрут и не прописать стат мак адрес
 то ичего работать небудет. система сама неможет найти MAC адрес 
 соседа.
 
 
 так вот как выгляидит тракт от пода до пода
 
 карта пода L3 (src 10.252.1.11/24 dst 10.252.2.3)-> vethL2-> cni0 ->  
 
 самое прикольно что сеть на поде \24
 так что когда он стучит на 10.252.2.3 тодля него это нелокльая сеть
 а другая сеть в которую он лезет чере прописанный на поде 
 маршррут
 
 default via 10.252.1.1 dev eth0
 
 10.252.1.1 = это адрес cni0
 
 а я то думал что на всех подах сеть \16 и под лезет через коммутацию
 безгейтвея думая что тот под лежит с ним водной LAn сети
 
 ан нет.
 
 получаетс поды сидят в одной сети только в рамкх сервера.
 а на другом сервере побы в другой сети.
 
 едиснвтенное в чем есть плюс это то что в связи неучаствует iptables и\или NAT
 
 
 пакет долетает до cni0 как гейтвей. 
 далее пакет оказывется в ядре линукса хоста
 
 хост смотрим в таблицу рутов
 и находит
 10.252.2.0/24 via 10.252.2.0 dev flannel.1 onlink
  

ядро пихает пакет в flanel.1  
одевает его во фрейм в котором dst MAC=MAC карты фланннеля на той стороне
ибо он указан гейтвеем в маршруте

далее flannel.1 одевает сверху еще один IP пакет в котром
dst_ip= внешний IP соседа, src_ip= внешний ip нашего сервера

и пакет пихается в ens160 а тот его без имзенееий просто выплеквыет в сеть

рисую

->cni0->ядро ->flannel.1->ens160-> сеть -> ens160 того компа -> flannel.1

на том компе пакет прилает и ens160 его тупо передает на flannel.1
 
 flannel.1 того компа снимает внешний ip пакет
 и видит что 
 
 dst_ip=пода src_ip=пода нашего компа dst_MAC=MAC фланнель
 
 далее на том компе ядро смотрит маршрут
 
 10.252.2.0/24 dev cni0  proto kernel  scope link  src 10.252.2.1
 
 тоесть ядро видит что этот пакет нужно всунуть в cno0
 рисуюб а тот его передает через коммутацию в под
  
 ->flannel.1->cni0-> pod
 
 походу наконец выяснено как устроена бекенд связи в кубернетесе 
 между подами
 
 поучется это классическая марушттизация, софт бриджи, vpxlan, стат MAC
 
 готово
 
 
 
 -----
 бльше всего выывает вопрос зачем фланнель.1 было давать IP такой страный и урщдодский а не обычный IP unicast
 
 прчием все эти 
 10.252.0.0
 10.252.1.0
 10.252.1.0
 

они  спокойно пиннгуются линуксом

# ping 10.252.0.0
# ping 10.252.1.1
# ping 10.252.3.0

как вообще линукс такоедопускаетт ??
============

далее нужно понять откуда берутся IP для сервисов и деплойментов
==
следущий этап попробвать устанвоить эалстик
которая упирется в helm и persiustent volume

===




статический MAC от flannel.1 на том конце (у соседнего сервера)
# ip neighbor show
10.252.0.0 dev flannel.1 lladdr 0e:d8:dc:0e:7c:cb PERMANENT


arp команду нужно забыть. она старая тупая. она многочего неоможет
она имеет дебилное описание вместо нее испольуем ip neigh....


удаляем его потом ыптвется восстановить связь


arp кэш на компе удаляется совсем не компндйр arp 
а

ip -s -s neigh flush all

--||
 ip route add 10.252.0.0/24 via 10.252.0.0 dev flannel.1 onlink

~# arp
Address     HWtype  HWaddress           Flags Mask       Iface
10.252.0.0  ether   0e:d8:dc:0e:7c:cb    CM              flannel.1



настроить самому руками связь между 3 серверами через vpxlan без фланнеля
с использованием дебильным IP адресов для vpxlan интерфейсов.
убедиться что сам по себе сервис фланнеля не играет никакой роли











фланнель мы ставим на мастер 
а потом куб сам ставит фланнель на дата ноды коогда мы их джойним
к кубу
таким образом фланнель пристуствует на всех нодах

как работает фланнель.
настройки он читает из etcd
там всего лищь настройки

из настроек он считывает вобщемто только сеть

(считать ее из живого etcd кубернетеса)

обычно там есть с префиксом 16

далее фланнель для каждого хоста из этой сети выделяет подлсеть размером \24
пример

далее фланнель на хосте создает vpxlan интерфейс с названием flannel.1

в этом интерфейсе он указыать local IP , транзитный dev
и с помощью bridge fdb фланнель прописывает адрес удаленных vtep 
соседей

странный IP на flannel,1

и посмтреть как пакеты направлвяются в фланнель


-
IP сети для сервисов и деплойментов
--

 (кстати прикол если контейнер создан через сам docker run то связь с ним
идет через процесс docker-proxy тоесть по другому чем когда контейнер
созда кубом разобрать потом)

===
 виртуалки test-etcdflannel-01
 test-etcdflannel-02
 
 на первой я стер etcd
 
 на оборих поменял ip
 
 так что связка etcd _ flannel сломаоась
 
 починить
  и сделтаь описанип подробное
  
  ==
  
записи в таблице маршрутизации
  
10.254.0.0/24 via 10.254.0.0 dev flannel.1 onlink
10.254.1.0/24 via 10.254.1.0 dev flannel.1 onlink
10.254.2.0/24 via 10.254.2.0 dev flannel.1 onlink
10.254.3.0/24 via 10.254.3.0 dev flannel.1 onlink
10.254.4.0/24 via 10.254.4.0 dev flannel.1 onlink
10.254.6.0/24 dev cni0  proto kernel  scope link  src 10.254.6.1

  
получается что на dst ip 10.254.6.0/24 связь идет через L3 порт cni0 но без гейтвея. в порт cni0 выплевывается фрейм. если в маршруте непрописан гейтвей что это значит. 
когда рутер хочет выплунуть за порт ip пакет его нужно при этом облечь снаружи во фрейм и указать dst MAC. какой dst mac указать? если гейтвей в маршруте не прописан то рутер шлет через этот порт запрос arp  с указанием dst ip из ip  пакета. если полуает из сети ответ то он облекает
ip пакет в dst MAC адрес и тогда уже плюет в этот порт. значит
при таком маршруте мы можем связаться за портом cni0 с компами которые 
лежат втой же ip сети что и 10.254.6.0/24 тоесть маршруте без гейтвея
позволяет связаться через наш порт с компами которые лежат в той же локалке что и сам порт. 

10.254.0.0/24 via 10.254.0.0 dev flannel.1 onlink

если в маршруте указан гейтвей то это значит что наш dst ip 10.254.0.0/24
находится не втойже локальной сети что компы за портом flannel.1 
наш ip находится где то гораздо дальше. но нам же нужно какойто MAC указать перед тем как плевать пакет в сеть. и будет послан запрос 
arp который спросит какой MAC у IP гейтвея 10.254.0.0

конкретно этот маршрут малек выглядит непривычно потому что обычно dst_ip
и гейтвей лежат в разных сетях.
например

8.8.8.8 via 192.168.1.1 dev ens160

но вот эта строчка 10.254.0.0/24 via 10.254.0.0 dev flannel.1 onlink
она просто значит что порт наш имеет ip который невходит в сеть 10.254.0.0

например наш порт flannel.1 = 192.168.1.1
и он сидит в одном броадкаст домене с портом с ip = 10.254.0.0
и как я уже писал как то ранее стало уже давно очевидно что два порта
сидящих не в одной ip сети вообще то неимеют никакой проблемы отослать
друг другу фрейм. и таким образом передать друг другу ip пакет.

КАК  походу работает kube-proxy
как получается что мы сидя на хосте Б пингуем сокет нашей локальной
сетевой карты а в итоге попадаем на под на другом хосте А ?

во первых вопрос . у нас на компе локалный ip = 192.168.1.57
мы пингуем 
> ping 192.168.1.57
вопрос какой src ip у нашего пинга. ответ = такой же. тоесть

src_ip=192.168.1.57 и dst_ip=192.168.1.57


пусть у нас на компе А есть сервис который работает 
на порту 30890.
тогда на компе Б куб запускает сервис куб-прокси и засталяет его 
начать слушать все карты на порту тоже 30890.
когда мы постучим на локальный ip нашего компа и порт 30890
к примеру наш комп имеет ip = 192.168.1.57
тогда мы стучим на dst_ip = 192.168.1.57:30890 этот запрос поступает на куб-прокси и он как я понял  делает новый пакет у которого
src_ip=192.168.1.57 а dst_ip вот тут интересно.

вот у нас есть под и он входит в состав сервиса.
ip сервиса:port1 --> ip пода:port2

куб делает так что он на каждой ноде прописывает одни и теже iptables rules
в которых прописано что когда у пакета dst_ip = ip сервиса :port1
то скорей всего ( прям так детально пока праивла iptabled наразбирал) сделай подмену dst ip и замени у пакета ip сервиса :port1 на 
ip пода:port2 .
получается внутри хоста где сидит под все работает отлично.
когда мы непросто поднимаем сервис а еще указываем порт на хосте.
то тогда мы попадаем на сервис нетлько кгда обрщаемся на ip сервиса
но и на лббой ip хоста. тогда цепочка выглядит так

ip хоста = 192.168.1.56:30890
ip сервиса = 10.113.20.16:9300
ip пода = 10.254.5.17:9300

тогда происходит несклоько преобразований dstIP

src_ip=192.168.1.56  dst_ip 192.168.1.56:30890 --> 10.113.20.16:9300 -> 10.254.5.17:9300

получается обратный пакет после все преобразований обратно уже прям когда мы его будем получать будет иметь

src_ip=192.168.1.56 dst_ip=192.168.1.56

это я к чему если у нас на компе есть iproute2 кастомная таблица маршрутизации котрая учитывает src_ip то она обязательно будет играть 
рояль и при прохождении пакета и туда и обратно. в этой таблице получается поскольку она другая чем общая таблица то в ней должны быть 
маршруты на flannel интерфейс. кпримеру. иначе после iptables преобразований она небудет знать куда ей выплеывать пакет с dst_ip=10.254.5.17 ведь ей его нужно выплнуть не в дефолтовый гейтвей
а в интерфейс фланнеля да еще через определенны гейтвей

10.254.0.0/24 via 10.254.0.0 dev flannel.1 onlink
10.254.1.0/24 via 10.254.1.0 dev flannel.1 onlink
10.254.2.0/24 via 10.254.2.0 dev flannel.1 onlink
10.254.3.0/24 via 10.254.3.0 dev flannel.1 onlink
10.254.4.0/24 via 10.254.4.0 dev flannel.1 onlink
10.254.5.0/24 via 10.254.5.0 dev flannel.1 onlink
10.254.6.0/24 dev cni0  proto kernel  scope link  src 10.254.6.1

потому что  связь с подами этого хоста идет во первых через cni0
а во вторых без гейтвея. окей насоклько я понимаю если мы без гейтвейя плюнем пакет в flannel.1 то они через L2 увязаны с cni0 и фрейм дойдет до
пода на нашем хосте. arp реквест для выясннеия MAC для dst IP пройдет.
а вот если под на другом хосте то arp запрос ничего не принемет поомму что
связь с другими подами на других хостах идет через уже L3 фланнель гейтвеи на каждом хосте. тоесть для того чтобы пакет долетел до 
пода нашего хоста хвати записи

10.254.6.0/24 dev cni0  proto kernel  scope link  src 10.254.6.1
либо такой
10.254.0.0/16 dev flannel.1 onlink

а вот чтобы долетть до пода 10.254.5.4 на другом хосте уже такой записи нехватит а нужна запись прям конкретрная
10.254.5.0/24 via 10.254.5.0 dev flannel.1 onlink




это все когда мы сидим на одном хосте с подом.

если мы сидим на другом хосте 
там прописаны почти такие же правила iptables и они делают почтти такое же преобразовние


dst_ip 192.168.1.57:30890 --> 10.113.20.16:9300 -> 10.254.5.17:9300

где 1.57 это ip другого хоста. значит когда стал понятен 
реальный конечный dst_ip 10.254.5.17 то в пакет все еще нахроится на том 
другом хосте. и в дело вступает таблица мрашрутизации

10.254.0.0/24 via 10.254.0.0 dev flannel.1 onlink
10.254.1.0/24 via 10.254.1.0 dev flannel.1 onlink
10.254.2.0/24 via 10.254.2.0 dev flannel.1 onlink
10.254.3.0/24 via 10.254.3.0 dev flannel.1 onlink
10.254.4.0/24 via 10.254.4.0 dev flannel.1 onlink
10.254.5.0/24 via 10.254.5.0 dev flannel.1 onlink
10.254.6.0/24 dev cni0  proto kernel  scope link  src 10.254.6.1

она говорит что в сеть 10.254.5.0/24 надо лезть через гейтвей 10.254.5.0
а гейтвей 10.254.5.0 как раз находится на исходном хосте где крутится 
под. попадание в на гейтвей 10.254.5.0 обеспечивает механизм vxlan фланнеля. тоесть перед вылетом с нашего второго компа ip пакет выглядит как

src_ip=192.168.1.57 dst_ip=10.254.5.17
далее он сверху запаковыается в совершенно другой ip пакет у которого
src_ip = внешний ip хоста Б dst_ip= внешний Ip хоста А

и в таком виде он летит через физ сеть между хостами.
там когда он прилетает драйвер vxlan все это сдирает 
 и внутрь уже поступает исходный ip пакет 
 src_ip=192.168.1.57 dst_ip=10.254.5.17 
 который благополучно и долетает до пода.
 
 ...
 ===
 
 походу вот как работает сеть в кубе.
 
 пусть на хосте есть  две физ карты сетевые.
 
 на первой карте ens160 стоит ip куба и настроена фланнель сеть.
 на второй карте ens192 есть некий ip2.
 
 вот я взял под на этом хосте. создал для него сервис на этом хосте. этот сервис  прокинул порт аж на хост сервера. на его внешние ip и ip2.
 
 и скажем прога которая крутится в поде она обращатся к другому поду
 на другом хосте через внешний ip2 того хоста + порт хоста.
 так вот.  на самом деле если коннект иниициирован самим подом 
 то связи между подами через карты ens192 небудет. хотя они и обращаются друг к другу через ip2 который на ens192.
 
 вместо этого праивилами iptables dst_ip буудет заменен на внутренний 
 ip пода соседа. а это автомаом привдеет к тому что связь между подамти 
 пойдет через фланнель порт а значит через карту ens160.
 
 таким образом вторая карта будет реально работать только для коннектов
 которые иниицированы снаружи и хост инициатора не принадлежит куб кластеру. вот так.
 в этом плане получается самое выгодное это связать все сетевые карты 
 что есть на сервере в единый L2 бондинг. ну и назначать тогда это вирт карте уже ip из разных сетей что нужны. а не разделяь ip по разным физ картам.
 
  я еще раз помотрел на графики. как я понимаю все же там система такая.
  вот у нас есть поды которые имеют проброшенные наружу на хосты порты.
  и в настройках прилоожения которое крутится на поде в качестве его 
  контакта указан ip хоста и порт хоста. так вот если общаются два пода
  которые лежат оба на хостах текущего куба то они даже если обращаются друг к дружке через Ip:port хоста все равно будут общаться через vxlan канал и vxlan карточку. vxlan карточка биндится ровно к той же карточке на котором сидит ip адрес по которому хост идентицифироуется на апи сервере. 
    а если общается два пода которые лежат в разных кластерах куба 
    или один лежит в кластере куба а второй просто во вне на физ сервере то в этом случае как я понимаю исопльзуется уже не vxlan канал а та вторая карточка сетевая ( у меня их две на сервере). 
  таким образом становтся понятно когда в строй вступает карточка с vxlan а когда доп карточка.
   в целом я по прежнему считаю что и быстрым и легким по архитеткурре решением является обьедеенить все сет карты сервера в l2 бондинг и вешать все ip адрес на эту вирт сет карточку нежели чем разбрасывать ip адреса по отделльным физ карточкам.   
 
 
 ==
 значит такой момент выяснился. 
 вот у меня под на хосте. и этот под проброшен наружу на порт хоста
 через NodePort сервис.
 
 далее получается я могу обратиться к этому поду послав запрос на внешний IP любого хоста. но важно понимать как это работает что при этом происходит.  если мы обратимся по ip хоста на котором под непосредственно щас сидит то вобщем то говоря пакет тут же попадет прямо в под. 
  а если мы обратимся на ip хоста на котором пода нет то службу куб-прокси + iptables правила запроксируют наш пакет он войдет в этот хост. далее у него будет подменен dst ip адрес и он из этого хоста 
  через vxlan интерфейс будет напрален на тот хост где непосредственно сидит под. с точки зрения внешней сети это будет выглядет так
  
  мы ---> хост Б ---> хост А (под)
  
  и ответ будет идти обратно точно таким же образом
  
  мы <--- хост Б <--- хост А (под)
  
  почему я уверен что ответ от хоста А непойдет сразу на клиент 
  потому что это плохая схема я уже с ней сталкивался она часто неработает
  поэтому обартный пакет пойдет в точности через все инстанции.
  опять же это совпадает  с определением проксирования. что мы обратились за информаций к хостуБ значит именно он и должен в конечном
  итоге нам выдать ответ.
  
  в чем жопа второго вариант это то что получается что мы перегружаем
  сетевую карту хоста Б ненужным ей трафиком плюс непонятно насколько трудлоемкий процесс vxlan трансоформаций.  в общем процесс похож на досту  к компу через гейтвей.
  
  какой из этого вывод - по возможности круто если мы знаем на каком хосте сидит под и обращаемся снаружи сразу на этот хост. 
  тогда сетевйо трафик напрягает куб значитаельно меньше.
  
  это я рассмотрел ситуацию когда у нас наружу проброшен сервис а у этого сервиса всего один под. а если подов несколько и они сидяти на разных хостах тогда как. тогда наверно нужно делать вот что.
  снаружи указываем все хосты куба. со стороны куба публикуем поды сервиса на всех хостах. и  на кубе можно такую настройку сделать чтобы 
  если прилетел запрос на хост то он его передает только на под этого сервиса который сидит только на этом хосте. поэтому если у нас на кажом хосте есть этот под то на какой бы хост мы непостучали мы тут же попадем на под на этом хосте и нам через vxlan лезть на другой под на другом хосте непридется. ну не нам точнее а системе проксировать непридется.
   в кубе эта фича наывается service topology
   https://kubernetes.io/docs/concepts/services-networking/service-topology/#only-node-local-endpoints
   
   работает она по метка которые мы зададим по которым куб будет решать на какой бекенд под сервиса ему направлять поступивший из вне трафик.
   выхолощенный упрощенный вариант  service topology  реализован 
   другой фичей 
	externalTrafficPolicy=Local
   если ее активировать то трафик пришедший на ноду будет направлен 
   только внутрь ноды на поды что там и категорически небудет перенаправлен на другую ноду. 
    service topology в этом плане гибче. на ней можно задать что мол 
	в первую очередь пробуй направлять трафик на под этого сервиса лежащий на этой ноде но если его там нет тоогда направляй на другой хост. это более резилиент вариант.

 если мы запрос послали на хост на котором лежит под который нам нужен 
 то vxlan при этом незадействвуейтся а это очень хорошо.
 
 если у нас на каждом хосте есть бекенд под сервиса тогда отлично.
 а если на части хостов есть а на части нет. как тогда красиво 
 трафик направляь только на нужные ноды?... 
  если хост проксирует через себя приешдший трафик и направляет на другой
  хост то при этом расхоуется впустую часть срупута карточки этого сервера и часть его цпу. и получается схема доступа как через гейтвей.
  это плохо.
    на вскидку как еще можно решать вопрос когда у нас ненавсех дата нодах лежат бекенд поды сервиса. типа можно на поде заставляеь его наружу сообщать хост на котором он сидит в какой нибудь zookeper а он тогдда будет делать настройки динамически типа на хапрокси . и клиент тогда будет лазить на сервис куба через  хапрокси.
 когда у нас у сервиса один под то тут легко - можно и руками задать.
когда на каждом хосте лежит бекенд под сервиса можно заюзать  externalTrafficPolicy=Local или service toopology. самый неудобный вариант кода нужные бекенд поды сервиса на части хостов есть а на части их нет.


  --
типичная картина загрузки одного хоста под эластиком.
некий обычный бейслайн
кластер из 5 физ хостов.

загрузка одного хоста
цпу 20-24%
сет карточка на которой vxlan
transmit rate ~ 0
receive rate = 1.5MB\s  (ктото что то от нас по vxlan читает)

сет карточка на которую приходят клиенсткие запросы
receive rate = 3.5 MB\s  ( связана с записью в эластик)
transmit rate = 8-12 MB\s (связано с чьением из индекса )
--
получается что при такой нагрузке если направлять запросы на хост на котором бекенд под сервиса нележит то можно выжрать 50MB\s срупута
карточки на хосте тольк за счет того что она будет перенаправлять проксировать через себя трафик для подов на других нодах. что жопа
--
что еще обнаружил так бывает что в сфере карта на графике назыаетмя vmnic0
но в в самой виртуалке это не первая карточка ens160 а наоборот вторая ens192.
--

  

  
  