| iptables
| bridge
| docker

здесь я в одном месте еще раз затрону тему такой ебалы
ка софт свич он же линукс бридж. потому что этот продукт
имеет ряд подьебок. и расмотрю его работу в связке  с iptables.


значит тут я еще раз коснусь проекта linux bridge. 
по факту это свич. железный бридж от железного свича отличается
тем что в свиче больше портов, также в свиче более мощное железо.
вот и вся разница. 

линукс бридж это софт бридж. тоесть L2 фреймы обраюатывает 
центральный процессор а не специализированные железки. 
еще они пишут что линукс бридж софт умеет побольше всяких фишек
делать чем железный свич. они пишут что линукс бридж (далее просто 
бридж) может делать файрволлинг и шейпинг на уровне L2.

вот страница с докой от бриджа - https://wiki.linuxfoundation.org/networking/bridge

что такое свич. это когда у нас куча L2 карточек на компах 
установлена. мы их все втыкаем в порты свича. и теперь зная MAC 
адрес любого компа можно с другого компа до этой карты достучаться
через свич. прямая связь между двумя сет картами двух компов на 
уровне L2. если у нас есть  одна какая то L2 сеть. и другая L2 сеть
состоящая из сетевых карточек на компах. и мы обе эти две сети 
втыкаем в свич то теперь компы этих двух сетей могут друг с другом
связаться. для этого нужно всего навсего знать MAC адрес компа 
в той сети в которой ты хочешь связватся.

свич в линуксе выгляди так. мы говорим компу создай свич(софт бридж)

# apt-get install bridge-utils
# brctl addbr bridge4

комп где то в своем ядре создает вирт свич. мы его не видим 
не чуствуем. но в сет неймспейсе хоста он создает новый NIC bridge4
тип этого интерфейса в терминологии линукса равен bridge. как
я понимаю это значит что данный порт ведет на вирт бридж ( а не 
в физ сеть например). кстати вот сколько "типов" сетевых портов
 в линуксе
   # man ip-link
  TYPE := [ bridge | bond | can | dummy | hsr | ifb | ipoib | macvlan | macvtap | vcan | vxcan | veth | vlan | vxlan | ip6tnl | ipip | sit | gre | gretap | erspan | ip6gre | ip6gretap | ip6erspan | vti | nlmon | ipvlan | lowpan | geneve | vrf | macsec ]


что при этом непонятно - это к какому типу относится обычный физ 
ethernet порт. (кстаи veth переводится как virtual ethernet)

как найти все сет карты у которых тип bridge

# ip -c link sh type bridge

как визуально распознать что сет интерфейс имеет тип bridge
помогает ключ -d

$ ip -c -d l sh docker0
8: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 02:42:69:49:5f:9f brd ff:ff:ff:ff:ff:ff promiscuity 0 
    bridge forward_delay

и вот в строке под link ищем слово bridge.
не путать со словом bridge_slave, 


например

$ ip -c -d l sh virbr0-nic
5: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN mode DEFAULT group default qlen 1000
    link/ether 52:54:00:a5:01:fd brd ff:ff:ff:ff:ff:ff promiscuity 1 
    tun 
    bridge_slave state 


bridge_slave означает  что этот интерфейс тоже смотрит в софт бридж

но порт который bridge_slave он отличается от порта который bridge.

порты которые bridge_slave они воткнуты в софт бридж, втыкаются 
они вот так.
создаю для примера dummy interface

# ip -c link add vasya1 type dummy

теперь втыкаю этот интерфейс в свич
# brctl addif bridge4 vasya1 

# brctl show bridge4
bridge name	  bridge id		     STP enabled	interfaces
bridge4		  8000.9e4d5aa4936b	      no		vasya1

проверяю что у порта vasya1 появился флаг bridge_slave

# ip -c -d link  sh dev vasya1
73: vasya1: <BROADCAST,NOARP> mtu 1500 qdisc noop master bridge4 state DOWN mode DEFAULT group default qlen 1000
    link/ether 1e:e5:b8:93:bc:87 brd ff:ff:ff:ff:ff:ff promiscuity 1 minmtu 0 maxmtu 0 
    dummy 
    bridge_slave state ...

так вот мы когда создаем софт бридж то у нас у него есть два типа портов.
один порт он как бы характеризует создаваемый свич. в нашем случае
это порт bridge4 он появляется при создании свича. этот порт 
создается в сет. стеке хоста и он ведет в вирт свич (по вирт проводу),
тоесть пакеты из вирт свича попадают в сет стек хоста по этому порту,
и вот этот порт имеет флаг bridge.

картинка

    bridge4--------------<порт>(свич) 
[сет стек хоста]       [другой сет стек]

как можно увидеть что порт bridge4 он втыкается в некий безымянный
L2 порт свича. мы этот порт никак не видим не чуствуем. но он есть.
почему я упомнянул про этот безымянный порт. это станет ясно позже.
повторю этот порт мы никак не чуствуем не ощущаем. 
такова ситуация после создания свича, и я обьяснил какую роль 
играет порт bridge4 при этом. но у свича есть еще другие порты.
они добавляются после создания свича. добавляются этой командой


# brctl addif bridge4 vasya1 

и когда этот порт мы добавляем в свич то этот порт становится L2
более того он теряет свой mac и свой IP. эта карта становится 
еще одним свичевым портом который ни мак адреса не имеет ни IP.


                          процессы хоста
                              ||
                   |          ||         |             свич    |
                   |       <bridge4>-----|------<порт>(    )   |
                   |                     |            (    )   |
  LAN2-------------|--------<vasya1>-----|------<порт>(    )   | 
  LAN3-------------|--------<vasya2>-----|------<порт>(    )   | 
                   |                     |                     |
                   |     сет стек хоста  |             ядро    | 


понятно что порт vasya1 он dummy но представим что этот порт 
ведет куда то во внешнюю сеть LAN2 ethernet сеть.
а порт vasya2 ведет во внешнюю LAN3 ethernet сеть.
тоесть порты vasya1 и vasya2 можно представить что это физ 
эзернет порты на компе который проводами ведут в некие внешние
физ свичи которые ведут в другие LAN сети. вот эти порты vasya1 
и vasya2 мы "втыкаем" в вирт свич. и в итоге у нас через порт 
bridge4 процессы сидящие на сет стеке хоста могут иметь доступ
и в LAN2 и в LAN3. хотя и без вирт свича наш комп имел туда
доступ но с вирт свичем у нас еще бонус у нас компы из LAN2 
получают доступ к компам LAN3. и компы из LAN3 дают доступ
к компам LAN2. порты vasya1 и vasya2 становятся транзитными прозрачными
и в промискуус моде. 

значит порт через который процессы на хосте могут получать доступ
к свичу называется мастер портом. а порты которые на компе
служат чтобы обьединить разобщенные сегменты сети на уровне L2
называются bridge_slave


наличие вирт свича на хосте можно определить несколькими 
способами либо через команду bridge из пакета iproute2
либо через команду brctl из пакета bridge-utils
либо через команду ip из пакета iproute2
(дело в том что невсегда ест позвможность поставить пакет 
bridge-utils а пакет iproute2 есть часто на всех хостах)

если мы знаем мастер порт то можно узнать bridge_slave порты

# ip -c link sh master docker0
15: veth91534ca@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default 
    link/ether ea:24:cf:5e:2e:c3 brd ff:ff:ff:ff:ff:ff link-netnsid 0
52: veth2456cc4@if51: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default 
    link/ether 62:8a:ca:2b:48:64 brd ff:ff:ff:ff:ff:ff link-netnsid 8
60: veth623cab8@if59: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default 
    link/ether 7a:9f:a0:09:29:33 brd ff:ff:ff:ff:ff:ff link-netnsid 9

# ip -c -d link show docker0
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:dd:d0:1a:24 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535 
    bridge forward_delay

видим признак bridge. значит это мастер порт

# bridge link show  
15: veth91534ca@if14: ... master docker0  
28: vethedd4829@if27: ... master br-81e0d9ff7558  
30: veth81b1acc2@docker0: ... master cni_bridge0  
31: vethd58787ee@docker0: ... master cni_bridge0  

прикольно. команда bridge она не показывает мастер порты совсем.
она показыает только bridge_slave порты (те которые обьединяют сегменты)
но в своствах кажодго порта она показыает какой у него мастер порт


# brctl show  docker0
bridge name	bridge id		   STP enabled	 interfaces
docker0		8000.0242ddd01a24	    no		 veth2456cc4
							                 veth623cab8
							                 veth91534ca

вот так можно узнать все мастер порты
e# ip -c link show type bridge
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
22: br-81e0d9ff7558: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
29: cni_bridge0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
32: cni_bridge2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
61: bridge2: <BROADCAST,MULTICAST> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
66: bridge3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
72: bridge4: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000

а вот так можно узнать какие из них щас UP
# ip -c link show  up type bridge

зная мастер находим его bridge_slave порты
# ip -c link sh master bridge3
68: veth4@if67: master bridge3 state UP 
    link/ether 52:c4:25:5b:29:3e brd ff:ff:ff:ff:ff:ff link-netns system6
70: veth5-bridge3@if69: master bridge3 state UP  
    link/ether 56:31:5e:f5:4c:cc brd ff:ff:ff:ff:ff:ff link-netns system7

отсюда мы увидели что слейвы ведут в неймспейсы system6 и system7
тогда можем узнать какие IP сидят   в тех нейисмпейсах на картах
которые там сидят

# ip netns exec  system6 ip -4 -c a s | grep inet
    inet 10.200.0.2/24 scope global veth4p1
# ip netns exec  system7 ip -4 -c a s | grep inet
    inet 10.200.0.3/24 scope global veth5p1-bridge3



вот сколко способом узнать если ли на компе вирт свич.
и какие порты в него входят в качестве мастер порта а какие
порты бриджевые\свичевые.


но это все было все еще вступление. далее очень серьезная вещь
детали которой я не понял но есть практическая суть. 
я покажу еще раз картинку чтобы было больше понятно


                          процессы хоста
                              ||
                   |          ||         |             свич    |
                   |       <bridge4>-----|------<порт>(    )   |
                   |                     |            (    )   |
  LAN2-------------|--------<vasya1>-----|------<порт>(    )   | 
  LAN3-------------|--------<vasya2>-----|------<порт>(    )   | 
                   |                     |                     |
                   |     сет стек хоста  |             ядро    | 


 если бы наш вирт свич был реальным физ свичем то  связь 
 между LAN2 и LAN3 заработала бы мгновенно. но на софт свиче 
 связи не будет. чтобы она заработала нужно 

 # cd /proc/sys/net/bridge
 # ls
 bridge-nf-call-arptables  bridge-nf-call-iptables
 bridge-nf-call-ip6tables  bridge-nf-filter-vlan-tagged
 # for f in bridge-nf-*; do echo 0 > $f; done

 (взял отсюда https://wiki.linuxfoundation.org/networking/bridge)

 а дело вот в чем у нас порты vasya1 и vasya2 они лежат в сет 
 стеке хоста. этот стек имеет правила iptables. это значит
 что к пакетам летающий через эти порты применяются правила iptables.
получеся что пакет который летит из LAN2 через наш хост в LAN3
с точки зрения iptables это цепочка FORWARD. а по дефолту она имеет
правило DROP. вот iptables и дропает нахрен все. так вот чтобы 
нам отменить обработку пакетов через правила iptables для тех 
пакетов которые летают друг к другу через свичевые порты vasya1
и vasy2 и прогоняется та шняга что указано выше.
можно пойти другим путем - можно вместо этого модифицировать цепочку
FORWARD.  

если у нас куча бриджей и вефов. то как по быстрому узнать 
в какие контейнеры они ведут.
# ip -c link sh dev veth-1
63: veth-1@if62: ... master bridge2 ...
    link/ether 3e:72:5b:ea:c2:4a brd ff:ff:ff:ff:ff:ff link-netns system3

во первых видно что мастер порт это bridge2
во вторых видно что другой veth ведет в сет неймспейс system3
зная неймспейс куда ведет веф мы можем узнать какие параметры
там имеет сетевая карта там

$ ip netns exec  system3 ip -c a

так вот дальше непонятно. если мы неотменяем прогонку пакетов
которые летают транзитом между vasya1 и vasya2 то в iptables
в цеопчку FORWARD почему то надо добавить правила касающиеся не
портов vasya1 и vasya2  а  мастер порта bridge4 ! хотя какое
нахрен он имееет отношение к трафику между vasya1 и vasya2 портами
сука! 
причем что еще непонятно подойдет любой одно из этих
двух правил:
	-A FORWARD -o bridge4 -j ACCEPT

	-A FORWARD -i bridge4 -j ACCEPT

это какойто пиздец

тоесть получается как бутто все фреймы влетающие в порт vasya1 или в
vasya2 они же в ядро попадают но как бутто они при этом также 
автоматом направляются в порт bridge4 а он при этом как бутто работае
в promiscous mode далее опять в ядро попадает пакет. там обрабатывается
и обратно этот пакет высирается через порт bridge4 в свич а оттуда
уже летит либо в vasya1 либо в vasya2. тоесть как бутто работает 
эта хуйня так
из LAN3 в LAN2 летит пакет. он влетает в vasya2 и попадает в ядро.
но почемуто он также направляется в порт bridge4 то есть для него
этот трафик -i потом он опять попадает  в ядро. оно его обрабатывает
и высирает обратно в свич через порт bridge4 при этом трафик 
стаовится уже -o bridge4 и потом он поадает в свич и оттуда уже 
через vasya1 летит в LAN2. это что за пиздец?  причем тут вообще
порт bridge4 сука? еще один вопрос . вот правила 
которые добаляет докер для своего свича docker0

-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT

так вот видно то что нужно добавляет правило как для потока 
туда так и для потока обратно. у меня же все работает если 
прописать разрешение либо для потока туда либо для потока обратно.
это что за пиздец ?

еще вот такое правило можно использовать и оно тоже решает
все проблемы

-A FORWARD -m physdev --physdev-is-bridged -j ACCEPT
про модуль physdev
This module matches on the bridge port input and output devices 
enslaved to a bridge device. This module is a part of the 
infrastructure that enables a transparent bridging IP firewall 
and is only useful for kernel versions above version 2.5.44.
[!] --physdev-is-bridged
Matches if the packet is being bridged and therefore is not 
being routed. This is only useful in the FORWARD and POSTROUTING chains.
это правило уже выглядит более логичным оно о том что (как я понимаю)
если у нас пакет влетел в бриджевый слейв порт и он хочет перепрыгнуть
за счет бриджинга на другой бриджслейв порт этого же бриджа то 
iptables его пропускает. 

далее еще один интересный момент. если я заускаю пинги между LAN2
и LAN3 и при этом я запукаю на мастер порту tcpdump без 
режима промискус (ключ -p)

# tcpdump icmp -p -i bridge4
то он ничего не показывает. пусто , оно и логично потому что 
для этого порта с его мак нет фреймов. 
но если запускаю tcpdump в режиме промискус опять же для мастер 
порта (без ключа -p)
# tcpdump icmp  -i bridge4
то мне tcpdump начинает сыпать логи
10.200.0.3 > 10.200.0.2: ICMP echo request
IP 10.200.0.2 > 10.200.0.3: ICMP echo reply
и я не могу понять а с хуя ли? у нас что свич работает как хаб 
чтоли? почему фрейм который свич должен четко кидать только 
между портами vasya1 и vasya2 еще по какойто причине кидается на
порт безымянный порт свича который уже ведет на порт bridge4 ?
НЕПОНЯТНО

я провел еще эксперимент. я стал посылать фреймы с LAN3 на
IP от bridge4 при этом я запустил tcpdump даже в промиск моде
на компе в LAN2. и ниаких дампов я не увидел. 

если мы сет карту вклчаем в режиме промискуити это значит что 
все к ней прилеело по проводу она с удовольстивем захыватывает
и проглатвыает и пихает это в ядро. но это совершенно не значит
что из ядра в карту полетит весь хрен пойми какой трафик. 
ядро будет пихать в карту только тот трафик который ей положено
выплюнуть наружу. а не весь подряд.

у меня сложилось в итоге впечатление что свич конечно не пихает 
во все дырки на выход тот пакет который в него прилетел снаружи.
он пихает его толко в ту дырку которая ему положена. но по всей
видимости когда я запускаю tcpdump на мастер порт в режиме
промискус моде то как я понимаю тот порт на свиче через который
мастер порт к свиче он превращается в хаб порт. тоесть все что влетает
в свич оно автоматом высирается и в этот хаб порт. я так понимаю
эту загадку. но что кссется бриджевых портов bridge_slave они 
во первых все время уже в режиме промискус. однако с ними такого
прикола не пороисходит. если в свич влетает фрейм то это не приводит
к тому что через эти порты он высирается наружу. нет. опять же 
вспминаю что входящий в порт трафик tcpdump показвыает до обрабоки
пакета через netfilter. но если бы в мастер порт ничего не было 
направлено то дамп был бы пустой для входящего трафика а он сука
не пустой. значит в том факте что tcpdump ловит трафик на мастер
порт независит от iptables а происхдоит от того что  код софт свича
реально кидает на мастер порт входящий трафик когда мы переводим
мастер порт в промискус режим. и тогда как я уже скаалзал полагаю 
что софт свич его код переводит свич порт который ведет на мастер
порт в хаб порт режим. я только так могу это обьяснить.
на циско есть такой фнукционал. зеркаьльный порт. тоесть у нас траф
ик влетает в порт fa1 и параельно он дубливается на порт fa2 на который
можно повесить анализатор трафика. 



такой по мне еще прикол. L2 порт на компе это порт который имеет
упрощенно говоря L2 адрес. MAC адрес. по которму в частнсти 
этот порт можно найти в сети. На свичах насколько я понимаю нет
никаких L2 портов. потому что для этого его порты должны иметь 
свои MAC адреса а у него их нет! поэтому сам свич обрабатывает 
фреймы которые в него влетели на уровне L2 но порты при этом
егоные нихуя не L2. я бы их назвал transparent L2. тоест они умеют
понимть и захыватывать трафик на уровне L2 но у них самих нет 
мак адреса. это как почтовая первалочная станция. письма приходят
на конечные почты. на перевалочных станциях адресов нет. но там
сортруют письма.  


