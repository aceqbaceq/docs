FC

как я понял ни ceph ни glusterfs неработает через fibre channel.


-------
согласно этому документу 

https://support.hpe.com/hpesc/public/docDisplay?docId=mmr_kc-0100588

HP StorageWorks 8/80 SAN Switch эквивалентен 	Brocade 5300

в этом документе
 https://h20195.www2.hpe.com/v2/getpdf.aspx/c04123373.pdf?ver=7

прописано какие трансиверы этот свич поддерживает:

нас интересует вот этот:
HP 8Gb Shortwave B-series Fibre Channel 1 Pack SFP+ Transceiver = AJ716B

далее я нашел в инете неодно упоминание 
(например вот здесь https://support.hpe.com/hpesc/public/docDisplay?docId=c01486152&docLocale=en_US)  о том что 
		AJ716A is End of Life (EOL), and is replaced by AJ716B because of RoHS compliance; however, the SKU is still orderable until supplies are diminished.


	тоесть то что AJ716A и AJ716B это типа одно и тоже. как японял B просто 
	более экологичный. дело в том что в магазинах сейчас B ненайти зато A попадается очень часто.
	
поэтому с трансиверами к этому свичу мы определились какие он официально
поддерживает.

итак свич есть, трансиверы есть
	
-------


согласно man isp из freebsd

(freebsd)# man isp

 
она поддерживает FC карточки от qlogic серии 

Qlogic 256x (aka 2532)
		   Optical 8Gb Fibre Channel PCIe cards.

 


-------
согласно этому документу 

https://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/HP.aspx?companyid=4


вот эта карточка 

HP StorageWorks 82Q 8Gb Dual Port PCIe Fibre Channel Host Bus Adapter  [AJ764A, QLE2562-HP, 489191-001]

эквиватлента

qlogic QLE2562

поэтому типа она заработает на фрибсд
с другой стороны так как это HP то думаю что со свичом HP она тоже
совместима  поэтому типа эта сет карточка нам подходит

в доке от карточки 
		https://h20195.www2.hpe.com/v2/getdocument.aspx?docname=c04164502#
		написано что вообще то в комплекте с ней идут трансиверы 
		но неуказано какие по модели, но вот в этом документе 
			https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-c04183658
			который типа говорит о запчастях к этой карточке
			сказано что к этой карте как запасная часть подходит
			трансивер вот такой 468507-001
			я поискал по этому номеру и нашел что это AJ716A.
			тоесть он либо идет с этой картой в комплекте либо по крайней
			мере он с картой совместим.
	

таким образом трансиверы + карточка + свич совместимы друг с другом.
и карточка совместима с freebsd.

хотя вот тут
	https://community.hpe.com/t5/bladesystem-virtual-connect/are-aj716a-and-aj716b-sfp-b-series-compatable/td-p/6680294#.X6BVwNUzY2w
	
	чувак пишет что (как я понял) что у него система на AJ716B незаработала а заработала она только на AJ716A и что типа они несовместимы. этот аспект непонятен.
	

ссылки 
		сет карта
			https://www.westcomp.ru/catalog/fc_8_16gb_s/adapter_hp_storageworks_82q_8gb_dual_port_pcie_fibre_channel_host_bus_adapter_aj764a_489191_001/
		
		здесь пишут что AJ716A и AJ716B трансиверы совместимы
			https://community.hpe.com/t5/hpe-eva-storage/difference-of-part-numbers-of-transceivers/td-p/5842347#.X6BU2NUzbIU
		
			https://forum.ixbt.com/topic.cgi?id=66:10191
		
		кабель
		
			https://www.westcomp.ru/catalog/transivery_opticheskie_kabeli/hp_premier_flex_lc_lc_om4_2f_5m_multi_mode_optical_cable_qk734a_656429_001/
		
		pdf от свича
		
			https://h20195.www2.hpe.com/v2/GetPDF.aspx/c04123373.pdf
		
		pdf вообще по свичам
		
			https://www.intesiscon.com/ficheros/manuales-tecnicos/96-Switch-fibra-HP.pdf
		
		здесь 
			https://it-hp.ru/product/aj716a/
		пишут вот такое:
		
			Общие
			Номер производителя AJ716A
			Название продукта Трансивер HP 8 Гб Shortwave B-series Fibre Channel 1 Pack SFP+

			Основные характеристики
			Тип приемопередающий модуль для коммутатора
			Устройство ЛВС 1 порт (Fibre Channel)
			Пропускная способность 8 Гбит/сек.
			Поддерживаемые стандарты IEEE 802.8 (Fibre Channel)

			Дополнительные характеристики
			Совместимость для систем хранения HP StorageWorks 4400EVA и коммутаторов StorageWorks 4/8 SAN/ 4/32B SAN/ 8/40 SAN/ 8/8 SAN/ 8/80 SAN/ DC SAN Backbone Director
			Интерфейсы 1 x Fibre Channel • SFP
			Системные требования Mini-GBIC

			Доп. информация Трансивер HP AJ716A
			Назначение система хранения • HP StorageWorks 4400EVA
			коммутатор • HP StorageWorks 4/8 SAN
			коммутатор • HP StorageWorks 4/32B SAN
			коммутатор • HP StorageWorks 8/40 SAN
			коммутатор • HP StorageWorks 8/8 SAN
			коммутатор • HP StorageWorks 8/80 SAN
			коммутатор • HP StorageWorks DC SAN Backbone Director

			Родительские устройства
			SAN коммутатор HP StorageWorks 4/8 SAN Switch, монтируемый в шкаф-стойку корпус, 8 портов, Fibre Channel, 1U, ret
			Система хранения информации HP StorageWorks 4400 Enterprise Virtual Array, внешн., жесткие диски: 8 x 400 ГБ, общей ёмкостью до 12 ТБ, черного цвета

		pdf от карточки
		
			https://h20195.www2.hpe.com/v2/getdocument.aspx?docname=c04164502#
		
		здесь пишут какая модель qlogic карточки сидит внутри карточки hp
		
			https://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/HP.aspx?companyid=4
		
		здесь пишут какие модели Brocade лежат внутри некоторых железок HP

			https://www8.hp.com/cn/zh/pdf/HP-RG-160-12_A4_SpreadsMar13_tcm_185_1431155.pdf
	
	
	

==============================


установка

качеаем http://ldriver.qlogic.com/firmware/ql2500_fw.bin
пересобираем initramfs 
 update-initramfs -u
 
 
 проверянм стату реэима раоты карты инициаотор или таргер
 cat /sys/module/qla2xxx/parameters/qlini_mode
 
 /sys/class/fc_host/host4/port_state = показывает вообще кабель то воткнуть есть ли сигнал на уровне L1
 
 =====
 fabric license и full fabric license это одно и тоже
 
 при наличии этой лицензии можно свичи друг с другом сединять
 но только по однрому порту.
 
 а если мы хотим друг сдргуом по несколким портам то нужна дополниельно
 ISL лицензия
 ==
 
 если стоит 
 full ports on demand license на 32 портов
 
 и стоит одновременно 
 second ports on demand license на 16 портов
 
 и еще может там какието он деманд лицензи
 так вот суммарное число активироавнных он деманд
 портов полюбому неможет превыщать число 32 которое указано
 в лицензии full ports on demand license
 =====
 если скорость портов указан через N
 например N8 
 то буква N ознавает что настройка порту = negotiate
 то есть она предлагает устройству конечному такую скорость.
 
 есть другой варианть настройки скорсти порта , когамы жестко
 задаем скажем 8ГБ.
 тогда буквы N не будет в статусе скрости поартов
 
 ====
<<<<<<< HEAD
 brocade
 
 default passwords
 
 root:fibranne
 admin:password
 user:password
 
 =======
 brocade
 без заебов не обошлось
 все для людей.
 
 значит в завимиости от версии FOS его веб морда работает т
 только с определенной версией jre
 
 https://downloads.hpe.com/pub/softlib2/software1/pubsw-windows/p199020807/v143212/FOS7.4.1d_RN.pdf
 
 страница 45
 
 это пишут люди и это написано в руководстве.
 
 например 
 
 FOS v7.4.0 is qualified and supported with Oracle Java version 7 update 76 and Java version 8 update 66.
 
 у меня конкретно работает такая связка:
 
 FOS v7.4.1d + jre8u66(64bit)
 FOS v7.4.1d + jre8u251(32bit)
 
 далее надо зайти в контрол панель в виндовсе в java и там
 в закладе security добавить путь к свичу как безопасный
 например 
 http://10.15.0.10
 
 и также суперважно что запускать веб морду надо не вбраузере там вобще ноль на массе 
 а в командной строке:
 
 "c:\Program Files\Java\jre1.8.0_66\bin\javaws.exe" http://10.15.0.10/switchExplorer_installed.html
 
  
=======
 имеем esxi
 на нем FC карты qlogic
 они проброшены напрямую в VM
>>>>>>> 12c389a31b34932076ddd8345ada0e6f251055cc
 
 сама карта FC воткнута в FC свич.
 
 пока VM выключена порт FC будет находится в состоянии выключено, в веб мордое свича это выглядит как буква U на порту.
 
 когда VM загрузилась то если все окей то буква в веб морде изменистя на F
 на самой виртуалке статус порта можно проверить командой
 
# more /sys/class/fc_host/host?/port_state
::::::::::::::
/sys/class/fc_host/host2/port_state
::::::::::::::
Online

тоесть он должен быть онлнайн.

на примерер убунту 16 и 20 = никаких паетов и настроек самому делать ненадо.
сразу с нуля если ВМ загрузиась то порт должен перейти в состояние онлайн.

единсвтенное что понятно что ВМ должна иметь дрова от FC карты.


вот как проверить что ВМ видит карту

/# lspci -nn | grep -i hba
0b:00.0 Fibre Channel [0c04]: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA [1077:2532] (rev 02)



 вот как проверить сколько в карте дырок
 
 /#  ls -l /sys/class/fc_host
total 0
lrwxrwxrwx 1 root root 0 May 15 10:52 host2 -> ../../devices/pci0000:00/0000:00:16.0/0000:0b:00.0/host2/fc_host/host2


здесь получается одна

вот как проверить какой WWN имеет порт на карте

/# more /sys/class/fc_host/host?/port_name
0x50014380186a0444


итак если это все видно значит без каких то настроек порт дожен быть в состоянии ONline.
если это нетак то проблема в самой VM.
типа надо попробовать другой голый дистрибутив.

ни на esxi ни на свиче. вобщем нигде ничего специально настривать ненужно.
если ВМ загрузислась то порт дрожен перейти в состояние онлайн.

если он непрешеол чтото нето с ВМ. я обычно пробуб новую голую ВМ другую.

==

 targetcli
 
 создание LUN на основе RAMDISK
 
 # targetcli
 
 идем туда где прописываются бекенд диски
 
 >  cd backstores/
 
 > create name=rd_backend size=40GB
Generating a wwn serial.
Created rd_mcp ramdisk rd_backend with size 40GB.


идем в настройки таргета в его луны чтобы создать лун который таргет
будет экспортировать наружу на основании нашего
бекенд диска

> cd /qla2xxx/50:01:43:80:18:6a:02:e8/luns/

> create /backstores/rd_mcp/rd_backend
Selected LUN 1.
Successfully created LUN 1.
Created mapped LUN 1 in node ACL 50:01:43:80:18:6a:04:44

сохранить конфиг

>saveconfig

это не все опписание. надо еще там и всякое разно сделать.
но это просто кусок который описывает если у нас бекенд 
это рамдиск.


далее на клиенте надо перезапустить скан лунов

клиент #  rescan-scsi-bus.sh
после fdisk сразу должен показать новый lun

===
targetcli 
как создать LUN на основе физ диска в качестве бекенда

# targetcli

создаем бекенд
> cd backstores/iblock/
> create name=ssd-200G  dev=/dev/sdc

экспортируем лун
> cd /qla2xxx/50:01:43:80:18:6a:02:e8/luns/
> create /backstores/iblock/ssd-200G
 
сохранить конфиг
> save config
 
 
 ===
 brocade
 
 настроить через cli адрес веб морды
 
 > ipaddrset
 
 ====
 на линуксе
 FC сервер настраиваем на основе targetcli
 а также ОС = ubuntu 14.04.6
 именно на этой ОС дрова от qlogic умеют 
 переключаться в режим таргета(сервера).
 в других версиях эти суки это выпилили. там тока как клиент
 может драйвер работать
 
 ====
 
 
что выяснилось из практики. есть FC карта на ней два порта.
оба порта выпилены из сферы чтобы их можно было отдать виртуалке по пасс-сру.
так вот сфера недаст один порт отдасть одной ВМ а второй порт отдать другой ВМ.
походу это так потому что порты принадлежат одной железке.
так что либо один порт либо все пробрасывай на одну ВМ. 
порты по отдельности на разные ВМ непробросить.


==

резултьтат такой получен

4k random read 100% , 1 поток

локальной на рейд контроллере latency = 0.17ms
а через FC 0,26мс

клиент и сервер сидят на одном физ хосте.
каждый на своей FC карте.

карте проброшены в VM's напрямую.

что касатеся FC сервера то использовалось два типа сервера
1. ubuntu 14 + targetcli
2. esos (scst)
результат один и тотже.

клиент это виндовс 2012 + iometer

==
esos

дальше на любой версии esos ( 3.0-0-3.0.12 ) если бекендом делать
голый диск /dev/sdc то при попытке к нему покдчючиться FC клиентом этот esos 
выпадает в kernel panic.

а вот если бекендом является blockio через LVM  то уже esos в кернел 
паник невыпадает

==

позвольте мне рассказать про FC и esxi.

значит тема Fc покрыта тайнами. как бутто это чтото и нечто.
 ажаль. как обычно понты и выебоны.
 
 правда такова:
 
 есть сервер - FC target.  у него есть FC карточка ( FC HBA ).
 эта карточка имеет wwwn это как mac в эзернете. wwwn используется для адресации
 передачи пакета или как он там называется.
 
 есть клиент. зовется он FC инициатор в терминологии. у него тоже FC карточка.
 она тоже имеет свой wwwn.
 
 между клиентом и севером находится FC свич. или FC свичи.
 они для клиента и сервера невидимы и прозрачны. также как  для эзрнет клиента и сервера
 невидны эзернет свичи.
 
 Fc свич после того как рассеять всю эту замудренную тему всего навсего
 делает такую простую вещь. на нем мы группируем порты ( дырки) в группы 
 и зовутся они в их терминологии Zones. это считай что аналог группирования портов на 
 эзернет свиче в виланы. порты из одной зоны невидят порты из другой зоны. вот и все.
 
 таким образом чтобы клиент и сервер через свич могли друг друга увидеть на свиче
 их порты куда воткнуты провода от их HBA должжны на свтиче входить в одну зону.
 при этом подчеркну на свиче никакие wwwn нтикуда невписываются, так же как на эзернет
 свиче при групировании порттов в вилан никакие mac адреса никуда не вписываются.
 да и wwwn и mac они обрабатыватся на свичах но при конфигурировании зон и виланов
 они никак непрописываются.
 
 таким образом если между клиентом и сервером стоит FC свич и порты (дырки ) куда воткнуты 
 клиент и сервер на свиче входят в одну зону то клиент и сервер друг друга должны видеть.
 дальеш на самом сервер конечно дополнительно можно настраивать а захочет ли сервер 
 давать этому клиенту (инциатору) доступ к своим лунам. 
 
 для тго чтобы клиент и сервер друг друга видели свич необязателен. можно напрямую 
 соединить hba клиента и сервера. никаких особых настроек , особых проводов как это есть 
 или было в эзернет сетях тут такого нет. 
 
 так что вначале можно вобще смело свич нахер исключить из этой цепи.
 
 так вот переходим к тому как обстоят дела с FC на esxi.
 
 для начала вспомним как обстоят дела с эзернет картами на esxi.
 
 реальная физ карта эзернет она на хосте esxi подключается к виртуальному esxi сетевому
 свичу ( standard или distributed switch ). потом на нем создается группа портов,
 далее в свойствах виртуалки ей добавляется виртуальная сетевая карта которая чисто виртуальная 
 и она как бы подключается через виртуальный провод в виртиуальный порт который входит 
 в группу портов на виртуальном свиче.
 
 таким образом еще раз цеопчка выгляди так:
 
 VM -> у нее есть вирт NIC -> эта сет карта подключется через вирт сетевой шнур в
 виртуальный порт (дырку) в виртуальный сетевой свич => а этот вирт сетевой свич
  через один из своих вирт портов через вирт шнур подключается к реальной физ карте.
  
  причем кстати говоря вот этот вирт свич. его функционал обрабатывется неким служебным
  процессом на esxi. условно говоря пакеты сетевые междлу виртуалками они прям 
  через цпу обрабатываются неким процессом служебным на esxi.
  
 
так вот получается что VM она к реальной сетевой карте хоста неимеет никакого 
доступа. а если на VM не поставить вирт сетевую карту то VM вообще к сети небудет
иметь никаокго доступа.


возврашается к FC. вот мы воткнули в сервер FC карту. далее в своствах сервера мы видим
что он видит что  у него есть карта Fc. возникает вопрос по аналогии с эзернет
а как добавти в виртуалку виртуальный FC адаптер.

ХА-ХА-ХА !!!  - ответ = НИКАК!

 в случае FC все работает по другому чем это есть с эзернет.
 
 HBA карты имеют wwwn. на fc сторадже мы увидим эти wwn. и мы разрешим какимто лунам
 FC стораджа быть видными для данных wwwn.
 
 что при этом произойдет на ESXI хосте. именно на хосте а не на ВМ.
 
 если мы зададим сканирование новых датасторов то мы увидим что нам стали доступны 
 новые луны ровно так как это бывает с локальным лунами когда мы их создаои на локальном
 рейд контроллере.
 
 таким образом когда мы на ESXI на основе появившегося нрового луна создадим новый датастор
 то мы сможем виртуалке прикрутить новый вирт диск ровно также как бутто это локальный диск 
 от рейд контроллера.
 
 таким образом виртуалка вообще неимеет понятия никакого ноль. о том что 
 данный диск прианадлежит FC стораджу а не локальному рейд контроллеру.
 
 таким образом ВМ никогда незнает тот диск который ей прикрутили он идет от локального
 рейд контрролера, или от NFS, или от ISCSI, или от FC. для ВМ это все диски 
 которые как бы доступны ей через ее виртуальный SCSI контроллер.
 
 
 отсюда вытекает одна суперважная последствие = если мы хотим чтобы наша виртуалка
 являлась FC стораджем сама, FC target то это можно сделать только одним способом =>
 надо тогда FC контроллер на esxi хосте напрямую пробрасывать внутрь данной виртуалку.
 вот о какой жопе  я к чему вел этот расказ. потому что никаким другим способом виртуалка
 вобще никак неможет дотянуться до FC карты. никак. 
 
  в чем при это неудобство. в том что если мы пробросим FC карту напрямую в виртуалку
  то хост в целом и конкретно все другие виртуалки на даннром хосте они немоугт пользлтвьатться
  данным FC картой и esxi хост и всего его виртуалки (кроме одной) немогут 
  ни подключиться к какому либо FC стораджу ни являться FC таргетом.
  
 это означает что на таком сервере нам надо иметь минимум два FC контроллера,
 чтобы один пробросить а второй нет. через второй мы будем подключаться к друнгим
 FC стораджам. а через первый мы будем сами раздавать луны наружу по FC.
 
 
 еще такая жопа котрая проверена. если на FC карте несоклько FC портов то нельзя
 один порт пробросить в одну ВМ а второй Fc порт пробросить в другую ВМ. 
 пробрасывается только вся железка целиком. да можно пробросить и один порт а второй 
 не пробросить но в лоюбому случае если железка частично занята одной ВМ вторая никак
 ей пользоваться неможет.
 
 хотя я нашео в инете что типа там есть какието карты которые типа как бы слеплены
 на одной плате но их две. и там якобы все таки прокатывает такой фокус. но вцелом
 это неработает.
 
 поэтому если мы хотим на одной ВМ использовать ее как FC таргет. 
 а остальные ВМ чтобы имели возможность подключаться к другим FC стораджам
 то на хосте надо иметь минимум две Fc карты воткнутые.
 
 кстаи возврашаясь к теме эзернет карт. всомним таую штуку как SR-IOV.
 она позволяет типа распилить физ эзернет адаптер на много виртуальных 
 адаптеров. и такой адаптер его можно как напрямую присобачить к ВМ , 
 также его можно присобачить к вирт свичу.
 тоесть опять же у FC все подругому.
 спрпашивается а зачем вобще этт sr-iov если итак наши ВМ имеют
 вирт сетвые карты и они могут дотягиваться до сети.
 щас скажу ясный внятный ответ - когда вм пуляет сетевой пакет в вирт
 сет карту которая идет в вирт свич. то этот пакет от процесится служебным
 процессом esxi на цпу хоста. тоесть грузится цпу.
 если sr-iov карту подключить к виртуалке , она тоже виртуальная но она 
 непросто виртуальная а она sr-iov виртуальная. 
 и разниа в том что она сразу ведет в физ сетевую карту. и вирт свич никак 
 несвязан с этой sr-iov вирт сетевой картой. и таким макаром сетевой пакет который
 ВМ плюет он неуходит в вирт свич и его необрабатывает служебный рпоцесс esxi и цпу негрузится
 при этом. вот в этом и фишка sr-iov вирт сетевых карт.
 но опятб же sr-iov сетевую карту можно присобавчить к вирт свичу если мы этого хотим
 и тогда все пакеты от ВМ которые подключены к свичу будут как и прежде обрабатвыаваться 
 цпу хоста. потому что вирт свич его функионал реализуется софтом , служебным процессом
 esxi OS.
 
 
 вернемся к FC. 
 есть еще одна хрент которая всплывает  в этой теме по Fc. 
 это NPIV.
 для нас она беполезна. щас раскажу что она дает.
 у hba на esxi хосте есть wwn. его видит FC сторадж. и какието луны экспоузит
 для данного wwn. и тогда esxi видит данный лун ровно так как бутто это лун 
 от локального рейд контрролллера.
 
 таким образом кодга мы создадим датастор на таком лун то мы можем нарезать вирт
 диски с этого датастора для любой ВМ которая сидит на данном хосте.
 
 так вот если мы зайдем в свойства ВМ и так включим NPIV. то при power on виртуалки
 наш физический HBA начнет из себя изрыгать нетолько свой родной wwn который ему дал
 esxi хост но и +1 новый wwn. это как плевать в сеть несколько MAC из одной физ эзернет 
 карты. таким образом FC сторадж увидит еще один wwn инициатор. с точки зрения FC сторадж
 появился +1 новый клиент. нехуже и нелучше чем тот первый. и на FC сторадлже можно
для нового wwwn заэкспоузить какито новые другие лун. укажу что конечно первый wwwn тоже никуда
недевался. просто пояявился второй. 
 так вот что сделает esxi вот с этим новым лун который сторадж заэкспузил для второго wwwn.
 этот новый лун он небудет виден в новых лун. нет. он станет виден как RDM диск 
 в свойствах виртуалки. где конкретно ( как я понял ). заходим в свойства виртуалки
 жмем доавить новый диск и там должен загореться значок "raw device mappings".
 таким макаром данный лун будет доступен и виден исключительно только данной
 ВМ.
 
 вобщем эта штука для того чтобы:
 
 предназначена толтько для того чтобы если на страдже мы хотим создать лун который
 будет 100% гарантиированно доступен только какойто одной конкретной виртуалке.
 
 например мы купили сверх  дорогой ssd диск. и мы хотим чтобы он был использован 
 толко и только для нашего sql сервера.
 
 если мы его сделаем просто общедоступным для всех вм на хосте 
 то тогда все зависит от человека который на этом хосте правит. 
 будем ли он помнить что даннй лун\датастор надо использовать только для sql vm
 и другим вм его недавать. 
 
 тоесть npiv позволяте исключит человеечкский фактор.
 
 
 в любом случае npiv абсолюбтно бесполезен для того чтобы 
 можно было из ВМ сделать FC таргет.
 
 проброс (или еше он называется directpath) и только проброс HBA карты
 внутрь вм. 
 
 к сожалению вся эта тема FC опутана какойто дуракой тайной на пустом месте.
 
 
=====
 
esos

запись 800МБ\с линейная
общая загрузка esos vm по показаниями esxtop = 0.8 ядра E5-2680v4

===
 
 
 
 4k read 100%
20 workerov x q=1

raid massiv Raid1 SmartArray P840  2xSSD Samsung 840 EVO 
61 000 iops
0.32 latency


FC (данные читаются с этого же контроллер но через FC)
41 000 iops
0.47 latency


загрузка cpu на сторадже esos
esxi показывает загрузку esos VM  по cpu:  1 ядро (E5-2680v4 2.4GHz)  идет на работу самой ВМ и 0.3 ядра идет на системную службу esxi os на обслуживние IO, итого 1.3 ядра цпу.
получается что внутри ВМ она должна показывать что загружено 1 ядро, однако
сам esos внутри себя показывает загрузку по cpu почему то 0.4-0.5 ядра ( а не 1 ядро как должно бы быть )
непонятно.  
бекенд у esos это LVM лун

а вот для сравнения скорость чтения на том же режиме 
4k read 100%
20 workerov x q=1
с другого контроллера  и других дисков
raid massiv Raid10 Smart Array P440ar  4xSSD KINGSTON SEDC500 
82 000 iops
0.24 latency

а вот еще.
контроллера нет, одиночный диск SSD Intel 3700 372GB
143 000 iops
0.14 latency
 


====

походу в esos такая схема

есть hba из него делается таргет(раздавальщик). к таргету крепится группа. 
в группу засовываются инициаторы(клиенты), и в группу засовыаюся бекенд луны.
таким макаром инициатор видит лун

когда у нас несколько HBA, то мы их пихаем в группу , это секция ALUA
не путать группу ALUA с группой инициаторов.

===

в очереной раз выянислоь что ssd smart path 
это отстой.

когда он активирован на лун то 
запись на диск даже с включенными буферами записи диска составляет
линейная ~ 200MB\s
а если отклчить ssd smart path и включить для луна controller cache 

то для ВМ скорость записи  линейная будет 600МБ\с на короткой дистанции и 450МБ\с
на длинной дистанции. esxtop будет конечно показвыать более низкую скорость 
потому что ВМ то пишет полуается в кэш рейд контроллера а уже ооттуда
данные пишутся на диск.

а потом еще раз заустил скороть поднялась вобще до 780МБ\с записи

пэтому ssd smart path в целом отстой.
===

также диски samsung 840 без обрезки их 
показывают низкую скорость записи на них.т
так что их надо резать.


====



