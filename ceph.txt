ceph


установка это поделки.
документация хуйня полная.


во первых придется взять ununtu попоследнее. 
потому что хотя и заявлено что они новые релизы выкладыавют для 
нескольких версий ubuntu это хуйня полная. поэтому если хотим
релиз ceph попоследнее то придется использовать убунт попоследнее.

я взял ubuntu 20.
это новая жирная тупорылая версия которую сразу надо допиливать
	надо убрать netplan -  https://disnetern.ru/disable-netplan-ubuntu/
		sudo vim /etc/default/grub
		GRUB_CMDLINE_LINUX="netcfg/do_not_use_netplan=true"
		update-grub
		apt install ifupdown
		заполняем /etc/network/interfaces
		rm -rf /etc/netplan/*.yml
		
	надо убрать модуль floppy - онзаебет его искать и тормозить систему
	с ошибками :
			$ sudo rmmod floppy
			$ echo "blacklist floppy" | sudo tee /etc/modprobe.d/blacklist-floppy.conf
			$ sudo dpkg-reconfigure initramfs-tools

продолжаем ставить ceph

$ sudo wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -


$ sudo apt-add-repository 'deb https://download.ceph.com/debian-octopus/ focal main'

где focal это кодовое имя релиза ubuntu 20


проверяем какая версия ceph доступна

$ sudo apt-cache madison ceph
      ceph | 15.2.5-1focal | https://download.ceph.com/debian-octopus focal/main amd64 Packages


ставим ceph, для этого достаточно поставить только пакет ceph.
он остальное вытяент что нужно

~$ sudo apt-get -y install ceph=15.2.5-1focal





в процессе установки надо будет сгенерироваь UUID
для этого в убунте юзаем 

# uuidgen

получим чтото типа того 

badd757e-6ef9-4d70-8d8b-9f28eded1c08

когда мы потом это посдунем ceph 
он нас пошлет нафиг, и скажет что UUID неверный.

оказвыается это я ошибся! и пришлось прочитать что такое uuid
моя ошбка была в том что я скопировал uuid с экрана и потерял одну
цфиру.

так вот что такое UUID

по определению
uudi это число длинной 128 бит , если мы это число типа конвертрруем 
в буквоцифры то полуичм 32 символа. обычно принимается что в итоге 
мы дожны получить 36 символов. а у нас 32. значит надо вставить четыре
минуса. обычно по таком правиалу 8-4-4-4-12. 

по ходу установки могу рекомендовать вобще ни на  йоту не отходить
от варианту установки. например неменять назвыание кластера
на свое . пусть будет дефолтовое!!!! иначе вобще ничего незваедетсяэ!!!

в доке указано без sudo и оно так неработает и еще там мудацкое
описание а в доке от ред хат указана очень полезная опция  
-o /var/lib/ceph/mgr/ceph-test-ceph-01/keyring

так что команда в итоге будет такая

	~$ sudo ceph auth get-or-create mgr.test-ceph-01 mon 'allow profile mgr' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mgr/ceph-test-ceph-01/keyring
	
		
		

sudo -u ceph mkdir /var/lib/ceph/mgr/ceph-test-ceph-01
		


тесты (бекенд один и тот же ssd диск)

локальный тест [r=6690,w=2280 IOPS]

# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=1000M --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.16
Starting 1 process
test: Laying out IO file (1 file / 1000MiB)
Jobs: 1 (f=1): [m(1)][100.0%][r=26.1MiB/s,w=9121KiB/s][r=6690,w=2280 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2145: Thu Nov  5 14:09:17 2020
  read: IOPS=11.0k, BW=43.1MiB/s (45.2MB/s)(750MiB/17378msec)
   bw (  KiB/s): min=23896, max=94976, per=100.00%, avg=44445.88, stdev=23771.52, samples=34
   iops        : min= 5974, max=23744, avg=11111.47, stdev=5942.88, samples=34
  write: IOPS=3689, BW=14.4MiB/s (15.1MB/s)(250MiB/17378msec); 0 zone resets
   bw (  KiB/s): min= 8384, max=31608, per=100.00%, avg=14851.76, stdev=7864.41, samples=34
   iops        : min= 2096, max= 7902, avg=3712.94, stdev=1966.10, samples=34
  cpu          : usr=3.56%, sys=13.10%, ctx=60174, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=191887,64113,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=43.1MiB/s (45.2MB/s), 43.1MiB/s-43.1MiB/s (45.2MB/s-45.2MB/s), io=750MiB (786MB), run=17378-17378msec
  WRITE: bw=14.4MiB/s (15.1MB/s), 14.4MiB/s-14.4MiB/s (15.1MB/s-15.1MB/s), io=250MiB (263MB), run=17378-17378msec

Disk stats (read/write):
    dm-0: ios=190453/63911, merge=0/0, ticks=854328/220184, in_queue=1074512, util=85.00%, aggrios=191133/64159, aggrmerge=760/228, aggrticks=858349/221327, aggrin_queue=621576, aggrutil=84.71%
  sda: ios=191133/64159, merge=760/228, ticks=858349/221327, in_queue=621576, util=84.71%





ceph [r=1675,w=595 IOPS]

o# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=3000M --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.16
Starting 1 process
Jobs: 1 (f=1): [m(1)][100.0%][r=6702KiB/s,w=2382KiB/s][r=1675,w=595 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2174: Thu Nov  5 14:15:07 2020
  read: IOPS=2484, BW=9939KiB/s (10.2MB/s)(2248MiB/231581msec)
   bw (  KiB/s): min=   96, max=53104, per=100.00%, avg=9937.84, stdev=5869.77, samples=463
   iops        : min=   24, max=13276, avg=2484.43, stdev=1467.45, samples=463
  write: IOPS=831, BW=3327KiB/s (3407kB/s)(752MiB/231581msec); 0 zone resets
   bw (  KiB/s): min=  272, max=17856, per=100.00%, avg=3333.73, stdev=1929.94, samples=462
   iops        : min=   68, max= 4464, avg=833.41, stdev=482.49, samples=462
  cpu          : usr=1.88%, sys=6.29%, ctx=752776, majf=0, minf=8
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=575394,192606,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=9939KiB/s (10.2MB/s), 9939KiB/s-9939KiB/s (10.2MB/s-10.2MB/s), io=2248MiB (2357MB), run=231581-231581msec
  WRITE: bw=3327KiB/s (3407kB/s), 3327KiB/s-3327KiB/s (3407kB/s-3407kB/s), io=752MiB (789MB), run=231581-231581msec


gluster [2810/975/0 iops]
(клиент размещен на хосте где и бекенд диски)

# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=1500M --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64
fio-2.2.10
Starting 1 process
Jobs: 1 (f=1): [m(1)] [100.0% done] [11240KB/3900KB/0KB /s] [2810/975/0 iops] [eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2712: Thu Nov  5 06:31:17 2020
  read : io=1124.5MB, bw=14888KB/s, iops=3722, runt= 77339msec
  write: io=384564KB, bw=4972.5KB/s, iops=1243, runt= 77339msec
  cpu          : usr=1.89%, sys=4.39%, ctx=158501, majf=0, minf=8
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued    : total=r=287859/w=96141/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: io=1124.5MB, aggrb=14888KB/s, minb=14888KB/s, maxb=14888KB/s, mint=77339msec, maxt=77339msec
  WRITE: io=384564KB, aggrb=4972KB/s, minb=4972KB/s, maxb=4972KB/s, mint=77339msec, maxt=77339msec




gluster [3733/1302/0 iops]
(клиент сидит ненахосте где бекенд диски)

1# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=1500M --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64
fio-2.2.10
Starting 1 process
Jobs: 1 (f=1): [m(1)] [100.0% done] [14932KB/5208KB/0KB /s] [3733/1302/0 iops] [eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2081: Thu Nov  5 06:47:54 2020
  read : io=1124.5MB, bw=16712KB/s, iops=4178, runt= 68898msec
  write: io=384564KB, bw=5581.7KB/s, iops=1395, runt= 68898msec
  cpu          : usr=2.26%, sys=3.55%, ctx=215041, majf=0, minf=8
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued    : total=r=287859/w=96141/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: io=1124.5MB, aggrb=16712KB/s, minb=16712KB/s, maxb=16712KB/s, mint=68898msec, maxt=68898msec
  WRITE: io=384564KB, aggrb=5581KB/s, minb=5581KB/s, maxb=5581KB/s, mint=68898msec, maxt=68898msec
root@test-gluster-03:/mnt/01#



----------
новый этап установки 

rados = reliable autonomous distributed object storage

как я понял радос это как OSI некая теоретическая модель на которую опираютс но 
котоаря в жизни в чистом виде несуществует.

OSD  = object storage device. включает в себя цпу, RAM,nic, hdd\ssd\raid.

RADOS состоит из кучи OSD и мониторов.
мониторы это это процессы. еще мониторы требут (как они пишут) немного локального дискового пространства

еще я понял что есть cluster map. в нем как то описана вся карта кластера.
это картой управляюи мониторы.

каждый раз когда OSD ломается то меняется cluster map. меняется это epoque.

данные распрееляются по Placement Groups (PG)

Чем больше рзмер кластера тем больлше число PG

данные суются в PG на основе типа имени обьекта. 

наколько я понял  в PG находятся данные которые являются типа квантом данных в том плане
что данные реплицируются целиком размером с PG

далее как то непнятно. они пишут что данные суются в конкретный PG исходя из имени обьекта.
сами PG рассовываются по OSD неким равномерно- раномным способом.

пищут что увеличение ил уменьшение кластера не приводит к тому чтобы эти PG начали 
перетасовываться по OSD


далее они еще раз пишут что данные релицируются не науровне OSD. а на уровне PG.
реплицируются именно пэгэшки. обычно на одном OSD хранится порядка 100 PG


далее. cluster map имеет в сеебе инфо о всех здоровых и нездоровых OSD 
а также их сетевые адреса

для каждого PG кластер мап знает на каких OSD он сидит. и какие из этих OSD
живы  а какие сдохли

если в кластер мап записано об OSD что он  жив и здооров ( состояние up) и запиано что на нем сидит конкретный
PG (состояние in) то он обслуживает реквест (io) об данных в этом PG.

OSD может быть (согласно кластер мап) быть в состоянии down+in это значит что OSD недоступен  а PG который 
на нем сидел еще система неусмела среплицировать на здоровыый OSD восстановив заданный уровень репликации


длаее мало понятно. но  ясно одно что OSD обмениваются друг с другом  мессагами сообщая друг другу
о -- и вот тут мало понятно что они друг другу собщают. то ли о том какие PG на них хранятся. то ли 
собщая полный  кластер мап. еще OSD обмениваются инфо с мониторами.


клиент пишет данные на OSD. а OSD потом реплицирует эти данные самостоятельно по кластеру

далее есть три варианта  репликации у цефа.
- primary-copy
- chain
- splay

primary-copy = клиент пишет на первичный OSD. он в паралель обновляет все копии на других OSD. 
и после этого возрвашает клиенту инфо что запись состоялась.
также вот этот первый OSD он обаратывает не только записи но и операции чтения

chain= как я понял клиент пищет на первичный OSD тот обновляет копии непаралеьно 
а одну за другой по очереди. после обновления последнего OSD идет возврат клиенту
инфо о том что запись прошла успешно.
 при этом операции ччтения первичный OSD не обслуживает.
обслуживает их тот OSD который самый последний в очереди на обновление

splain == как этот метод репликации я непонял


как уже говорил что OSD друг с другом а также с монитором обвенимются  мессагами (messages)
о правда не сомвсес понятно о чем. то ли от тех PG которые хрантся на том или ином OSD. 
то ли о полной карте кластера.  так вот еще OSD посылвает такие же мессаги клиенту.
и может быть и клиен посылает к OSD мессаги но эт неточно.
точно то что месаги имеют epoque поэому типа OSD всегда знает какая мессага относится к более раннему
неактуальному состояию кластера чем это записано в OSD либо прищедшая месага относистя к более новой 
эпохе тоесть отаражает более атуальное состсоение кластера.

насколько я понял из описания - то клиенты цефа они получают карту с PG и таким образом не кластер изначально 
принимает реквесты от клиента и рооутит их на нужный OSD а сам клиент цефа имея карту направляет запрос 
на нужный OSD для доступа к данным с опреденного PG.


вот у нас есть PG и он лежит на таком то колчестве OSD.  так вот среди этих OSD есть первичный osd.
тот который как я понял прежде всего и принимает на себя операции записи.

далее может так случиться что OSD на чтение будет доустпен части клиентов а части небудет.
ии будет доступен части других OSD а части небудет.  тогда част клиентов будет продолжать с него считвать
втовреся как это делать ненадо. 

они вроде решабт проблему так. если копиия на чтение  не получила от своих peer товарией хартбит 
значит она больше операции чтения не обслуживает. дефотовй интервал хартбита 2с.
непонятно как это решает проблему если чтото случилось впределах меньше 2с.

из того что я понял OSD знает PG которые на нем харняться. и он знает для каких PG он не primary PG.
тогда он отправляет для primary PG OSD сообщение о том что тот по его инфо является примари PG.
примари PG узнает что он примари PG.
и этот примари также узнает где лежит реплика PG.
в ответ примари PG шлет всем этим репликам какую важную инфо по этой PG.

насколько я понял процессом восстановление на другие реплики руководит примари PG.
детали неясны.

на монииорах хранисят мастер копия  от cluster map.
нужно чтобы был кворум среди мониторов. чтобы они могли договориться и понять о том какая в итоге мастер 
копия верна.

выбирается некий мастер монитор .далее он собирает вокруг себя кворум мониторов. им разрешаается
рассылть амктуальную версию кластер мапа к OSD и клиентам.

далее есть lease time. по деолфту 2с. через 2с мастер монитор должен своим пирам прислать новый месседж
о том что он жив. тогда они понимаю что мастер жив. если он не прислал значит он сдох и выбирает
нового лидера среди мастеров


к мониторам обрашение за картой кластера от OSD или клиентов приходят редко если все окей.
оно приходит только если клиент обратился к OSD а он мертый. тогда он звонит к монитору. 
или OSD обнаружил мертвого соседа. а так если все норм то всем хватает информации о кластер мапе 
от соседей. клиент получает от OSD а OSD друг от друга. если поломоалась сразу куча OSD а на них лежала куча PG
то на монитор прилетит куча обращений.  чтобы этого небыло OSD  каждый полуслучаный интервал времени пигуют только я непонял
что . может соседей? и это как то охраняет монитор от шквала запрососв в случае обвала  кучи OSD,







до этого моменты мы рассматривали RADOS. это чисто сферический конь в вакуууме на котоором строится
ceph. но по факту цеф состоит из нескоько других компонертов.

ceph состоит из:
мониторы
OSD демоны
менеджеры
метадата серверы


мониторы содержат мастер копию кластер мапа. клиенты получают кластер мап от монитора. это странно
потому что в радосе написано что клтиенты получают кластер мап от OSD. 

OSD демон проверяет свое состояние и состтоние других OSD юнитов и докалдывает об этом мониторам.
это тоже странно. в радосе этого компоненты нет. OSD сами себя тестируеют и тестируют соседей. 
и если што докладывают мониторам.

A Ceph Manager acts as an endpoint for monitoring, orchestration, and plug-in modules. = это вобще
непоняно что значит. но вот еще нашел
A Ceph Manager daemon ( ceph-mgr ) is responsible for keeping track of runtime metrics and the current state of the Ceph cluster, including storage utilization, current performance metrics, and system load.
Тоесть походу это прсто типа мониторинговая хрень ни на что не влияющая.
хотя вот еще нашел = Since the 12.x (luminous) Ceph release, the ceph-mgr daemon is required for normal operations. The ceph-mgr daemon is an optional component in the 11.x (kraken) Ceph release. тоесть в 12 версии вроде бы он уже обязательно 
нужен . бардак какойто

метадата сервер хранить метадату файлов когда к цеф конектятся как  к файловой системе. получается если 
мы конектится к цеф как к блочному устроуству то метадата сервер нам нахер ненужен.

получется металата сервер нахен не встрался, менеждер тоже нахер невстрался, осатаются рельно важные компоенты
это мониторы и OSD демоны.

OSD = object based storage deice

в статеть они пишут что фишка OSD в том что они  содержат инфо не о блоках как на жестких дисках ибо этих блоков просто 
дохера а всего навсего об обьектах. обьектов мало по сравнению с блоками. тоесть блоки это некий лоу-левело инфо 
которая явлется внутрненней для OSD и для внешних чувачков она не предоставляется. наружу проеоставляется инфо 
об обьектах котораых хранстя на OSD и это типа компактная информация. поэтому типа реплиация блочных дисков это 
типа жопа. незнаю в ккаком смсыле. а репликация обьектов это типа очень компактно

далее они пишут вот о чем - что есть фундаметальная проблема сторадж у которого дохрена дисков или OSD без разницы.
проблема в том что если мы добавлем новые диски то они будут либо пустые. либо на них будут только новые даныные.
и прблема тут  в том что диски в системе будут загружены не равномерно. либо будут пахать толдько диски старые
со старыми данными либо только новые диски с новыми данными. а нам надо чтобы пахали все диски равномерно.
тогда приходит идея что если мы добавили новые диски то надо на них скопировать рандомные куски со старых дисков
тогда типа все дисики будут загружены равноемерно.

и вот  они пишут что они разработали алгоритм CRUSH (Controlled replication Under scalable hashing)
алгоритм береь обьект либо идетиификатор обьекта и применяя функцию рассчитывает список девайсов (видимо список OSD)
на которые надо засунуть этот обьект

фишка CRUSH в том что цефу ненужно иметь никакой элемент архитекткру в котором бы хранилась информация о метаданных.
если у клиента есть карта кластера (список здоровых OSD) то клиент может сам чисто по формуле высчитать на какой OSD
надо начать писать данные.  это как если бы клиент фс мог по формуле сам выситать в каких блоках сидит файл 
без обращения к каталогам

CRUSH берет как параметр входной id обьекта, прикладывает к нему некие "placement rules" и кластер мап и на выходе
по формуле получает список OSD куда надо запихать этот обьект x.

кластер мап состоит из devices и buckets. бакеты в себя включают другие бакеты и девайсы.
все эти хрени имеют веса.  девайс имеет вес назначенный админом . вес указывает обьем инфо который девайс
может хранить

дальше по тексту идет куча малопонятной хрени как раобтает crush 


вобщем клиенты и OSD демоны используют CRUSh чтобы самостоятельно определять для обьекта его группу OSD
вместо того чтобы для этого лазить кудато на сторону в кластер

неважно как клиент пытается работать с цеф - толи как с ФС, толи как с блочным устройством, толи как 
с оббьект сторажем => в любом случае внутри цеф влетевший кусок инфо записывается как обьект.

дальшер написнаан непонятная хрень = Ceph OSD Daemons handle read, write, and replication operations on storage drives.
это отличается от радос в котором вобще нет такого элемента архитектуры. репликацией там занимается примари OSD.
чтением занимается либо примари OSD либо он сообщит клиенту с какого OSD читать данные.
также это отличается от тго что выше было написано в доке про демона OSD = OSD демон проверяет свое состояние и состтоние других OSD юнитов и докалдывает об этом мониторам. пока непонятно.
хотя наверное я понял!!! osd daemon про которого говорится в ceph это  OSD в литературе по rados!
теперь все стало понятно. osd в терминах rados это osd daemon в терминах ceph! все теперь стало понятно.
тогда еще раз рассмотрим из чего состоит цеф:

ceph состоит из:
мониторы
OSD демоны
менеджеры
метадата серверы


менеджеры это мусор, метедатат это тоже мусор если мы к цеф неподключаемся как к ФС.
тоода остаются ровно теже комоеннты что и в радос литературе = кластер из мониторов и  OSD штуки. которые 
в цеф называют OSD демоны. 

далее OSD и OSD daemon это одно и тоже.

OSD хранит данные  в виде обьектов. информация об обьектах внутри OSD хранится в виде плоской таблицы. тоесть 
там нет никакой иерархии каталогов. идет просто идентиификатор обьекта + тело данных + доп метаданные.
считай что это как key-value база данных. метедаднные обьекта тоже имеют вид ключ+значение.
какие там метаданные будут прописаны для обьекта - цефу похер, метаданные опредеяет клиент цеф.

object ID уникален в рамках всего кластера OSD

в обычыных стораджах клиент обращается к центральному входному устройству. в цеф этого нет.
в цем клиент на основе CRUSH высчттыаеи к какому OSD ему надо обратиться и лезет сразу туда.
получаем децентрализацию. 



клиенты и OSD должны иметь актуальный "cluster map"

кластер мап состоит из 5 кусков:
- монитор мап
- osd мап
- pg мап
- crush мап
- mds мап


монитор мап содержит инфо о мониторах.
смотрим:

# microceph.ceph mon dump

epoch 4
fsid d64966ad-7849-4b7b-a2f9-79461128965c
last_changed 2023-05-04T14:05:18.608758+0000
created 2023-05-04T14:04:01.430028+0000
min_mon_release 17 (quincy)
election_strategy: 1
0: [v2:10.103.1.27:3300/0,v1:10.103.1.27:6789/0] mon.lxd-02
1: [v2:10.103.1.24:3300/0,v1:10.103.1.24:6789/0] mon.lxd-03
2: [v2:10.103.1.13:3300/0,v1:10.103.1.13:6789/0] mon.lxd-04
dumped monmap epoch 4


итак в микроцефе у нас три монитора.


osd мап содержмит инфо о пулах, osd, pg
смотрим:

# microceph.ceph osd dump

epoch 139
fsid d64966ad-7849-4b7b-a2f9-79461128965c
created 2023-05-04T14:04:03.133182+0000
modified 2023-05-08T19:10:01.088603+0000
flags sortbitwise,recovery_deletes,purged_snapdirs,pglog_hardlimit
crush_version 19
full_ratio 0.95
backfillfull_ratio 0.9
nearfull_ratio 0.85
require_min_compat_client luminous
min_compat_client jewel
require_osd_release quincy
stretch_mode_enabled false

pool 1 '.mgr' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 19 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr

pool 2 'lxd' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 25 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd

max_osd 3

osd.0 up   in  weight 1 up_from 125 up_thru 138 down_at 124 last_clean_interval [103,117) [v2:10.103.1.27:6802/1659,v1:10.103.1.27:6803/1659] [v2:10.103.1.27:6804/1659,v1:10.103.1.27:6805/1659] exists,up 45509eb5-d0a5-4455-b9d9-dc6286a1ba4d

osd.1 up   in  weight 1 up_from 126 up_thru 132 down_at 125 last_clean_interval [105,117) [v2:10.103.1.24:6802/1627,v1:10.103.1.24:6803/1627] [v2:10.103.1.24:6804/1627,v1:10.103.1.24:6805/1627] exists,up fb86c6af-5bab-4921-a9b0-8978f3637432

osd.2 up   in  weight 1 up_from 125 up_thru 138 down_at 115 last_clean_interval [109,113) [v2:10.103.1.13:6802/
1669,v1:10.103.1.13:6803/1669] [v2:10.103.1.13:6804/1669,v1:10.103.1.13:6805/1669] exists,up 74cc655c-6e8c-49c3-8902-dfe38343aa47

blocklist 10.103.1.24:0/2221645565 expires 2023-05-09T19:09:28.925243+0000
blocklist 10.103.1.24:0/3638844332 expires 2023-05-09T19:09:28.925243+0000
blocklist 10.103.1.24:0/2375246677 expires 2023-05-09T19:09:28.925243+0000
blocklist 10.103.1.24:6811/834 expires 2023-05-09T19:09:28.925243+0000
blocklist 10.103.1.24:6810/834 expires 2023-05-09T19:09:28.925243+0000
blocklist 10.103.1.24:0/2720725004 expires 2023-05-09T19:09:28.925243+0000


pg мап содержит инфо о pg



crush map содержит инфо 
об сторедж девайсез. 
посмотреть можно через 

$ ceph osd getcrushmap -o {filename}
$ crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}

к соажалению на микроцефе это несрабатывает


mds мап содержит инфо об метадата серверах 
смотрим:

v# microceph.ceph fs  dump
e23
enable_multiple, ever_enabled_multiple: 1,1
default compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2,10=snaprealm v2}
legacy client fscid: -1
 
No filesystems configured
Standby daemons:
 
[mds.lxd-03{-1:194107} state up:standby seq 1 addr [v2:10.103.1.24:6800/4107889023,v1:10.103.1.24:6801/4107889023] compat {c=[1],r=[1],i=[7ff]}]
[mds.lxd-02{-1:194112} state up:standby seq 1 addr [v2:10.103.1.27:6800/1283618052,v1:10.103.1.27:6801/1283618052] compat {c=[1],r=[1],i=[7ff]}]
[mds.lxd-04{-1:194131} state up:standby seq 1 addr [v2:10.103.1.13:6800/2866487613,v1:10.103.1.13:6801/2866487613] compat {c=[1],r=[1],i=[7ff]}]
dumped fsmap epoch 23


соотвестенно микроцеф неиспользует фс на цефе. поэтому сами метадата серверы есть. но как видно 
никкой цеф фс несоздано


прежде чем клиент начнет юзать цеф ему нужно получить кластер мап. для этого он лезет к монитору.
мониторы обетдиенны в кластер. чтобы был кворум мониторы должны иметь большинство доступных тоесть
1, 2:3, 3:5, 4:6



далее. для аутентификции клиента цеф юзает shared secret метод.
это метод кога и сервера и уклиента есть тот же самый secret key
аутентификация работает типа как у керберос так они пишут.
как я понял клиент обращается на монитор. тот его аутентиифицирует и обратно возвращает session тикет. причем этот тикет 
зашифрован тем же secret key что есть и у клиента. далее с помощью этого session тикет клиент повторно обращетеся
к мониторму и закаызаывает сервисы которые нужны от OSD. монитор возврашает новый тикет с помощью которого 
уже клиент будет аутентифицироваться+авторизоваться уже на OSD. все эти тикеты они временные. тоесть они протухают 
во времени и их нужно получать заново.

чтобы заюзать аутентификацию человеек должен обартиться к цеф и создать юзера. при этом цей создать учетку и secret key
и сообщит обратно. и также сохорнаить копию на мониторе.

и еще раз проговрим как раобтает аутентификация у цефа. вот мы завели юзера на цефе. и получили secret key.
теперь для аутентификации мы обращаемся через клиент к мониотору. вводим свой secret key. нам воттвет монитор 
создает session ticket , шифрует его нашим secret key и возвраащает нам. мы с помощьью нащего secret key расшироываем
и получаем на руки session ticket. он валиден какоето время. потом его надо заново получать. он валиден 
как говорится на период нашей сесссии.  тееперь юзая наш session ticket (правда непонятно как кокнретно) мы обращаемся
на монитор и он нам создаем еще один тикет (названия  у него нет). и именно этот тикет мы предьявляет на OSD
для аутентиификации+авторизации


в цефе клиент, мониторы, OSD все знают кластер мап. поэтому могут обаться друг с другом напрямую.

OSD периодически шлет на монитор мессаги о том что он жив здоров. если это прекратилось то монитор 
делает вывод что OSD сдох

OSD также может опредеить то что сосед OSD сдох и тоже об этом доложить монитору.

OSD ежедневно делает легкий scrub. тоесть обращется к соседним OSD на которых хранятся реплики PG.
и сравнивает метаданные у себя локальные с метаданными у соседей для конкретного PG.
а еженеделно OSD делает уже тяжелый scrub. когда уже сраванивает по контрольным суммам  тело или данные для PG 
у себя с тем же самым у других OSD у которых хрантся реплики данного PG

клиент как я понял по object id опредяет pool и PG для обьекта. и потом применяя формулу узнает primary OSD.
OSD делая туже хрень узнает все множствео OSD на которых должны хранится реплики для этого PG.
клиент пишет на примари OSD. а этот примари OSD пишет на все реплики. и уже когда все реплики записаны 
то пишет обратно клиенту что процесс записи успешно завершен

обьект в цеф имеет принадлежность к пулу.
кто опредяет параметры пула пока неясно
но ясно то что своствами пула являются

Ownership/Access to Objects
The Number of Placement Groups, and
The CRUSH Rule to Use.


пул имеет какое количество PG.
CRUSH алогоритм делает маппинг object-id --> pool --> pg --> список OSD 
так как кластер мап меняется то для разных кластер мапов для одного и того же object-id будет пполучатся
разный список OSD сереров

для того чтбы клиент узнал список OSD ему в качестве входных парамтров нужно:
- object-id
- pool-id
- cluster map

кластер мап клиент получает от монитора.
object-id клиент должен сам знать 
откуда брать pool-id пока неясно

применяя object-id + pool-id на кластер мап клиент получается список OSD серверов

у меня такое ощущение что пулы создает сам клиент предваретельно в цефе. и кода клиент хочет сохранить обьект 
то клиент должен сам определится в какой пул он хочет засунуть обьект.

насколько я понял все OSD которые хранят PG тестно друг с другом общаются провреяя здоровье друг друга.
и если чтото нетто то стучат об этом монитору

primary OSD - он один для PG. он и только он принимает запрос от клиента на запись в эту PG

весь набор OSD кооыре хранят PG назыается Acting set. те серверы который в Acting set находятся в состонии UP
называются UP SET.

erasure coding = чтото ттипа raid5 кодиования.  в чем прикол. прикол в том что экономитс место как я понял
если у нас голые реплики то мы теряем много места. например если у нас 2 реплики то три OSD хранят инфо по обьему
одного OSD. а если унас raid5 кодиорвание. то 3 OSD могут хранить инфо по обьему как 2 OSD. 
так вот как я понял уровень erasure coding определеяется на уровне пула.


из того что я понял про формулу эрейзинг кодирования. если указано 3+2 к прмиемеру это значит
что данные делятся на 3 части. каждая часть сохранятеся на свой OSD. и еще высчтываются 2 части доп данных 
это коды кодирования. эти части сохрнатяются на отдельные OSD. итого на 100% обьема данных будет еще 
дполнительно заниматься 40% под коды. 

у цефа есть еще такая прокладка как cache tier.
типа можно иметь кэш таир из быстрых дисков и обыкновенный бекенд из медленных дисков.
кэш работет прозрачно. детали надо читать дополниетельно

естьтакая штука bluestore. как я понял это метод с помощью котлрого хрнаятся данные на дисках на OSD
машинах. это как ихяяя цефоская файловая система на дисках.


BlueStore opens block devices in O_DIRECT and uses fsync frequently to ensure that data is safely persisted to media. 

я помотрел что значит O_DIRECT. значит в линуксе мы открваем файл на запись. или устойство на запись
и при этом указыаем флаги. флаг O_DIRECT формально означает что мы говорим ядру что мы нехотим чтобы
при записи использовася кэш линукса (page cache) при записи. мы хотим чтобы данные сразу летели в устройство.
однако как написано на стековерфлоу по факту это нихрена незначит что когда мы пишем на диск и нам прилетает 
подтверждение что данные приняты то данные реально записаны на диск. они могут застрять где то там в кэше диска
или еще где а до пластин диска еще не долететь. поэтому этот фалг негарантирует что данные записаны на диск
как нам о том сообщит ядро. а только как я понял дает то что данные не будут дублироваться в page cache линукса.

fsync как я понял - да. он гарантирует что данные были записаны на диск. 


You can evaluate a drive’s low-level write performance using fio. For example, 4kB random write performance is measured as follows:


# fio --name=/dev/sdX --ioengine=libaio --direct=1 --fsync=1 --readwrite=randwrite --blocksize=4k --runtime=300

далее они пишут что диск в линукск может работать в двух режимах : write back и write thgrouh.
когда на диске активирован его дисковый встроенный кэш то он работает в реэиме write back.
проверть в каком реэиме работает диск можно через 

# cat /sys/class/scsi_disk/*/cache_type

прикол в том что для дисков на яндекс облаке вобще такой папки нет. 
и у nvme дисков такой папки тоже нет.

вчем прикол с этим кэшем. прико в том что данные пишутся не надиск а в этот кэш а уэе потом переносятся на диск.
так вот если запускать fsync то эта команда говорит - перенеси все данные из этого кэша уже на пластины.
поскольку цеф постоянно запускает fsync то вместо того чтобы сазу писать на диск  ос пишет сначалаа в кэш 
диска а потом опустошает этот кэш перенося данные на пластины. как пишет дока это существенно повысить
лейтенси для иопсов. при записи иопса с выклченных кэшем у нас иопс будет писаться долго, а с включеным кэшем
у нас иопс пишется быстро но потом долго ждать пока опустошится буфер перенося все что в нем есть
из кэша на пластины. и якобы это будет суммарно дольше чем сразу писать на пластины


они даже предалгаюь udev правило чтоб руками это дело ненастраивать

# cat /etc/udev/rules.d/99-ceph-write-through.rules
ACTION=="add", SUBSYSTEM=="scsi_disk", ATTR{cache_type}:="write through"


вотт тут https://docs.ceph.com/en/latest/start/hardware-recommendations/ в самом низу
указано сколько ядер цпу и дисков должно быть на  мониторах и OSD и метадата нодах

получается cpu\RAM на минималках

OSD: 1cpu 
     4GB

монитор: 2cpu
         4GB

mds: 2cpu
     2GB




у цеф есть  алгоритм страйпинга. я не очень понял как он конфигурится. такое ощущение что это все настраивется на клиенте.
идея вробы бы такая - берется исходный обьект. и делится на несколько обьектов цефа. посолькуо каждый обьект 
хранится на своем списке OSD то запись идет так: пишется небольшой кусок первогоо обьекта. потов небольоой кусок 
втрого обьекта. причем писать на первый и второй обьект можно одновременно. и так достигается увеличение скорости. 

по архитектуре цеф вобщем то для начала все.


я продолжаю описывать как работает архитекутура цефа.
цеф на выходе выдает три типа ресурсов: блочное устройство (RBD),
файловую систему (CephFS), S3 (rados gateway)
s3 это чтото типа фтп сервера. еще s3 называют object storage.
я поискал в чем разница ftp vs s3 и зачем нужен s3 если есть ftp.
ничего ненашел. 


еще раз поговорим про компоненты цефа:

OSD - типа хрень на которой лежат диски на которых реально сохоаняются данные
это бекенд. как  я понял на одном физ компе можно поднять несклько OSD. 
пока мало понятно как это сделать . тем не менее.
OSD = Ceph Object Storage Daemon

manager 
monior
librados


object name, pool, cluster-map


pool.
для клиента пул это один из парамтров которыый нужно указать когда клиент хочеь 
записать обьект на цеф. клиет обязательно указывает пул в который он хочет это записать.

для самого цефа  это как LUN на рейд контроллере. это как раздел на диске. 
это первый такой уроень логический чтобы разрезать резделить весь кластер данных
на куски. 

пулы на цефе создает админ цефа. имя  или номер пула админ цефа передает клиенту
вместе с логином и паролем.

клиенту чтобы сохранить обьект на цефе нужно иметь:
- обьект который мы хотим сохранить. 
- имя пула
- логин пароль
- установленную на компе клиент цефа (который опирается на билиотеку librados)
- cluster-map от цефа который клиент должен получить от MON (монитора) цефа

данные хранятся на OSD юнитах. что такое OSD еще буду прояснять.

насколко я понял клиент высчитыывает на какой PG (placement group) нужно засунуть
обьект. высчиав PG клиент высчитывает на каком OSD (каждый OSD имеет IP адрес)
этот PG хранится. узнав IP клиент стучится на этот OSD и соощает ему мол запиши обьект
в такой то PG такого то пула. дальше уже работа OSD куда и как внуьрь себя пихать обьект.
что такое PG. наверно это некий аналог папки на ФС. как обьекты конкретно напиханы внутри PG сам цеф (его мониторы) незнают и не парятся. это некая внутреннняя кухня OSD. 
насколько я понял cluster-map которую хранят в себе MON цефа хранят в себе только OSD 
карту и как я понимаю карту распределения PG по этим OSD (но это пока неточно).

еще раз освящу момент как клиент работает с цефом.  админ цефа созддает на цефе
и логин пароль. и передает клиенту. клиент ставиит клиент цефа который основан (нез наю
зачем это они уточняют) на librados билиотеке. нам в принципе похуй на какой библиотеке.
также админ цефа сообщает какие IP адреса имеют мониторы цефа.
тогда клиент когда хочет читать писат на цеф некий обьект он . берет логин пароль 
и клиент цефа через логин пароль стучит на монитор цефа котоырй указан в конфиге 
и получается cluster-map. 
в нем как я понимаю прписаноы пулы, osd и PG(распредение pg по osd и пулам).
клиент говорит клиенту цефа что хочет записать обьект (файл например) в цеф в такой то пул.
клиент цефа выщитывает на какой PG будет записан этот обьект и на каком OSD этот PG лежит.
полсе этого клиент звонит на OSD и сообщает что хочет сделать. 

хочу поговоть про пулы и PG. пул предтсвляет собой набор PG на ряде OSD. 
PG этот чтото типа папки на ФС. потому что внтри PG сохранены обьекты клиентов.
кода создатеся пул то там укаызается сколько PG он будет содержать. причем что непонятно 
это то что никак не оговриывается размер кажодго PG. они динамические чтоли?
далее как я понял цеф он распределяет эти PG пустые по всем OSD равномерно. вот эти PG 
это как бытто папки . которые растут динамическт во времени. если колчиество OSD вырасстает 
то происходит пререаспредленеие PG по OSD. есть еще момент при созании пула оговаривается 
уроень редунданси данных. наприер 3. это значит что каждый обьект доолжен иметь три копии
в цефе. тоесть оригинал и еще две штуки. а так как обтект сохранятеся в PG то получается что
если мы записали обькт в цеф то он записался в какие то три PG. поэтму у нас полкчается что 
скажем у нас 10 OSD а scale=3. и получается что цеф старется разбрасыывать по три файла 
все время на раные oSD чтобы суммарно они были заполнены более менее равномерно по обьему.
может получсится такая ситууация . мы записали 10 000 файлов по 4KB  каждый из них имеет три
копии на 10 OSD. так как файлы мелкие то в целом заолнеение по обьемы будет более менее
равеноменое. поулчется что примерно кажлый OSD будет заполенен на 12MB каждый OSD.
а потм мы взяи и запиали 1 файл а 400МБ. поулчсется что на какие то три OSD будет 
записано по 400MB. и получется что все OSD заполенены на 12 MB а три OSD на 412MB.
вот так неравномерно.

на что влияет число PG при создании пула. представим что на кластер всего один пул так 
что про сам пул можно забыть. это как диск с одним разделом. то есть похеру на пул.
остаются PG. на что влияет число PG.  пусть  у нас 3 OSD. и какая разница у нас 3PG
или 3000 PG. 

если у нас 3 PG. и scale=3 то каждый обьеки будет помещатся на один OSD а его две копии
на два других OSD.  мне эти PG напомниают шарды в эластике. скажем у нас 
три ноды и мы создаем индекс . индекс это аналог пула. пусть у нас кластере эластика
из трех нод создан один индекс из трех шардов. один щард для записи и чтения а два для
чтения. мы пишем обьект в одну ноду в ее шард и остальые две копи улетают на
две другие ноды на ее шарды. по мне так если ноды три создавать более чем три шарда 
нет никакого смысла!  единственный смысл создавать более чем три шарда это если мы планиурем 
в дальнейшем расштряться расширять число OSD тогда при увеличении числа нод (OSD) нам надо 
и увеличивать число PG. число шардов потому что если шарда три а нод стало 10 то семь нод
просто будет стоять пустые.  также возможна друая ситуация у нас 10 OSD и у нас 10PG
и у нас сломалась одна нода. осталось 9 OSD это значит что шард (PG) с 10 умерешей ноды
будет создан скопирован склонирован на одну из оставихся нод.  получается на 8 нодах 
у нас по 1 PG а на девятой ноде у нас 2 PG. получается неравновесно некрасиво. 
а теперь положим что у нас было 10 OSD и 90 PG распределенных по этим 10 OSD. получатеся 9PG
на одну OSD
если одна нода сдыхает то нужно восстановить 9 PG. и распеделить их равномерно по 9OSD
и мы получаем красивую картину потому что 9 PG отлично делится на 9OSD.
и тоода у нас будет 10 PG на каждом OSD. будет равнмоменое распределение по обьему
или скажем по числу PG на кжадом OSD. это хорошо. плюс получсется что в случае кода 
 унас 90 PG было в пуле то каждый PG меньше по размеру чем размер PG кодга их было 10.
 это значит что воссоздать каждый pG из 9 пропавших гораздо быстрее поэтому 
 восстаовление потерынных даных будет гораздо быстрее. это тоже хорошо. как уже стало 
 понятно при поломке OSD данные восстанвливются не на уровен отделных обьектов а на уровне
 PG. в случае долбавеия либоу удаления OSD  у нас движуха между OSD идет на уровне
 PG а неотдельрых обьектов. как и в эластике между нодами дивгаются шарды а не отдельрые 
 обьекты. я не помню уже как в эластике можно ли там иметь неколько шардов на запись 
 в индексе или только один шард на запись а остальыне на чтение. но в цефе схема такая.
 у нас ест пул. он состоит из кучи PG. мы пишем обьект в некоторый PG и если scale =3
 то еще в два PG будут записаны копии. когда мы пишем другой обьект то он скорей всего 
 будет записан в другой PG и еще две другие копии в еще другие PG. поэтому у нас как бы полчается все время что меняется шард на запись. для разных обьектов будет другой 
 шард на запись. и получается вот что если  унас умирает две ноды (два OSD) 
 то для какойто части данных так совпадет что умрет две копии этих данных останется только 
 одна копия этих данных. если у нас умрет три ноды из всего набора то как я понимаю 
 для каойто части обьектов умрут все три копии. и тут получается как я понимаю похуй
 сколько PG было в пуле.  тоесть если у нас 4 ноды и  хоть 4 PG хоть 1044 Pg если
 умрет три ноды при scale =3 то для 1\3 всех данных у нас умрут все три копии. 
 и здесь спасает только либо увеличение scale. либо увеличение числа OSD. 
 действительно если у нас скажем 1ТБ данных разделить по 10OSD и умрет 3 нода при scale=3 то погибнет некотора ячасть данных. а если  у нас 100 OSD и умрет 3 OSD то тоже какая то 
 часть обьектов при этом потеряет все свои копии. но это будует гораздо меньшая часть данных.
получается интересная вещь - увеличение числа PG вобщем то никак не влияет на выжиываемость
дьюрабили данных. она зависит от другого. она зависит от scale. чем больше scale тем больше
выжиываеомсть. и чем больше OSD тем при томжк scake у нас погибнет меньше данных при гибели 
того числа нод чему равен scale. тоесть если scale=4 и у нас умерло 4 нода в составе 10 нод.
то точно данные будет потеряны какя то часть. и если умрет 4 нода в составе 100 нод то тоже
будут потери номеньше. но будут. увеличение числа PG позволяет при помирании ноды 
оставшие PG равномерно распределить по нодам . и восставновить их гораздо быстрее.
тоесть число PG влияет на равномерность забистости места на нодах при увеличении числа
OSD или уменьшении. и влияет на быстроту восстаноавления потерянных проавших PG при аварии.
PG напинают мух. а ноды напоминают банки. чем больше мух тем легче их распределить по банкам
равномерно. причем если мух много то увеличение и уменьшегие числа банок не будет влиять 
на равномерность рспределение.  прадв тут еще такое дополнение если мух становится болше
то их размер ставноися меньше. либо можно сравнить вот так - чем ментше мух тем
они заменяются более крупными живаотными скажем слонами а чем боьше мух тем они заменяются
микробаами. поэтому елси у нас скажем 10 банок и три слона то их тяежлело перетскивать с банки
в банку. а если у анс 10 банок и 90кошек . то случае поломки банки взять и засунуть в кажду
из 9 банок по одной кошке это оченб быстро. а если у нас 100 банок и 1000 мух. это получется
по 10 мух на одну банку. если разбить одну банку то надо восстаровить 10 мух. тоесть взят
10 банок и туда засунуть по одной мухе. это вобще быстро.
я бы сформулировал правило указания числа PG при создании пула вот так. - все зависит
от числа OSD. если  у нас 10 нод. то надо чтобы на одной ноде было 10-1 PG. 
это это занчит что суммарно должно быть (10-1)*10 PG. если n это число OSD
то число pg должны быть (n-1)*n.   тогда при поломке 1 OSD у нас   в каждй из оставищихся
oSD нао будет засунуть ровно 1 PG.  
если oSD = 10 то PG = 90
если OSD = 3 то PG = 6
ну и надо конечно прикидывать поломку скольктх OSD мы хотим заложить. если скажем
у нас 10 OSD а мы хотим захложить поломку 2 OSD . то надо чтобы PG равнмоерно раскладывался
и на 9 и на 8 OSD.  получается 90 на 9 делится. а на 90 на 8 нет. 
тость число должно делится на цело на 10 и 9 и 8 . получается надо найти наиментшее
общее кратное для 10 и 9 и 8 это 360. получается надо создавать пул из 360 PG.
это по 36 PG если все ок. и по 45 PG если оставлось 8 OSD. ну и если мы будем расширтся
до 12 OSD то более менее оно равнмерно ложится и на 11 и на 12 нод.

при записи в цеф мы пишем в один PG в один OSD. а этот OSD сам находит и пишет в два 
других OSD (если scale=3) и толко потом возрашвает программе сигнал что данные
записаны. 


отойду малек в сторону. цеф как софт состоит  из нескольких процессов демонов:
вот пример с хоста где микроцеф крутится

ceph-mds -f --cluster ceph --id nl-test-01
ceph-mgr -f --cluster ceph --id nl-test-01
ceph-mon -f --cluster ceph --id nl-test-01
ceph-osd --cluster ceph --id 2

тоесть когда мы говорим про например  OSD то это по факту прям один процесс "ceph-osd"
тоесть когда мы говоиим про MON, OSD, MGR то это на нижнем уровне прям конкретные сингл
процессы. 

OSD - этот демон имеет у себя на бекенде диск или диски. на них по факту и сохраняются
данные. это конечна точка куда прилетают данные. ниже уже нет ничего. (Object Storage Daemon)
OSd нетолько хранит данные или отдает данные. он еще обается с соседями. проверяет
их здоровье. делает репликацию PG. когда мы говорим про OSD мы представлем себе не 
комп вместе с демоном и дисками. а именно сам демон. это уже и есть OSD.



MON - эта хрень не про банальный мониторинг. нет. это супер важный демон.
он хранится cluster-map. также я прочитал что он отвечает за аутентификацию.
и он ответчает за мембершип других демонов кластера. A Ceph Monitor maintains a master copy of the cluster map. A cluster of Ceph monitors ensures high availability should a monitor daemon fail. Storage cluster clients retrieve a copy of the cluster map from the Ceph Monitor. (monitor)

MGR  - с этим демоном непонятно. якобы он просто делает мониторинг. но 
в последних версиях почемуто эта хрень является обязательной.. пока неясно с этим демоном
нахер он сдался.тем более обязстельно.  Ceph Manager acts as an endpoint for monitoring, orchestration, and plug-in modules. но пока это мало что говорит мне. (manager)

MDS - этот демон нужен вроде как для того если мы юзаем cephFS. если мы его неюзаем
то нахер он несдался. (MDS) manages file metadata when CephFS is used to provide file services. (metadata server)

далее. раньше на дисках OSD хранил данные на базе xfs.
но щас у них свой какойто родной формат хранения называется BlueStor. как это 
там выглядит на практике. как это можно посмотреть. какие утилиты это могут показать
пока хер его знает


из этих четырех видов демонов состоит цеф софт кластер

как я понял обьекты хратся на bluestor без папок. 
обьект в цефе имеет уникальный ID в рамках всего цеф кластера

цеф демоны цеф софт прдразумевает что клиенты и OSD демоны (сокращенно OSD) 
знаю топологию кластера. для этого они обращаются к мониторам и оттуда скачивают
cluster-map


переходм из чего состоит cluster-map:
она состоит из пяти кусков


1) monitor map.
ее можно заценить вот так

# microceph.ceph mon dump
epoch 4
fsid 2f48afc6-22a3-4688-a09f-4d8a011bb7da   <=== это id кластера
last_changed 2023-06-22T18:26:56.404865+0000
created 2023-06-22T18:19:09.236944+0000
min_mon_release 17 (quincy)
election_strategy: 1
0: [v2:172.16.10.10:3300/0,v1:172.16.10.10:6789/0] mon.nl-test-01  <== адреса мониторов
1: [v2:172.16.10.11:3300/0,v1:172.16.10.11:6789/0] mon.nl-test-02
2: [v2:172.16.10.12:3300/0,v1:172.16.10.12:6789/0] mon.nl-test-03
dumped monmap epoch 4


это буквально из чего состоит monitor map кусок



2) OSD map
ее можно буквально посмотреть вот так

# microceph.ceph  osd dump
epoch 100
fsid 2f48afc6-22a3-4688-a09f-4d8a011bb7da
created 2023-06-22T18:19:11.449007+0000
modified 2023-06-29T22:51:52.209296+0000
flags sortbitwise,recovery_deletes,purged_snapdirs,pglog_hardlimit
crush_version 13
full_ratio 0.95
backfillfull_ratio 0.9
nearfull_ratio 0.85
require_min_compat_client luminous
min_compat_client jewel
require_osd_release quincy
stretch_mode_enabled false
pool 1 '.mgr' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 20 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr
pool 2 'k8s' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 30 lfor 0/0/28 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd
pool 3 'kubePool' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 38 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd
pool 4 'cephfs_data' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 50 lfor 0/0/46 flags hashpspool stripe_width 0 application cephfs
pool 5 'cephfs_metadata' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 50 lfor 0/0/46 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 6 'KubePool2' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 20 pgp_num 20 autoscale_mode on last_change 82 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd
max_osd 3
osd.0 up   in  weight 1 up_from 92 up_thru 96 down_at 91 last_clean_interval [56,85) [v2:172.16.10.11:6802/6095,v1:172.16.10.11:6803/6095] [v2:172.16.10.11:6804/6095,v1:172.16.10.11:6805/6095] exists,up e40e7c92-e3d0-4dfc-bfeb-f55761e32f6e
osd.1 up   in  weight 1 up_from 91 up_thru 96 down_at 90 last_clean_interval [57,85) [v2:172.16.10.12:6802/4500,v1:172.16.10.12:6803/4500] [v2:172.16.10.12:6804/4500,v1:172.16.10.12:6805/4500] exists,up 83133952-1abc-420c-9db7-ee36b4edc540
osd.2 up   in  weight 1 up_from 96 up_thru 96 down_at 95 last_clean_interval [60,85) [v2:172.16.10.10:6802/4382,v1:172.16.10.10:6803/4382] [v2:172.16.10.10:6804/4382,v1:172.16.10.10:6805/4382] exists,up e012050f-a1c4-44b5-b248-1741caddf48c
blocklist 172.16.10.10:0/1291720937 expires 2023-06-30T08:15:32.668281+0000
blocklist 172.16.10.10:0/1043716579 expires 2023-06-30T08:15:32.668281+0000
blocklist 172.16.10.10:0/2242167631 expires 2023-06-30T08:15:32.668281+0000
blocklist 172.16.10.10:6811/812 expires 2023-06-30T08:15:32.668281+0000
blocklist 172.16.10.10:6810/812 expires 2023-06-30T08:15:32.668281+0000
blocklist 172.16.10.10:0/1603527195 expires 2023-06-30T08:15:32.668281+0000
blocklist 172.16.10.11:6809/2585122472 expires 2023-06-30T08:15:12.258239+0000
blocklist 172.16.10.11:6808/2585122472 expires 2023-06-30T08:15:12.258239+0000


из того что я понимаю в этой карте. то тут есть список хостов
где сидят OSD демоны
нпример

osd.0 up   in  weight 1 up_from 92 up_thru 96 down_at 91 last_clean_interval [56,85) [v2:172.16.10.11:6802/6095,v1:172.16.10.11:6803/6095] [v2:172.16.10.11:6804/6095,v1:172.16.10.11:6805/6095] exists,up e40e7c92-e3d0-4dfc-bfeb-f55761e32f6e

его IP = 172.16.10.11


еще в нем есть инфо про пулы в кластере:
сразу скажу про два пула. это 'cephfs_data' и 'cephfs_metadata'. если мы развернули
MDS демон то эти два пула будут созданы автоматом.

пример пула из списка вверху
pool 6 'KubePool2' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 20 pgp_num 20 autoscale_mode on last_change 82 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd

пул имеет название 'KubePool2' его id=6,
 у него 20 PG (pg_num 20)
еще у него есть такая очень хуево описанная вещь как pgp_num = 20
вчем же разница между pg_num и pgp_num ? я искал пару часов пока наконец собрал
собираееьный образ и понял в чем разница и кто за что отвечает.
вот у нас есть pool . это по сути кучка PG. скажем 100 штук. тоесть пул это как коробка
а внутри нее кирпичики лего. PG это кирпичик лего.  значит для понимания скажу еще одну
вещь когда клиент цефа определяет в какой PG его пихать то он это делает на основе
CRUSH алгоритма я его будут ниже разбирать но в первом приближении то в какой PG надо
засунуть обьект зависит от hash значения вычисленного от имени обьекта. там берется
вычисляется хэш обьекта и берется число PG состоавляющих пул все это вместе
както суется в формулу и в итоге она выдает в какой из PG надо совать обьект. 
еще вжано понимать что клиент цефа берет информацию о том сколкьо  PG в составе пула
из cluster_map.  так вот насколько я понимаю pgp_num это сколько PG на данный момент 
может использовать клиент в своих расчетах. а pg_num это сколько PG  в реальности 
прописано в свойствах пула. но эта цифра как я понимаю не участвует в расчетах клиента цеф.
как же так может получится что pg_num отличается от pgp_num.  а вот как - можно задать команду
и она изменит число pg_num изменит число PG в пуле. что при этом произойдет. они это называют
так что часть или все PG будут расщеплены (split) на две части. если скажем было у нас
pg_num=16 а стало pg_num=32 то это будет занчит что каждый из PG будет расщепден на две части
действиетльно как из 16 поулчить 32. надо каждый кусок из состава 16 разделить по полам.
но эти суки они не пишут того что разделение выглядит так что старый PG он не трогается .
он остается как есть. а просто рядом с ним создается новый пустой PG. таким образом
при этой операции с данными ничего не проихсодит. со старыми PG тоже ничего не проиисходит.
прлсто нарезаются новые пустые PG. и фишка еще в том что эти новые PG на них неразрешается
ни писать новые данные от клиента ни перетаскивать данные из старых PG в новые PG. то есть
на самом деле никакого split суки не происходит. просто создается ряд пустых болванок. 
но в них даже новые данные писать нельзя. потому что как я уже сказал как я пнимаю клиент
когда смотритв cluster_map то не смотрит на pg_num он смотрит только на pgp_num.
поэтому если мы изеили только pg_num то кроме создания нвоых пустых PG  в которые 
ни данные от клиента новые ни данные из старых pG никогда не попадут (как они пишут усебя
pg_num не участвует в CRUSH алгоритме). тоесть скажем у нас было

pg_num=16 pgp_num=16. окей все понятно. мы изменили на
pg_num=32 pgp_num=16. по факту вобщем то почти что нихуя в пуле не изменилось.
а далее мы начинаем делать очень интерсуню операцию мы увеличиваем pgp_num скажем на 1
pg_num=32 pgp_num=17 и вот теерь начинает происходит очень важная хуйня.
клиент при запросе кластер мапа уже увидит что в пуле стало не 16 PG  17 PG 
поэтому +1 новый пустой Pg начинает участовать в том что в него начнут пихаться новые данные.
и также как я понимаю OSD сам автоматом начнет из какого то старого PG перекачивать часть
данных в этот +1 PG. если мы хотим чтобы у нас при этом процессе перформанс не ушел в ад
то мы будем уувеличивать pgp_num потихоньку. а если конечно мы увеличим pgp_num 
за один раз до 32 то конечно начнется ад. как я понимаю ад начентся не из за того что
из за клиентских запросов в эти новые +16 PG тоже полетят данные. а из за того что
цеф OSD начнет массово пеерносить часть данных из старых PG в новые PG.
кстати нахера цеф начнет пееносить данные. он начнет это делать чтобы небыол такой херни
то в новых PG лежат только новые данные и эти новые PG станут горячей точкой отказа.
когда унас и старые данные и новые перемещаны равномерно по всем PG это несоздает
узких точек отказа. 
поэтому pg_num это число Pg  в пуле вместе со старыми и новыми PG. одни с данными
вторые пустые. а pgp_num это то число PG которое доступно клиенту для использования
в том числе для алгоритма CRUSH. поэтому pgp_num это как бы внешняя цифра для клиента
а pg_num это цифра раскрыающая внутренюю кухню пула.
очень странно что значение pgp_num вооббще мегахуево описано в инете я заебался 
это искать. 



