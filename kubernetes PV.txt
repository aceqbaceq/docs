kubernetes persistent volume

PV - persisytent volume это кусок диска который кластер имеет как ресурс
для того чтобы его выдавать потом подам. 

persistent volume claim - это свойство пода. под через клейм заказывает
у кубе кусок диска. тоесть pv это кусок диска имеющийся на стороне 
куба а claim это заказ со стороны  пода  к кубу мол заказываю
10ГБ диска.

разница между PV и PVc. pv это параметр кластера это сколько есть у кластера
а pvc это запрос от юзера заказ на то чтобы сколько отрезать от кластера.

когда мы заранее готовим некий набор PV's это называется статическая подготовка PV . есть еще динамическая. когда PV в система появляется незаранее
а по запросу. это можно сравнить с тем что диски на компе могут быть предустановлены в комп.  а могут быть(предположим) появится в нем по запросу.ну по типу того что послал программный запрос и к виндовсу подключился iscsi lun.

динамическое появление PV связано со сторадж клаассами . об этом потом.

шарманка работает в целом так , в от PVC в куб поступает запрос на то чтобы он выделил
кусок диска. куб ищет есть ли  у него PV который удовлетворяет 
параметрам заказа в PVC. он ищет в предустановленных статических PV если там 
нет то пытается динамически создать PV , чтобы динамически создать PV - PVC должен обратиться к storage классу

походу чтобы минимально чтото заработало надо иметь сетевой сторадж
типа NFS. так что надо собрать NFS

так. значит начал я делать пробовать создать PV потом создать
PV claim потом создать pod который бы запрашивал PV.

значит вначале я попробовал создать PV тип которого hostPath
хотя в yaml нигде не указано hostpath 

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

так вот с этой хренью ничего не получилось . потому что вроде как 
в одном месте я нашел что hostpath pv можно использовать 
только якобы на куб кластере с одной нодой.
у меня их три.
короче когда я пытался создать этот PV то мне куб выдавал ошибку.

окей. 

стало понятно что надо ставить nfs сервер. и использовать PV nfs типа.

как поставит nfs сервер

# chown nobody:nogroup /var/nfs
# apt-get install nfs-kernel-server

# cat /etc/exports
/var/nfs    172.16.102.0/24(rw,sync,no_subtree_check,no_root_squash)

/var/nfs = папка на сервере которую мы расшариваем
172.16.102.0/24 = клиенты которым можно подключаться
no_root_squash = эта опция нужна потому что при копировании файлов 
клиент также и устанавливает пермишнс на копируемый файл. так вот 
по дефолту на nfs сервере запрещено в расшаренной папке уставливать
владельцем файла пользователя root. и когда мы под рутом копируем файла
на nfs папку то клиентский линукс неможет установить владельцем файла рута.
и вылезает ошибка. вот чтобы рут мог стаовиться владельцем файлов и папок
в nfs папке испольщуется опция no_root_squash


# exportfs -a
# systemctl enable nfs-server
# systemctl start nfs-server

как посмотреть какие шары наш nfs сервер опубликовал. 

первый способ

# exportfs -v
/var/nfs        172.16.102.0/24(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)
/var/nfs2       172.16.102.0/24(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)

второй способ

# showmount -e localhost
Export list for localhost:
/var/nfs2 172.16.102.0/24
/var/nfs  172.16.102.0/24

кстати в случае nfs шары называются экспортами


как подключаться к nfs серверу:
на линуксе клиенте нужно поставить пакет

# apt-get install nfs-common
после этого nfs папку можно монтировать через mount 

# mount -t nfs  test-nfs-01.mk.local:/var/nfs /mnt/nfs

окей. сервер nfs установили настроили. в целом как маунтить эту папку
на клиенте понятно. возвращаемся к кубернетесу

надо обязательно поставить пакет nfs-common на все дата ноды
иначе при попытке mount -t nfs будет писать ошибку. 

теперь привожу Pv.yaml 
скажу сразу что примеры из доков куба гавно. они нерабочие.
поэтому пришлось искать примеры на стороне.
тип бекенда прописан nfs 

apiVersion: v1
kind: PersistentVolume
metadata:
  name: portal-info-data-pv
spec:
  accessModes:
    - ReadWriteMany
  mountOptions:
    - hard
    - vers=4.0
    - timeo=60
    - retrans=10
  capacity:
    storage: 35Gi
  nfs:
    server: test-nfs-01.mk.local
    path: "/var/nfs"
  persistentVolumeReclaimPolicy: "Recycle"


начнем разбирать эту мантру пока поверхностно.
name: portal-info-data-pv=имя PV под которым будем к нему обращаться


mountOptions:
    - hard
    - vers=4.0
    - timeo=60
    - retrans=10
это параметры которые куб будет использовать  в команде mount -t nfs 

вот я раскопал какой mount потом использует куб для маунта нашей nfs 
папки при создании пода

mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60 test-nfs-01.mk.local:/var/nfs /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv

nfs: = насколько я понял это мы указали что тип  бекенда PV это nfs сервер

 server: test-nfs-01.mk.local
    path: "/var/nfs"
параметры nfs сервера

а если бы тип PV был hostpath то вместо nfs секции(которую только 
что разобрал) было бы 

 hostPath:
    path: "/mnt/data"

устанавливаем этот PV

# kubectl apply -f PV.yaml

и проверяем что он успешно установился

# kubectl get pv
# kubectl get pv
NAME                 CAPACITY    CLAIM    STATUS
portal-info-data-pv  10Gi                 Available

значит видно что pv появился  и что поле CLAIM у него пока пустое

далее устанавливаем pv claim

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: portal-info-data-pvc
spec:
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  volumeName: "portal-info-data-pv"
  storageClassName: ""


name: portal-info-data-pvc   = имя клейма по которому мы будем к нему обращатся

volumeName: "portal-info-data-pv"   = имя PV к которому мы обращаемся нашим клеймом

(пока поверхностно анализируем yaml)

навскидку про клеймы непонятно вот что - про  storage: 10Gi 


устанавливаем клейм и смотрим на него
# kubectl apply -f claim.yaml
# kubectl get pvclaim

NAME                       STATUS    VOLUME                CAPACITY
portal-info-data-pvc       Bound     portal-info-data-pv   10Gi            

теперь смотрим как изменился статус PV

# kubectl get pv
NAME                 CAPACITY    CLAIM                          STATUS
portal-info-data-pv  10Gi        default/portal-info-data-pvc   Bound


еще раз подобью бабки об именах pv и клейм. ибо они длинные
portal-info-data-pv   (pv)
portal-info-data-pvc  (клейм)


pod
теперь наконец создаем под который использует pv
цепь такая , под обращается к claim а клейм обращается к pv

apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: portal-info-data-pvc
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
		  

кусочек в поде который прописывает клейм.
volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: portal-info-data-pvc

насколько  я помню в докере  volumes  это хрень когда мы хотим папку в
контейнере пробросить на фс хоста. значит как это было в докере - нужно
было указать папку в контейнере и папку на фс хоста. значит
на данном этапе мы указываем куда "вовне" контейнера мы будем пробрасывать
папку контейнера. тоесть теперь когда мы видим слово вольюм в yaml 
мы понимаем что это некий внешний по отношению к ФС контейнера обьект в который мы будем пробрасывать внутреннюю папку контейнера
здесь мы создаем вольюм с именем task-pv-storage. но вольюм 
это просто обьект с названием ему нужен бекенд. бекендом указывается клейм
 portal-info-data-pvc. итак к контейнеру будет присобачен внешний 
 volume task-pv-storage который свои бекенд получит от клейма portal-info-data-pvc

 
далее в поде мы прописываем какую папку в поде будем пробрасывать в этот вольюм
volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
		
папка внутри контейнера /usr/share/nginx/html
будет проброшена во внешний вольюм task-pv-storage

как видно папка привязывается не к клейму а к вольюму.

таким образом полная цепочка

папка пода -> volume -> claim -> pv

публикем под. и проверяем завелся ли он.
если да то вся цепочка срослась.

все сработало.

теперь начинаем копаться.
например где на диске находится этот маунт


значит я когда неустановил на дата ноды пакет nfs-common 
после публикации пода он незаводился а в его логе была ошибка

Warning  FailedMount  11s  kubelet  MountVolume.SetUp failed for volume "portal-info-data-pv" : mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv --scope -- mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60 test-nfs-01.mk.local:/var/nfs /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv
Output: Running scope as unit run-r44a9aee0e2b44d809700914c5b983839.scope.
mount: wrong fs type, bad option, bad superblock on test-nfs-01.mk.local:/var/nfs,
       missing codepage or helper program, or other error
       (for several filesystems (e.g. nfs, cifs) you might
       need a /sbin/mount.<type> helper program)

       In some cases useful info is found in syslog - try
       dmesg | tail or so.

что в этой ошибке можно интересного увидеть. это как кубернетес
непосредственно монтируем nfs шару и куда он ее монтирует

mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60 test-nfs-01.mk.local:/var/nfs /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv

соотсвтевенно монтируем через mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60

откуда test-nfs-01.mk.local:/var/nfs
куда /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv

потом я поставил пакет nfs-common на дата ноды , удалил и заново
создал под и под завелся.


из самого маунтинга приходят вопросы а где у нас прописана вся 
эта специфика указанная в маунт.
в поде ли , в клейме ли , в pv ли.
если поднятся выше и посмотреть то вся специфика маунтинга прописывается 
в PV.yaml

как видим нигде в маунтинге неуказан никакой размер который будет окупирован
подом. но это и логично это ведь файловая шара. какие тут лимиты.
все место что в шаре есть все твое.
куб я думаю вопрос поедаемого размера на шаре контролирует както подругому
нечерез mount

также видно что nfs шару куб монтирует в /var/lib/kubelet/pods/
так как это сетевая шара то выделять под этот путь отдельный диск 
на нодах чтоб он незабился нет смысла ибо это сетевая шара и на ноде
она ничего по размеру незанимает.
вспоминаем что это вообще за папка в кубе /var/lib/kubelet/pods/
и также надо выяснить вот этот вот маунтинг куб делает на какой ноде? логично
что ровно на той на которой крутится сам под. проверим
также заметим что b78b0ac0-d65d-4807-b085-64771dbab78c это видимо ID
пода. проверим.
интересно что вот мы обьявили pv в кубе и он вроде как немонтирует его
при этом никуда. что он его монтирует каждый раз походу в каждвый
конкретный под. проерить.
также надо выяснить как этот маунт найти из своств пода.
также надо найти этот маунт уже в свойствах самих контейнеров 
пода а не  в поде.
да... pv вызывает много вопросов.

самые интересные из них это то что при создании pv куб его никуда немонтирует
чтобы раздавать его из одной точки. а монтирует его для каждого
пода индивидуально. взможно так еть только для nfs потому что 
врядли такое он делает для iscsi шар.

второй момент это куда конкретно куб монтирует щару для пода. и 
как это реализовао уже на уровне контейнера

также надо разобрвтся про связь между PV и клеймами. как они делят место.
есть ли такая штука что один клейм можно подключаить только к однму 
поду а не к нескольким.
вопросы вопросы..



>>>
еще вопросы
это может ли куб иметь несколько мастеров
чтобы была редунданси.
как там сплит брейн решен.

ответ да может. сплит брейна там нет.
апи сервер выступает в роли апликейшн сервера. а данные хранться в единой
базе etcd

что происходит когда мастер невидит ноды и поды поому что скажем сфере
ей насрать. виртуалки будут работать на серверах.
>>>

>>>
ceph
>>>

клеймы позволяют указать в каком режиме будет доступ
к куску диска который они отожрали
ReadWriteOnce - R\W для одного хоста          (куб обозначает RWO)
ReadWriteMany - R\W для нескольких хостов     (куб обозначает RWM)
ReadOnlyMany - R для нескольких хостов        (куб обозначает ROM)

как я понял в клейме из параетров  указывается размер диска который берется в lease у PV
и режим доступа к этому куску.

далее куб пишет что поскольку юзерам нужны диски у которых можно 
выбрать нетолько размер и режим доступа но и скажем скорость 
диска то клеймы это не позволяют выбирать и на помощь приходить
storage class

если PV заготавливать заранее руками это назыавется статический метод
заготовки PV.
тоесть скажем мы руками загоняем в куб 7 PV. и они там висят готовые 
к труду и обороне. 

как я понял в клейме мы можем указать как название PV так и storage class.
если мы указали PV то это мы обращаемся к заранее опубликованному в кубе
статическому PV. если в клейме мы указываем сторадж класс то куб
через сторадж класс будет пытаться создать PV в автоматическом режиме.
такой способ создания PV называется динамическим.

при запросе статического PV клейм указывает storageClas="", и имя PV
а при динамическом клейм делает запрос к сторадж классу с именем 
и без PV имени.

чтобы активировать в кубе фичу динамических PV надо чтобы на апи сервере
куба была указан параметр DefaultStorageClass в опции --enable-admission-plugins.
посмотрим указан ли такой параметр по дефолту в запущенном апм сервере
на мастере выполняем
# ps aux | grep kube-apiserver


kube-apiserver 
--advertise-address=172.16.102.31 
--allow-privileged=true 
--authorization-mode=Node,RBAC 
--client-ca-file=/etc/kubernetes/pki/ca.crt 
--enable-admission-plugins=NodeRestriction 
--enable-bootstrap-token-auth=true 
--etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt 
--etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt 
--etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key 
--etcd-servers=https://127.0.0.1:2379 
--insecure-port=0 
--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt 
--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key 
--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname 
--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt 
--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key 
--requestheader-allowed-names=front-proxy-client 
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt 
--requestheader-extra-headers-prefix=X-Remote-Extra- 
--requestheader-group-headers=X-Remote-Group 
--requestheader-username-headers=X-Remote-User 
--secure-port=6443 
--service-account-key-file=/etc/kubernetes/pki/sa.pub 
--service-cluster-ip-range=10.96.0.0/12 
--tls-cert-file=/etc/kubernetes/pki/apiserver.crt 
--tls-private-key-file=/etc/kubernetes/pki/apiserver.key

как видим в опции --enable-admission-plugins нашего параметра нет по дефолту

--enable-admission-plugins=NodeRestriction

тогда возьмем и добавим наш плагин. то есть 
--enable-admission-plugins=NodeRestriction,DefaultStorageClass

как это сделать в итоге.
правим файл 
/etc/kubernetes/manifests/kube-apiserver.yaml
и перезапускаем kubelet

# systemctl restart kubelet

ну и проверить что все сработало

# ps aux | grep apiserver




далее. поды при описании volumes обращаются к клеймам.это мы уже видели.
напомню как при описании volume в pod мы обращаемся к claim
kind: Pod
...
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: portal-info-data-pvc
		
если pvc используется подом то pvc нельзя удалить.
точнее команда на удаление пройдет но pvc будет висеть неудаленный
до тех пор пока жив под который его юзает и в свойствах pvc
в статусе будет указано terminating и в строке finalizers будет строка kubernetes.io/pvc-protection

# kubectl describe pvc hostpath
...
Status:        Terminating
Finalizers:    [kubernetes.io/pvc-protection]

тоже самое для pv. если дать команду удалиь pv то он будет висеть
неудаленный пока неисчезнет pvc который его юзает.		

# kubectl describe pv task-pv-volume
...
Finalizers:      [kubernetes.io/pv-protection]
Status:          Terminating

походу вот еще чо! к одному PV можно подключить только один pvc!
тоесть два pvc нельзя подключить к одному PV.
и тут вылезает вот эта проблема ! под каждый под нужен свой предварительно
созданный PV! это же жопа! число подов динамически меняется и надо 
либо иметь  всегда подготовыленные PV с запасом. это катстрофически неудобно.
надо чтобы PV создавались автоматически при запросе от пода.
так что статический метод формирования PV это абсолютнонам немодходит.
нам походит только динаимческий метод формирования PV получается.

далее написано. если мы удалили pvc то то что дальше будет с PV к которому он 
был подсоединен зависит от  persistentVolumeReclaimPolicy:  которая прописана в PV
пример

apiVersion: v1
kind: PersistentVolume
...
 persistentVolumeReclaimPolicy: "Recycle"

из бывает три штуки:
Retain
Delete
Recycle

Retain - когда удалили pvc то pv как обьект остается в кубе, содержимое 
этого pv остается нетронутым , статус pv = released но этот pv он недоступен 
для автоматического повторнгого подключения к нему другого pvc.
в целом как я понял он вообще больше недоступен для повторого использования.
надо руками удалить этот pv, затем почемуто надо удалить те данные которые
лежат на pv и потом заново создать этот pv.  в общем полезно если данные
после работы пода нам нужны и иудалять их нельзя типа как база эластика.
но какая то дурацкая система что повторно заюзать с другим pvc нельзя

Delete - как я понял при удалении pvc куб автоматом удалит и pv и даже хуже
того еще и данные удалить на этом pv

Recycle - пишу что это устаревшая опция типа неюзайте ее. а юзайте
динамичесеий провижионинг. а так типа опция работает так что она удаляет
данные что лежат на pv и данный pv снова доступен для того чтобы его другой 
pvc мог заюзать.

теперь надо проверять на практике то что написано.

далее написано что если у нас есть PV то он будет заюзан первым на очереди pvc который удовляряет ему. а если мы хотим чтобы прям конкретный pvc заюзал
наш pv то надо в PV указать имя привилигированного pvc который только и имеет
право заюзать pv. причем как надо и pv указать имя pvc и в pvc указать
имя pv. но все таки главная мысль что в pv надо указать имя pvc.тоесть
в pvc мы указываем типа что нам желательно подключиться к такому то pv.
а вот в pv мы говорим что обязательно к этому pv может подключться только 
такой то pvc. тоесть сам pv решает какой pvc к нему имеет право подкючиться.

пример
pvc foo-pvc желает поключиться к pv foo-pv

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
spec:
  volumeName: foo-pv

а pv  foo-pv жестко прописывает что только pvc foo-pvc имеет право к нему
подключиться

apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  claimRef:
    name: foo-pvc
    namespace: foo
  
  про динамический провижиониннг. 
  из того что я пока увидел тоже неполучается нормально решения.
  да нам нужно иметь созданный PV так как он создается автомтом при запросе
  из клейма. но! для того чтобы под заработал надо иметь предварительно созданный клейм.хм.. скажем имеет мы деплоймент в котором указано
  8 подов. так что как нам динамический указать 8 клеймов.
  тоесть дело в том что мы же неуказываем руками 8 ямль файлов для пода 
  который состоит в дейплойменте. так вот надо както научиться чтобы 
  ненужно было статически предваориетльно создавать клеймы под будудущие поды.
  чтоб также как для деполймента 1 раз указвам ямль пода а потом хоть 1000
  подов имеем автоматом из дплоймента. также надо научиться  1 раз 
  укзать для пода клейм а потом при размножении подов чтобы и клеймы
  автомтом множились.
  
  схма динамического провижионинга
  
  pod -> claim -> storage class -> provisioner -> pv 
  
  под обращается к клейму тот к сторадж классу. класс обращается к провижионеру. провижинер создат pv.
  
  
  насколько я понял из эксперимента статус pv будет released тока
  тогда когда будет удален pvc который к pv подсоединен. это тупо 
  что надо pvc удалять.
  
  значит что я в итоге выяснил на счет 
  устновки elasticsearch через helm.
  установил то что 
  
  во первых что helm почемуто игнориует насйтрокйи в файле values.yaml
  поэтому запускаем установку вот так
  
#   helm install el10 elastic/elasticsearch --set replicas=1 --set resources.requests.memory=1Gi --set volumeClaimTemplate.storageClassName=managed-nfs-storage --set volumeClaimTemplate.resources.requests.storage=5Gi

во вторых что единстыенный способ прописать pv которые будет
использоват эластик это только через storageclass. и никак иначе.
а значит надо сделать nfs provisioner.
сейчас он у меня падает как только к нему обращется pvc.

задача - написать устойчтвый нормальый nfs provisioner

я вот этот nfs provisioner никак нехотел запускаться.
оказалось что вся жопа заключается в той штуке которую
я уже давно немог понять. в том что мы когда куб инициализируем
мы указываем там сеть для подов 

# kubeadm init  --pod-network-cidr=10.252.0.0/16 ...

и эту сеть по идее потом должен использоваться flannel
по всем конфигам фланнеля он ее недолжен использовать так как куб
нифига непрописывает эту сеть в настройки фланнеля , в конфигах 
фланнеля всегда фигурирует другая сеть. но по факту каким то неведомым
образом фланенль все таки использует именно 10.252.0.0/16 хотя
в его настройках указано совсем другое. тоест связь работает как мы и 
указали в kubeadm init хотя непонятно как такое возможно ибо в конфигах
фланнеля указан другая ip сеть которая по дефолту прописана в 
yaml фланнеля. так вот . эта штука нифига немешала однаков в конец
концов она вылезла!!! именно из за этого несовпдаения файл provisioner.go
(который я толком непонял толи принадлежит самому кубу толи конкретно
nfs provisiner пакету который мы ставим) выходит на ошибку

# kubectl logs nfs3-nfs-client-provisioner-5c6c68fb6b-nlgww --previous
F1026 12:27:13.801481       1 provisioner.go:180] Error getting server version: Get https://10.96.0.1:443/version?timeout=32s: dial tcp 10.96.0.1:443: i/o timeout

рещение - чтобы в кубе ничего не пересустанавливать
надо зайти на mapping для flannel в кубе
и поменять там сеть с 10.244.0.0\16 на ту которую мы
указали в kubeadm init 10.252.0.0\16

И ТОГДА СРАЗУ И ПОДЫ nfs provisoner заработают и pvc сразу
перейдут в состояние bound !!!!

а как от этого дерьма уберечься изначально  - это надо устанавлвать 
фланнель не из yaml из интернета. а скачать его на комп
отредактировать там параметры сети руками чтобы они были равны

--pod-network-cidr=10.252.0.0/16

и только потом ставить flannel.


как отредактировать mapping для flannel в кубе

# kubectl edit cm -n kube-system kube-flannel-cfg
# edit the configuration by changing network from 10.244.0.0/16 to 10.252.0.0/16
# kubectl delete pod -n kube-system -l app=flannel

ура!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  
  но на этом прикшючения незакончились.
  pvc теперь успешно крепится к pv.
  
  но под с эластиком незапускается.
  почему 
  
  потому что вот что пишет ошибка в поде который не может запуститься
  
  # kubectl describe pods elasticsearch-master-0
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  4h27m  default-scheduler  0/3 nodes are available: 3 Insufficient cpu.

тоесть оно говорит что мол нет нод потому что на них нет достаточно 
цпу ресурса.

выясним сколько cpu требует под эластика под себя.
  
# helm get  all el10 | grep cpu
    cpu: 1000m
    cpu: 1000m
            cpu: 1000m
            cpu: 1000m

el10 - это имя инсталляции эластика через helm

из доков следует что 1000m = 1 ядро.
таким одразом как я понимаю под эластика требует под себя ядро.
прям типа резервирует.(навскидку это меня смущает. мне ненужна никакая
заранее резервация цпу. )


теперь смотрим сколько cpu уже зарезервивано на дата нодах

# kubectl describe nodes test-data-02

 Resource           Requests    Limits
  --------           --------    ------
  cpu                100m (10%)  100m (10%)
  memory             50Mi (2%)   50Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)

на test-data-02 есть 1 ядро. (это я знаю заранее) и из них 10%
ужк зарезервировано. 
поэому как я понимаю под неразворачивается так как нет cpu ресурса 
под него единолично.

как прописываются ресурсы

cpu ,  1 ядро = 1000m = 1
memory , 1GB = 1Gi = 1024 Mi

с чем столкнулся. во первых прежде всео нужно удалять pvc  а только 
затем pv. так вот если удаляешь инсталляицю в helm

# helm delete el10
то почему то все удаляется кроме pvc.
их приходится удалять руками

далее  я выяснил что эластик 7.7.1 он минимум жрет 430МБ если его 
пустой запустить.


вот так я пока запускаю elasticsearech установку

# helm install el10 elastic/elasticsearch \
-f values.yaml \
--set replicas=1 \
--set resources.requests.memory=2Gi \
--set volumeClaimTemplate.storageClassName=nfs-client \
--set volumeClaimTemplate.resources.requests.torage=5Gi \

узнал из справки helm то если настройки конфликтуют превалируют опции
которые стоят правее всего. тоест берется левая настройка потом на нее
сверху накладывается сосед справа и все что конфлитукт оно перетирает

 настройка replica в values.yaml
 это не реплики в смысле рпелик эластика . нет!
 это реплики в смысле ReplicaSet куба. 
 поэтому replica=1 означает что будет развернуть 1 под.
 а реплика = 0 означает что будует развернуто 0 подов.


\\значит эластик пока незаработал.


что важно сказать 
в эластике естьнастройка bootstrap_memory_lock=true
о чем она. она дает команду эластику при старте попробовать 
всю память процесса пометить особым образом чтобы  витоге 
он всегда висел в памяти и не попал никогда в свап. для этого
используется систесный вызов mlockall. если у эластика это получается
то хорошо если нет то он просто грузится дальше.
чем хороша эта настройка это тем что тогда ненужно на виртуалке 
на которой крутится эластик дизейблить свап. это хорошо.

причем когда применяют насйтроку bootstrap_memory_lock=true
тогда нередко меняют настройку ulimit. в целом это неочень понятно.
дело в том что ulimit это встроенная функция в bash ( проверяется 
это командой typa -a ulimit ) и ее назначение это ограничить максимальное
использование того или иного вида ресурса для процесса который запускается из bash для данного пользователя. получается если 
мы запустили из bash кучу процессов под пользователем vasya то эта 
группа процессов неможет использовать ресурсов хоста больше чем
задано. еще раз какие ключевые моменты ulimit. во первых процесс
должен быть запущен из bash. но ведь далеко невсе процессы запускаются
из bash например сервисы которые запускатся из systemd они запускаются
не из bash и в том числе elasticsearch. то есть если процесс запущен
не из bash к нему ограничения ulimit некасаются. поэтому к примеру
непонятно, если мы запустили elstivcsearch из systemd то каким
боком к нему относится ограничение ulimit. возникает
интересный вопрос - как определить что процесс был запущен из bash.
(научиться это определять и посмотреть эластик из systemd он в итоге
запущен из bash ?)
так вот про ulimit. процесс дожлн быть запущен из bash, процесс запущен
от некоторого пользователя. тогда на него а точнее нетолько на него
а на всю группу в целом которая была запущена под этим пользователем
и все они были запущены из bash накладывается ограничения прописанные
в ulimit. ограничения ulimit можно изменять как из командной строки
так и в файле limits.conf
в частности в ulimit можно указать чтобы юзер немог залочить в памяти
больше такого то обьема. залочить это значит что эта память всегда
будет в памяти и непопадет в свап никогда. тут опять же важно понять
что ulimit лишь ограничивает и все. но чтобы эта память реально
непопала в свап надо еще чтобы процесс сделал вызов к системному
вызову mlockall, установка лимита через ulimit это еще не все дело
этого недостаточно это лишь указатель ограничения сверху. а вызов 
mlockall должно делать сам процесс.  как видно конечно есть некотоая 
связь между bootstrap_memory_lock=true и ulimit но по мне это только
если эластик запускается из bash.
итак еще раз ulimit прописывает ограничение использования ресурсов хоста 
1) для пользователя
2) процесс от имени эттго пользователя должен быть запущен 
из bash
3) ограничение указывает суммарную цифру всумме не для одного
процесса запущенного из баш и от имени данного пользователя а для 
всех таких процессов в сумме. для группы таких процессов.


  



ulimit в лиеуксе
docker + ulimit
docker run --net=host -e "bootstrap_memory_lock=true" --ulimit memlock=-1:-1 -v ~/.data/:/usr/share/elasticsearch/data/ --name ES elasticsearch:7.7.1

память будет отожрана java min - java max и плюс 
остальными частями процесса. скажем если java max = 1GB
то в итоге процесс в памяти занимает 1.3ГБ например

мой эластик в кубе падает с ошибкой
  "Native controller process has stopped - no new native processes can be started" }

я запустил эластик в докере и он упал с такойже ощибкой пока я не втсавил
в докер такую опцию -e "discovery.type=single-node" из доков эластика 
 # docker ... -e "discovery.type=single-node" ...

поэтому в кубе эластк может падает потому что там только одна нода
стартует и другие он неможет найти ? как задать в хельм эту опцию
надо узнать.

еще надо вот эти оппции отработаь

esJavaOpts: "-Xmx800m -Xms100m"

resources:
  requests:
    cpu: "100m"
    memory: "300Mi"
  limits:
    cpu: "900m"
    memory: "900Mi"

из того что здесь написано джава heap может быть от 100 до 800МБ макс.
а размер всего процесса может быть от 300 до 900 МБ.
тут тоже надо все продумать

думаю надо вначале дать процессу памяти точно чтобы ему хватило.
и если все равно будет падать то уже смотреть в сторону
опций дискаверинга

\\


 
 в итоге эластик заработал!
 
 при вот такой установке
 
# helm install el13 elastic/elasticsearch \
 -f values.yaml \
 --set volumeClaimTemplate.storageClassName=nfs-client \
 --set volumeClaimTemplate.resources.requests.storage=5Gi

values.yaml имеет вот такой состав




# cat values.yaml | grep -v '#'

---
clusterName: "elasticsearch"
nodeGroup: "master"

masterService: ""

roles:
  master: "true"
  ingest: "true"
  data: "true"

replicas: 1
minimumMasterNodes: 1

esMajorVersion: ""

esConfig: {}

extraEnvs: []

envFrom: []

secretMounts: []

image: "docker.elastic.co/elasticsearch/elasticsearch"
imageTag: "7.7.1"
imagePullPolicy: "IfNotPresent"

podAnnotations: {}

labels: {}

esJavaOpts: "-Xmx1g -Xms1g"

resources:
  requests:
    cpu: "1000m"
    memory: "2Gi"
  limits:
    cpu: "1000m"
    memory: "2Gi"

initResources: {}

sidecarResources: {}

networkHost: "0.0.0.0"

volumeClaimTemplates:
  - metadata:
      name: elasticsearch-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi
      storageClassName: nfs-client


rbac:
  create: false
  serviceAccountName: ""

podSecurityPolicy:
  create: false
  name: ""
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim

persistence:
  enabled: true
  annotations: {}

extraVolumes: []

extraVolumeMounts: []

extraContainers: []

extraInitContainers: []

priorityClassName: ""

antiAffinityTopologyKey: "kubernetes.io/hostname"

antiAffinity: "hard"

nodeAffinity: {}

podManagementPolicy: "Parallel"

protocol: http
httpPort: 9200
transportPort: 9300

service:
  labels: {}
  labelsHeadless: {}
  type: ClusterIP
  nodePort: ""
  annotations: {}
  httpPortName: http
  transportPortName: transport
  loadBalancerIP: ""
  loadBalancerSourceRanges: []

updateStrategy: RollingUpdate

maxUnavailable: 1

podSecurityContext:
  fsGroup: 1000
  runAsUser: 1000

securityContext:
  capabilities:
    drop:
    - ALL
  runAsNonRoot: true
  runAsUser: 1000

terminationGracePeriod: 120

sysctlVmMaxMapCount: 262144

readinessProbe:
  failureThreshold: 3
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 3
  timeoutSeconds: 5

clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

schedulerName: ""

imagePullSecrets: []
nodeSelector: {}
tolerations: []

ingress:
  enabled: false
  annotations: {}
  path: /
  hosts:
    - chart-example.local
  tls: []

nameOverride: ""
fullnameOverride: ""

masterTerminationFix: false

lifecycle: {}

sysctlInitContainer:
  enabled: true

keystore: []

fsGroup: ""


что непонятно это что helm из values.yaml 
неможет прочитать вот эти параметры


--set volumeClaimTemplate.storageClassName=nfs-client
--set volumeClaimTemplate.resources.requests.storage=5Gi

и их приходится насильно добавлять в строку.

это какойто дебилизм.





  
  ---------------------------------------------------------------
  вопрос >юзер спейс права под которым процесс работает в одном проснтранстсве
  и другом. чтоб процесс не мог вылезти из pid пронстратснва
  
  -------------------
  
  
значит быстрые вопросы
1) связь с подом идет по pod IP. ура
2) что там еще за service ip итп
3) как из пода логи вытащить наружу
4) научиться с обьемом памяти и цпу лимитами под эластик задавать
6) чтоя еще выяснмл. что nfs provisioner он несоздает на nfs сервере
новые папки которые он экспортирует. нет. он создает внутри папки
которую я экспортировал одну заранее кучу подпапок которые и есть
pv для подов. что очень круто. с точки зрения самого nfs сервера.
тоесть получается что провижионер он незаходит на сервер и ничего там
неменеяет. это очень круто.
7) также я выяснил походу пьесы что helm эластик так настроен что 
он мастер ноды эластика  размещает только на разных дата нодах куба.
тоесть небудет запущено две мастер ноды эластика на одной дата ноде куба.

на этом этапе я случайно стер куб на test-kub-01 через
apt-get purge kube*. Но! - при этом убунту сохранил все конфиги
поэтому  что я обратно накатил пакеты apt install -y kubelet kubeadm kubectl и все поды появились и завелись. и системные и обычные.
ура.
 


отсюда супер важный вывод - надо чтобы в кластере куба
было несколько контроль панелей . чтобы если одну сломаешь то другие 
все спасают, и еще надо чтобы кластер etcd был вообще отдельно 
чтобы туда вообще нелазить через сеанс ssh. тогда куб будет надежный.


наконец заработал кластер из 3-ех нод. каждая мастер.
мастер ставится только на отдельный куб дата ноду.

заметил в свойствах пода вот такую переменную

# kubectl describe pod ...

  Environment:
      node.name:                     elasticsearch-master-2 
      cluster.initial_master_nodes:  elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,
      discovery.seed_hosts:          elasticsearch-master-headless

я имею ввиду discovery.seed_hosts: elasticsearch-master-headless

из описания эластика эта переменная юзается на дата нодах 
чтобы указать мастеров. обычно юзается Ip адрес. но походу тут юзается
dns имя.

так вот что выяснилось интересное.

во первых я узнал какой домен прописывает куб для подов в их /etc/hosts

если под имеет имя то его полное доменное имя = 

имя-пода.elasticsearch-master-headless.default.svc.cluster.local

ну может потому что данные под входит в состав headless service 
или как он там назыавется.
так вот я подключился к coredns через nslookup
и  посмотрел какой ip имеет dns имя elasticsearch-master-headless

и оказалос что он имеет сразу три IP , ровно те IP которые имеют
три мастера.

> elasticsearch-master-headless.default.svc.cluster.local


Name:   elasticsearch-master-headless.default.svc.cluster.local
Address: 10.252.1.17
Name:   elasticsearch-master-headless.default.svc.cluster.local
Address: 10.252.3.2
Name:   elasticsearch-master-headless.default.svc.cluster.local
Address: 10.252.2.20
>

это супер удобно с точки зрения масштабирования эластика.
в конфиге при добавилении нового мастера на других нодах ничего
менять ненужно , переменная 

discovery.seed_hosts: elasticsearch-master-headless

останется такойже. просто в DNS надо будет к этой dns имени 
присобачить +1 IP адрес.

класно!

единственное что мне неочень понятно. 
1) что за типа DNS записи имеет это имя. 
ответ: если при запросу одного dns имени вылезает несколько ip
это значит что на dns сервере прписано несколько A записей с одним
dns именем и разными ip.

2) эластик как клиент он в итоге понимает ли что данная запист имеет
несколько IP или он берет первую попавшуются или как.
неочень понятно тогда какой из кучки полученных ip юзает клиент.
ответ: какой ip из полученного списка будет юзать клиент зависит от самого
клиента. какието клиенты используют всегда первый в списке какието 
пробуют первый если он недступен то второй. какието берут рандомный.
получается что лучше всегда счиать что клиент тупой и он возьмет 
первый в списке. так вот dns сервер может сам по себе каждый раз 
возвращать список в котором IP располжены рандомно тогда самый тупой клиент в итоге будет иметь фейловер. я проверил кубовый coredns 
возрвщает список IP в рандомном порядке.  теперь понятно что в таком
случае эластик будет работать хорошо и нода будет рапортовать к разным
мастерам. то есть есть фейловер и обрашения к разным мастерам.


я создал в эластие два индекса,
потом я удалил эластик через helm 
при этом в кубе остаются pvc.
так вот потом я накатил эластик обоатно через helm
и данные сохранились.

далее если я удаляю pvc которые оппираются на strage classs то pv исчезает автомтом.

в чем яубеждаюсь уже неодин раз. конфиг хельма он использует структуры
похожие на структуры куба но они отличаются. изза это путаницы 
ошибки. 

пример 

в кубе

volumeClaimTemplates:
- metadata:
    name: pv-data
  spec:
    accessModes: 
      - ReadWriteOnce
    resources:
      requests:
        storage: 50Mi
		
в values.yaml helm-а

volumeClaimTemplate:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "nfs-client"
  resources:
    requests:
      storage: 1Gi

хотя бы даже взять то что в кубе оканчивается на templates
а в хельме на template.

тоесть зачастуб нельзя взять конфиг из куба и напрямую
накатить в конфиг хельма

значит как я продвинулся дальше.
я узнал что чтобы мне опубликовать раздельно мастер ноды
и раздельно дата ноды то мне надо делат два релиза хельма

# helm install el13 elastic/elasticsearch -f ./values-master.yaml
# helm install el13-data elastic/elasticsearch  -f ./values-data.yaml


вначале я публику только мастеров а потом публикую только дата ноды.
для этого использую два разных файла с насройками

самая главная разница файла с настройками мастеров с файлом настроек
дата нод

///
основные настройки values.yaml для мастеров:

clusterName: "elasticsearch"
nodeGroup: "master"

masterService: ""
///


///
основные настройки values.yaml для дата нод:

clusterName: "elasticsearch"
nodeGroup: "data"

masterService: "elasticsearch-master"

antiAffinity: "soft"
///

nodegroup это по момему хрень самого хельма.
важно другое на дата нодах в masterService:
мы указываем clusterName и nodeGroup разделив их минусом. 

masterService: "elasticsearch-master"

nodegroup указваыем тот который в конфиге хельма для мастеров.
именно благодаря masterService дата ноды смогут найти мастер ноды.


ну и еще одна настройка antiAffinity: "soft" она дает то что 
при запуске подов куб небудет против чтобы несколько штук дата 
подов работали на одном куб хосте, потому что в конфиге мастеров
antiAffinity: "hard" и это приводит к тому что куб недает запустить
более одного пода мастер эластика на одном куб хосте



ну а так собственно вот основные настройки которые я менял в 
файлах values.yaml для мастера и дата нод

\\\
# cat ./values-masters.yaml


# grep -v '#'  values-master.yaml
---
clusterName: "elasticsearch"
nodeGroup: "master"

masterService: ""

roles:
  master: "true"
  ingest: "true"
  data: "true"

replicas: 3
minimumMasterNodes: 2

...

esJavaOpts: "-Xmx1g -Xms1g"

resources:
  requests:
    cpu: "500m"
    memory: "1.4Gi"
  limits:
    cpu: "1000m"
    memory: "1.6Gi"

...

volumeClaimTemplate:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "nfs-client"
  resources:
    requests:
      storage: 1Gi


antiAffinity: "hard"
\\\


\\\
# cat ./values-datas.yaml


# grep -v '#'  values-data.yaml

---
clusterName: "elasticsearch"
nodeGroup: "data"

masterService: "elasticsearch-master"

roles:
  master: "false"
  ingest: "true"
  data: "true"

replicas: 3

esJavaOpts: "-Xmx1g -Xms1g"

resources:
  requests:
    cpu: "500m"
    memory: "1.4Gi"
  limits:
    cpu: "1000m"
    memory: "1.6Gi"



volumeClaimTemplate:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "nfs-client"
  resources:
    requests:
      storage: 1Gi


antiAffinity: "soft"
\\\



значит в настройках мы укаываем размер Java heap
а еще указываем размер процесса в целом. так вот по моим
наблюдениям размер процесса целиком на 30% а лучше на 40% больше 
чем размер java heap. скажем если -Xmx1g то memory: надо ставить"1.4Gi"





=================================
провел такой эксперимент
в папке  с маунт понитами куда смотрит локал провижионер
там все маунт поинты одинакового размера.

в итоге провижионер создал в кубе PV одинакого размера.
потом я зашел на ноду руками отмонтировал одну из точек монтирования
расширил ее у подмонтировал обратно, но размер PV в кубе остался
таким же самым. так что условно говоря после создания PV его бекенд 
менять бесполезно.

=======================



>>>>>>>>>>>>>> остановился здесь!


  
  