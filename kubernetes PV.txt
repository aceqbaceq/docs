kubernetes persistent volume

PV - persisytent volume это кусок диска который кластер имеет как ресурс
для того чтобы его выдавать потом подам. 

тут я хочу сказать что PV вольюмы они неуничтожаются кубом при уничтожении пода. 
под погибает а вольюм остается. в этом фишка. их будем тут рассматривать. так вот 
надо сказать что есть вольюмы которые после уничтожения пода также уничтожаются
они назваются ephemeral volumes. но тут они не рассматриваются. они например нужные тем
подам которые обеспечивают кэш. 

persistent volume claim - это свойство пода. под через клейм заказывает
у кубе кусок диска. тоесть pv это кусок диска имеющийся на стороне 
куба а claim это заказ со стороны  пода  к кубу мол заказываю
10ГБ диска.

разница между PV и PVc. pv это параметр кластера это сколько есть у кластера
а pvc это запрос от юзера заказ на то чтобы сколько отрезать от кластера.

когда мы заранее готовим некий набор PV's это называется статическая подготовка PV . есть еще динамическая. когда PV в система появляется незаранее
а по запросу. это можно сравнить с тем что диски на компе могут быть предустановлены в комп.  а могут быть(предположим) появится в нем по запросу.ну по типу того что послал программный запрос и к виндовсу подключился iscsi lun.

динамическое появление PV связано со сторадж клаассами . об этом потом.

шарманка работает в целом так , в от PVC в куб поступает запрос на то чтобы он выделил
кусок диска. куб ищет есть ли  у него PV который удовлетворяет 
параметрам заказа в PVC. он ищет в предустановленных статических PV если там 
нет то пытается динамически создать PV , чтобы динамически создать PV - PVC должен обратиться к storage классу

походу чтобы минимально чтото заработало надо иметь сетевой сторадж
типа NFS. так что надо собрать NFS

так. значит начал я делать пробовать создать PV потом создать
PV claim (PVC) потом создать pod который бы запрашивал PV через pvc.

значит вначале я попробовал создать PV тип которого hostPath

apiVersion: v1
kind: PersistentVolume

metadata:
  name: task-pv-volume
  labels:
    type: local

spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

так вот с этой хренью ничего не получилось . потому что вроде как 
в одном месте я нашел что hostpath pv можно использовать 
только якобы на куб кластере с одной нодой.
у меня их три.
короче когда я пытался создать этот PV то мне куб выдавал ошибку.

окей. 

стало понятно что надо ставить nfs сервер. и использовать PV nfs типа.


/// как поставит nfs сервер ///
/// начало блока            ///
значит если ставим его внутрь lxd то контейнер надо запускать с особыми опциями. иначе 
незаработает

# lxc launch local:dcfe2a671f1d nfs-02   -c security.privileged=true -c raw.apparmor="mount fstype=rpc_pipefs, mount fstype=nfsd,"

(параметры подсмотрел вот здесь = https://github.com/HSBawa/nfs-server-on-linux-container)

супер важна вот эта вот запятаая на конце. без нее выдаст ошибку:
raw.apparmor="mount fstype=rpc_pipefs, mount fstype=nfsd,"


либо если контейнер уже запуще без спец параметров надо его оставоить и зайти в редактор конфига
контйнера и в раздел config: добавить опции

# lxc config edit имя_контейнера

config:
  raw.apparmor: mount fstype=rpc_pipefs, mount fstype=nfsd,
  security.privileged: "true"

опять же повторю супер важна запятая в конце


# mkdir /var/nfs
# chown nobody:nogroup /var/nfs
# apt-get install -y nfs-kernel-server

# cat << EOF >> /etc/exports
/var/nfs    172.16.102.0/24(rw,sync,no_subtree_check,no_root_squash)
EOF


/var/nfs = папка на сервере которую мы расшариваем
172.16.102.0/24 = клиенты которым можно подключаться
no_root_squash = эта опция нужна потому что при копировании файлов 
клиент также и устанавливает пермишнс на копируемый файл. так вот 
по дефолту на nfs сервере запрещено в расшаренной папке уставливать
владельцем файла пользователя root. и когда мы под рутом копируем файла
на nfs папку то клиентский линукс неможет установить владельцем файла рута.
и вылезает ошибка. вот чтобы рут мог стаовиться владельцем файлов и папок
в nfs папке испольщуется опция no_root_squash


# exportfs -a
если lxd контейнер запущен без спец параметров то на этом этапе вылезет ошибка
exportfs does not support NFS export


# systemctl enable nfs-server
# systemctl start nfs-server

как посмотреть какие шары наш nfs сервер опубликовал. 

первый способ

# exportfs -v
/var/nfs        172.16.102.0/24(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)
/var/nfs2       172.16.102.0/24(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)

второй способ

# showmount -e localhost
Export list for localhost:
/var/nfs2 172.16.102.0/24
/var/nfs  172.16.102.0/24

кстати в случае nfs шары называются экспортами


как подключаться к nfs серверу тоесть как проверить как примаунтить на другом сервере 
эти NFS шары:

ставим nfs-client
# apt-get install nfs-common
после этого nfs папку можно монтировать через mount 

# mount -t nfs  test-nfs-01.mk.local:/var/nfs /mnt/nfs
или
# mount IP_nfs_сервера:/var/nfs/general /nfs/general

где 
/var/nfs/general = в точности путь к папке шары на nfs сервере


/// как поставит nfs сервер ///
/// конец блока             ///


окей. сервер nfs установили настроили. в целом как маунтить эту папку
на клиенте понятно. возвращаемся к кубернетесу

надо обязательно поставить пакет nfs-common на все дата ноды
иначе при попытке mount -t nfs будет писать ошибку. 

теперь привожу Pv.yaml 
скажу сразу что примеры из доков куба гавно. они нерабочие.
поэтому пришлось искать примеры на стороне.
тип бекенда прописан nfs 


apiVersion: v1
kind: PersistentVolume

metadata:
  name: portal-info-data-pv


spec:
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  mountOptions:
    - hard
    - vers=4.0
    - timeo=60
    - retrans=10
  capacity:
    storage: 35Gi
  nfs:
    server: test-nfs-01.mk.local
    path: "/var/nfs"
  persistentVolumeReclaimPolicy: "Recycle"


начнем разбирать эту мантру пока поверхностно.
name: portal-info-data-pv=имя PV под которым будем к нему обращаться

далее интееренсная опция  volumeMode: Filesystem 
она говорит в каком виде этот вольюм будет прикручен к поду. если файлсистем то будет прикручен 
в виде папки. есть еще  volumeMode: Block . тодга PV будет прикручен к поду в виде block device.
так вот интересно то что volumeMode: говорит о том в каком виде вольюм будет прикручен к поду но
дело в том что бекендом для этого вольюма может служить разная хрень. например бекендом для pv
может быть nfs. тоесть уже готовая папка (фс). в этом случае проблемы нет. на бекенде есть 
готовая папка и маунтим мы ее к поду в виде папки. а вот бекендом еще может быть блочное устройство
если напимер тип pv равен rbd а втоже время для него мы указываем volumeMode: Filesystem 
тогда вопрос как же это блочное устройство станет файловой папкой внутри пода. значит ведь 
это блочное устройство ктото должен разметить и отформатровать. так вот наскоько я прочитал 
в описании к8 это автоматом делает сам к8. либо же драйвер этого бекенда для к8. таким макаром
это удобно. например на ceph мы выделяем блочное устройство , его к8 автоматом прозрачно
форматирует нам об этом заботится ненадо. а внутрь пода уже маунтится этот pv в форме папки.


далее еще тонкий момент.

mountOptions:
    - hard
    - vers=4.0
    - timeo=60
    - retrans=10
это параметры которые куб будет использовать  в команде mount -t nfs 

вот я раскопал какой mount потом использует куб для маунта нашей nfs 
папки при создании пода

mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60 test-nfs-01.mk.local:/var/nfs /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv

nfs: => это мы указали что тип  бекенда PV это nfs сервер
есть еще другие типы бекенда. такие например как 
cephfs -  CephFS volume. тоесь это когда на цефе создается фс и мы ее биндим
csi   - это интересная штука. пока мало инфо
rbd  - это когда из цефа мы импортируем блочное устройство


 server: test-nfs-01.mk.local
    path: "/var/nfs"
параметры nfs сервера

а если бы тип PV был hostpath то вместо nfs секции(которую только 
что разобрал) было бы 

 hostPath:
    path: "/mnt/data"

устанавливаем этот PV

# kubectl apply -f PV.yaml

и проверяем что он успешно установился

# kubectl get pv
# kubectl get pv
NAME                 CAPACITY    CLAIM    STATUS
portal-info-data-pv  10Gi                 Available


значит видно что pv появился  и что поле CLAIM у него пока пустое

тут я привожу дескрайб для аналогичного PV

# kubectl describe  pv  pv-01
Name:            pv-01
Labels:          <none>
Annotations:     <none>
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    
Status:          Available
Claim:           
Reclaim Policy:  Recycle
Access Modes:    RWX
VolumeMode:      Filesystem
Capacity:        1Gi

de Affinity:   <none>
Message:         
Source:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    10.42.147.49
    Path:      /data
    ReadOnly:  false
Events:        <none>

 
а вот еще момент. мы задали в формате yaml манифест для PV. 
накатили его на к8. 
мы можем посмотреть как выглядит полный манифест для PV уже когда его принял к8 
и развернул.



# kubectl    get  pv   pv-01  -o yaml

apiVersion: v1
kind: PersistentVolume

metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"PersistentVolume","metadata":{"annotations":{},"name":"pv-01"},"spec":{"accessModes":["ReadWriteMany"],"capacity":{"storage":"1Gi"},"mountOptions":["hard","vers=4.0","timeo=60","retrans=10"],"nfs":{"path":"/data","server":"10.42.147.49"},"persistentVolumeReclaimPolicy":"Recycle","volumeMode":"Filesystem"}}
  creationTimestamp: "2023-06-23T23:18:18Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pv-01
  resourceVersion: "2590075"
  uid: 6135d61e-8e92-42d0-935d-e87b2937f81c


spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 1Gi
  mountOptions:
  - hard
  - vers=4.0
  - timeo=60
  - retrans=10
  nfs:
    path: /data
    server: 10.42.147.49
  persistentVolumeReclaimPolicy: Recycle
  volumeMode: Filesystem
status:
  phase: Available


и как видно в этом полном ямль формате 
есть тип pv это nfs 
и есть "volumeMode: Filesystem" который оббясняет в каком виде этот PV (фс или блочное устройство)
будет прикручен к поду 


далее устанавливаем pv claim



apiVersion: v1
kind: PersistentVolumeClaim

metadata:
  name: portal-info-data-pvc

spec:
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  volumeName: "portal-info-data-pv"
  storageClassName: ""



name: portal-info-data-pvc   = имя клейма по которому мы будем к нему обращатся из пода
чтобы заказать себе PV

volumeName: "portal-info-data-pv"   = имя PV к которому мы обращаемся нашим клеймом.
дело в том что в этом примере мы создали PV руками а не через storageclass. поэтому 
такой PV через pvc можно найти только по имени. поэтому для нахождения статического PV 
мы в PVC указываем имя PV


volumeMode: Filesystem  = вот мы тут опять прописываем какой вид PV нам нужен.
а именно что мы хотим только такйо PV который можно в формате папки примонтировать (а не блочного
устройства)

  accessModes:
    - ReadWriteMany
эта хрень прописывает что на данный pV можно писать с разных нод.
пока мало понятно но тем не менее. видимо это говорит кубу что под можно запускать 
на любой ноде и с нее можно дотянуться до этого PV. потмоу что есть такие PV до котороых
можно дотянуться только с опрделенной ноды. занчит под надо запускать только на той ноде.

  storageClassName: ""
сторадж класс пустое имя. это значит что мы заказываем через наш PVC статичекий предподготовленный PV 
тоесть он уже существует в к8. а если имя сторадж класса непутое то это значит что PV на к8 такого
щас нет. и цепочка такая. pvc обращется к sc а он уже создает динамически +1 PV.

storage: 10Gi 
типа прописвает какой размер PV нам нужен. ни больше ни меньше.


а вот не отходя от кассы как выглядит pvc когда мы заказываем динамический PV
через storageClass

# cat pvc.yml 
---
kind: PersistentVolumeClaim
apiVersion: v1

metadata:
  name: ceph-rbd-sc-pvc

spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50M
  storageClassName: ceph-rbd-sc


как видно здесь мы обращаемся к сторадж классу и закывазем только размер PV
и тип доступа. 



возврашаемся к нашему исходному клейму.

устанавливаем клейм и смотрим на него
# kubectl apply -f claim.yaml
# kubectl get pvclaim

NAME                       STATUS    VOLUME                CAPACITY
portal-info-data-pvc       Bound     portal-info-data-pv   10Gi            

теперь смотрим как изменился статус PV
вот так было у PV до того как мы накатили PVC

# kubectl get pv
NAME                 CAPACITY    CLAIM                          STATUS
portal-info-data-pv  10Gi        default/portal-info-data-pvc   Available

а вот так стало после накатки PVC

# kubectl get pv
NAME                 CAPACITY    CLAIM                          STATUS
portal-info-data-pv  10Gi        default/portal-info-data-pvc   Bound

изменилось поле STATUS, 
было Available а стало Bound



еще раз подобью бабки об именах pv и клейм. ибо они длинные
portal-info-data-pv   (pv)
portal-info-data-pvc  (клейм)


возникает вопрос - нахер этот PVC вообще нужен.
если PV уже создан то как бы нахер он нужен. если PV динамический то PVC по факту
порождает создание PV. окей. вопрос почему незаказывать PV прямо сразу из пода.
зачем нужен PVC. 
также замечу что ни накатка pv ни накатка pvc в случае nfs неприводит к маунтингу этого nfs
папку куда бы тони было. это произовйдет только при уже создании пода.
возвращаеся к впопросу нахер нужен PVC. почему нельзя заказывать PV изнутри пода из его ямл
манифеста. а дело вот в чем нам нужна какойто идентификатор PV на кластере который не зависит
от манифеста пода. еси бы мы заказыали PV из ямля пода то при удалении пода 
у нас бы удалялся и PV потому что его манифест содержался бы  в поде. а так у нас под удаляется
а ссылка на PV сохранятеся в PVC. создание и удаление пода никак не влияет на PV.
PV это сам диск а PVC это заявка на него или указатель на него. 
удаление PVC приведет (в зависиомсоти от настроек толи PV толиPVC) к тому что PV либо будет
удален либо он будет освобожден для захвата другим PVC.

PVC в случае наличия в нем стораж класса создает динамический PV и звхатвыает в его 
в свою власть.
в случае отсутвтвия стораж класса в PVC он неосздает PV он ищет уже созданный и захывает
его так что другой PVC не будет трогать этот PV. 

когда PV захвачем PVC мы можем этот PV уже присоединять  к тому поду к которому
мы захотим. PVC это ниточка к PV. PV без PVC он как бы обезличенный. без номера условно 
говоря. а  когда мы захватили PV чрез PVC то наш PV как бы получает идентификационный номер
по которому мы можем отличать этот PV от всех других. 

таким образом создание PVC ведет к тому что создается по его заказу PV (если мы говорим про динамический
PV ) ведь до PVC новый PV просто несуществует. и у нас появяляется ниточка к этому PV 
через PVC. 


pod
теперь наконец создаем под который использует pv
цепь такая , под обращается к claim а клейм обращается к pv

apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: portal-info-data-pvc
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
		  

кусочек в поде который прописывает клейм.
volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: portal-info-data-pvc

здесь получается "claimName: portal-info-data-pvc" это имя клейма через который
мы хотим получить PV.
а поле "name: task-pv-storage" это уже имя диска под которым мы  его присоденияем
к контейнеру
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage

это какйото дебилизм. почему нелььзя было обращаться из контейнера к Pv через имя клейма.
нахер еще раз имя переопределять. непонятно.


что сущесвтенно - сущесвтенно то что если мы юзаем NFS PV то к8 в этой точке монтирования 
несздаст ниаких новых подпапок. если у нас на нфс сервере эта точка монтирования просто 
голая папка то и в поде будет просто голая папка

далее еще интересный момент. вот мы узнали на каком сервере у нас запущен под.
тогда мы можем найти в какую папку к8 примонтировал нашу nfs шару. главное не ступить
и неискать nfs примонтированный не натом сервере где крутится под.

# mount | grep nfs
10.42.147.49:/data on /var/lib/kubelet/pods/13daf157-f212-4e3b-9ee2-717d2b1f3a15/volumes/kubernetes.io~nfs/pv-01 type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=60,retrans=10,sec=sys,clientaddr=10.42.147.2,local_lock=none,addr=10.42.147.49)

отсюда видно где к8 хранит файловые системы всех своих подов - папка /var/lib/kubelet/pods/
в чем прикол этой папки - можно там зайти в любой под в его volume и напрямую руками 
чтото отредактировать.

что еще интересно. что папка которая идентифицирует под то есть 13da.... 
это uid в манифесте пода

# kubectl get   pods pod1  -o yaml | grep uid
  uid: 13daf157-f212-4e3b-9ee2-717d2b1f3a15

таким макаром можно решить интересную задачу. мы берем под. 
опредяем его uid. и таким макаром знаем куда нам надо лезть в /var/lib/kubelet/pod/..
(обрати внимание не uuid а uid)


также приколно то что созданный руками шара на nfs сервере превращается ровно в один PV
получается для одного пода. а если мы хотим кучу PV то это надо руками кучу 
шар создавать на nfs сервере. или создавать nfs storage class и nfs provisioner который 
будет делать это за нас.

еще хочу скзаать что если к8 крутится внутри lxd контейнера то к такому к8 ноде
не получится внутрь подключить PV на основе ceph. неполучится потому что  и щас скажу почему:
вот я сооздал кластер к8 без lxd на котором работает монтирования Pv на основе ceph.
я взял под у которого PV идет из ceph. выяснил его uid

# kubectl get  pods  ceph-rbd-pod-pvc-sc -o yaml | grep uid
  uid: bf6a9989-bbdd-413d-bbb5-9a151f3bf75d

выяснил хост на котором он крутится. 
пошел на тот хост и так как под у нас крутится в папку вида /var/lib/kubelet/pod/uid
и так как вольюм монтируется внутрь этой папки то мы можем найти строку монтиорования используя uid
и я ее нашел

# mount | grep /bf6a9989-bbdd-413d-bbb5-9a151f3bf75d
/dev/rbd0 on /var/lib/kubelet/pods/bf6a9989-bbdd-413d-bbb5-9a151f3bf75d/volumes/kubernetes.io~csi/pvc-e3afa980-35f5-41a9-81e6-e2e553870972/mount type ext4 (rw,relatime,discard,stripe=64,_netdev)

и по ней мы видим что на хосте у нас из цеф создано устройство /dev/rbd0 блочное 
и оно примонтировано в папку пода. и становится понятно почему монтирования такого PV неработает 
на хосте к8 который крутится внутри lxd  контейнера. потому что создать на лету устройство
в папке /dev/ внутри lxd контейнера невозможно. это можно сделать только на хосте.
поэтому PV на основе ceph не примонтируешь внутрь lxd контейнера. и получается +1 каждый новый
pv на основе ceph бдует создавать на хосте +1 /dev/rbdX устройство.
может конечно можно примонтировать cephFS как то внутрь lxd контейнера но это пока непонтяно 
как сделать.

в целом это неудивительно ведь даже нфс сервер на оснвое lxd контейнера можно создать только при доп настройках 
для этого lxd контейнера. 
зато к ноде к8 которая крутится внутри lxd контейнера можно  подключить NFS PV



двигаем дальше.
насколько  я помню в докере  volumes  это хрень когда мы хотим папку с хоста
пробросить в папку в контейнере. 
вольюм  это просто обьект с названием ему нужен бекенд. бекендом указывается клейм
 portal-info-data-pvc. итак к контейнеру будет присобачен внешний 
 volume task-pv-storage который свои бекенд получит от клейма portal-info-data-pvc

 
далее в поде мы прописываем какую папку в поде будем пробрасывать в этот вольюм
volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
		
папка внутри контейнера /usr/share/nginx/html
будет проброшена во внешний вольюм task-pv-storage


таким образом полная цепочка

папка пода -> claim -> pv

публикем под. и проверяем завелся ли он.
если да то вся цепочка срослась.

все сработало.


выше я рассматривал PV  типа NFS.
а щас для примера рассмотрю PV типа cis

# cat pv.yml 
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-rbd-sc-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50M
  storageClassName: ceph-rbd-sc

как видно в нем невиден тип PV потому что обращение идет к сторадж классу.


посмотрим на сторадж класс


# cat ceph-rbd-sc.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass

metadata:
  name: ceph-rbd-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"

provisioner: rbd.csi.ceph.com
parameters:
   clusterID: 2f48afc6-22a3-4688-a09f-4d8a011bb7da
   pool: kubePool
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: ceph-admin
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/controller-expand-secret-name: ceph-admin
   csi.storage.k8s.io/controller-expand-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: ceph-admin
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard


кхм.. тут тоже невидно.

окей на спасение приходит ямль манифест от PV который нам покажет сам к8. 
когда мы уже накатили на него наш PV


# kubectl get pv pvc-e3afa980-35f5-41a9-81e6-e2e553870972 -o yaml
apiVersion: v1
kind: PersistentVolume

metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: rbd.csi.ceph.com
  creationTimestamp: "2023-06-23T00:05:10Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pvc-e3afa980-35f5-41a9-81e6-e2e553870972
  resourceVersion: "3824"
  uid: e1183cf7-3641-4413-8aa0-4cd92f79b707

spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 48Mi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: ceph-rbd-sc-pvc
    namespace: default
    resourceVersion: "3819"
    uid: e3afa980-35f5-41a9-81e6-e2e553870972
  csi:
    controllerExpandSecretRef:
      name: ceph-admin
      namespace: default
    driver: rbd.csi.ceph.com
    fsType: ext4
    nodeStageSecretRef:
      name: ceph-admin
      namespace: default
    volumeAttributes:
      clusterID: 2f48afc6-22a3-4688-a09f-4d8a011bb7da
      csi.storage.k8s.io/pv/name: pvc-e3afa980-35f5-41a9-81e6-e2e553870972
      csi.storage.k8s.io/pvc/name: ceph-rbd-sc-pvc
      csi.storage.k8s.io/pvc/namespace: default
      imageFeatures: layering
      imageName: csi-vol-9e916d54-1159-11ee-b8e4-bab4468ed06d
      journalPool: kubePool
      pool: kubePool
      storage.kubernetes.io/csiProvisionerIdentity: 1687478381426-8081-rbd.csi.ceph.com
    volumeHandle: 0001-0024-2f48afc6-22a3-4688-a09f-4d8a011bb7da-0000000000000003-9e916d54-1159-11ee-b8e4-bab4468ed06d
  mountOptions:
  - discard
  persistentVolumeReclaimPolicy: Delete
  storageClassName: ceph-rbd-sc
  volumeMode: Filesystem
status:
  phase: Bound


и тут уже видно что тип вольюма это "cis" вольюм.
и то что "volumeMode: Filesystem" это значит что к поду будет маунтится папка.(а не блочное устройство)
я там потом позже будут рассатривать этот PV .  бекендом для этого PV является блочное устройтво
из цефа. и вот к8 через драйвер его аавтоматом форматириует

    driver: rbd.csi.ceph.com
    fsType: ext4

что меня тут удивлется это то что почемуто тип Pv это не rbd а cis. это пока загадка.


пока откладываю этот цеф pv. и
возвращаемся к нашему nfs PV
начинаем копаться.
например где на диске находится этот маунт


значит я когда неустановил на дата ноды пакет nfs-common 
после публикации пода он незаводился а в его логе была ошибка

Warning  FailedMount  11s  kubelet  MountVolume.SetUp failed for volume "portal-info-data-pv" : mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv --scope -- mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60 test-nfs-01.mk.local:/var/nfs /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv
Output: Running scope as unit run-r44a9aee0e2b44d809700914c5b983839.scope.
mount: wrong fs type, bad option, bad superblock on test-nfs-01.mk.local:/var/nfs,
       missing codepage or helper program, or other error
       (for several filesystems (e.g. nfs, cifs) you might
       need a /sbin/mount.<type> helper program)

       In some cases useful info is found in syslog - try
       dmesg | tail or so.

что в этой ошибке можно интересного увидеть. это как кубернетес
непосредственно монтируем nfs шару и куда он ее монтирует

mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60 test-nfs-01.mk.local:/var/nfs /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv

еще раз видно что поды их вольюмы монтируются в папку 
/var/lib/kubelet/pods/uid/...


соотсвтевенно монтируем через mount -t nfs -o hard,nfsvers=4.0,retrans=10,timeo=60

откуда test-nfs-01.mk.local:/var/nfs
куда /var/lib/kubelet/pods/b78b0ac0-d65d-4807-b085-64771dbab78c/volumes/kubernetes.io~nfs/portal-info-data-pv

потом я поставил пакет nfs-common на дата ноды , удалил и заново
создал под и под завелся.


из самого маунтинга приходят вопросы а где у нас прописана вся 
эта специфика указанная в маунт.
в поде ли , в клейме ли , в pv ли.
если поднятся выше и посмотреть то вся специфика маунтинга прописывается 
в PV.yaml



как видим нигде в маунтинге неуказан никакой размер который будет окупирован
подом. но это и логично это ведь файловая шара. какие тут лимиты.
все место что в шаре есть все твое.
куб я думаю вопрос поедаемого размера на шаре контролирует както подругому
нечерез mount

также видно что вольюм куб монтирует в /var/lib/kubelet/pods/
еще раз это повторю.



так как это сетевая шара то выделять под этот путь отдельный диск 
на нодах чтоб он незабился нет смысла ибо это сетевая шара и на ноде
она ничего по размеру незанимает.


вспоминаем что это вообще за папка в кубе /var/lib/kubelet/pods/
и также надо выяснить вот этот вот маунтинг куб делает на какой ноде? логично
что ровно на той на которой крутится сам под. проверим
также заметим что b78b0ac0-d65d-4807-b085-64771dbab78c это видимо ID
пода. это uid пода


# kubectl get pods pod2 -o yaml  | grep uid
  uid: 88bdc69f-197e-40cb-99a8-0252dd38f54b




интересно что вот мы обьявили pv в кубе и он вроде как немонтирует его
при этом никуда. что он его монтирует каждый раз походу в каждвый
конкретный под. 




самые интересные из них это то что при создании pv куб его никуда немонтирует
чтобы раздавать его из одной точки. 

вообще это полностью логично ! под это процесс. обычный процесс.
процесс работает на каком то хосте. и нужно маунтить вольюм именно 
на этом хосте где под. не будет же такого что под крутится
на одном хосте а вольюм примаунтен на другом хосте. это был бы дебилизм.

еще важно понять что все вольюмы подов маунтятся в конечном итоге
через классический маунт. тоесть еще раз схема работает такая - 
под это процесс. процессу нужно дать вольюм. вольюм это папка (обычно)
папку маунтят через классический маунт.


монтирует вольюмы для каждого
пода индивидуально через mount. 


далее я проделал такой эксперимент я задался вопросом - можно 
ли создать два пода которые подключены к одному вольюму.
подключены к одному вольюму через тот же самый клейм. 
и ответ да. это работаает.


вот мой PV

apiVersion: v1
kind: PersistentVolume

metadata:
  name: pv-01


spec:
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  mountOptions:
    - hard
    - vers=4.0
    - timeo=60
    - retrans=10
  capacity:
    storage: 1Gi
  nfs:
    server: 10.42.147.49
    path: "/data"
  persistentVolumeReclaimPolicy: "Recycle"


вот мой PVC

# cat pvc.yml 

apiVersion: v1
kind: PersistentVolumeClaim

metadata:
  name: pvc-01

spec:
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  volumeName: "pv-01"
  storageClassName: ""



вот мой первый pod

s# cat pod1.yml 
apiVersion: v1
kind: Pod

metadata:
  name: pod1

spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-vol

  volumes:
    - name: nginx-vol
      persistentVolumeClaim:
        claimName: pvc-01


вот мой второй pod

# cat pod2.yml 
apiVersion: v1
kind: Pod

metadata:
  name: pod2

spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-vol

  volumes:
    - name: nginx-vol
      persistentVolumeClaim:
        claimName: pvc-01



и рельно я зашел в поды и они  реально 
смотрят в один вольюм


и вот даже такие пруфы

# mount | grep nfs
10.42.147.49:/data on /var/lib/kubelet/pods/13daf157-f212-4e3b-9ee2-717d2b1f3a15/volumes/kubernetes.io~nfs/pv-01 type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=60,retrans=10,sec=sys,clientaddr=10.42.147.2,local_lock=none,addr=10.42.147.49)

10.42.147.49:/data on /var/lib/kubelet/pods/88bdc69f-197e-40cb-99a8-0252dd38f54b/volumes/kubernetes.io~nfs/pv-01 type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=60,retrans=10,sec=sys,clientaddr=10.42.147.2,local_lock=none,addr=10.42.147.49)


маунт одного и того же nfs ресурса в два пода. главное что один и тот же 
nfs ресурс

замечу что у меня PV и PVC созданы в режиме ReadWriteMany.
это напомню означает что на данный PV могут писать поды с разных 
нод. это значит что куб просто обязан создавать поды на РАЗНЫХ нодах
подключенных  этому вольюму.

я создал третий под.
# kubectl get pods  -o wide | grep -E "READY|pod"
NAME                                  READY   STATUS    RESTARTS      AGE     IP              NODE       NOMINATED NODE   READINESS GATES
pod1                                  1/1     Running   0             13h     10.233.95.95    nl-k8-02   <none>           <none>
pod2                                  1/1     Running   0             50m     10.233.95.96    nl-k8-02   <none>           <none>
pod3                                  1/1     Running   0             46s     10.233.80.220   nl-k8-03   <none>           <none>






>>>
еще вопросы
это может ли куб иметь несколько мастеров
чтобы была редунданси.
как там сплит брейн решен.

ответ да может. сплит брейна там нет.
апи сервер выступает в роли апликейшн сервера. а данные хранться в единой
базе etcd

что происходит когда мастер невидит ноды и поды поому что скажем сфере
ей насрать. виртуалки будут работать на серверах.
>>>

>>>
ceph
>>>

клеймы позволяют указать в каком режиме будет доступ
к куску диска который они отожрали
ReadWriteOnce - R\W для одного хоста          (куб обозначает RWO)
ReadWriteMany - R\W для нескольких хостов     (куб обозначает RWM)
ReadOnlyMany - R для нескольких хостов        (куб обозначает ROM)


что существенно отметить - что режим RWO он позволяет 
многим подам одновременно читать и писать в один вольюм
правда но одно хосте. но главное что куча подов может делать а 
не один под.


как я понял в клейме из параетров  указывается размер диска который берется
 в lease у PV
и режим доступа к этому куску.

далее куб пишет что поскольку юзерам нужны диски у которых можно 
выбрать нетолько размер и режим доступа но и скажем скорость 
диска то клеймы это не позволяют выбирать и на помощь приходить
storage class

если PV заготавливать заранее руками это назыавется статический метод
заготовки PV.
тоесть скажем мы руками загоняем в куб 7 PV. и они там висят готовые 
к труду и обороне. 

как я понял в клейме мы можем указать как название PV так и storage class.
если мы указали PV то это мы обращаемся к заранее опубликованному в кубе
статическому PV. если в клейме мы указываем сторадж класс то куб
через сторадж класс будет пытаться создать PV в автоматическом режиме.
такой способ создания PV называется динамическим.

при запросе статического PV клейм указывает storageClas="", и имя PV
а при динамическом клейм делает запрос к сторадж классу с именем 
и без PV имени.

чтобы активировать в кубе фичу динамических PV надо чтобы на апи сервере
куба была указан параметр DefaultStorageClass в опции --enable-admission-plugins.
посмотрим указан ли такой параметр по дефолту в запущенном апм сервере
на мастере выполняем
# ps aux | grep kube-apiserver


kube-apiserver 
--advertise-address=172.16.102.31 
--allow-privileged=true 
--authorization-mode=Node,RBAC 
--client-ca-file=/etc/kubernetes/pki/ca.crt 
--enable-admission-plugins=NodeRestriction 
--enable-bootstrap-token-auth=true 
--etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt 
--etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt 
--etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key 
--etcd-servers=https://127.0.0.1:2379 
--insecure-port=0 
--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt 
--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key 
--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname 
--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt 
--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key 
--requestheader-allowed-names=front-proxy-client 
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt 
--requestheader-extra-headers-prefix=X-Remote-Extra- 
--requestheader-group-headers=X-Remote-Group 
--requestheader-username-headers=X-Remote-User 
--secure-port=6443 
--service-account-key-file=/etc/kubernetes/pki/sa.pub 
--service-cluster-ip-range=10.96.0.0/12 
--tls-cert-file=/etc/kubernetes/pki/apiserver.crt 
--tls-private-key-file=/etc/kubernetes/pki/apiserver.key

как видим в опции --enable-admission-plugins нашего параметра нет по дефолту

--enable-admission-plugins=NodeRestriction

тоесть должно там быть типа такого 

--enable-admission-plugins=NodeRestriction,DefaultStorageClass

после знака "=" указываются плагины так назыаемого admission
контроллера


и тут я еще дополняю о том что ряд плагинов адмишн контроллера 
они активируются по дефолту и поэтому их не будет в конфиге апи сервера

как узнать какие плагины админшн контроллера активрированы по дефолту
для этого надо залезть в справочную систему апи сервера. 
для этого надо зайти внутрь апи сервера и вызывать его в командной строке 
с флагом -h


# kubectl exec -it  kube-apiserver-nl-k8-01     -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'

--enable-admission-plugins strings       admission plugins that should be enabled in addition to default enabled ones (NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, PodSecurity, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, RuntimeClass, CertificateApproval, CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, MutatingAdmissionWebhook, ValidatingAdmissionPolicy, ValidatingAdmissionWebhook, ResourceQuota). Comma-delimited list of admission plugins: AlwaysAdmit, AlwaysDeny, AlwaysPullImages, CertificateApproval, CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, DefaultStorageClass, DefaultTolerationSeconds, DenyServiceExternalIPs, EventRateLimit, ExtendedResourceToleration, ImagePolicyWebhook, LimitPodHardAntiAffinityTopology, LimitRanger, MutatingAdmissionWebhook, NamespaceAutoProvision, NamespaceExists, NamespaceLifecycle, NodeRestriction, OwnerReferencesPermissionEnforcement, PersistentVolumeClaimResize, PersistentVolumeLabel, PodNodeSelector, PodSecurity, PodTolerationRestriction, Priority, ResourceQuota, RuntimeClass, SecurityContextDeny, ServiceAccount, StorageObjectInUseProtection, TaintNodesByCondition, ValidatingAdmissionPolicy, ValidatingAdmissionWebhook. The order of plugins in this flag does not matter.

в этой портянке выцепляет кусок
(NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, PodSecurity, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, RuntimeClass, CertificateApproval, CertificateSigning, CertificateSubjectRestriction, DefaultIngressClass, MutatingAdmissionWebhook, ValidatingAdmissionPolicy, ValidatingAdmissionWebhook, ResourceQuota)

ив нем мы видим DefaultStorageClass
значит dynamic PV по дефолту активированы на апи сервера

но если бы нам надо все таки было активировать руками 
то делается это так 

--enable-admission-plugins=NodeRestriction,DefaultStorageClass


правим файл 
/etc/kubernetes/manifests/kube-apiserver.yaml
и перезапускаем kubelet

# systemctl restart kubelet

ну и проверить что все сработало

# ps aux | grep apiserver




далее. поды получают себе PV через  клеймы.это мы уже видели.
напомню как при описании volume в pod мы обращаемся к claim
kind: Pod
...
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: portal-info-data-pvc
		
если pvc используется подом то pvc нельзя удалить.
точнее команда на удаление пройдет но pvc будет висеть неудаленный
до тех пор пока жив под который его юзает и в свойствах pvc
в статусе будет указано terminating и в строке finalizers будет строка kubernetes.io/pvc-protection

# kubectl describe pvc hostpath
...
Status:        Terminating
Finalizers:    [kubernetes.io/pvc-protection]

тоже самое для pv. если дать команду удалиь pv то он будет висеть
неудаленный пока неисчезнет pvc который его юзает.		

# kubectl describe pv task-pv-volume
...
Finalizers:      [kubernetes.io/pv-protection]
Status:          Terminating


походу вот еще чо! к одному PV можно подключить только один pvc!
тоесть два pvc нельзя подключить к одному PV.
и тут вылезает вот эта проблема ! под каждый под нужен свой предварительно
созданный PV! это же жопа! число подов динамически меняется и надо 
либо иметь  всегда подготовыленные PV с запасом. это катстрофически неудобно.
надо чтобы PV создавались автоматически при запросе от пода.
так что статический метод формирования PV это абсолютнонам немодходит.
нам походит только динаимческий метод формирования PV получается.

далее написано. если мы удалили pvc то то что дальше будет с PV к которому он 
был подсоединен зависит от  persistentVolumeReclaimPolicy:  которая прописана в PV
пример

apiVersion: v1
kind: PersistentVolume
...
 persistentVolumeReclaimPolicy: "Recycle"

из бывает три штуки:
Retain
Delete
Recycle

Retain - когда удалили pvc то pv как обьект остается в кубе, содержимое 
этого pv остается нетронутым , статус pv = released но этот pv он недоступен 
для автоматического повторнгого подключения к нему другого pvc.
в целом как я понял он вообще больше недоступен для повторого использования.
надо руками удалить этот pv, затем почемуто надо удалить те данные которые
лежат на pv и потом заново создать этот pv.  в общем полезно если данные
после работы пода нам нужны и иудалять их нельзя типа как база эластика.
но какая то дурацкая система что повторно заюзать с другим pvc нельзя

Delete - как я понял при удалении pvc куб автоматом удалит и pv и даже хуже
того еще и данные удалить на этом pv

Recycle - пишу что это устаревшая опция типа неюзайте ее. а юзайте
динамичесеий провижионинг. а так типа опция работает так что она удаляет
данные что лежат на pv и данный pv снова доступен для того чтобы его другой 
pvc мог заюзать.

теперь надо проверять на практике то что написано.

далее написано что если у нас есть PV то он будет заюзан первым на очереди pvc который удовляряет ему. а если мы хотим чтобы прям конкретный pvc заюзал
наш pv то надо в PV указать имя привилигированного pvc который только и имеет
право заюзать pv. причем как надо и pv указать имя pvc и в pvc указать
имя pv. но все таки главная мысль что в pv надо указать имя pvc.тоесть
в pvc мы указываем типа что нам желательно подключиться к такому то pv.
а вот в pv мы говорим что обязательно к этому pv может подключться только 
такой то pvc. тоесть сам pv решает какой pvc к нему имеет право подкючиться.

пример
pvc foo-pvc желает поключиться к pv foo-pv

apiVersion: v1
kind: PersistentVolumeClaim

metadata:
  name: foo-pvc

spec:
  volumeName: foo-pv


здесь "volumeName: foo-pv" указвывает имя PV 
который мы хотим подключить к PVC
вот не долбоебы ли вместо того чтобы в PVC указать pv_name
поле а они указывают "volumeName:" зачем так делать. 


а pv  foo-pv жестко прописывает что только pvc foo-pvc имеет право к нему
подключиться

apiVersion: v1
kind: PersistentVolume

metadata:
  name: foo-pv

spec:
  claimRef:
    name: foo-pvc
    namespace: foo


  

  про динамический провижиониннг. 
  PV можно можно создавать автоматом но на основе клейма.
  так вот вопрос а кто будет автоматом клеймы создавать?
  для того чтобы под заработал надо иметь предварительно созданный PV а для этого нужно иметь предвариетельно созданный клейм.хм.. 
  тут насколько я понимаю на помощь приходит claimtemplate,
  он позволяет автомтом создавать клеймы по шаблону.
  тогда динамическй провижн обретает на конец практический смысл

  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi


 только прикол в том что volumeClaimTemplates: нельзя использовать
 для пода например. а можно использовать только в ямле statefull set
 это не клево. тоесть ни под ни деплоймент немогут исползововать
 volumeClaimTemplates:

 этот аспект остатеся пока открытым.
я считаю что если нельзя легко создавать клеймы динамически
то это круто ставит крест на динамическом создании PV.




  схма динамического провижионинга
  
  pod -> claim -> storage class -> provisioner -> pv 

  в этой схеме порблема такая что клейм нам надо создавать руками.
  нето чтобы это смертельно но неудобно.с другой стороны под
  щтука штучная значит и +1 клейм создать руками не вопрос.

  у стейтфулл сета у которго указываются реплики
  там ситуация такая

  statefull set -- > шаблон пода ---> шаблон клейма --> storage class --> provisioner -> pv

  таким образом мы в магифесте стейтфулл сета указываем 
  шаблон для одного пода\клейма а стейтфулл сет уже сам создает
  гору подов и клеймов. это норм подход.


  реплику сет не рассматриваем так как она приудмана для служебных
  целей деплоймента. и не предназанчена чтобы ее создавали руками


  деплоймент. вот тут интересно...
  
  
рассмотрим как обстоят дела с PV у подов деплоймента.


деплоймент его поды и вольюмы (PV). что я выяснил.
чего сделать нельзя => нельзя сделать так чтобы поды (реплики)
деплоймента пользовалоись несколлькими  PVC то есть несколькими PV. 
такое невозможно. все поды деплоя должны обязаны использовать один PV
на всех. либо вобще ни одного.

скажем мы деплоим жигкс через деплой. у него 50 реплик.
так вот нельзя сделть так чтобы к ним примаунтить несолько PV.
это нельзя. а что можно . можно их всех примаунтить к одному PV.
причем и на запись тоже. они все смгуть писать в этот PV.
еще какое ограничение - если PV имеет свойство ReadWriteMany 
то тогда нет проблем с тем на кких хостах можно запускать
поды реплки - на всех хостах. тоесть под запускатеся на любом хосте 
и куб без прбоблем маунтит на тот хост этот PV.
а если PV имеет свойство ReadWiritneOne то это значит что этот PV
можно приманутить только на один хост. любой один хост. и это значит
что поды тоже должны все быть запуены только на одном хосте. 
еслиони запущены на одномхосте то проблем  с тем чтобы примаунттть их 
всех к этому PV нет.  есть только прблема в том что деплой он тупой
и у него хватвает мозгов порять что все поды на дозаусктаь на одном хосте.
поэтому этот придурок будет пытаться запускать поды на разных 
хостах. а потом будет ругаться о том что мол дескать наш PVC 
он уже примаунчен другим подом. так вот еррор мессадж мудацкий 
и мислидинг. как бутто намекаюзий что нащ PV может быть примаунчен
толкьо к однму поду. это еще раз скажу что это хуйня полная. 
прблема только в том что надо заставить куб опудликовать все поды
на одном хосте. прблема решаается тем что надо в манифесте деплоя
укзаать какйнибудь лейбл ноды. который в итоге обьяснит делою и заставит
его публиковать поды на одном хосте. и тогда никаких проблем
насоздавать милллиард подов на этом хосте и примаунить их всех 
к этому PV нет вообще.

и тогда если это конфиг файл. то все поды деплоя будут иметь 
одинаковый конфиг. а значит обрабаыать входящий поток скорей всего
одинаково. и значит они будут стейтлесс.

хотя один под будет стейтфулл. через деплой без прблем можно разворвааичвать стетйтфулл приложения. надо толкьо чтобы реплик число
было 1. при удалении пода будет создан новый. а PV у него остаентся
прежний. так что проблем с потерей данных нет.


далее хотет осветить вот эти две ебнытые настройки в деплое:
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25
      maxUnavailable: 0


а именно   maxSurge:   и   maxUnavailable:

значит когда идет обновление деплоя то унас создается +1 новая rs
и в ней разворачиваются обновленные поды а в старой rs 
поды удаляются. самая хитрост по сколько подов создается и по скольку 
удаляется.

вот взята картинка в процессе обноавления деплоя

# kubectl get rs

NAME               DESIRED   CURRENT   READY   AGE
depl3-5fb976c98    33        33        8       22s
depl3-7786f864df   42        42        42      17m


одна реплика со старыми подами а вторая с новыми.
так вот maxUnavailable  она задает то макс число подов в
состоянии READY которые может быть недоступно для клиента 
в  течение процесса деплоя. еще раз - дело в том что  в процессе 
деплоя на старой реплике часть реплик удаляется а на новой 
оно создается. причем надо понимать что  есть лаг между 
состояниями DESIRED и READY.  DESIRED это заказанное на сейчас
количество подов в реплике с состояием READY. так вот понятно что
когда мы удаляем поды уменьшая DESIRED то поды удаляются не мгновенно
поэтому есть некотоырй лаг между тем кода мы уменьшили  DESIRED
скажем на 5 подов и тем временем кода они реально удалились.
еще больший лаг есть межжду тем как мы увеличили число желанных подов
в новой реплике в DESIRED и тем моментом когда они завелись и получили
статус READY. таким образом у нас в каждй момент вермени есть каоето
суммарное количество старых пдов в сосотоянии READY и новый подов
в состоянии READY. это суммарное число определяет редунданси приложения.
так вот если мы зададим  maxUnavailable = 0
то к8 будет так хитро создавать новые поды и ждать у них READY 
статуса и так хитро удалять старые поды что в любой момент времени
суммарное число новых и старых подов будет оставаться ровно тому
чему у нас равно желаемое число реплик. то есть редунданси приложения
меняться не будет. я считаю что это то что надо и эту настройку 
надо выставлять всегда именно в ноль.
а еси ее выставить скажем = 5 . это даст то что в процессе деплоя
у нас  суммарное число новых подов в состоянии READY и старых подов
в состоянии READY будет меньше исходного числа реплик на 5.
возникает вопрос а нахуй надо на момент публикации уменьшать 
число редунданси для приложения? делать его меньше и тоньше.


второе число maxSurge
оно вот о чем. когда запускается публикация то создаетс новая реплика
и ее desired занчение мгновенно прыгает на +maxSurge 
величину. тоесть скажем если maxSurge = 10 то мы как бы 
сразу заказывает у куба +10 новых подов. казалось бы что
это число должно ускорять влиять на скорост генерации новых подов. 
так вот я проверил и это хуйня и совсем не так. какое бы мы число 
это большим не выставляли по факту скачкоообразно увеличивается в новой
реплике только число DESIRED подов но число READY подов в новой реплике
растет всего навсего на +1 и все. там нетакого что раз и на +10 у нас
создалоссь READY подов. поэтому на мой взгляд  ставить это число 
больше чем равным 1-3 нет смысла. 
я тестировал это на примере деплоя подов жинкса даже без монтирования
внешнего вольюма. единсвтенное что у меня было ограничение что поды
должны лежать тлько на одной дата ноде. возможно если у нас нет этого ограничения и в кластере 500 дата нод. то возможно этот процесс дествительно распаллеливается по числу дата нод и тогда увеличение 
maxSurge дает ускорения формирования новых подов а значит и ускорение
скорости деплоя.
так что суммарная рекомедация это выставлять 

maxUnavailable = 0 

всегда чтобы редунданси приложения в процессе деплоя не менялось. 
тоесть чтобы количество здоровых READY подов в сумме оставалось 
равным желаемому числу реплик

а 

maxSurge = 1-2

или равным числу дата нод. 


вообще же график\закон прибавления новых подов и удаления
старых подов он какойто ебнутый. привожу график
обноавления делоймента для числа реплик = 10 
и maxUnavailable = 0 и maxSurge = 5
будет четко видно что выставлени максурдж в 5 совершенно 
не ускоряет генерацию новых подов а значит не ускряет деплой в целом

NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   0         0         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   5         5         0       57m
depl3-7786f864df   10        10        10      56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   7         7         2       57m
depl3-7786f864df   8         8         8       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   9         9         4       57m
depl3-7786f864df   6         6         6       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        5       57m
depl3-7786f864df   5         5         5       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        5       57m
depl3-7786f864df   5         5         5       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        5       57m
depl3-7786f864df   5         5         5       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        5       57m
depl3-7786f864df   5         5         5       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       115m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        6       57m
depl3-7786f864df   4         4         4       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       116m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        8       57m
depl3-7786f864df   2         2         2       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       116m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        9       57m
depl3-7786f864df   1         1         1       56m
NAME               DESIRED   CURRENT   READY   AGE
depl3-58568b99c    0         0         0       116m
depl3-5fb976c98    0         0         0       39m
depl3-674c68c4b8   10        10        10      57m
depl3-7786f864df   0         0         0       56m

чтоб понять надо смотреть на столбец с READY.
видно что число новый READY реплик прибавляет по одному. 
и похеру на это число 5 в максурдже.

также следует обратить внимание что как только стартует 
деплой то число DESIRED реплик в новой rs скачок увеличивается
на макс сурдж. однако как я уже сказал это соверщенно не приводу к
тому что чтобы в READY подах также число скачкоообразно на 5 прибавлялось.
прибавляется оно по одному.

и в продолжение темы есть команда в к8 
типа отслеживать прогресс деплой.

# kubectl rollout status deployment/depl3
Waiting for deployment "depl3" rollout to finish: 5 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 5 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 5 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 6 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 7 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 7 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 8 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 8 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 9 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 9 out of 10 new replicas have been updated...
Waiting for deployment "depl3" rollout to finish: 6 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 5 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 5 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 5 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 4 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 4 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 4 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 3 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 3 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 3 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 2 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "depl3" rollout to finish: 1 old replicas are pending termination...


так вот чем ее наебалово крутейшее. она вместо того чтобы показыват
число READY подов в новой rs  а это нам и надо чтобы понимать 
насколко деплой завершился эта дура показыает число DESIRED 
подов в новой rs. вот мы это видим в первой строке вывода


Waiting for deployment "depl3" rollout to finish: 5 out of 10 new replicas have been updated...


и получается misleading сообщение о том что якобы 5 подов уже обновлено
а это хуйня полная. пройдет огромное время пока эти 5 desired подов
превратятся в 5 READY подов. поэтму оно так и выгляидит ебануто и 
непонятно если незнаешь секрет что запускаешь эту команду
он выоводит в первой строке число DESIRRD подов в новой реплике
которое равно макс сурдж числу. именно на это число в новой репике 
прыгает число подов с момента старта деплоя. а потом сидишь идва часа
ждеш пока это число увеличиитсяь хоотя бы на 1.

поэтому если запустил эту кманду и на экране выскочило мгновенно

Waiting for deployment "depl3" rollout to finish: 99 out of 100 new replicas have been updated...

то это соверешенно не значит что мгнвоенно было обновлено 99 подов.
нет это наебалово от к8. 
это значит что в манифесте деплоя дебил както выставил maxsurge=99
и  в новой реплике desired число подов выставилось =99 штук.
но пройдет огромное время пока к8 поодной поду переведет эти 99 подов
в READY состояние.



насколько я понял из эксперимента статус pv будет released тока
тогда когда будет удален pvc который к pv подсоединен.


|| nfs provisioner

до этого я монтировал nfs через ручное создание PV
для кажого нового пода. 
переходим к провижионеру nfs который будет создавать PV динамически


общая инфо по поводу как работает nfs сервер:
по поводу доступов на  nfs сервер. как я понимаю там все просто.
на самом nfs сервере мы задаем то что некая папка /data (для примера) будет доступна 
на какомто то другом сервере 10.42.147.10

# cat /etc/exports  | grep -v '#'
/data    10.42.147.0/24(rw,sync,no_subtree_check,no_root_squash)


 поэтому с того другого сервера 10.42.147.10
может ЛЮБОЙ юзер конектиться на nfs сервер и маунтить эту папку. повторю любой
юзер. и никакой аутентификации делать ненужно.
маунтится nfs папка очент просто 
на сервеерер  10.42.147.10 под любым юзером делаем

$ mount  nfs_ip:/data /mnt/1

в чем еще прикол nfs.
вот у нас расшарена папка /data
если мы сосздадим подпапку /data/02
то мы можем смонтировать эту подпапку.
поэтому провижионеру очень просто работать. ему чтобы создать PV надо просто в указаннрй
папке /data создать подпапку. а затем к8 очень  простотоже остается 
надо тоьлько примонтировать эту подпапку в под. полная халява с этиим nfs


| переходим к установке провижионера
ставим проивижионер руками

клоинируем
  https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner


проверяем что на к8 активирован RBAC
# kubectl api-versions | grep rbac
rbac.authorization.k8s.io/v1

это означает что rbac активирован.


далее они пишут что если юзер под которым мы ходим в к8 имеет неймспейс
отличный от default то надо  в rbac.yml
и в deployment.yaml замениить название неймспейса

# Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed
$ NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
$ NAMESPACE=${NS:-default}
$ sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
$ kubectl create -f deploy/rbac.yaml


в первой строке определяется неймспйес для нашего текущего к8 юзера
а потом этот нейспейс подставляется в файлы. 

что интересно первая команда показыает все контексты включая и текущий
помеченный зведочкой и также покаывает неймспейс

# kubectl config get-contexts
CURRENT   NAME                             CLUSTER         AUTHINFO           NAMESPACE
*         kubernetes-admin@cluster.local   cluster.local   kubernetes-admin   

а команда котоаря показывает чисто текущий контекст она непоказыавет неймспейс
# kubectl config current-context


итак заменили если нужно неймспейс в файлах. и накатили rbac.yml


далее заходим в deployment.yaml
и правим там IP nfs сервера и  название папки на nfs сервере которую мы расшарили на
nfs сервере. вдеплойменте это всего пара мест

например вот nfs ip =  10.42.147.49
а папка которая там расшарена это /data
тогда


          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 10.42.147.49
            - name: NFS_PATH
              value: /data


      volumes:
        - name: nfs-client-root
          nfs:
            server: 10.42.147.49
            path: /data


провижионер работает очень просто. он создает под. в него монтирует нашу nfs папку
 а когда надо создать новый PV то он просто в своем поде внутри этой папки создает 
 подпапку вот и готов новый PV.


вот такой еще интересный важный момент. оказывается что внутрь пода можно примонтировать
вольюм без использования PVC. это собсвтенно видно из манифеста деплоймента этого проивижионера. также я нашел пример и в разделе volumes на к8.

вот пример из доков к8. сразу для одиночоного пода

apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /my-nfs-data
      name: test-volume
  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com
      path: /my-nfs-volume
      readOnly: true


меня интересует вот этот кусок

  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com
      path: /my-nfs-volume
      readOnly: true


здесь вместо указания имени PVC идет описание подключения к nfs шаре.
вот для сравнения как выглядит блок volumes: через pvc

  volumes:
    - name: nginx-vol
      persistentVolumeClaim:
        claimName: pvc-01


 а вот подключение вольюма в деплойменте без PVC

       containers:
        - name: nfs-client-provisioner
          image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 10.42.147.49
            - name: NFS_PATH
              value: /data


      volumes:
        - name: nfs-client-root
          nfs:
            server: 10.42.147.49
            path: /data


таким оразом можно нахер послать PVC как случае stand alone пода
так и в случае шаблона пода в деплойменте.
это очень интересно.


итак мы отредактировали текст маифеста деплоя. теперь накатыаем его

# kubectl apply -f deployment.yaml

проверим что в поде от этого деплоя произошел маунт nfs
опредяет на какой ноде крутится под этого деплоя

# kubectl get pod -o wide  | grep -E "READY|nfs" 
NAME                                      READY   STATUS    RESTARTS       AGE     IP              NODE       NOMINATED NODE   READINESS GATES
nfs-client-provisioner-7d8d7f5fc4-8495m   1/1     Running   0              2m24s   10.233.95.99    nl-k8-02   <none>           <none>


определяем uid этого пода
# kubectl get pod -o yaml nfs-client-provisioner-7d8d7f5fc4-8495m | grep uid
    uid: e44b9145-11a1-44d6-83cf-f7f43fe448f0
  uid: 93d41039-9fe2-41b3-ada3-ad6d55b7de08

значит это  uid: 93d41039-9fe2-41b3-ada3-ad6d55b7de08
идем на ту ноду и смотрим маунты

# mount | grep  nfs | grep 93d
10.42.147.49:/data on /var/lib/kubelet/pods/93d41039-9fe2-41b3-ada3-ad6d55b7de08/volumes/kubernetes.io~nfs/nfs-client-root type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.42.147.2,local_lock=none,addr=10.42.147.49)

и да. там есть этот маунт.

далее деплоим StorageClass

# kubectl apply -f class.yaml 
storageclass.storage.k8s.io/nfs-client created


типа все. провижионер развернут.

остатется создать PVC и под.


apiVersion: v1
kind: PersistentVolumeClaim

metadata:
  name: pvc-nfs-01

spec:
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  storageClassName: "nfs-client"
  resources:
    requests:
      storage: 1Gi



# kubectl apply -f pvc-nfs-01.yml 
persistentvolumeclaim/pvc-nfs-01 created


создаем под в котором используем этот PVC

# cat pod5.yml 
apiVersion: v1
kind: Pod

metadata:
  name: pod5

spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-vol

  volumes:
    - name: nginx-vol
      persistentVolumeClaim:
      claimName: "pvc-nfs-01"



# kubectl apply -f pod5.yml 
pod/pod5 created


делаем дескрайб пода и видим свойства его вольюмов

    Mounts:
      /usr/share/nginx/html from nginx-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7rk57 (ro)
Volumes:
  nginx-vol:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc-nfs-01
    ReadOnly:   false

тоесть все сработало.

посмотрим куда на nfs сервере в итоге сделан маунт.
смотрим свойства pvc

# kubectl get pvc pvc-nfs-01 -o yaml | grep volumeName
  volumeName: pvc-7806896a-cc2d-4576-a735-9a02e9de1e67

  смотрим какаие папки у нас нас на nfs сервере

/data/default-pvc-nfs-01-pvc-7806896a-cc2d-4576-a735-9a02e9de1e67/

вот такая папка создана внутри /data

проверим что именно она примонтирована в наш под

# kubectl get pod -o yaml pod5 | grep uid
  uid: f117b69f-d606-4a90-9cf0-70345d097ff1


 # mount | grep nfs | grep f117b69
10.42.147.49:/data/default-pvc-nfs-01-pvc-7806896a-cc2d-4576-a735-9a02e9de1e67 
on 
/var/lib/kubelet/pods/f117b69f-d606-4a90-9cf0-70345d097ff1/volumes/kubernetes.io~nfs/pvc-7806896a-cc2d-4576-a735-9a02e9de1e67 
type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.42.147.177,local_lock=none,addr=10.42.147.49)

так что в наш под примонтровна папка 
/data/default-pvc-nfs-01-pvc-7806896a-cc2d-4576-a735-9a02e9de1e67

что совпадает с /data/default-pvc-nfs-01-pvc-7806896a-cc2d-4576-a735-9a02e9de1e67/

кстати вот как разнится описание volume в поде
когда он заказан через PVC и когда закзан  без него 

Volumes:
  nginx-vol:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    10.42.147.49
    Path:      /data
    ReadOnly:  false



Volumes:
  nginx-vol:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pvc-01
    ReadOnly:   false
  

если стаить провижионер nfs не руками а через helm 
то это делатеся вот так

$ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
$ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
    --set nfs.server=10.42.147.49 \
    --set nfs.path=/data


