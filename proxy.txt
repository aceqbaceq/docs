| proxy
| socat
| nc

тема раскрывает как работает прокс

значит как юзер приложение (процесс) связывается с удаленным
компом ? 
юзер приложение делает с ядру запрос чтобы ядро создало tcp конект с удаленным
процессом. ядро это делает. а нашему процессу возвращает указатель (файл дескриптор)
в который мы можем писать байты. любые байты. мы суем в указатель байты а ядро их тогда
подхватыывает упаковыавает в TCP+IP+ETHERNET и сует в сеть. это все летит на удаленный комп
там обратный процесс и удаленный процесс получает ровно те байты котоыре мы сунули в
наш указатель.  итак с точки зрения нашего процесса запись в сеть это всего навсего 
запись байтов в указатель. вот так просто. и не более того. для процесса как бы сети и нет
нет всех этих сложных подробностей сетевых протоколов. для процесса это просто процесс
записи тех байтов которые процесс хочет передать удаленному процессу в указатель.
для процесса это все выглдяит вцелом ровно также как если бы процесс писал в указатель
за котррым сидит pipe ядра. тоесть если два процесса связать пайпом то для процессов
все будет вылядеть вцелом также. что в пайп писать что в сеть. это все остается прозрачно
невидимо с точки зрения как это выглядит и запаривает процесс.
при записи в файл на диске для процесса все вяглядит также. мы просим ядро открыть файл.
ядро его както там открывает. а процессу возвращает указатель. тогда процесс точно также 
сует байты в этот указатель а ядро их уже сует на диск. тоесть для процесса скрыты все подро
бности сети. процессу всего навсего нужно совать байты в указатель. а все остальные подробности
которые нужны для преедачи по сети делает ядро совершенно невиимо прозрачно для процесса.

итак у нас есть юзер программа. она в ос выглядит как процесс.  вобласти памяти ядра есть кусок
памяти в которой находятся разные структуры в которых ядро записывает разные хрени которые
ядро использует при обслуживании этого процесса. там же лежат файл дескрипторы этого процесса.
в /proc мы из юзер спейса можем посмотреть часть этих служебных переменных и параметров храня
щихся в памяти ядра про наш процесс. тамже можно увидеть номера всех файл дескрпиторов которые 
ядро создало для нашего процесса. можно увиедеть ряд подробностей что там кроется на бекенде
от этих файл дескрипторов. напрмиер можно увидеть что данный файл дескриптор ведет на файл
на физ диске. или ведет на сетевое соединение.  так вот lsof умеет вытаскивать прикольную
информацию о том что скрывается за файл дескриптором. 

у нас за каждоый файл дескриптом скрывается какйото бекенд - либо файл на диске, либо пайп (буфер в
ядре), либо сетевое соедиенние. 
соовтесвтенно как работает запись в указатель. юзер процесс запускает glibc функцию с параметрами ,
эта функция запускает сисколл. сисколл переключает цпу на исполненение ядерного кода. 
он запускается и он видит вот что - что юзер процесс его вызывал для того чтобы скопировать 
байты которые лежат в памяти начиная с такого то адреса в файл дескриптор. для ядра файл 
дескриптор это фуфло ширма прикрытие. ядро смотрит за что отвечает данный файл дескприптор.
например он указывает на сетевое соединение. тогда ядро берет байты из памяти. упакоывает их 
в TCP+IP+ETHER пакет и сует в сетевую карту. и оно вылетает в сеть. если за файл дескпритптором
кроется файл на диске то он эти байты сует на диск. 

итак если процессу нужно передать на удаленный процесс какойто поток байтов(пусть это будет
текст в формате HTTP) по сети. то процесс решает через какой стек протоколов он хочет передать
этот поток байтов. например мы решили что мы будем передавать через связку TCP+IPv4
то мы узнаем dest_IP и dest_port. далее наш процесс делает ряд вызоовов функций glibc в 
через которые мы обьясняем ядру что мы хотим чтобы ядро установило TCP конект с удаленным
компом который имеет IP=dest_IP и TCP_port=dest_port . ядро делает этот конект и в таблице
памяти нашего процесса создает +1 файл дескриптор номер которого он и возвращает нашему процессу.
теперь наш процесс можем используя glibc фнукции пихать любые байты в этот указаетель и тогда
ядро и прочие хрени доставят эти байты тому удаленному процессу который на удаленном компе 
"привязан" удаленным ядром к тому dest_IP+dest_port.

таким образом если я знаю что у меня процесс пишет данные в сеть по TCP то это значит что у него
есть файл дескриптор у которого в свойствах будет указаны параметры сетевого соединенеия 
котроое ядро установило с удаленой системой а именно 
src_IP+src_port и dest_IP+dest_port и это всегда можно посмотреть используя lsof
итак ядро обеспечивает нам конект. и выдает процессу ссылку. а процесс со своей стороны сует
в ссылку полезные данные. которое потом ядро через конект сует в сеть.
конект у нас определяется ip адресом и портом. адрес задает другой хост. порт задает другое
прилоожение на том хосте. ядро изготовлено так что оно привязывает конкретное прилоожоение
к порту tcp. 

предположим что с нашего компа-А мы хотим переслать пакет с dest_TCP_port=443 на комп-Е.
и у нас на файрволле нашего компа-А или какогтто компа-Б в сети на пути пакета к 
конечному компу-Е закрыт проход. зато файрволл разрешает пролетание пакетов например 
на порт 4444
и у нас есть  доступный комп-Д у которого если с него слать пакеты на dest_TCP_port=443
то такой tcp пакет пройдет успешно. значит было бы прикольно установить конект от
нашего компа-А до компа-Д чтобы мы стучались с нашего компа-А на IP-Д на порт 4444
а тот комп пересылал наши пакеты на комп-Е на 443

один из методов это можно на том транзитном компе-Д настроить iptables правила что если пакет
прилетает  с IP-А нашего компа то делать DNAT тоесть менять dest_IP c IP-Д на IP-E и менять
dest_port c 4444 на 443

эта хрень будет работать на уровне ядра компа-Д и никакие юзер процессы для этого на том
компе ненужны. 

однако минус в том что это негибко! программа на компе-А он сейчас хочет достучаться до компа-Е
а потом до компа-Ж а потом до компа-З и на компе-Д через iptables ненастроить так правила
чтобы можно как то было переключаться. тоесть нет ниакой гибокость чтобы выбирать налету
и налету менять иптейбл правила на какой комп переправлять пакеты от нашего компа. 
iptbales отлично подходит для устанолвения "статических" перенаправлений. 

поэтому придумали прокси. поэтому на сцену выходит новый инстурмеент прокси. 
если iptables это набор правил в ядре. и обрабатывает эти привила ядро то прокси это всегда
юзер программа. сетевой поток если мы говорим о прокси всегда обрабатываеся юезер программой
тоесть юзер процессом. юзер кодом. тоесть унас обязательно в списке процессов должен виесть
какойто процесс. который ообрабатывает сетевой поток. 
помимо процесса также был придуман новый проотокол. это протокол выше транспортного уровня.
они так придумали. поэтому он обрабатывается не ядром а исклюительно юзер спейс кодом. 
так повелось что протоколы уровня транспортного L4 и ниже они обычно вмонтрованы в ядро. 
а юзер процесс работает с ними через  функции glibc. условно говоря что мы решаем какой
транспортный l4 протокол мы хотим заюзать. вызываем socket() там указываем этот транспортный 
протокол и его параметры. ядро нам возвращает указатель. и дальше мы просто пихаем данные
полезные именно нашей программы. незаботясь  о подроностях протокола L4 и все что ниже ,так
как этим занимется ядро.  а протоколы уровня L5 и выше уже проблема юзер кода. юзер
программа должна в своем коде их сама реализовывать. так вот они решили что прокси протокол
это будет протоокол выше чем   L4.  наверно при желании прокси протокол можно было бы вкорячить
в само ядро. и юзер программа бы работала с ним через новые функции glibc. но они так не 
захотели. поэтому поток данные байты от прокси протокола их формиорвание и оббаботка это 
искючиетльно проблема юзер спейс программы. 
прокси протокол нам что дает - он нам дает то что мы берем нашу программу на компе-А которая
хочет достучаться до компа-Ж на порт 444 и мы звоним комп-Д где работает ююзер процесс
который понимает прокси протокол и мы туда обращаемся через прокси протокол и в нем мы
обьясняем или задаем тот комп-Ж его dest_IP и dest_port 443 на котрый мы хотим постучать.
и прокси программа на компе-Д нам пробросит наш трафик туда куда мы заказываем. 
если мы хотим попасть на комп-З то мы меняем параметры в прокси протоколе . формируем новый
прокси пакет. звоним на комп-Д. и он читает наш прокси пакет. и перенаправляет трафик легко
и непринужедденно на комп-З. таким образом в отичие от iptables мы можем используя прокси
протокол и прокси процесс легко и динамично выбирать тот конечнйы комп на который мы 
хотим обратиться. в этом и есть главная фишка придумки прокси протокола в отличие от инстурмента
iptables который  я привел вначале. вот ответ на вопрос нхрен был придуман прокси протокол
еслои у нас уже был iptables.
потом еще такой момент. вот нашей программе нужно куда то попасть. если юзать iptables то 
нужно получать рут доступ на тот комп-Д и там под рутом менять правила иптейблс.
если же мы юзаем прокси хрень то нам на компе-Д ненужен никакой доступ в плане к командной 
строке. мы через сеетевой поток через сетевой прокси протокол спокойно сами без посредников
задаем на какой комп мы хотим в итоге достучаться. мы управляем потоком сами прям с нашего компа.
а в случае иптейбл нужно когто просить причем постоянно. плюс все те минусы что я наверху
расписал.

ткперь более конекретно как работаем передача через прокси сервер.
у нас комп-А у него IP-А на нем крутится наше приложение процесс-А1 на удаленном компе-Д
крутится процесс-Д1 он сидит на биндинге IP-Д и TCP_port-Д=4444

наш процесс-А1 обращается к ядру устнвить TCP конект с IP-Д:4444
ядро устаналивает конект с параметрами пакетов сететвого тарфика который будет обслуживаться
этим конектом вот с такими параметрами

  локальный IP=IP-А   локальыйн TCP_port=50345
  удаленный IP=IP-Д   удаленный TCP порт = 4444

соовтсвтнненно в рамках данного TCP конекта наше ядро будет обарабывать пакеты с параметрами

  src_IP = IP-А  src_port = 50345     dst_IP = IP-Д  dst_port = 4444
либо 
  src_IP = IP-Д  src_port = 4444      dst_IP = IP-А  dst_port = 50345

и нашему процессу-А выдается указатель fd/4 
далее теперь наш процесс если он сует байты в этот указатель то ядро будет паковать 
эти байты в TCP+IP+ETHER пакет с параметрами

  src_IP = IP-А  src_port = 50345     dst_IP = IP-Д  dst_port = 4444

и высирать через сетвую карту в сеть.

а если из сети будет приетать пакет с паараметрами 

  src_IP = IP-Д  src_port = 4444      dst_IP = IP-А  dst_port = 50345

то ядро будет принимать этот пакет вскрыать его. вытаскивать содержимое и передавать
процессу

если мы посмотрим через lsof на свойства этого процесса-А1 и своства его файл дескриптора fd/3
там будет вот такое на экране написано


	TCP IP-А:50345->IP-Д:4444 (ESTABLISHED)

оно нам покзывает на каком TCP+IP биндиге сидит через этот указатель наш процесс
и на каком биндинге сидит удаленный процесс до которого это указатель позволяет достучаться 
таким образом мы наданный момент пока что только достучались до компа-Д 
до процесса-Д1

кстати это означает что на удаленном компе-Д тоже работает процесс. у него тоже 
есть указатель. и этот указатель покзывает тоже самое только наоброт

	TCP IP-Д:4444->IP-А:50345 (ESTABLISHED)



ИНТЕРЕСНАЯ ВСТАВКА=НАЧАЛО
яхочу этим сказать что если мы со совего компа инииицировали TCP конект то это значит
что обязательно где то там далеко на какомто компе обязательно живет еще один процесс
что на той стороне конекта обязательно сидит именно процесс. код исполняемый в юзер 
спейсе как и уменя. а не ядро. что tcp конект это всегда именно способ связать два процесса.
обязательно будет два процесса. в отичие от случая например когда мы делаем ICMP конект(кстаи
icmp он неюзает ни tcp ни udp. он просто сразу оборачивается снаружи IP пакетом). так
вот когда мы на нашем компе  с юзер процесса шлем icmp в сеть и нам прилетает ответ то 
на той стороне ниакой юзер процесс не принимает и не обрабатывает наш поток! в этом случае
на том стороне это делает ядро! то удаленное ядро! вот что важно понимать про tcp конект.
если он  у нас есть то с  обоих сторон обязательно есть два процесса. то хрени кртящиеся
в юзер спейсе. чьи то частные програмки.
тут есть нюанс - в конечном итоге это верно. на той стороне в конечном итоге точно сидит 
гдетто юзер проесс на том конце конекта. но между нами могут быть компы которые тоже участвуют
в передаче трафика и там вместо юзер процесса будет ядро. щас оббьсню. положим мы шлем 
пакет с нашего компа на dest_IP=1.2.3.4 tcp_dest_port=443
трафик прилетает на тот комп. а там его принимает ядро и там настроен iptables который
делает DNAT и он меняет dst_IP на какойто другой скажем IP=7.8.9.10
пакет от нас влетает в комп 1.2.3.4. его обрабатывает ядро через iptables без участия
юзер процессов. и пакет летит дальше на комп 7.8.9.10 и там уже его ждет юзер процесс.
поэтому да - на нашей стороне сидит запущеннй юзер процесс который явялется одним концом
тцп конекта и на 7.8.9.10 тоже сдит юзер процесс который принимает в конечном итоге наш пакет.
но между ними могут сидеть куча промежуточных компов с iptables которые пропускают 
через себя этот трафик и меняют TCP\IP параметры этих пакетов используя код ядра. у нас
конечно изначльный dest_ip был 1.2.3.4 и на нем нет юзер процесса. но где то потом в конце
цепочки юзер процесс обязательно есть получаетс просто необязательно что этот юзер процесс
сидит именно на том хосте который имеет ip=1.2.3.4 . вот это интеересный вывод про tcp 
конект. тоесть тцп конект всегда служит для связи двух процессов. их обязательно два. а
не один например. 

еще интеерснейший момент в том что мы привыкли смотреть текущие конекты на компе через 
скажем утилиту ss
а она в свою очередь смотрит это в /proc

так вот в /proc она это смотрит как я понимаю анализирую свойства файл дескрипторов 
у всех процессов на данный момент которые показаны в /proc

но фишка в том что на компе есть сетевые конекты которые никак не привязаны ни к одному
файл дескриптору.  а значит в /proc о них нет никкакой инфомрации! значит
утиита ss частично слепа!

речь идет например о tcp конектах которые транзитно редиректятся через наш комп.
тоесть в наш комп влетает сеетевой поток который был порожден на процессами нашего компа
и этот сетевой поток не предназначен для процессов нашего компа.  самый простой пример 
это ктото пингует наш комп. влетающий icmp поток он не будетпередан ни одному прцессу нашего
компа. поэтому не будет ни одного файл дескриптора где бы инфомрации об этом потоке была
была бы отображена. однако поток влетает. и он забираетв ядре всякие буферы куски памяти.
тоесть он напрягает систему. он составляет некий конект. но ss об этом потоке не покажет 
нихерна. это вот если мы будем со совего компа пиновать удаленный комп. то так как поток
породается одимиз процессов нашего компа. то будет файл дескрпиотор и ss покажет инфо 
об этом потоке. это да. 

другой пример такого невидимого потока это трафик который просто роутится на нашем компе 
тоесть влетает в одну сет карту и тут же вылетает через другую куда то дальше в сеть
без измнения IP\TCP парамтеров потока. хотя все таки и в таком потоке наше ядро будет
нагружаться потому что нужно  в этом потоке менять TTL у IP заголовка. а это циклы цпу. 
нагрузка на память. 

так вот увидеть полную карину всех конектов на компе можно через команду 

		# conntrack -L

для справки читаем 

        $ man conntrack

и тут я понял одну вещь - что команда ss она конечно же не лазиит в /proc и не парсит
файл декрипторы. это было бы охренено долго. 
щас я это докажу

	# strace -e openat ss -4n 2>&1| grep proc
	пусто

вместо ss юзает совершенно другой механмзм получения информации. а именно юзер процесс может
к ядру сделать сисколл чтобы создать сокет. но это не просто локальный UNIX_DOMAIN сокет
и не сетевой сокет. это AF_NETLINK сокет. это получается такой канал связи между ядром
и юзер процессом. тоесть часть информации о своей ядерной требухе ядро выставляет в /sys
и /proc папке но это же далеко не все. так вот ядро позволяет получит канал мостик 
связи между собой и юзер процессом. через спец сокет. 

# strace -e socket ss -4n | grep socket
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_SOCK_DIAG) = 3
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_SOCK_DIAG) = 3
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 4

вот каким образом ss получает инфомрацию о сетевых конектах!

также как в случае классичесокго сетевеого сокета первый параетр AF_NETLINK задает более общий
класс сокетов. а трретий парметр задает уже более конкртный вид сокета внутри класса.

описание этих типов сокета

	$ cat /usr/include/linux/netlink.h | grep NETLINK_SOCK_DIAG
	#define NETLINK_SOCK_DIAG	4	/* socket monitoring				*/

	$ cat /usr/include/linux/netlink.h | grep NETLINK_ROUTE
	#define NETLINK_ROUTE		0	/* Routing/device hook				*/

справка по этим сокетам
		
		 $ man  7 netlink

NETLINK_ROUTE
              Receives routing and link updates and may be used to modify the routing tables (both IPv4 and IPv6), IP addresses,  link  parameters,  neighbor
              setups, queueing disciplines, traffic classes, and packet classifiers (see rtnetlink(7)).

 NETLINK_SOCK_DIAG (since Linux 3.3)
              Query information about sockets of various protocol families from the kernel (see sock_diag(7)).

посмотрим man sock_diag
и там видим что 

		sock_diag - obtaining information about sockets

	   #include <sys/socket.h>
       #include <linux/sock_diag.h>
       #include <linux/unix_diag.h> /* for UNIX domain sockets */
       #include <linux/inet_diag.h> /* for IPv4 and IPv6 sockets */

       diag_socket = socket(AF_NETLINK, socket_type, NETLINK_SOCK_DIAG);

походу под sock_diag они понимают  NETLINK_SOCK_DIAG
и с помощью такого сокета можно из ядра зарпщиапть информацию о сокетах в ядре.
насколько я понял можно посмотреть инфо о двух больших классах сокетов это UNIX сокеты
и  IPV4\IPV6 сокеты.

как я уже писал в файле про СИ то сокет это не имеется ввиду парочка чисел IP:port
это биндинг. а сокет это стурктура данных в памяти ядра которую ядро создает для процесса
при исользвании глибс фнкции   

		socket()

в памяти ядра создаетая стркутара котоаря будет оббслуживать сетевой поток а процессу возвращается
дескиптор на эту струтуру. 

так вот ВАЖНО насколько я понял вот эта хрень

       diag_socket = socket(AF_NETLINK, socket_type, NETLINK_SOCK_DIAG);


она позволяет посмореть инфомрацию исключительно о сокетах которые юзер процессы создали в ядре.
но как  я уже говорил у нас есть например сквозоной трафик на хосте к которму либо прмиенятеся
SNAT\DNAT либо нет. но все равно такой трафик все равно обррабатывается(как минмиум в пакетах менятеся TTL и менятеся ETHER оболочка) . под это нужны струкотуры в памяти ядра так вот я вот
не знаю для таких потоков трафика создает ли само ядро для себя сокеты или нет. или это уже не
сокеты а какието друие струкрутуыры. 

в люом случае из моих эксприментов я дела вывод что для сквзного трафика (наприме унас на хосте
ест виртуаока и я с втуалки пингую гугл и ос стек хоста при этом работает как ротуер плюс к 
потому применяется SNAT). так вот для такого трафика я не вижу чтобы ss показал инфо что такойй
поток есть!

информацию о таком потоке покзывает утилита 

   # conntrack -L


я посмотрел чем она пользуется

# strace -e openat,socket conntrack -L
openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libmnl.so.0", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libnetfilter_conntrack.so.3", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libnfnetlink.so.0", O_RDONLY|O_CLOEXEC) = 3
socket(AF_NETLINK, SOCK_RAW, NETLINK_NETFILTER) = 3

соовтесвтенно она получается юзает тот же способ получения информации как AF_NETLINK
тоесть через спец сокет  с ядром. НО! эта прога юзает другой субкласс сокетов 

	NETLINK_NETFILTER


в man 7 netlink про него сказано

     NETLINK_NETFILTER (since Linux 2.6.14)
              Netfilter subsystem.

и не более того. но походу именно иза него conntrack показывает расшиеную инфомрацию
по когектам внутри ядра!

также  я даже без этой утилиты нашел на саомом деле инфо о конектах в /proc/net
там прям она разбита по типаом конектов

/proc/net/udp/tcmp/icmp/raw
также 
/proc/net/protocol
/proc/net/nf_conntrack

например то что выводит на экран команда conntrack это можно руками увидеть в nf_conntrack

таким образом какя понимаю ss она показывает инфо об сетевых сокетах тоесть об сущностях
которые сидят в памяти. но сетевой сокет это еще не сетевой поток. да ессли создан сетевой
сокет то в его свойствах есть сетевой поток. но как я понимаю сетевой сокет через socket() 
создается тлоько по запросу юзер процесса. а если у нас трафик транзитный в которомо не принимает участия юзер процесс то и сокет не создается а сетевой поток пакетов при этом все равно есть.
поэтому я бы предположил что ss покзывает сетевы потоки порожденные юзер процессами. 
но это не весь сетеовй поток проходящий через комп. и полный сетеовой поток надо смотреть
либо в /proc/net/... либо через conntrack

тут я еще узнал что у линукса есть сокеты типа packet. они предназначены чтобы юзер приложение
могло само форимровать все уровни сетевого пакета выше L2. тоесть мы пишем в такой сокет
и далее ОС надевает на наши байты сверху  L2 (ETHERNET) оболочку. а наша задача получается
сформирорвать в наших байтах все что выше тоесть L3,L4, ... L7
для сравнения AF_INET сокеты они уровня L4. тоесть юзер приложение должно само содавать
через байты которые оно пшет в сокет уровни L7-L7
в ss можно посмтреть такие сокеты через 

   # ss -o

а еще можно почитать man 7 packet

я так и непонял как через ss посмтреть статистике по icmp потокам которые мы сами
запустили с хоста!

а вот я нашел почему был придуман нетлинк:
	Why do the above features use netlink instead of system calls, ioctls or proc filesystems for communication between user and kernel worlds? It is a nontrivial task to add system calls, ioctls or proc files for new features; we risk polluting the kernel and damaging the stability of the system. 

тоесть вместо того чтоы придумывать кучу новых сисколлоов для получения статисткии придумали
просто один сокет. а уже через него будут херачит всякие статисткиики. нето чтобы супер
но навервно чуть получше. ведь эту стаистику все равно нужно будет создавать внутри ядра.
просто несколько другой метод получения доступа к ней. фрнтенд метод

а вот еше нашел:
	When you are writing a linux application that needs either kernel to userspace communications or userspace to kernel communications, the typical answer is to use ioctl and sockets.

я так и непонял раницу между сокетом нетлине и ioctl. какой из методов выглядит проще
сточки зорения испольования в приложении. в каком методе нужно вводить болше данных и мудить

нетлинк позволяет нетолько читать из ядра его требуху но наоброт - чтото менять внутри
ядра ! подкручивать там настройки.

значит статистика по icmp потокам можно руками посмтреть  в /proc/net/icmp (а не в raw)
выглядит она вот так 

 sl  local_address rem_address   st tx_queue rx_queue tr tm->when retrnsmt   uid  timeout inode ref pointer drops             
   19: 00000000:002E 00000000:0000 07 00000000:00000000 00:00000000 00000000  1000        0 14128362 2 0000000002d5d055 0      

странно при этом то что невозможно определить на какой IP мы посылаем пинги.
также важно сказать что в этом файле показан icmp трафик который обязательно на нашем компе
приходит на юзер приложение. тоесть трафик генерируется нашим юзер приложением.если
через комп идет транфизтный трафик icmp то здесь такой поток не отражается!

на счет команды conntrack я не могу грантировать точно но вроде бы в ядре есть две связанные
но вроде бы все такие разные системы одна из них это netfilter это ровно как раз те
самые правила которые можно редатировать через строчную утилиту iptables а вторая хрень
в ядре это conntrack. это подсистема которая занмается отслеживанием поток трафика. 
и согласно вот этой картинке 
	https://en.wikipedia.org/wiki/Netfilter#/media/File:Netfilter-packet-flow.svg
подсистема netfilter заглядывает в подсистему conntrack для принятия решений об потоке сетевом.
ищи на картинке черный овал  с надписью conntrack.
в простом смысле коннтрак  в ядре держит таблицу о сетевых соединениях. тоесть грубо 
говоря вот отправили мы первый TCP пакет с флагом SYN тогда коннтрак в своей таблице 
создает новую строчку и помечает что статус этого соедиенеия [NEW ] и возможно еще как [SYN-SENT]
прилетел нам ответ он там модифицирует статус этого соединения. 

в iptables комнаде можно указывать так называемые модули например модуль tcp.
модули это такие расширения которые нам позволяют все более изошренно анализировать 
и обрабатывать трафик. например 
вот например пример анализа тарфик на основе модуль -m udp

	-A FORWARD -i br+ -o wlp2s0 -p udp -m udp --dport 123 -j ACCEPT

у этого модуля есть доп опции
	
	-m udp --dport 123

инфорацию о модуле иптейблc можно найти в мануале 

	$ man  iptables-extensions

например там про модуль udp написано 

udp
       These extensions can be used if `--protocol udp' is specified. It provides  the  following
       options:

       [!] --source-port,--sport port[:port]
              Source  port or port range specification.  See the description of the --source-port
              option of the TCP extension for details.

       [!] --destination-port,--dport port[:port]
              Destination  port  or  port  range  specification.   See  the  description  of  the
              --destination-port option of the TCP extension for details.


так вот тамже есть и модуль conntrack
у эттого модуля есть доп опция

 [!] --ctstate statelist
              statelist  is  a  comma separated list of the connection states to match.  Possible
              states are listed below.

там же нарисано какие стейты есть у сетевого потока с точки зрения коннтрак

	   INVALID
              The packet is associated with no known connection.

       NEW    The packet has started a new connection or otherwise associated with  a  connection
              which has not seen packets in both directions.

       ESTABLISHED
              The  packet  is  associated  with  a  connection  which  has  seen  packets in both
              directions.

       RELATED
              The packet is starting a  new  connection,  but  is  associated  with  an  existing
              connection, such as an FTP data transfer or an ICMP error.

       UNTRACKED
              The  packet  is  not  tracked at all, which happens if you explicitly untrack it by
              using -j CT --notrack in the raw table.

       SNAT   A virtual state, matching if the original source address  differs  from  the  reply
              destination.

       DNAT   A  virtual  state,  matching  if  the  original  destination differs from the reply
              source.


вот пример такого конекта и его статуса с точки зрения коннтрак

	# conntrack -L | head -n 3
	tcp      6 136571 ESTABLISHED src=192.168.220.1 dst=1.1.1.1 sport=60916 dport=443 src=146.190.207.220 dst=192.168.220.1 sport=443 dport=60916 [ASSURED] mark=0 use=1

а вот кстати как видит коннтрак icmp поток

	# conntrack -L |   grep 8.8.8.8
	icmp     1 29 src=192.168.51.1 dst=8.8.8.8 type=8 code=0 id=48 src=8.8.8.8 dst=192.168.51.1 type=0 code=0 id=48 mark=0 use=1

понятное дело что у icmp нет нкиакого статуса конекта. статус конекта есть только в tcp
потоков

а вот самое известное правило модуля conntrack которое обычно можно найти в любой таблице 
правил iptables

	-A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

тость мы юзаем модуль иптейблс conntrack и его фичу ctstate
таким образом мы разрешаем проходить пакетам которые принадлежат TCP потокам у которых 
в таблице коннтрак статус потока с точки зрения коннтрак равен RELATED или ESTABLISHED

conntrackd это юзер демон - он нужен только для того если пытаться построить откзауо
стойчивый кластер на основе иптейбл. ибо он будет копировать таблицу конекшенов на другую
ноду - ровно также работает pfsense кластер
вот тут очень много полезного я узнал про иптейбл и коннтрак
	
	https://serverfault.com/questions/1030236/when-does-iptables-conntrack-module-track-states-of-packets

еще раз справка по модулям iptables

	https://manpages.ubuntu.com/manpages/oracular/en/man8/iptables-extensions.8.html


получается такой момент - если трафик инициируется на юзер процессе на нашем компе
или трафик снаружи прилетает на юзер процесс нашего компа то информация об этом сетевом
пакете будет отражена в ss. потому что трафик породит файл дескриптор который будет привязан
к процессу. если трафик транзитный никак не связанный с юзер процессом тоггда в ss небудет
о нем инфо. но в любом случае инфомрация об трафике этом будет отражена в conntrack таблице.
тоесть если заюзать команду conntrack то там инфо обо всем трафике на компе будет отображена


сcылки
https://github.com/mwarning/netlink-examples/blob/master/articles/Why_and_How_to_Use_Netlink_Socket.md



ИНТЕРЕСНАЯ ВСТАВКА=КОНЕЦ



вовзращаюсь к основному вопросу про прокси
мы составляем  в нашем процессе-А1 поток байтов  согласго формату прокси протокола
(назовем этот поток байтов которые сует процесс как пакет. пакет имеет формат прокси протокола) и суем их в указатель.

далее ядро берет наш прокси пакет оборачивает его в тцп пакет потом оборачивает 
в ип пакет потом обрачиваем в эзернет пакет. сует в сетвую карту и эта матрешка полетела в сеть.

прилетела на комп-Д . там ядро разобрало эту матрешку и в процесс-Д1 поступает только вкусное
содржимое а именно прокси пакет. все остальное это была лишь транспортная упаковка. 

так вот в этом прокси пакете будет написано на какой IP и на какой порт наш процесс-А1 
на самом деле хочет попасть. например нам там напсиано 

   IP = IP-Ж  TCP порт = 443

процесс-Д1 понимает формат пакетов прокси протокола поэтому он все понимает что от него хотят.
тогда процесс-Д1 обращается к ядру и создает тцп коннект к компу-Ж

на данный момент тогда мы имеет следующую ситуацию в плане коннектов:

   комп-А 
   процесс-А1 
      IP-А:45345 -> IP-Д:4444


   комп-Д
   процесс-Д1 
      IP-Д:4444 -> IP-А:45345


но также процесс-Д1 имеет и второй коннект

   комп-Д
   процесс-Д1 
      IP-Д:4444  -> IP-А:45345
      IP-Д:56345 -> IP-Ж:443


тоесть у процесса-А1 один сетевой сокет а у процесса-Д1 два сетевых сокета

	процесс-А1 <----------->  процесс-Д1 <---------------------> процесс-Ж1
IP-А:45345 -> IP-Д:4444       IP-Д:4444  -> IP-А:45345           IP-Ж:443 -> IP-Д:56345
                              IP-Д:56345 -> IP-Ж:443

так значит статья которая обьясняет как выглядит пакет в формате SOCKS4 вот она

 	  https://ftp.icm.edu.pl/packages/socks/socks4/SOCKS4.protocol
еще вот тут можно посмотреть
	 https://en.wikipedia.org/wiki/SOCKS


значит когда наш процесс шлет прокси пакет на комп-Д он шлет пакет в виде 

		+----+----+----+----+----+----+----+----+----+----+....+----+
		| VN | CD | DSTPORT |      DSTIP        | USERID       |NULL|
		+----+----+----+----+----+----+----+----+----+----+....+----+
     	   1    1      2              4           variable       1


цифры внизу это число байт - сколлько байт занимает то или иное поле в пакете.
VN - это номер сокс протокола. в текущем случае это версия 4
CD - это команда которую просим выполнить у прокси процесса. может быть всего две 
команды это CONNECT и BIND. конект это если мы хотим сделать исходящее соедиенеие 
например сходить на сайт. а бинд это если снаружи к нашему процессу ктот должен достучаться
через прокси стучась снаружи на прокси сервер. я эту хрень для простоты рассатривать щас
не буду. если мы хотим сделать конект то CD должен быть равен 1. 
DSTPORT - все понятно это TCP порт на который мы хотим чтобы прокси процесс достучался
DSTIP  - тоже все понятно
USERID - это id юзера под которым мы типа обращаемся на прокси процесс. 
как я понял в теории прокси процесс может обратсят к ident серверу (ident это такой протокол)
и проверить можно ли такому id оббрашаться на такой dest ip и dest port. 
в нашем случае  наш прокси процесс нихрена этого неделает. он разрешает всем id лезть на 
все хрени в интернете. вот такой пакет и посылает наш процесс-А1 к процессу-Д1
а именно


		+----+----+----+----+----+----+----+----+----+----+....+----+
		| 4  | 1  | 443     |      IP-Ж         | 345          |NULL|
		+----+----+----+----+----+----+----+----+----+----+....+----+
 
тоесть получается наш прокси процесс создает новый сокет к компу-Ж на основе 
инфо в этом прокси пакете. от этгого зависят параметры нового создваемого им сокета.
то бишь нового сетевого соедеинененеия. 

если бы мы использовали ipables на компе-Д для модификации сетвого трафика то непонятно
как от приложения передать ядру эту инфо о том какое правило нужно создать ядру для DNAT\SNAT
входящего потока от компа-А

если прокси приложение успешно создает сокет до компа-Ж то оно обратно нашему процессу-А1
шлет прокси пакет вот такого формата


        +----+----+----+----+----+----+----+----+   
		| VN | CD | DSTPORT |      DSTIP        |
		+----+----+----+----+----+----+----+----+
     	   1    1      2              4

где цифры - это число байт в каждом поле.
в стстье что я привел написано что важно только первое поле. а остальные игнориуются
клиентом принимаюшим ответ. а первое поле в ответе содержит уже не номер сокс протокола 
как это было в запросе а содержит код возврата была ли операция успешна. а именно

VN:
    90: request granted
	91: request rejected or failed
	92: request rejected becasue SOCKS server cannot connect to
	    identd on the client
	93: request rejected because the client program and identd
	    report different user-ids


поэтому если все успешно то по идее должно вернуться VN=90

а дальше начинается самое интересное  - теперь наш процесс-А1 просто может пихать любые 
байты (тоесть уже требования к тому чтобы это были байты в формате прокси отпадает) в файл дескриптор который овтечает за сетевой конект до прокси процесса-Д1 , они в рамках тцп конекта
будут долетать до процесса-Д1 тоесть это будет выгляеть вот так

	процесс-А1 " {  [ (любые байты) ] TCP } IP }  ETHERNET " ---------> процесс-Д1

соотвесвтенно ядро на компе-Д вскрывает эту матрешку и в процесс-Д1 передает только пейлоад
от тцп тоесть (любые байты) и процесс-Д1 просто беерет эту мешанину байтов и ему плевать
соотвствуют ли они какотому формату или протоколу и просто тупо сует эти байты в свой второй
сетевой сокет который ведет на комп-Ж !

	процесс-Д1 " {  [ (любые байты) ] TCP } IP }  ETHERNET " ---------> процесс-Ж1


получается процесс-А1 на ушко шепчет процессу-Д1 полезный контент и говооит передай 
этот контент на комп-Ж. и процесс-Д1 берет переданный контент и уже через свой сокет сетевой
передает в комп-Ж
если попробовать осознать как это выглядит то выглядит это так что как бутто у нас байты
которые генерирует процесс-А1 на самом деле генерирует процесс-Д1 и от себя (через свой
сетевой сокет) уже шлет на конечный сервер-Ж на который бы хотел это послать процесс-А1.
это выглядит так что вася хочет позвонить маше но ему нельзя. тогда он звонит пете и говорит
позвони маше и скажи ей "ку". петя звонит и говорит "ку". маша ответчает пете "дза" и петя
звонит васе и говорит ему "дза"

прокси протокол очень простой. в общем то два прокси пакета нужно предать туда и обратно 
а потом уже можно преедаавать голые данные. также получается что легко можно менять на какой
комп мы хотим попасть в итоге. также классно то что прокси процессу в итоге глубоко плевать
в каком формате байты прилетают к нему от клиента. он их просто берет и пересылает (релей) на
конечный комп. получается сокс прокси хорош тем что он непривязывается к байтам от юзер
процесса. поэтому через прокси можно передавать и ssh и HTTP и HTTPS 
суть процесса проксирования получается в том что наш процесс через тцп коннект связывается
с прокси процессом. через простой прокси пакет обьясняет куда хочет попасть. прокси процесс
делает тцп конект с этим дестинейшн компом. для тцп конекта абсолютно все равно какие байты
в него пихают. он просто берет и их пересылает. у нас получается два тцп конекта. 
от клиента до прокси процесса. и от прокси процесса до конечного компа. для простоты скажу
что прокси протокол и прокси процесс исползуются для релея именно тцп конектов. то есть 
прокси процесс вместо нашего исходного процесса  устанавливает тот самый тцп коннект  который
неможет устанвить наш исходный процесс. тоесть ICMP+IP через прокси процеесс неполучится
запроксировать. мы проксируем только тцп конект от имени клиента. по крайней мере в SOCK4
а про пятый я нехочу разбирать. 
получается мой процесс хочет сделать тцп конект с конечным компом. но ему нельзя запрешено.
тогда он делает тцп конект с прокси компом и прокси процессом. и просит его сделать 
ровно тот самый тцп конект который не может сделать мой процесс до конечного компа. 
и в итоге мы имеем два тцп конекта. 

так как прокси протокол такой простой и такой мощный то написать прокси клиент и прокси
сервер очень просто. или например скажем что добавить в программе фукционал прокси клиента
очень просто!

есьт несклько протоколов проксирвания: SOCKS4, SOCKS5. потом яне очень понимаю вроде как 
протоколом проксирования может выступать HTTP протокол. но я это неразбирал. также вроде
как haproxy придумали свой протокол проксирования. а еще есть SOCKS4a, SOCKS5, SOCKS5h прото
колы. 
в чем фишка SOCKS5h - в том что вот нам надо в прокси пакете указать DEST_IP. а теперь
предтсавим что мы обращаемся с браузера на прокси процесс. значит мы хотим попасть на сайт
скажем google.com значит нам нужно в прокси пакете указать DEST_IP сервера накотором крутится
google.com , а для этого наш браузер должен как то САМ зарезолвить домен в IP адрес чтобы
повторюсь его указат в прокси пакете который он будет отсылать прокси процессу. 
ну получается для этого браузер (пусть даже это будет программа curl которая по факту веб клиент
считай что простой браузер) должен либо через NSS зарезолвить домен в айпи либо либо как то
сам это пытасят сделать например как это делает хром которйы резолвить сам через DOH.
так вот представим что мы нехотим чтобы процесс-клиент сам резволил доменные имена в айпи.
тогда можно попросить это сделать сам прокси процесс. и такой функционал ест как раз у 
протокола SOCKS5h
А есть более диковинные прокси процессы когда он не один а их скажем два. причем они 
сидят на разных хостах. таким примером является ssh. на своем компе можно запустить так 
ssh что он на ноутбуке запустить процесс который прокси клиент. мы с нашего процесса например
барузера обращааеимся на этот прокси процесс. а он через шифрорванный тонель пересылает наши
полезные данные не наконеный сервер куда мы хотим попасть а на другой процесс который куртися
на другом сервере. и уже тот процесс ииницирует тцп конект к тому компу к которому хочет
попасть наш исходый браузер.

тоесть вот обычная схема

наш комп               прокси сервер
наш процесс   ------>  прокси процесс --------> конечный сервер


вот более сложная схема как у ssh


наш комп               наш комп                 прокси сервер
наш процесс   ------>  прокси процесс --------> еще один прокси процесс   ------> конечныый сервер

такая схема используется потому что SOCKS протоколы они не шифруют трафик. ну скажем
нам и ненадо ибо мы хотим предавать TLS трафик котоырй уже шифрованный. но вспоминаем что
сам сокс протокол он же тоже требует отослать и принять два прокси пакета. и когда 
мы это делаем в рамках одного компа то оокей - их никто в сети не подсмотрит не прочитает.
а вот когда наш прокси звонит на второй прокси процесс он же тоже передает прокси пакет
незащищенный. и уже по сети. и поэтому нужно чтобы эта передача шла внутри шифрованого тонеля.
поэому между двумя прокси прцоессами натянут ssh тонель. чтоб никто не подсмотрел и непоменял
паратры  пакета в формате прокси протокола socks. 

плюс SOCKS прокси протокола в том что чтобы он заработал нам всего навсего нужно чтобы 
файрвол между нами и удаленным сервером где сидит прокси процесс позволял нам сделать 
TCP конект на IP того компа  на любой TCP порт. тоесть нам нужно чтобы наш файрволл ползволял
нам всего навсего сделать тцп конект до прокси сервера. а тцп конекты это вообще база всех
сетевых конектов. на какой нибудь тцп порт наш файрволл да должен разрешать конект на тот сервер.
тоесть сокс протокол хорош тем что он базируется на тцп. а тцп насктлко сильно используется во
многих конектах что файрволл должен разрешать конект тцп хотя бы на какойто порт на удаленном
компе. тоесть мы хорошо проникаем сквозь файрволл. тоесь запретить на файрволле все тцп содеинеия
это как с Земли откачать весь воздух. сразу все сдохнут. второй плюс SOCKS прокси протокла в том
что ему абсолютно плевать какой L5-L7 протокол (поток байтов) будет через него прокачиваться.
ему на это настолкьо же плевать насколько плевать тому же самому тцп протоколу. для прокси 
процесса это просто поток байтов в которых неужно разбираться. просто возьми эти байты 
из одного тцп конекта и вставь их в другой тцп конект и все. тоесть прокси протокол такой классный
что просто напросто прокси процесс вытаскиват байты из одной коробки и кладет в другую. 
и воще похер какой формат этих даных. 

SOCKS4 протокол хорош тем что он базируется на TCP. а так как тцп конекты лежат в основе
очень многих более высоких протоколов то чтобы с клиента на компе достучаться до прокси
процесса на прокси сервере достаочно чтобы наш файрволл разрешал исходяшее тцп содиениение
хотя  бы на какойто тцп порт. также как пишут заумными словами СОКС4 протокол он типа 
протокол агностик. говоря нормальным языком они имеют ввиду что если мы хотим через прокси
сервер передать любой протокол который лежит выше чем TCP то соскс протокол и сокс прокси
процесс это сделают отлично. потому что им абсолютно плевать какому формату подчиаяются
байты которые прилетают от клиента на прокси процесс через тцп конект. прокси процессу 
это вобще неважно. он прсто беерет эти байты и преереклвдывает из одного тцп пакета в другой
тцп пакет. СОКС4 протокол он подразумевает что мы с клиента стучим на прокси процесс через
именно TCP конект хотя если вот так подумать то ничего немешает чтобы клиент стучал на прокси
процесс через любой транспортный протокол надежный . и сокс4 протокол подарзуметвает что 
прокси процесс он потом когда достукивается до конечного сервера до делает он это тоже ТОЛЬКО
через тцп конект и в прокси пакете который прилетает от клиента укзыавется в графе ПОРТ именно
TCP порт конечного сервера на который нужно постучать. ИМЕННО TCP порт. а не какогото друогого
протокола порт.
прокси процесс принимает от клиента через тцп конект контент (байты) которые передаются 
внутри ТЦП пакетов.  вытаскивает эти байты. и ничего в них неменяя перекладыват их внутрь
других ТЦП пакетов которые он уже отправляет на конечнй сервер. это означает что байты внутри
ТЦП пакетов от клиента могут быть любые. поэтому клиент через СОКС4 прокси может отправтлять 
байты любого протокола который базируется на TCP а именно для прмиера это могут быть байты
формата(протокола) telnet, ftp, finger, whois, gopher, http, tls, ssh  

минусы СОКС4 - неумеет работать с UDP, нет аутентсциикации по паролю, нет шифрования.
поскольку работа с прокси процессом состоит из двух фаз. первая фаза это обмена пакетами
именно в формате ПРОКСИ. и они незашифрваны. то если прокси сервис находится на удаленном
компе то чтобы защитится от злоодея то нужно получается защщать конект до прокси сервера 
через ширфванный тонель. на вторйо фазе уже просто клиент передает свои "голые" данные
на прокси процесс. если эти данные это TLS то на этой фазе получается защита и ненужна потому
что защиту даст сам TLS хотя и он неидален потому что он нешифрует SNI. но на первой фазе защита шированием нужна по любому. 
нет аутентфиикации по паролю - хотя есть контроль доступа через IDENT протокол который поодходу
уже никто неислольует. 
неумеет работать по UDP. это значит что сокс4 протокол предпиывает сокс4 серверу чтобы он 
дозванивался до конечного сервера исклтительно через TCP конект. и больше никак. поэтому и 
через UDP тоже это запрещено делать. это значит что если мы будем  с клиента пихать байты 
в формате протоколов которые базируются на UDP например опенвпн то нас ждет провал. потому что
наш прокси сервис будет дозваниться до опенвпн сереваера исклчтительно по TCP конекту!
тоесть мы должны четко понимать что сокс4 сервис будет звонить на сервер который мы ему укажем
исключительно через TCP конект! а если нам надо позвонить на кончный сервер через UDP конект
то он этого делать НЕБУДЕТ! поэтому СОКС4 прокси сервис подходит только если нам надо достучаться
через него до удаленного сервера через TCP конект и передать внутри этого конекта контент
который базируется на TCP конекте. тоесть протокол который базируется на тцп конекте.
получается можно будет через прокси донести такие протоколы как DNS, IMAP, HTTP, TLS, NTP, POP, 
SMTP, SSH, Telnet. охренеть. круто.

SOKCS5 сервис умеет уже дозванится до удаленного сервера нетолько через TCP но и через UDP,
также умеет аутенцировать клиента через логин пароль. правда толку от этого если он не защищает
себя шифрованием. также SOCKS5h прокси сервис умеет по просьбе клиента делать dns резолвинг. делает он его опираясь на NSS подсистему.

HTTP прокси протокол - это на самом деле всего навсего кусочек от обычного протокола HTTP 1.1
а именно это всего навсего одна команда(в протоколе команды назвыаются диркетивы) в протоколе 1.1, 
а именно команда CONNECT тоесть в протоколе есть команды GET, PUT, POST итд. и вот есть команда
CONNECT. и схема получсется точнот такая же самая как в протоколе SOCKS. тоесть клиент должен
послать команду CONNECT в рамках HTTP запроса на прокси сервер. в формате вот таком
(показываю заголовок HTTP запроса)

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

этот HTTP запрос улетает на прокси сервер. в строке CONNECT мы указываем хост на который 
мы хотим досутчаться в кончном итоге и порт

значит про эту директиву CONNECT расписано вот здесь

  https://www.ietf.org/archive/id/draft-luotonen-web-proxy-tunneling-01.txt


для примепа я покажу как выгляит обычный GET запрос 

> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


я это привел к тому что послденнее поле в строке CONNECT стоит HTTP/1.1 спрашивается зачем
оно там? что оно обозначает? оно обьячняет веб серверу на который прибудет реквест что ээтот
реквест создан на базе протокола HTTP 1.1 тоесть формат входящего сообщения. и также это
намекает веб версерру на каком формате вебсервер должен вернуть ответ на запрос. 
вот смотри

  > CONNECT google.com:443 HTTP/1.1

  > GET / HTTP/1.1

то есть в обоих директивах мы указываем для веб севрера какой формат прилетевшего сообщения.
тоесть поле HTTP/1.1  оно не относится к параметрам GET или CONNECT а эта глобалная настройка
в каком формате ВЦЕЛОМ составлен полный текст запроса. я считаю что это деилиизм вставлять 
глобаяную настройку в частную строку. это путает.
я считаю что это додждно было бы быть лучше вот так

для гет запроса
> GET / 
> HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


для коннект запроса
> CONNECT google.com:443 
> HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

так вот на счет строчки 

  CONNECT google.com:443 HTTP/1.1

итак ее смысл. итак опция HTTP/1.1 это глобальная обьяснялка коорая говорит веб серверу
что весь запорос отформатирован согласно формату HTTP/1.1 
CONNECT google.com:443 означает к какому серверу и на какой порт нужно сделать TCP коннект веб
серверу (или прокси веб серверу) на который прилетел запрос. тоесть клиент просит прокси
сервер устанвитть тцп конект с серврером google.com на тцп порт 443
потворюсь пока что просто тцп конеект. о протоколах более выского уровня речи неидет

в частности из это вытекает что конект может быть нетолько на 443 порт. а на любой тцп порт
например 


  CONNECT google.com:1518 HTTP/1.1


и важно понимать что речь идет о том чтобы прокси сервер сделал всего навсего ТЦП конект. 
о том какой протокол вышего уровня будет передаваться чрез этот конект речи не идет. 
это неоговариваетя неограничивается! а опция HTTP/1.1 гоорит не о том что через порт 1518
пойдет HTTP поток . нет! опция HTTP/1.1 к соажению в этой строке всунута неудачно она неимеет
отношения к опции команды CONNECT. опция HTTP/1.1  сообщает что весь реквест целиком который
прислан на прокси веб сервер отформатрован на основе HTTP/1.1 формата и что клиент ожидает ответ
тоже в формате HTTP/1.1

далее прокси веб сервер делает тцп конект к google.com:443 и остлыает клиенту ответ 
в формате HTTP/1.1

    HTTP/1.1 200 OK  Connection established

ну или полный ответ

  HTTP/1.1 200 Connection established
  HTTP/1.1 200 Connection established

первая строка идет в хидере а вторая это тело соообщения в ответе
в овтете более умно сделано - формат ответа указан с самого начала ответа а не гдето
в конце первой строчки

итак еще раз мы с нашего клиента шлоем на прокси сервер запрос в формате HTTP протокола 
и просим чтобы прокси сервер устанвоил тцп конект с google.com:443

  > CONNECT google.com:443 HTTP/1.1
  > Host: google.com:443
  > User-Agent: curl/8.10.1
  > Proxy-Connection: Keep-Alive

прокси сервер принимает зазпорос. и устанвливает этот тцп конект. 
и нам в ответ сообщает что он это сделал

  < HTTP/1.1 200 Connection established
  HTTP/1.1 200 Connection established

а дальше происходит то что прокси сервер просто сидит ожидает что мы ему пришлем в тцп
пакете. он это вытаскивает и просто тупо кидает в тцп конект к google.com:443

тоесть HTTP прокси сервер ралтботает точно по такому же принципу как и SOCKS сервер. 
асболютно такой же прринцип работы!

и поэтому это значит что HTTP прокси сервер он может форвардить аболютно любой юзер протокол
который базиурется на базе TCP а не только HTTP\HTTPS протоколы!
тоесть я где то встречал что СОКС протокол может форвардить любые юзер протоколы базирующиеся
на ТЦП конектах а якобы HTTP прокси может форврдиь толко HTTP проткоол - ТАК ВОТ это полная 
хуйня! и SOCKS и HTTP прокси могут оба форваодить любые блять юзер протколы бащирующиеся
на TCP.

не вижу никких проблем чтобы такой HTTP прокси сервер форвардил нетолько HTTP поток
но и HTTP+TLS = HTTPS поток. 

прмиером HTTP прокси явялется программа 

  tinyproxy


и вот как можно через curl обраттся в интернте через HTTP прокси

  $ curl -x http://172.16.10.11:2080  -p --dump-header - -v  https://google.com


обращаю внимание что только при сипользовании обоих ключей и -x и -p у нас курл будет
делать реквест используя CONNECT! если ключ -p опустить то курл сгенериурет вот такой
запрос 

  $ curl -x http://172.16.10.11:2080   --dump-header - -v  https://google.com 
> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 Connection established
HTTP/1.1 200 Connection established

кхм.. странно  до этого без ключа -p директиа CONNECT неиспользовалось

на счет поля "Host:" я хотел пояснить. какой его смысл. в пртоколе 1.1 это обязательное поле
для каждого реквеста. здесь так написано

  https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html

и это поле ознаает вот что. вот мы достучались до веб севрера  и туда влеает HTTP текст.
так вот процесс веб сервера может обслуживать несколко сайтов в этом нет никаких проблем.
так вот заголовое "Host:" дебильное название потому что он ообозначает не хост а он обо
значает доменное имя сайта на который мы хотим досутчаться. скажем наш веб сервер процесс
обсивает сайты "vasya.com" и "petya.com" и вот влетает наш реквест и там написнао

    Host: petya.com

и процесс веб сервер понимает какой веб сайт нужно выдать в ответе!
так вот честно говортя в реевесте CONNECT

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive


у нас инфомрация на какой веб сайт мы ломися получается дублиуется ! с одной
стороны она указан в строке CONNECT а вдругой сторне она укаана в строке Host:
ненужная дупликация. хотя это не совсем так.
дело в том что то что указано в строке CONNECT оно используется для установления TCP конекта.
тоесть речи ни о каких HTTP сайтах нихуя не идет! имя сервера в строке CONNECT берется.
дальше оно чрез DNS резолвится в IP. и далее через этот IP и порт делается tcp коннект
а хосту! на этом хосте никаких веб сервров процессов может не быть в поимне! например на этом
хосте может быть SMTP или DNS или FTP сервер! поэтому доменное имя в строке CONNECT необозна
чает никакой веб сайт! он обозначает DNS имя уаленного сервера. который через DNS резволтся
в IP адрес. и с этим IP адресом уснваливет прокси ТЦП коннект!
а вот поле Host: испольщуется в том случае если мы делаем именно реквест к веб серверу
к веб сайту через соотсветеущие директиы например диерктиву GET. и тогда достучавшись до 
процесса веб сервера и передва ему GET реквест 


> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


то веб сервер процесс через поле Host: понимает про какой веб сайт который он обслуживает
из списка несклольких веб сайтоы мы хотим получить обратно ответ!

да! дело в том что сам ХОСТ сам сервер может иметь в системе DNS одно доменное имя.
например 

   cyber.com  A    1.2.3.4

и на этом серверер крутится веб сайт процесс и он соглласно найтрокам в жинкс
обслуживает два веб сайта
  
   vasya.com
   petya.com

у жинкса в это секции server{} опция servername

дапонятно что архитекрутно это дебильно сделано. что выгоднее в DNS сделать вот такие
две записи 

   vasyacom  A    1.2.3.4
   petya.com A    1.2.3.4


так удобнее но это необязательно. у нас доменное имя сервера может быть одним. 
а веб сайты(их доменные имена) которые на нем обслуживает жинкс могут быть совершенно другие.

тогда в веб клиенте мы как должны постуить. мы берем доменное имя срвера cyber.com
и через днс превращаем его в IP. далее мы создаем TCP конект с IP:443
а теперь чрез этот тцп конект мы суем в этот конект HTTP текст в виде

> GET / HTTP/1.1
> Host: vasya.com:443
> User-Agent: curl/8.10.1
> Accept: */*

и нет никаких проблем!

поэтому так как HTTP базируется часто но невсегда на TCP то нам вначале унжно создать тцп
конект. а уже потом через него кидать HTTP запрос. так вот это две большие разницы между
доменным именем сервера который имеет IP. и через этот IP (а значит и это дменное имя ) мы
вначале устанвливаем тцп конект.
а внутри серевера может  быть куча веб процессов кажый из которых обслуживает кучу разных
веб сайтов со своими доменами . и вот эти домены веб сайтов необязаны пересекаться с доменом
самого сервера. поэтому Host: относится к HTTP тексту который приетает на веб сервер процесс
и сообзает этому процессу покажи мне такойто сайт. а домен  в CONNECT он относится не к
HTTP тексту и не для веб серер процесса. он относисятся к той фазе когда мы создает тцп
конект.

тогда спршивается нахер в этом запросе вообще это поле Host:

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

ведь наш прокси сервер это не веб сервер. и он не хостит у себя никкакой веб сайт. 
ну как я понял из документа что привел выше что тупо в протоколе HTTP/1.1 обязательно 
в каждый реквест вставлять поле Host: и там где оно имеет смысл и где смысла в нем нет.

привожу еще раз описание смысла поля Host:

 The Host request-header field specifies the Internet host and port number of the resource being requested, as obtained from the original URI given by the user or referring resource (generally an HTTP URL,

as described in section 3.2.2). The Host field value MUST represent the naming authority of the origin server or gateway given by the original URL. This allows the origin server or gateway to differentiate between internally-ambiguous URLs, such as the root "/" URL of a server for multiple host names on a single IP address.

       Host = "Host" ":" host [ ":" port ] ; Section 3.2.2

A "host" without any trailing port information implies the default port for the service requested (e.g., "80" for an HTTP URL). For example, a request on the origin server for <http://www.w3.org/pub/WWW/> would properly include:

       GET /pub/WWW/ HTTP/1.1
       Host: www.w3.org

A client MUST include a Host header field in all HTTP/1.1 request messages . If the requested URI does not include an Internet host name for the service being requested, then the Host header field MUST be given with an empty value. An HTTP/1.1 proxy MUST ensure that any request message it forwards does contain an appropriate Host header field that identifies the service being requested by the proxy. All Internet-based HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code to any HTTP/1.1 request message which lacks a Host header field. 

тоесть когда мы достучались до веб сервер процесса то он обслуживает несклоько веб сайтов.
а у веб сайта есть доменное имя и есть дальнейший URI который обозначает путь к ресурсу.
так вот у разных веб сайтов может быть одинаковый путь к ресурсу а в запросе GET там укаызвается
именно путь к ресурсу. и из только строки GET непонятно от какго веб сайта этот путь относится.
и тут нам на помощь и приходит поле Host:
я так понимаю этот смысл.

забавно также что вот так деильше повелось что путь к ресурсу в строке GET он раньше совпдаал
с точн таким же путем к этому ресуру на файловой системе линукса. поэтому эти два понятия 
смешиватся. хотя они разные! это четко видно если веб сервер берет ресурсы через php.
потому что если мы возьмем путь к php файлу из GET то мы такой файл на файловой системе
не обнаружим! потому что это разыне вещи которые иногда совпдают.

немножко отойдем в сторону. чем оличается DNAT и прокси. вот расмотрим что мы с компа-А
из процесса пороидили сеетевой поток и он полетел по сети. если поток проходит через прокси
на другом компе-Б то он на прокси машине из сеетвой карты летит в ядро а оттуда 
доходит до юзер спейсе - в процесс в юзер спейсе тоесть поток из машины-А в итоге через ядро
на компе-Б влетает в юзер спейс. и там заканчивает свой путь в рамках одного и тогоже
тцп конекта между компом-А и компом-Б.  что значит заканчивает свой путь. это значит что содер
жимое ТЦП пакета вытаскивается , сам тцп пакет отправляется в мусор. а его контент передается
в процесс. тоесть тцп\ип пакет он полностью уничтожается на этом компе. толлько от него осатется
полезный контент который лежит в пейлоад зоне тцп пакета. а сам пакет уничтожается ядром.
если же сеетвой поток от машины-А влетает в роутер на компе-Б на котором происходит DNAT то в этом случае сетевой поток во первых он недоходит до юзер спейса. он крутится только в районе ядра
этого компа-Б. далее насколько я понимаю нетфильтр\иптейблс в ядре оно не уничтожает прилетевшие
тцп пакеты а просто меняет в них тцп порты и ип адреса в ip части пакета. хотя точно я незнаю
ядро уничтожает тцп\ип заголовки вытаскивает тцп пейлоад и перекладывается в новые тцп\ип 
упоковки или все таки оно только модифицирует имеющиеся тцп\ип заголовки. точно непонятно.
но далее эта хрень сидящая в ядре выплеывается дальше в сеть. повторюсь все обработку делает
толкьо ядро. юзер спейс никак не участвует. правда я не знаб как это вынлядит со стороны таблица
коннтрак. там два потока сетвых в итоге записывается - влетающий и вылетающий или один?
я проверил - поток получается один. с точки зрения записей в таблице коннтрак.
дело в том что коннтрак таблица заполняется на определенном этапе проходждени иптейбл цепочек(ищи картинки в интернете). так вот что я имею. яимею виртуалку у нее IP=172.16.10.11
она сидит на ноутбуке за сетевым бриджем br0
я на виртуалке запускаю команду

    (виртуалка 172.16.10.11) $ nc -v   64.50.236.52 21

смотрю какая запись или записи для этого TCP конекта есть в коннтарк таблице внутри вртуалки

    (виртуалка 172.16.10.11) # cat nf_conntrack | grep 64.50.236.52 | grep EST
    ipv4     2 tcp      6 431993 ESTABLISHED src=172.16.10.11 dst=64.50.236.52 sport=51582 dport=21 src=64.50.236.52 dst=172.16.10.11 sport=21 dport=51582 [ASSURED] mark=0 zone=0 use=2

получается мы имеем пакет 

    TCP  172.16.10.11:51582 -> 64.50.236.52:21

теперь  я  отслеживаю пакет на хосте на ноутбуке. ноутбук имеет карту wlp2s0 IP=192.168.51.1
(удивительный IP потом что гейтвей это 192.168.51.34)
смотрим через коннтрак

  (нотубук хост) $ sudo conntrack -L  | grep 64.50.236.52
  tcp      6 431956 ESTABLISHED src=172.16.10.11 dst=64.50.236.52 sport=51582 dport=21 src=64.50.236.52 dst=192.168.51.1 sport=21 dport=51582 [ASSURED] mark=0 use=1

тоесть пакет уже в стеке хоста выглядит до попадания на SNAT преобразование как

    172.16.10.11:51582 -> 64.50.236.52:21

и после SNAT преобразования как 

    192.168.51.1:51582 -> 64.50.236.52:21

но в любом случае при проходдении стека на хосте компа пакет до преобования NAT
и после преобразования с точки зрения коннтрак принадлежит одному потоку
видно что при преобразовании только меняется SRC IP. а src port неменяется как был 51582
так и остался. потому что этот порт не был занят на хосте. так зачем его лишний раз менять.

таким образо получается что транзитный SNAT поток через комп он во первых обрабвыатется
искочиетльно ядром и не попдаает в юзер спейс это первое отличие от прокси обработки потока.
второе отличие что  с точки зрения ядра и поток котоыйр влетает в комп и поток который из него
вылетает это один и тотже поток только слегка преобрвзанный.  а если поток летит через прокси
приложение то у нас  сточки зрения ядра будет два потока. один который влетает и заканвиается
на юзер процессе. а второй поток который новый он начинается на юзер процессе и улетает в 
интернет и заканчивтся на удаленном хосте. тоесть мы имеем два потока и в таблице коннтрак
будет две записи. в этих двух моментах отличие проксиования сетевого потока от его NAT-ирования.

а вот ксатти как этот поток выглядит ст очки зрения tcpdump. напомню что тисипидамп
ловит поток если исходящий то уже после обработки его НАТ правилами. а если входящий
поток то ДО обработки его НАТ правлами.

    (ноутбук 192.168.51.1) # tcpdump -i wlp2s0 -n tcp port 21
    192.168.51.1.51582 > 64.50.236.52.21

(ну это так чисто для српавлки я првиел)

таким образом я поииследовал чем отличается сетеовй поток котоый влетает  в комп и он 
проксируется на этом компе и высирается наружу либо он натируется на этом компе и высирвется
наружу. в чем разница такой обработки.

получется если поток проксируется то он прям "терминируется"  на юзер процессе(тоесть он умирает заканчивается на юзер процессе. и юзер процесс вытаскивает конеткнт из потока. и далее юзер процесс
создает новый поток и туда перекладаывает карго из певрого потока. и получается два тцп потока
у юзер процесса. и с точки зрения ядра тоже у нас два потока).
а если он натится то он до юзер процессе недолетает  а просто его ядро модифицирует
и высирает наружу и с точки зрения ядра и коннтрак у нас один поток. 

значит есть wireshark но это графическая прога и также пакета pacman в манжаро нет.
есть вместо эттго консольная утилита tshark и можно в ней записать трафик в файл вот так

    # sudo tshark -i eth0 -w test.pcap -F libpcap

правда проблема в том что если я буду трафик как то фильтровать то к сожалению
тогда его в файл прога откажется запмываться.

а после как мы записали трафик в файл то его можно загрузит в другую прогу она есть 
аналог wireshark но работает исклюичетьно сразу в терминале через псевдографику.
это прога

    termshark

в ней фильтры срабатывают точно такие как и в вайршарк

некая неочень хорошая инстрауция от нее 
    
    https://github.com/gcla/termshark/blob/master/docs/UserGuide.md#filtering

пример фильтроов от вайршарк

    https://wiki.wireshark.org/DisplayFilters

как я уже писал пакет tinyproxy который дает функционал HTTP прокси

статья про вайршарк и прочее в арч

  https://wiki.archlinux.org/title/Wireshark



значит вот я хочу потерстировать прокси через curl. для этого мне хочется удобно видеть 
какой трафик сам curl пихиает   в сетевой сокет. без всяких там вайршарков. и дейтсиельно
это можно удобно видеть если заюзать socat.
вот так запускаем socat

  $ sudo socat -v tcp-listen:8080,fork tcp:172.16.10.11:2080

или даже вот так чтобы только ipv4

  $ sudo socat -v tcp4-listen:8080,fork tcp:172.16.10.11:2080

опция fork нужна для того чтобы когда в него прилетает новый конект то он его 
обслуживал в рамках олдльеного нового прцесса. чобы если конект закроет клиент
то процесс погибает но родетлський процесс работает как ни в чем ни бывало. чтобы
его неперезапускать.

тогда он вешается слушать tcp на 0.0.0.0:8080  и работает как прозрачный прокси.
тоесть все что в него влетает через тцп конект имеется ввиу пейлоад от тцп пакетов он 
это вытаскивает и всатвляет в новый тцп пакет который связан с новым тцп конектом который
идет на   172.16.10.11:2080 где у меня работает SOCKS/HTTP прокси.

тоесть socat это реально тоже прокси только он нетребует от приложения разговаривать с ним
по протоколу прокси ему это ненадо потому что он в отличие от протоколов прокси делает
проксирование только на один фиксированный IP:port а так это реальное проксирование.
и плюс его в том что он все то что получает в себя он на экране печатает. а это супер то что
надо. таким макаром я вижу какой контент curl пихает в сетевой сокет.

далее я запускаб curl вот так

  $ curl -4 -x socks4://127.0.0.1:8080 -p   http://google.com 

либо вот так 

  $ curl -4 -x http://127.0.0.1:8080 -p   http://google.com 

тоест курл связыватся с socat а тот прозрачно передает тот что внутри тцп пакетов 
уже далее на прокси сервер используя новый тцп конект и на терминале где висит сокат
я вижу каакой контент протекает через него

ну неуоодобно то что когда курл получает ответ от веб сервера то он берет и сам закрывает
конект и прекращаеьт свою жизнь. чтобы сделать что курл висит бесконечно чтобы
можно было его поииследовать надо еше вот как сделать. 
надо запустить nc

  #  while 1; do nc -l -p 7777; done

он позволяет усатнвить с ним тцп конект и он его незакрывает. и висит так бесконечно долго.
тогда curl надо модифицирвать и запускать вот так

  $ curl -4 -x socks4://127.0.0.1:8080 -p  --noproxy "localhost"   http://google.com   http://localhost:7777

тоесть мы ему говорми что нужно ходить в сеть через прокси. но если доменное равно localhost
то прокси не юзай иди напрямую. далее мы просим его открыть две ссылки
вначале эту

  http://google.com

а потом вот эту 

  http://localhost:7777

первая будет открыта чере прокси. а вторая напрямую. и посколоку от втторого "вебсервера"
небудет ответа но тцп конкт будет то курл будет висеть и ждать ответа и незакрываться.
поэтому его удобно в это время исскледовать



кстати про формат строки в conntrack

  tcp      6 431994 ESTABLISHED src=172.16.10.11 dst=146.190.207.220 sport=33794 dport=443 src=146.190.207.220 dst=192.168.51.1 sport=443 dport=33794 [ASSURED] mark=0 use=1

как видно у нас две пары src+dest и src+dest так вот первая пара

  src=172.16.10.11 dst=146.190.207.220 sport=33794 dport=443

а вторая пара 

  src=146.190.207.220 dst=192.168.51.1 sport=443 dport=33794

так вот если соединенеие ИСХОДЯЩЕЕ то первая пара и src будет IP локальной сетевой карты.
а если оно ВХОДЯЩЕЕ то в первой паре src IP будет какогото компа из интернета.
поэтому в данном случае показано исходящее с компа соединение.
щас покажу на еще одном примере. я сижу на виртуалке и делаю входящее содениенеие по ssh
для виртуалки это входящее содениение а для хоста это исходящее посмотрим как выглядит
конект в двух случаях
смотрим на хосте. ищем сам конект

    #lsof -Pn -p 1515986   | grep TCP
ssh     1515986  TCP 172.16.10.1:56016->172.16.10.11:22 (ESTABLISHED)

ищем конект в conntrack
    # conntrack -L | grep 56016
tcp      6 431994 ESTABLISHED src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22 src=172.16.10.11 dst=172.16.10.1 sport=22 dport=56016 [ASSURED] mark=0 use=1

смотрим чему равно первая пара

    src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22

и вот реально это исходящее соединение потому что в src стоит IP локального компа 172.16.10.1
а в dst стоит IP удаленного компа 172.16.10.11

теперь смотрим как этот поток выглядит  изнутри вируталки

  # echo $$
  584

  # pstree -Asp 584
  systemd(1)---sshd(474)---sshd(560)---sshd(566)---bash(567)---sudo(582)---sudo(583)---bash(584)---pstree(641)

  # lsof -Pn -p 566 | grep TCP
  sshd    566 noroot    4u  IPv4              18466      0t0    TCP 172.16.10.11:22->172.16.10.1:56016 (ESTABLISHED)

  # conntrack -L | grep 56016
  tcp      6 431999 ESTABLISHED src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22 src=172.16.10.11 dst=172.16.10.1 sport=22 dport=56016 [ASSURED] mark=0 use=1

смотрим первую пару 

  src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22

и смотрим чему равен src 172.16.10.1 это ип адре удаленного компа. и это подтрвеждает что
это входящее содениениение. ксати порт 56016 такойже как порт на коннкете не хосте что
еще раз потдрвеждает что это тот же самый поток.




кстати еще полезный контент про прокси я написал в файле apt.txt ищи по "| proxy"




далее. из коннтрак можно увиетт какие конекты подвергаются NAT преобразованиям.
вот например 

# conntrack -L -n
tcp      6 431990 ESTABLISHED src=172.16.10.11 dst=146.190.207.220 sport=56308 dport=443 src=146.190.207.220 dst=192.168.51.1 sport=443 dport=56308 [ASSURED] mark=0 use=1

смотрим на первую пару 

  src=172.16.10.11 dst=146.190.207.220 sport=56308 dport=443

смотрим на вторую пару 

  src=146.190.207.220 dst=192.168.51.1 sport=443 dport=56308

так вот эти пары покывают вид пакетов котоырые в этом потоке влетают  и вылетают из
компа. если нат преоборазвания нет то они будутт зеркальны. а если есть то небудут зеркаьны.
что и видно.
значит мы видим что в наш комп пакет влетает в виде

  src=172.16.10.11 dst=146.190.207.220

потом он вылетает из компа 
а влетает обратно в наш комп в виде 

  src=146.190.207.220 dst=192.168.51.1

тоесть видно что кода комп из инета стреиться к нам обратно то он имеет dst=192.168.51.1
а не 172.16.10.11 . тоест имеет место NAT. в данном случае SNAT тоесть кода пакет вытает
из компа то у него меняяется src=172.16.10.11 на src=192.168.51.1 поэтому обратный пакет
или летит на 192.168.51.1



далее.
я походу понял смысл настроек в файрфоксе в плане разделения у них  в меню 
на HTTP прокси и HTTPS прокси. дело вот в чем. я пока незнаю на освое какого
документа но походу пьесы HTTP проткол позвоялет  создавать прокси двух видов.
один прокси он для https ссылок и работает через CONNECT. и для такого прокси процесса
запрос от клиента выгдяит как тцп конект через которы будут проукаычиваться байты. 
а что там внутри байтов непонятно. тоесть клиет подлкючается. говоритсоединись  с сайтом
vsya.com , мы дл клиента звоним на vasya.com по тцп. а далее через этот тцп тупо 
релейим форвардим те байты которые лиент шлет по тцп.  если мы даже подозреваем что 
внутри потока байтов както там скрыт http запрос наш прокси процесс никак не может
это разгдяеть потом учто например хттп запрс зашифрован через TLS. так вот минус 
такого прокси подхода в том что мы неможем на прокси кэшировать запросы от клиентов. 
потому что никак нельзя понять что за запросы шлет клиент. поэтому для таких https
запроссов юзается простой кондовый HTTP CONNECT прокси.
а вот если к нам приетает прям HTTP запрос напрокси то мы его можем кэшировать потому что
мы на прокси процессе видим какой HTTP запрос мы пересылаем. ну есствеенно что прокси длжен
понимать как выглдяит формат HTTP запросов. и тогда мы можем на прокси наачть кэшировать ответы
которые приетаюи из интета. и если клиент запрашивает то что у нас уже хранится в кеше 
мы можем без лазииния в инет ему отдать . например такой прокси умыйн это squid. 
поэтому для http запросов можно юзать отдеьный пркси сверер. вот пооэтому в файрвофокс и идет
разделение на http прокси и https прокси.  но это дебилная форулироваька. потому что оба 
этих прокси это HTTP прокси.  было бы правльне в в файрооксе в меню назвать так

   http urls via http proxy
   https urls via http proxy (more smart proxy with cache)

тоест форулироваь в файрфокс меню очень некоретная. 

а вот насчет можно ли через squid кэшировать TLS трафик
Unless a proxy is intercepting the HTTPS traffic (i.e. SSL bump) and thus gets access to the decrypted content, it cannot cache the traffic. When just being a non-intercepting HTTPS proxy squid will just build a tunnel to the final server whenever a client issues a CONNECT request and will forward all traffic without any changes. Even if the client would access the same resource again the proxy will not realize this since the traffic is encrypted differently, i.e. different encryption key and initialization vector. Similar it would not be possible to reply with a cached (encrypted) response since the encryption key used between client and final server would be different from the last access to this resource.

хотя можно все таки затерминировать TLS на сквиде используя SSL_BUMP фичу. 
читаю https://wiki.squid-cache.org/Features/SslBump
  Squid-in-the-middle decryption and encryption of straight CONNECT and transparently redirected SSL traffic, using configurable CA certificates. While decrypted, the traffic can be analyzed, blocked, or adapted using regular Squid features such as ICAP and eCAP.

как я понимаю сквид будет налету гениррвать фейковые сертфиикаты которые он удет подпсиывать
своим CA.  а далее уже сам от себя делать конект на внешние сайты. тут я вижу хрень в том
что ну типа браузеру я доверяю как он проверяет валидность сертфиктаа а вот как это будет 
делать сквид это еще вопрос и как он будет меня информиррвать об этом... 


далее 
HTTP прокси проктолол тоесть по факту CONNECT диркетива она позволяет нам проксировать нетолько
HTTP\S протокол но и любой протокол который базируется на TCP ровно также как это позволяет 
SOCKS протокол
например возьмем curl как пронрамму которая умеет делаь HTTP реквесты 
и в том числе курл умеет работать через HTTP прокси и также курл умеет рабоатть через FTP
протокол который как раз таки работает поверх TCP тоесть я хоу показть доказать что HTTP proxy
позволет проксировать нетлко HTTP\S но и другие TCP based протколы. главное чтобы клиент
который звонит на hTTP прокси умел с ними рабоать.
curl умеет ходиьт на ftp
щас тогда запускаем curl через HTTP прокси и чтобы курл зашел по ftp протоколку пропущенному
через прокси на фтп сервер

запускаем соккат чтобы видеть трафик высираемый курл хотя курл может нам его
и так показать через ключ -v

  # socat -v tcp-listen:8080,fork tcp:172.16.10.11:2080


запускаем curl 

  $ curl -4 -p  -x http://127.0.0.1:8080  ftp://ftp.slackware.com -v


хотя можно и просто вот так без сокат

  $ curl -4 -p  -x http://172.16.10.11:2080  ftp://ftp.slackware.com -v


вот что шлет curl на пркси сервер через HTTP протокол

  CONNECT ftp.slackware.com:21 HTTP/1.1
  Host: ftp.slackware.com:21
  User-Agent: curl/8.10.1
  Proxy-Connection: Keep-Alive

тоесть идет CONNECT запрос 

прокси срвер нам шлет ответ  тоже через HTTP протокол

  HTTP/1.1 200 Connection established

а дальше уже тупо клиент начинает на прокси сервер через тцп конект слать байты в формате
ftp прткола а прокси сервер тупо это переслыает на уденный фтп сервер 

220-\r
220----------------------------------------------------------------------------\r
220-                      R S Y N C . O S U O S L . O R G\r
220-                          Oregon State University\r
220-                              Open Source Lab\r
220-\r
220-       Unauthorized use is prohibited - violators will be prosecuted\r
220----------------------------------------------------------------------------\r
...
...
226 Directory send OK.
QUIT\r
...
...
221 Goodbye.



про ключ -p у курл  . ключ -p   заставялет курл когда мы ему указали юзать HTTP proxy через 
-x http://172.16.10.11:2080 посылать на HTTP прокси сервер именно CONNECT запрос.
потому что я еще не понял почему но если мы указали курл и другим веб клиентам юзать HTTP
прокси и открыает http:// урл то веб клиент вместо CONNECT  послылает GET запрос на прокси
сервер. да он отличатеся от обычного GET запроса но тем неменее! прокси сервер понимает
этот запрос и проксирует его. итак -x http://172.16.10.11:2080 без ключа -p породит
то что курл на прокси пошлет GET запрос. а если -p -x http://172.16.10.11:2080 то курл
пошлет на прокси CONNECT запрос.  ну и плюс такой схемы в том что : вот когда мы послыаем CONNECT
запрос на прокси серввер то он устнвлаие конект с удаелнеемы компом. далее он сообщает клиенту
что тцп конект установлен и только потом кллиент шллет прокси сревреру тот самый запрос на
уаленный срервер. а прокси он уже не анализирует что ему шлет клиент   а тупо эти байты преслыает
на удаленный срврер. если же вместо CONNECT клиент присылвает на прокси уже прям конечный запрос
на удаленный сервер то прокси сразу видит и понимает и конечный запрос клтента на удаленный
сервер и он его  уже от себя зпускает к тому удаленному сервреру и прокси ополуается что знает 
какой был запрос. и может сохранить ответ от сврере. и если потом от лиента прилетит тот же
самйы запрос то пркси саомжет клиенте отдать ответ из кэша. 

тоесть вот так отрабывает схема при 

  -p -x http://172.16.10.11:2080 

первый шаг. клиет шлет на прокси
  > CONNECT google.com:80 HTTP/1.1
  > Host: google.com:80
  > User-Agent: curl/8.10.1
  > Proxy-Connection: Keep-Alive

прокси делает пока просто тцп конект на удаленый сревер  google.com:80 
и клиенту возврашет ответ что тцп конект устанолвен  с гуглом

  <HTTP/1.1 200 Connection established

теперь на втором шаге клиент на прокси шлет свой рельный запрос
  > GET / HTTP/1.1
  > Host: google.com
  > User-Agent: curl/8.10.1
  > Accept: */*

прокси сревер уже не анализирует не парсит что мы шлем. он тупо эти байты
пересылет на удаленный сервер через тца конект.
веб суврер ответчает и прокси нам возварщает овтет

  < HTTP/1.1 301 Moved Permanently
  < Location: http://www.google.com/
  < Content-Type: text/html; charset=UTF-8
  ...
  ...


атеперь как работает схема без ключа -p 
запускаю я вот такой курл

    $ curl -x http://127.0.0.1:9999  http://example.ex:8080/index.html  -v

я ломлюсь на example.ex через прокси  127.0.0.1:9999 (прокси у меня это tinyproxy который
пишет логи в journald). еще раз замечу что ключ -p в курл отсуствует. значит веб клиент
тоесть курл будет обащаться на прокси сервер точно не через CONNECT директиву. а 
как то подругому
и вот что видно как курл звонит на прокси сервер за счет ключа -v

  > GET http://example.ex:8080/index.html HTTP/1.1
  > Host: example.ex:8080
  > User-Agent: curl/8.10.1
  > Accept: */*
  > Proxy-Connection: Keep-Alive


мы еще вернемся к анализу этого запроса
далее tinyproxy в journald фиксирует какой приелет в него запрос

  tinyproxy Request: GET http://example.ex:8080/index.html HTTP/1.1

что полностью совпадает что нам написал curl

далее домен example.ex  в /etc/hosts записано что имеет ip=127.0.0.1 так что тинипрокси
направит свой запрос на 127.0.0.1:8080 но там сидит не веб сервер а сокат вот такой 

    # socat -v tcp4-listen:8080,fork tcp:127.0.0.1:8888


тоесть он слушает 127.0.0.1:8080 и прозрачно передает поток на 127.0.0.1:8888
а уже там у меня сидит жинкс
наличие между тинипрокси и жинксом соката дает то что мы увидим на его экране как выглядит 
трафик который тинипрокси направляет на веб сервер а сокат позволяет визуализировать этот трафик

так вот вот что покажет сокат

    GET / HTTP/1.1\r
    Host: example.ex:8080\r
    Connection: close\r
    Via: 1.1 tinyproxy (tinyproxy/1.11.2)\r
    User-Agent: curl/8.10.1\r
    Accept: */*\r


это тот трафик который тинипрокси из себя высирает

итак еще раз сравним трафик который веб клиент сует на HTTP прокси его самые 
главные строчки

    > GET http://example.ex:8080/index.html HTTP/1.1
    > Host: example.ex:8080

и трафик который тинипрокси шлет на веб сайт

    GET / HTTP/1.1
    Host: example.ex:8080

тоесть разница только в строке GET

так вот я прочитал про метод GET здесь

    https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html

там написано что в нем укзывается так назвыемый Request-URI
а что же такое Request-URI я прочитал вот тут

    https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html

так вот Request-URI это URI от ресурса. 
значит написано что если ури идет в форме аболютного(полного) пути то это всегда означает
что этот реквест предназначен для ПРОКСИ. тоесть если мой веб клиент хочет обратиться на 
веб сайт то он в своем запросе недолжен никогда юзать полный путь. полный путь это когда 
у нас в запросе указан домен. если клиент в запросе формирует полный путь то это значит
что наш клиент собирается сделать свой запрос направить именно на прокси сервер а не на
конечный сервер.  это что касается отправик. если у нас есть процесс и он получил веб 
запрос с полным путем то это значит что тот кто ему этот запрос направил думает что 
наш процесс это прокси. HTTP прокси. и процесс если он реально прокси должен нарпавить
этот реквест на таргет сервер. либо выдать ответ из своего кэша. так написано в хттп стандарте.
также хттп прокси имеет право направить запрос на другой прокси сервер кстати!
также написано что если запрос поступил на веб сервер то веб сервер обязан принять этот 
запрос единственное что я не понял что наш веб сервер приняв такой запрос должен с ним делать.
хорошо если запрос реаьно хочет получить ответ с домена который наш веб сервер обслуживает 
а если наш веб сервер домен из запроса не обслуживает тогда что? надо проверять!

теперь становится понятно почему запрос от курл программы на хттп прокси выглядит так

    > GET http://example.ex:8080/index.html HTTP/1.1
    > Host: example.ex:8080


наш курл должен составить реквест на хттп прокси. вот он и составил. он указал полный 
аболсютный путь к ресурсу в URI. который только и используется в HTTP для того чтобы 
сформировать реквест к хттп прокси серверу! все верно!  реквест означает эй прокси сервер
доставь ка мне ресурс /index.html c веб сайта  example.ex
тут правда такая стремная тема. у нас есть сервер до которого надо по тцп достучаться.
сервер вобще то нам нужен в форме IP. а если нам дали его в форме домена то мы резолвим 
его в IP. а потом добравшись до сервера церез tcp конект (который в себя включает IP) 
мы там уже сообщаем процессу название веб сайта на который мы звоним и адрес ресурса.
так вот доменное имя сервера и домен веб сайта вообще то необязаны совпадать. 
наскоько я понимаю доменное имя сервера задается в строке с GET  

    example.ex:8080

причем тамже указан и tcp порт. именно в этом месте мы узанем доменное имя именно сервера!
и тцп порт. и можем тогда устанвливаеть тцп конект. а имя веб сайта мы узнаем из строки 

    Host: example.ex:8080

правда зачем в этой строке тцп порт непонятно. ибо инфомрация из этой строки она передается
уже внутрь процесса. уже выбор порта позади. ну да ладно.
в нашем конкетно случае доменное имя сервера и домен веб сайта к счастью совпадают.
кстати согласно HTTP/1.1 поле все что ниже первой строчки это назыается header запроса.
тоесть в первой строке запрос. далее хидер. далее уже тело запроса.
так вот в хидере обязательно всегда должна быть строка 

      > Host: ...


итак еще раз смотрим на запрос который курл отослал на хттп прокси

  > GET http://example.ex:8080/index.html HTTP/1.1
  > Host: example.ex:8080



теперь понятно почему запрос так выглядит. потому что мы сказали курлу чтобы он послал запрос
http://example.ex:8080/index.html  ненапрмую а через прокси и не просто прокси а хттп прокси.
а в http стандарте написано что если мы хотим направить запрос на хттп прокси то для этого 
надо взять метод который  бы мы использовали для досутпа на конечный сервер например GET
и там использовать не относительный URI а абсолютный. 

    GET http://example.ex:8080/index.html HTTP/1.1

(HTTP 1.1 на конце обьясняет процессу который примет запрос что он прилетел  в формате HTTP/1.1)

и все такой запрос и является прокси запросом!  или форматом запроса через хттп прокси!

тоесть если мы знаем адрес хттп прокси и хотим через него получить ответ мы берем составляем
обычный HTTP запрос как бутоо мы его ооправяем на обычный веб сервер. 
а в конце когда запрос составили мы просто напросто  в нем добавляем имя сервера и порт 
в первой строке  в URI! и все запрос к http серверу через хттп прокси готов!

сооттсвенно наш прокси сервер этот запрос принимает и в логах journald об этом
отчитывается 

    GET http://example.ex:8080/index.html HTTP/1.1

а сам прокси сервер после  этого уже от себя запускает "обычный" хттп запрос на конечный
сервер


    GET / HTTP/1.1
    Host: example.ex:8080


как мы видим это уже "обычный" хттп запрос. ибо здесь в первой строке мы видим обычный
относительный URI=/


поэтму сумарно получается что HTTP протокол включает в себя прокси субпротокол так скажем.
тоесть в рамках хттп протокола можно делать прокси запросы. и делать это можно двумя
путями. первый путь это когда мы формируем обычный хттп запрос и в реквесте используем абсол
ютный URI. а второй путь это когда мы юзаем метод CONNECT
насколлко я понимаю если мы говорим HTTP прокси то он должен понимать оба формата запросов.
запросы через CONNECT они более универсальные. они позволяют проксировать через прокси любые
пртоколы базирующиеся на TCP. минус в том что роль прокси тольк в том что он создает тцп
конект до конечного сервера а что через этот тцп конект потом передается он не анализирует. 
он это рассматривает просто как поток байтов. если у нас в потоке байтов используется шифрование
тодаже для одинакового запроса из за шфирования поток байтовы каждй раз будет другой поэтому
при таком способе прокисрования у нас прокси сервер не может кэшировать запросы и ответы
потому что он просто нихрена их не понимает так скажем. еще раз подчеркну что такой способ
позволяет приложению проксировать (тоесть прогонять черз прокси срвер)любые протколы базирую
щтеся на TCP тоесть это и HTTP и HTTPS(HTTP over TLS) и SMTP и FTP (правда FTP испоьзует
несколько а именно два тцп конекта но по крайней мере общаться с фтп серером через дата канал).
а вот проксирование через обычные
HTTP запросы с ипользованием аболютных URI дает возможность проксировать только HTTP запросы.
тоесть такой метод подразумевает что хотя и через прокси но мы хотим всегда связаться
именно с веб сервером на той стороне. именно веб сервером и ничем другим. мы конечно можем
в запросе указать порт  за которым сидит скажем FTP сервис например 

    GET http://example.ex:21/index.html HTTP/1.1

но фишка в том что смысла в этом нет. потому что прокси наш создаст тцп конект 
вот сюда  example.ex:21
и потом через этот тцп конект передаст вот такой запрос


    GET / HTTP/1.1
    Host: example.ex:21

такой текст прилетит в фтп сервис и он просто не поймет что за хрень ему прислали

на счет того что можно ли HTTPS прогнать через хттп прокси через обычные HTTP запросы с абсолют
ным URI. ответ нет. потому что : что значит HTTPS это значит что на первом этапе клиент
и удаленный сервер уставливают TLS сеанс через тцп конект. грубо говоря договариваются каким
ключом симеетрчиного шифрвания будут через тцп конект обениваться шифрованной информацией.
а уже на втором шаге наша прога генериует HTTP запрос, потом его шифрует через TLS ключ и потом
уже пихает в тцп конект. так вот проблема в том что установка TLS конекта никак не может 
быть сделана через HTTP реквесты. ну никак! мы неможем послать реквест вида 


    > GET http://example.ex:8080/index.html HTTP/1.1
    > Host: example.ex:8080

тоесть причем здесь этот реквест и TLS сеанс. нипричем.

поэтому итого - с помощью этого метода можно проксировать только самые что ни на есть HTTP 
реквесты и больше никакие! тоесть через этот метод можно прогонять через прокси только HTTP
реквесты и все!

итак я разобрал два метода проксирования через HTTP протокол как протокол проксирования.
поэттому HTTP via TLS aka HTTPS у нас работает только через метод CONNECT.
у нас клиент шлет на прокси сервер HTTP реквест CONNECT


    > CONNECT google.com:443 HTTP/1.1
    > Host: google.com:443
    > User-Agent: curl/8.10.1
    > Proxy-Connection: Keep-Alive

прокси сервер открвает тцп конект  к   google.com:443
и шлет обратно к клиенту ответ о том что это получилось

  < HTTP/1.1 200 Connection established

после этого курл начинает в тцп конект кидать байты по протоколу tls на прокси сервер.
а тот эти байты тупо пререклывдает в тцп конект до веб сервера. 
сервер и клиент обмениваются tls байтами туда и сюда исольузу прокси серер тупо как тцп релей.
когда tls хрень заканчиватеся. то тогда курл уже генерирует обычный HTTP реквест

> GET /index.html HTTP/2
> Host: google.com
> User-Agent: curl/8.10.1
> Accept: */*


потом обоарчивает его в TLS шифрование. и этот поток байтов бросает в тцп конект к прокси.
а тот этот поток байтов прееклдывает в тцп конект до сервера где веб сайт.

вот так эта хрень работает.
ктстати я понял смысл ключа -p  в курл  
он предписывает курлу использовать метод CONNECT при обращении к хттп прокси даже в
случае когда я хочу проксировать HTTP протокол. потому что без этого ключа наш курл
при обрашении к хттп прокси бы использовал обычные хттп методы ( а не CONNECT) плюс абсолют
ные URI

тут надо еще раз подчеркнуть что у нас есть протокол проксирования через котоырй клиент
общается с прокси сервисом.  а есть протокол который мы хотим передавать через протокол
проксирования. вот это надо различать.  также надо понимать что если протокол проксирования HTTP
то в рамках этого протокола ест два метода проксирования. это тоже надо понимать

теперь становится понятно смысл меню в файрфокс где написано 

    HTTP  proxy:
    HTTPS proxy:
    SOCKS host:


первый и второй прокси означает на самом деле один и тот же тип прокси или что тоже самое
один и тотже протокол проксирования HTTP. разница в том что вторая строчка обозначает прокси
сервер к которому файрфокс будет обращатсья через HTTP метод CONNECT. а первая
строчка указывает хттп прокси сервер к которому будет идти обращение через другой метод
проксирования тоже через HTTP проктол но через другие http методы (GET, POST, DELETE) плюс
использовать абсолютные URI.  прикол в том что через первую строчку будут проксироваться только
HTTP запросы. тоесть урлы которые имеют вид http://... будут проксирваться через первую строку.
а урлы вида https:// или ftp:// или все другие урлы кроме урлов http:// будут проксироваться
через вторую строку. 

если я пропишу IP:port только в первой строчке то файрфокс сможет мне октрывать толкьо 
урлы вида http:// и больше никакие в том числе он не сможет открыать урлы вида https://

если я пропишу IP:port только в второй строке то он сможет открывать любые урлы.

если указать их оба то урлы вида http:// будет клиент октрыать через сервер в первой строке
а все остальыне урлы через сервер во второй строке

через третью строчку SOCKS можно открывать любы урлы.
не очень понятно какие строчки будет юзать клиент если я пропишу серверы для всех трех строчек
сразу например 



    HTTP  proxy: 1.2.3.4:1080
    HTTPS proxy: 2.4.5.6:2080
    SOCKS host:  8.9.1.1:3333

вот непоняно какой из этих сервров будет использоваться файрфоксом для конекта.



некокторые вопросы после темы:
>а можно как то curl перенаправить в nc а уже с него в интернет. чтобы через nc наблюдать
дамп того что высирает curl ?

прям такого сделать нельзя.  потому что как программа ходить в инет. она делает запрос
к ядру открыть тцп сокет. ядро возвраещает файл дескрпитор. потом прога в частности
курл пихает данные в этот дескпритор.  каждый раз когда мы идем на другой сайт
то прога открыает новое тцп соедиение а значит получает новый дескприптор по номеру.
ну получется надо чтобы наш nc както прехыватывал трафик котоырый прога пишет в этот 
дескриптор. при том что каждый раз ноер его другой. вот мы привыкли через pipe связывать
проги но это всего навсего дескприор 1 через ядро увызывается с дескрпитором 0 у другого
прцоесса. повторю что курл при конекте на удаленный сайт делает это через тцп конект а значит
для этого ядро выдывает курлу новый файл дескриптор с номером нам заранее неизвстным. поэтому
пайп не прокатит.короче невозможно фигня. 
выход другой. мы говорим проге чтобы она трафик свой пускала через прокси сервер 127.0.0.1:8888
на этот бинд мы сажаем nc или socat и говорим ему чтобы он входящий трафик перенправлял  а точнее
прозарчно проксровал на 127.0.0.1:1080 где уже сидит наш прокси сервер
тогда курл посылает сетевой поток всегда на 127.0.0.1:8888 там его принмиает nc\socat
и рисует на экране что там прилетело. и потом через новый тцп конект передеает контент 
который прилет от клиента по тцп в новый тцп конект до 127.0.0.1:1080 где конент принимает
прокси сервер.  а мы наэкране терминала где запущен nc\socat наблюдаем наш трафик который 
приложение пихает в свой файл дескриптор. тоесть тот поток который лежит выше TCP
вот строчка для этого через socat

  
    # socat -v tcp4-listen:8080,fork tcp:172.16.10.11:2080

сокат слушает порт tcp 8080 и то что получает оттуда вытаскивает пейлоаз тцп пакета
и переклывает в новый тцп пакет который летит в тцп конект к 172.16.10.11:2080 тоесть 
это такой tcp relay. или tcp forward. это аналогично DNAT через iptables. только иптейблс
у нас процессит исключительно ядро. а здесь мы еще юзер процесс привлекаем.  в принципе
запуск сокат требет прав рут поэтому по сути без разницы что добавит DNAT правило в иптейлбс
что запустить сокат. 
fork нужен для ттго что каждй входяший тцп конет будет обслуживаться клоном этого процесса
поэтому он может принимать все новые и нвоые входящие конекты. а закрытие конекта на клеинте
неубивает голвной процесс. удобно

через nc это делаетс вот так. а оказывается nc так делать не умеет. не умеет он делать 
tcp форвадинг. и для этого сокат подходит гораздо лучше

    https://unix.stackexchange.com/questions/293304/using-netcat-for-port-forwarding


предодположим у нас приложение не умеет разговариать на языке прокси протоколов.
нужно это приложение связать с удаленным сервером но напрямую не получится а можно
только через прокси сервер. но как это сделать если приложение не умеет говорить на языке
прокси протоколов. есть прогрммы соксификаторы. они принимают конект как тцп. а потом
сами свызываются с прокси сервером разгоывариют с ним на прокси пртколе.  а потом от клиента
просто пересыалают его байты. так вот socat умеет быть соксификатором. смотри справку.
также есть прграмма redsocks я ее описал в какомто другом месте . она тоже умеет.


> fireofox у него настройки есть HTTP/HTTPS прокси.  - а какая разница?
на это я уже отвеил



> как ssh юзает nc для доступа к серверу через прокси?
как работает что делает nc  в простом обьясеннии.
по дефолту она делает тцп конект к удаленному серверу. а далее вот что. 
если мы чтото пришлем на STDIN этой проги то она это перешлет через тцп конект
удаленному процессу. если удаленнй процесс чтото нам пришлет по тцп конекту то прога
нам это вывдедет на терминал. вот так просто!

  $ nc 127.0.0.1 8888  
GET / HTTP/1.1 <====
Host: localhost <====

HTTP/1.1 301 Moved Permanently
Server: nginx/1.26.2
Date: Sat, 09 Nov 2024 17:06:05 GMT
Content-Type: text/html
Content-Length: 169
Connection: keep-alive
Location: https://vasya.com

<html>
<head><title>301 Moved Permanently</title></head>
<body>
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx/1.26.2</center>
</body>
</html>


стрелочками я показал какие команды я ввел с клавы на STDIN этого процесса.
он их переслал на дуаленный сервер через тцп конект.
и далее вывод на терминал того что приллслалв ответ дуаленный сервер

тоеть еще раз я запускаю nc указывают ему IP +port он до того сервера делает тцп конект.
далее то что я ввожу в терминале тоесть в его fd/0 онже STDIN эту инфо програ передает 
в тцп конект на удаленнй процесс. а то что даленнй процесс через тцп шлет обратно то 
наша програ выдает на экран

ну кончно можено через пайп передать на STDIN в nc то что мы хотим пераедать на удалный 
сервер


$ echo -e "GET / HTTP/1.1\nHost: localhost\n\n\n" | nc 127.0.0.1 8888 | head -n3 
HTTP/1.1 301 Moved Permanently
Server: nginx/1.26.2
Date: Sat, 09 Nov 2024 17:15:27 GMT


теперь о физическом смысле nc (неткат) - когда мы юзаем cat мы читаем 
 файлы с диска. а nc нам позволяет "читать"  
не  файлы на диска а из тцп конекта IP:port
тоесть это типа как tcpcat по смыслу. плюс помимо читать еще можно и писать

спрашивается нахрен эта программа нужна. какой от нее практичккий смысл. 
ну получается что еще раз работает это так - то что мы этой программе кидаем на STDIN
байты и программа их перекидывает через тцп конект на удаленный процесс. например мы указали
nc чтобы она связалась с  1.2.3.4:21 тоесть это FTP сервер . далее мы кидаем "5678" на
STDIN (через терминал или через pipe) и программа эти байты перебрасывает на  1.2.3.4:21
а то что удаленный тцп сервис обратно присылает через тцп конект на nc то оно нам выдает
на STDOUT 
ну а вопрос   а зачем мне "в ручном" режиме через терминал "разговаривать" с тцп сервисом?
обычная программа разговаривет через тцп с удаленным сервисом через свой код. а теперь
мы разговариваем с удаленным сервисом через тцп "руками". так нахрен нам это надо ?

один из случаев когда эта прогармма нам может помочь в чем то практическом -  





у жжнкса один воркер может иметь неолкко открытых тцп конектов.
попробвать использовать жинкс как хттп прокси
оказывается почемуто можно помлать прсто HTTP запрос на прокси типа GET и он зафоровадит
наш запрос дальше. это рабоатет только для HTTP запроса не HTTPS
окзывается еще прокси бывают такие что у нас между клиентом и прокси связь идет
через HTTPS!

как жинкс можно использовать как прокси сервер?

как происходит преерключение с HTTP/1.1 на HTTP/2 и самое интересное на HTTP/3
и в свете этого QUIC через xray может работать ?


выше я распиал примеры прокси протоколов. теперь надо скзаать названия программ которые умеют
работать с прокси протоколами. это :  ssh, squid, dante, haproxy, nginx(это неточно)


реверс прокси и форвард прокси ???

кстати при TCP конекте мы через 3 пакетный хендшейк мы убеждаемся что на той стороне реально
есть ктото живой! в отличие от UDP когда мы нихрена не знаем есть ли  там ктот живой. мы
просто отослали и хер знает живой там ктото или нет
 
netcat (nc)    vs   socat?


поробвать все таки squid попровать потавить с фичей ssl_bump ?

как вот так сделатть чтоы создать неймспейс сетвой привзяь его к tap2 даеть ему IP и
чтобы он был за br0
и на его базе запустит процесс sing-box. тоесть чтобы без виртуалки?


ссылки:
https://en.wikipedia.org/wiki/SOCKS

