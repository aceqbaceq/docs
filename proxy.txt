| proxy
| socat
| nc
| curl
| HTTP


тема раскрывает как работает прокси

значит как юзер приложение (процесс) связывается с удаленным
компом ? 
юзер приложение делает с ядру запрос чтобы ядро создало tcp конект с удаленным
процессом. ядро это делает. а нашему процессу возвращает указатель (файл дескриптор)
в который мы можем писать байты. любые байты. мы суем в указатель байты а ядро их тогда
подхватыывает упаковыавает в TCP+IP+ETHERNET и сует в сеть. это все летит на удаленный комп
там обратный процесс и удаленный процесс получает ровно те байты котоыре мы сунули в
наш указатель.  итак с точки зрения нашего процесса запись в сеть это всего навсего 
запись байтов в указатель. вот так просто. и не более того. для процесса как бы сети и нет
нет всех этих сложных подробностей сетевых протоколов. для процесса это просто процесс
записи тех байтов которые процесс хочет передать удаленному процессу в указатель.
для процесса это все выглдяит вцелом ровно также как если бы процесс писал в указатель
за котррым сидит pipe ядра. тоесть если два процесса связать пайпом то для процессов
все будет вылядеть вцелом также. что в пайп писать что в сеть. это все остается прозрачно
невидимо с точки зрения как это выглядит и запаривает процесс.
при записи в файл на диске для процесса все вяглядит также. мы просим ядро открыть файл.
ядро его както там открывает. а процессу возвращает указатель. тогда процесс точно также 
сует байты в этот указатель а ядро их уже сует на диск. тоесть для процесса скрыты все подро
бности сети. процессу всего навсего нужно совать байты в указатель. а все остальные подробности
которые нужны для преедачи по сети делает ядро совершенно невиимо прозрачно для процесса.

итак у нас есть юзер программа. она в ос выглядит как процесс.  вобласти памяти ядра есть кусок
памяти в которой находятся разные структуры в которых ядро записывает разные хрени которые
ядро использует при обслуживании этого процесса. там же лежат файл дескрипторы этого процесса.
в /proc мы из юзер спейса можем посмотреть часть этих служебных переменных и параметров храня
щихся в памяти ядра про наш процесс. тамже можно увидеть номера всех файл дескрпиторов которые 
ядро создало для нашего процесса. можно увиедеть ряд подробностей что там кроется на бекенде
от этих файл дескрипторов. напрмиер можно увидеть что данный файл дескриптор ведет на файл
на физ диске. или ведет на сетевое соединение.  так вот lsof умеет вытаскивать прикольную
информацию о том что скрывается за файл дескриптором. 

у нас за каждоый файл дескриптом скрывается какйото бекенд - либо файл на диске, либо пайп (буфер в
ядре), либо сетевое соедиенние. 
соовтесвтенно как работает запись в указатель. юзер процесс запускает glibc функцию с параметрами ,
эта функция запускает сисколл. сисколл переключает цпу на исполненение ядерного кода. 
он запускается и он видит вот что - что юзер процесс его вызывал для того чтобы скопировать 
байты которые лежат в памяти начиная с такого то адреса в файл дескриптор. для ядра файл 
дескриптор это фуфло ширма прикрытие. ядро смотрит за что отвечает данный файл дескприптор.
например он указывает на сетевое соединение. тогда ядро берет байты из памяти. упакоывает их 
в TCP+IP+ETHER пакет и сует в сетевую карту. и оно вылетает в сеть. если за файл дескпритптором
кроется файл на диске то он эти байты сует на диск. 

итак если процессу нужно передать на удаленный процесс какойто поток байтов(пусть это будет
текст в формате HTTP) по сети. то процесс решает через какой стек протоколов он хочет передать
этот поток байтов. например мы решили что мы будем передавать через связку TCP+IPv4
то мы узнаем dest_IP и dest_port. далее наш процесс делает ряд вызоовов функций glibc в 
через которые мы обьясняем ядру что мы хотим чтобы ядро установило TCP конект с удаленным
компом который имеет IP=dest_IP и TCP_port=dest_port . ядро делает этот конект и в таблице
памяти нашего процесса создает +1 файл дескриптор номер которого он и возвращает нашему процессу.
теперь наш процесс можем используя glibc фнукции пихать любые байты в этот указаетель и тогда
ядро и прочие хрени доставят эти байты тому удаленному процессу который на удаленном компе 
"привязан" удаленным ядром к тому dest_IP+dest_port.

таким образом если я знаю что у меня процесс пишет данные в сеть по TCP то это значит что у него
есть файл дескриптор у которого в свойствах будет указаны параметры сетевого соединенеия 
котроое ядро установило с удаленой системой а именно 
src_IP+src_port и dest_IP+dest_port и это всегда можно посмотреть используя lsof
итак ядро обеспечивает нам конект. и выдает процессу ссылку. а процесс со своей стороны сует
в ссылку полезные данные. которое потом ядро через конект сует в сеть.
конект у нас определяется ip адресом и портом. адрес задает другой хост. порт задает другое
прилоожение на том хосте. ядро изготовлено так что оно привязывает конкретное прилоожоение
к порту tcp. 

предположим что с нашего компа-А мы хотим переслать пакет с dest_TCP_port=443 на комп-Е.
и у нас на файрволле нашего компа-А или какогтто компа-Б в сети на пути пакета к 
конечному компу-Е закрыт проход. зато файрволл разрешает пролетание пакетов например 
на порт 4444
и у нас есть  доступный комп-Д у которого если с него слать пакеты на dest_TCP_port=443
то такой tcp пакет пройдет успешно. значит было бы прикольно установить конект от
нашего компа-А до компа-Д чтобы мы стучались с нашего компа-А на IP-Д на порт 4444
а тот комп пересылал наши пакеты на комп-Е на 443

один из методов это можно на том транзитном компе-Д настроить iptables правила что если пакет
прилетает  с IP-А нашего компа то делать DNAT тоесть менять dest_IP c IP-Д на IP-E и менять
dest_port c 4444 на 443

эта хрень будет работать на уровне ядра компа-Д и никакие юзер процессы для этого на том
компе ненужны. 

однако минус в том что это негибко! программа на компе-А он сейчас хочет достучаться до компа-Е
а потом до компа-Ж а потом до компа-З и на компе-Д через iptables ненастроить так правила
чтобы можно как то было переключаться. тоесть нет ниакой гибокость чтобы выбирать налету
и налету менять иптейбл правила на какой комп переправлять пакеты от нашего компа. 
iptbales отлично подходит для устанолвения "статических" перенаправлений. 

поэтому придумали прокси. поэтому на сцену выходит новый инстурмеент прокси. 
если iptables это набор правил в ядре. и обрабатывает эти привила ядро то прокси это всегда
юзер программа. сетевой поток если мы говорим о прокси всегда обрабатываеся юезер программой
тоесть юзер процессом. юзер кодом. тоесть унас обязательно в списке процессов должен виесть
какойто процесс. который ообрабатывает сетевой поток. 
помимо процесса также был придуман новый проотокол. это протокол выше транспортного уровня.
они так придумали. поэтому он обрабатывается не ядром а исклюительно юзер спейс кодом. 
так повелось что протоколы уровня транспортного L4 и ниже они обычно вмонтрованы в ядро. 
а юзер процесс работает с ними через  функции glibc. условно говоря что мы решаем какой
транспортный l4 протокол мы хотим заюзать. вызываем socket() там указываем этот транспортный 
протокол и его параметры. ядро нам возвращает указатель. и дальше мы просто пихаем данные
полезные именно нашей программы. незаботясь  о подроностях протокола L4 и все что ниже ,так
как этим занимется ядро.  а протоколы уровня L5 и выше уже проблема юзер кода. юзер
программа должна в своем коде их сама реализовывать. так вот они решили что прокси протокол
это будет протоокол выше чем   L4.  наверно при желании прокси протокол можно было бы вкорячить
в само ядро. и юзер программа бы работала с ним через новые функции glibc. но они так не 
захотели. поэтому поток данные байты от прокси протокола их формиорвание и оббаботка это 
искючиетльно проблема юзер спейс программы. 
прокси протокол нам что дает - он нам дает то что мы берем нашу программу на компе-А которая
хочет достучаться до компа-Ж на порт 444 и мы звоним комп-Д где работает ююзер процесс
который понимает прокси протокол и мы туда обращаемся через прокси протокол и в нем мы
обьясняем или задаем тот комп-Ж его dest_IP и dest_port 443 на котрый мы хотим постучать.
и прокси программа на компе-Д нам пробросит наш трафик туда куда мы заказываем. 
если мы хотим попасть на комп-З то мы меняем параметры в прокси протоколе . формируем новый
прокси пакет. звоним на комп-Д. и он читает наш прокси пакет. и перенаправляет трафик легко
и непринужедденно на комп-З. таким образом в отичие от iptables мы можем используя прокси
протокол и прокси процесс легко и динамично выбирать тот конечнйы комп на который мы 
хотим обратиться. в этом и есть главная фишка придумки прокси протокола в отличие от инстурмента
iptables который  я привел вначале. вот ответ на вопрос нхрен был придуман прокси протокол
еслои у нас уже был iptables.
потом еще такой момент. вот нашей программе нужно куда то попасть. если юзать iptables то 
нужно получать рут доступ на тот комп-Д и там под рутом менять правила иптейблс.
если же мы юзаем прокси хрень то нам на компе-Д ненужен никакой доступ в плане к командной 
строке. мы через сеетевой поток через сетевой прокси протокол спокойно сами без посредников
задаем на какой комп мы хотим в итоге достучаться. мы управляем потоком сами прям с нашего компа.
а в случае иптейбл нужно когто просить причем постоянно. плюс все те минусы что я наверху
расписал.

ткперь более конекретно как работаем передача через прокси сервер.
у нас комп-А у него IP-А на нем крутится наше приложение процесс-А1 на удаленном компе-Д
крутится процесс-Д1 он сидит на биндинге IP-Д и TCP_port-Д=4444

наш процесс-А1 обращается к ядру устнвить TCP конект с IP-Д:4444
ядро устаналивает конект с параметрами пакетов сететвого тарфика который будет обслуживаться
этим конектом вот с такими параметрами

  локальный IP=IP-А   локальыйн TCP_port=50345
  удаленный IP=IP-Д   удаленный TCP порт = 4444

соовтсвтнненно в рамках данного TCP конекта наше ядро будет обарабывать пакеты с параметрами

  src_IP = IP-А  src_port = 50345     dst_IP = IP-Д  dst_port = 4444
либо 
  src_IP = IP-Д  src_port = 4444      dst_IP = IP-А  dst_port = 50345

и нашему процессу-А выдается указатель fd/4 
далее теперь наш процесс если он сует байты в этот указатель то ядро будет паковать 
эти байты в TCP+IP+ETHER пакет с параметрами

  src_IP = IP-А  src_port = 50345     dst_IP = IP-Д  dst_port = 4444

и высирать через сетвую карту в сеть.

а если из сети будет приетать пакет с паараметрами 

  src_IP = IP-Д  src_port = 4444      dst_IP = IP-А  dst_port = 50345

то ядро будет принимать этот пакет вскрыать его. вытаскивать содержимое и передавать
процессу

если мы посмотрим через lsof на свойства этого процесса-А1 и своства его файл дескриптора fd/3
там будет вот такое на экране написано


	TCP IP-А:50345->IP-Д:4444 (ESTABLISHED)

оно нам покзывает на каком TCP+IP биндиге сидит через этот указатель наш процесс
и на каком биндинге сидит удаленный процесс до которого это указатель позволяет достучаться 
таким образом мы наданный момент пока что только достучались до компа-Д 
до процесса-Д1

кстати это означает что на удаленном компе-Д тоже работает процесс. у него тоже 
есть указатель. и этот указатель покзывает тоже самое только наоброт

	TCP IP-Д:4444->IP-А:50345 (ESTABLISHED)



ИНТЕРЕСНАЯ ВСТАВКА=НАЧАЛО
яхочу этим сказать что если мы со совего компа инииицировали TCP конект то это значит
что обязательно где то там далеко на какомто компе обязательно живет еще один процесс
что на той стороне конекта обязательно сидит именно процесс. код исполняемый в юзер 
спейсе как и уменя. а не ядро. что tcp конект это всегда именно способ связать два процесса.
обязательно будет два процесса. в отичие от случая например когда мы делаем ICMP конект(кстаи
icmp он неюзает ни tcp ни udp. он просто сразу оборачивается снаружи IP пакетом). так
вот когда мы на нашем компе  с юзер процесса шлем icmp в сеть и нам прилетает ответ то 
на той стороне ниакой юзер процесс не принимает и не обрабатывает наш поток! в этом случае
на том стороне это делает ядро! то удаленное ядро! вот что важно понимать про tcp конект.
если он  у нас есть то с  обоих сторон обязательно есть два процесса. то хрени кртящиеся
в юзер спейсе. чьи то частные програмки.
тут есть нюанс - в конечном итоге это верно. на той стороне в конечном итоге точно сидит 
гдетто юзер проесс на том конце конекта. но между нами могут быть компы которые тоже участвуют
в передаче трафика и там вместо юзер процесса будет ядро. щас оббьсню. положим мы шлем 
пакет с нашего компа на dest_IP=1.2.3.4 tcp_dest_port=443
трафик прилетает на тот комп. а там его принимает ядро и там настроен iptables который
делает DNAT и он меняет dst_IP на какойто другой скажем IP=7.8.9.10
пакет от нас влетает в комп 1.2.3.4. его обрабатывает ядро через iptables без участия
юзер процессов. и пакет летит дальше на комп 7.8.9.10 и там уже его ждет юзер процесс.
поэтому да - на нашей стороне сидит запущеннй юзер процесс который явялется одним концом
тцп конекта и на 7.8.9.10 тоже сдит юзер процесс который принимает в конечном итоге наш пакет.
но между ними могут сидеть куча промежуточных компов с iptables которые пропускают 
через себя этот трафик и меняют TCP\IP параметры этих пакетов используя код ядра. у нас
конечно изначльный dest_ip был 1.2.3.4 и на нем нет юзер процесса. но где то потом в конце
цепочки юзер процесс обязательно есть получаетс просто необязательно что этот юзер процесс
сидит именно на том хосте который имеет ip=1.2.3.4 . вот это интеересный вывод про tcp 
конект. тоесть тцп конект всегда служит для связи двух процессов. их обязательно два. а
не один например. 

еще интеерснейший момент в том что мы привыкли смотреть текущие конекты на компе через 
скажем утилиту ss
а она в свою очередь смотрит это в /proc

так вот в /proc она это смотрит как я понимаю анализирую свойства файл дескрипторов 
у всех процессов на данный момент которые показаны в /proc

но фишка в том что на компе есть сетевые конекты которые никак не привязаны ни к одному
файл дескриптору.  а значит в /proc о них нет никкакой инфомрации! значит
утиита ss частично слепа!

речь идет например о tcp конектах которые транзитно редиректятся через наш комп.
тоесть в наш комп влетает сеетевой поток который был порожден на процессами нашего компа
и этот сетевой поток не предназначен для процессов нашего компа.  самый простой пример 
это ктото пингует наш комп. влетающий icmp поток он не будетпередан ни одному прцессу нашего
компа. поэтому не будет ни одного файл дескриптора где бы инфомрации об этом потоке была
была бы отображена. однако поток влетает. и он забираетв ядре всякие буферы куски памяти.
тоесть он напрягает систему. он составляет некий конект. но ss об этом потоке не покажет 
нихерна. это вот если мы будем со совего компа пиновать удаленный комп. то так как поток
породается одимиз процессов нашего компа. то будет файл дескрпиотор и ss покажет инфо 
об этом потоке. это да. 

другой пример такого невидимого потока это трафик который просто роутится на нашем компе 
тоесть влетает в одну сет карту и тут же вылетает через другую куда то дальше в сеть
без измнения IP\TCP парамтеров потока. хотя все таки и в таком потоке наше ядро будет
нагружаться потому что нужно  в этом потоке менять TTL у IP заголовка. а это циклы цпу. 
нагрузка на память. 

так вот увидеть полную карину всех конектов на компе можно через команду 

		# conntrack -L

для справки читаем 

        $ man conntrack

и тут я понял одну вещь - что команда ss она конечно же не лазиит в /proc и не парсит
файл декрипторы. это было бы охренено долго. 
щас я это докажу

	# strace -e openat ss -4n 2>&1| grep proc
	пусто

вместо ss юзает совершенно другой механмзм получения информации. а именно юзер процесс может
к ядру сделать сисколл чтобы создать сокет. но это не просто локальный UNIX_DOMAIN сокет
и не сетевой сокет. это AF_NETLINK сокет. это получается такой канал связи между ядром
и юзер процессом. тоесть часть информации о своей ядерной требухе ядро выставляет в /sys
и /proc папке но это же далеко не все. так вот ядро позволяет получит канал мостик 
связи между собой и юзер процессом. через спец сокет. 

# strace -e socket ss -4n | grep socket
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_SOCK_DIAG) = 3
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_SOCK_DIAG) = 3
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 4

вот каким образом ss получает инфомрацию о сетевых конектах!

также как в случае классичесокго сетевеого сокета первый параетр AF_NETLINK задает более общий
класс сокетов. а трретий парметр задает уже более конкртный вид сокета внутри класса.

описание этих типов сокета

	$ cat /usr/include/linux/netlink.h | grep NETLINK_SOCK_DIAG
	#define NETLINK_SOCK_DIAG	4	/* socket monitoring				*/

	$ cat /usr/include/linux/netlink.h | grep NETLINK_ROUTE
	#define NETLINK_ROUTE		0	/* Routing/device hook				*/

справка по этим сокетам
		
		 $ man  7 netlink

NETLINK_ROUTE
              Receives routing and link updates and may be used to modify the routing tables (both IPv4 and IPv6), IP addresses,  link  parameters,  neighbor
              setups, queueing disciplines, traffic classes, and packet classifiers (see rtnetlink(7)).

 NETLINK_SOCK_DIAG (since Linux 3.3)
              Query information about sockets of various protocol families from the kernel (see sock_diag(7)).

посмотрим man sock_diag
и там видим что 

		sock_diag - obtaining information about sockets

	   #include <sys/socket.h>
       #include <linux/sock_diag.h>
       #include <linux/unix_diag.h> /* for UNIX domain sockets */
       #include <linux/inet_diag.h> /* for IPv4 and IPv6 sockets */

       diag_socket = socket(AF_NETLINK, socket_type, NETLINK_SOCK_DIAG);

походу под sock_diag они понимают  NETLINK_SOCK_DIAG
и с помощью такого сокета можно из ядра зарпщиапть информацию о сокетах в ядре.
насколько я понял можно посмотреть инфо о двух больших классах сокетов это UNIX сокеты
и  IPV4\IPV6 сокеты.

как я уже писал в файле про СИ то сокет это не имеется ввиду парочка чисел IP:port
это биндинг. а сокет это стурктура данных в памяти ядра которую ядро создает для процесса
при исользвании глибс фнкции   

		socket()

в памяти ядра создаетая стркутара котоаря будет оббслуживать сетевой поток а процессу возвращается
дескиптор на эту струтуру. 

так вот ВАЖНО насколько я понял вот эта хрень

       diag_socket = socket(AF_NETLINK, socket_type, NETLINK_SOCK_DIAG);


она позволяет посмореть инфомрацию исключительно о сокетах которые юзер процессы создали в ядре.
но как  я уже говорил у нас есть например сквозоной трафик на хосте к которму либо прмиенятеся
SNAT\DNAT либо нет. но все равно такой трафик все равно обррабатывается(как минмиум в пакетах менятеся TTL и менятеся ETHER оболочка) . под это нужны струкотуры в памяти ядра так вот я вот
не знаю для таких потоков трафика создает ли само ядро для себя сокеты или нет. или это уже не
сокеты а какието друие струкрутуыры. 

в люом случае из моих эксприментов я дела вывод что для сквзного трафика (наприме унас на хосте
ест виртуаока и я с втуалки пингую гугл и ос стек хоста при этом работает как ротуер плюс к 
потому применяется SNAT). так вот для такого трафика я не вижу чтобы ss показал инфо что такойй
поток есть!

информацию о таком потоке покзывает утилита 

   # conntrack -L


я посмотрел чем она пользуется

# strace -e openat,socket conntrack -L
openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libmnl.so.0", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libnetfilter_conntrack.so.3", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libnfnetlink.so.0", O_RDONLY|O_CLOEXEC) = 3
socket(AF_NETLINK, SOCK_RAW, NETLINK_NETFILTER) = 3

соовтесвтенно она получается юзает тот же способ получения информации как AF_NETLINK
тоесть через спец сокет  с ядром. НО! эта прога юзает другой субкласс сокетов 

	NETLINK_NETFILTER


в man 7 netlink про него сказано

     NETLINK_NETFILTER (since Linux 2.6.14)
              Netfilter subsystem.

и не более того. но походу именно иза него conntrack показывает расшиеную инфомрацию
по когектам внутри ядра!

также  я даже без этой утилиты нашел на саомом деле инфо о конектах в /proc/net
там прям она разбита по типаом конектов

/proc/net/udp/tcmp/icmp/raw
также 
/proc/net/protocol
/proc/net/nf_conntrack

например то что выводит на экран команда conntrack это можно руками увидеть в nf_conntrack

таким образом какя понимаю ss она показывает инфо об сетевых сокетах тоесть об сущностях
которые сидят в памяти. но сетевой сокет это еще не сетевой поток. да ессли создан сетевой
сокет то в его свойствах есть сетевой поток. но как я понимаю сетевой сокет через socket() 
создается тлоько по запросу юзер процесса. а если у нас трафик транзитный в которомо не принимает участия юзер процесс то и сокет не создается а сетевой поток пакетов при этом все равно есть.
поэтому я бы предположил что ss покзывает сетевы потоки порожденные юзер процессами. 
но это не весь сетеовй поток проходящий через комп. и полный сетеовой поток надо смотреть
либо в /proc/net/... либо через conntrack

тут я еще узнал что у линукса есть сокеты типа packet. они предназначены чтобы юзер приложение
могло само форимровать все уровни сетевого пакета выше L2. тоесть мы пишем в такой сокет
и далее ОС надевает на наши байты сверху  L2 (ETHERNET) оболочку. а наша задача получается
сформирорвать в наших байтах все что выше тоесть L3,L4, ... L7
для сравнения AF_INET сокеты они уровня L4. тоесть юзер приложение должно само содавать
через байты которые оно пшет в сокет уровни L7-L7
в ss можно посмтреть такие сокеты через 

   # ss -o

а еще можно почитать man 7 packet

я так и непонял как через ss посмтреть статистике по icmp потокам которые мы сами
запустили с хоста!

а вот я нашел почему был придуман нетлинк:
	Why do the above features use netlink instead of system calls, ioctls or proc filesystems for communication between user and kernel worlds? It is a nontrivial task to add system calls, ioctls or proc files for new features; we risk polluting the kernel and damaging the stability of the system. 

тоесть вместо того чтоы придумывать кучу новых сисколлоов для получения статисткии придумали
просто один сокет. а уже через него будут херачит всякие статисткиики. нето чтобы супер
но навервно чуть получше. ведь эту стаистику все равно нужно будет создавать внутри ядра.
просто несколько другой метод получения доступа к ней. фрнтенд метод

а вот еше нашел:
	When you are writing a linux application that needs either kernel to userspace communications or userspace to kernel communications, the typical answer is to use ioctl and sockets.

я так и непонял раницу между сокетом нетлине и ioctl. какой из методов выглядит проще
сточки зорения испольования в приложении. в каком методе нужно вводить болше данных и мудить

нетлинк позволяет нетолько читать из ядра его требуху но наоброт - чтото менять внутри
ядра ! подкручивать там настройки.

значит статистика по icmp потокам можно руками посмтреть  в /proc/net/icmp (а не в raw)
выглядит она вот так 

 sl  local_address rem_address   st tx_queue rx_queue tr tm->when retrnsmt   uid  timeout inode ref pointer drops             
   19: 00000000:002E 00000000:0000 07 00000000:00000000 00:00000000 00000000  1000        0 14128362 2 0000000002d5d055 0      

странно при этом то что невозможно определить на какой IP мы посылаем пинги.
также важно сказать что в этом файле показан icmp трафик который обязательно на нашем компе
приходит на юзер приложение. тоесть трафик генерируется нашим юзер приложением.если
через комп идет транфизтный трафик icmp то здесь такой поток не отражается!

на счет команды conntrack я не могу грантировать точно но вроде бы в ядре есть две связанные
но вроде бы все такие разные системы одна из них это netfilter это ровно как раз те
самые правила которые можно редатировать через строчную утилиту iptables а вторая хрень
в ядре это conntrack. это подсистема которая занмается отслеживанием поток трафика. 
и согласно вот этой картинке 
	https://en.wikipedia.org/wiki/Netfilter#/media/File:Netfilter-packet-flow.svg
подсистема netfilter заглядывает в подсистему conntrack для принятия решений об потоке сетевом.
ищи на картинке черный овал  с надписью conntrack.
в простом смысле коннтрак  в ядре держит таблицу о сетевых соединениях. тоесть грубо 
говоря вот отправили мы первый TCP пакет с флагом SYN тогда коннтрак в своей таблице 
создает новую строчку и помечает что статус этого соедиенеия [NEW ] и возможно еще как [SYN-SENT]
прилетел нам ответ он там модифицирует статус этого соединения. 

в iptables комнаде можно указывать так называемые модули например модуль tcp.
модули это такие расширения которые нам позволяют все более изошренно анализировать 
и обрабатывать трафик. например 
вот например пример анализа тарфик на основе модуль -m udp

	-A FORWARD -i br+ -o wlp2s0 -p udp -m udp --dport 123 -j ACCEPT

у этого модуля есть доп опции
	
	-m udp --dport 123

инфорацию о модуле иптейблc можно найти в мануале 

	$ man  iptables-extensions

например там про модуль udp написано 

udp
       These extensions can be used if `--protocol udp' is specified. It provides  the  following
       options:

       [!] --source-port,--sport port[:port]
              Source  port or port range specification.  See the description of the --source-port
              option of the TCP extension for details.

       [!] --destination-port,--dport port[:port]
              Destination  port  or  port  range  specification.   See  the  description  of  the
              --destination-port option of the TCP extension for details.


так вот тамже есть и модуль conntrack
у эттого модуля есть доп опция

 [!] --ctstate statelist
              statelist  is  a  comma separated list of the connection states to match.  Possible
              states are listed below.

там же нарисано какие стейты есть у сетевого потока с точки зрения коннтрак

	   INVALID
              The packet is associated with no known connection.

       NEW    The packet has started a new connection or otherwise associated with  a  connection
              which has not seen packets in both directions.

       ESTABLISHED
              The  packet  is  associated  with  a  connection  which  has  seen  packets in both
              directions.

       RELATED
              The packet is starting a  new  connection,  but  is  associated  with  an  existing
              connection, such as an FTP data transfer or an ICMP error.

       UNTRACKED
              The  packet  is  not  tracked at all, which happens if you explicitly untrack it by
              using -j CT --notrack in the raw table.

       SNAT   A virtual state, matching if the original source address  differs  from  the  reply
              destination.

       DNAT   A  virtual  state,  matching  if  the  original  destination differs from the reply
              source.


вот пример такого конекта и его статуса с точки зрения коннтрак

	# conntrack -L | head -n 3
	tcp      6 136571 ESTABLISHED src=192.168.220.1 dst=1.1.1.1 sport=60916 dport=443 src=146.190.207.220 dst=192.168.220.1 sport=443 dport=60916 [ASSURED] mark=0 use=1

а вот кстати как видит коннтрак icmp поток

	# conntrack -L |   grep 8.8.8.8
	icmp     1 29 src=192.168.51.1 dst=8.8.8.8 type=8 code=0 id=48 src=8.8.8.8 dst=192.168.51.1 type=0 code=0 id=48 mark=0 use=1

понятное дело что у icmp нет нкиакого статуса конекта. статус конекта есть только в tcp
потоков

а вот самое известное правило модуля conntrack которое обычно можно найти в любой таблице 
правил iptables

	-A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

тость мы юзаем модуль иптейблс conntrack и его фичу ctstate
таким образом мы разрешаем проходить пакетам которые принадлежат TCP потокам у которых 
в таблице коннтрак статус потока с точки зрения коннтрак равен RELATED или ESTABLISHED

conntrackd это юзер демон - он нужен только для того если пытаться построить откзауо
стойчивый кластер на основе иптейбл. ибо он будет копировать таблицу конекшенов на другую
ноду - ровно также работает pfsense кластер
вот тут очень много полезного я узнал про иптейбл и коннтрак
	
	https://serverfault.com/questions/1030236/when-does-iptables-conntrack-module-track-states-of-packets

еще раз справка по модулям iptables

	https://manpages.ubuntu.com/manpages/oracular/en/man8/iptables-extensions.8.html


получается такой момент - если трафик инициируется на юзер процессе на нашем компе
или трафик снаружи прилетает на юзер процесс нашего компа то информация об этом сетевом
пакете будет отражена в ss. потому что трафик породит файл дескриптор который будет привязан
к процессу. если трафик транзитный никак не связанный с юзер процессом тоггда в ss небудет
о нем инфо. но в любом случае инфомрация об трафике этом будет отражена в conntrack таблице.
тоесть если заюзать команду conntrack то там инфо обо всем трафике на компе будет отображена


сcылки
https://github.com/mwarning/netlink-examples/blob/master/articles/Why_and_How_to_Use_Netlink_Socket.md



ИНТЕРЕСНАЯ ВСТАВКА=КОНЕЦ



вовзращаюсь к основному вопросу про прокси
мы составляем  в нашем процессе-А1 поток байтов  согласго формату прокси протокола
(назовем этот поток байтов которые сует процесс как пакет. пакет имеет формат прокси протокола) и суем их в указатель.

далее ядро берет наш прокси пакет оборачивает его в тцп пакет потом оборачивает 
в ип пакет потом обрачиваем в эзернет пакет. сует в сетвую карту и эта матрешка полетела в сеть.

прилетела на комп-Д . там ядро разобрало эту матрешку и в процесс-Д1 поступает только вкусное
содржимое а именно прокси пакет. все остальное это была лишь транспортная упаковка. 

так вот в этом прокси пакете будет написано на какой IP и на какой порт наш процесс-А1 
на самом деле хочет попасть. например нам там напсиано 

   IP = IP-Ж  TCP порт = 443

процесс-Д1 понимает формат пакетов прокси протокола поэтому он все понимает что от него хотят.
тогда процесс-Д1 обращается к ядру и создает тцп коннект к компу-Ж

на данный момент тогда мы имеет следующую ситуацию в плане коннектов:

   комп-А 
   процесс-А1 
      IP-А:45345 -> IP-Д:4444


   комп-Д
   процесс-Д1 
      IP-Д:4444 -> IP-А:45345


но также процесс-Д1 имеет и второй коннект

   комп-Д
   процесс-Д1 
      IP-Д:4444  -> IP-А:45345
      IP-Д:56345 -> IP-Ж:443


тоесть у процесса-А1 один сетевой сокет а у процесса-Д1 два сетевых сокета

	процесс-А1 <----------->  процесс-Д1 <---------------------> процесс-Ж1
IP-А:45345 -> IP-Д:4444       IP-Д:4444  -> IP-А:45345           IP-Ж:443 -> IP-Д:56345
                              IP-Д:56345 -> IP-Ж:443

так значит статья которая обьясняет как выглядит пакет в формате SOCKS4 вот она

 	  https://ftp.icm.edu.pl/packages/socks/socks4/SOCKS4.protocol
еще вот тут можно посмотреть
	 https://en.wikipedia.org/wiki/SOCKS


значит когда наш процесс шлет прокси пакет на комп-Д он шлет пакет в виде 

		+----+----+----+----+----+----+----+----+----+----+....+----+
		| VN | CD | DSTPORT |      DSTIP        | USERID       |NULL|
		+----+----+----+----+----+----+----+----+----+----+....+----+
     	   1    1      2              4           variable       1


цифры внизу это число байт - сколлько байт занимает то или иное поле в пакете.
VN - это номер сокс протокола. в текущем случае это версия 4
CD - это команда которую просим выполнить у прокси процесса. может быть всего две 
команды это CONNECT и BIND. конект это если мы хотим сделать исходящее соедиенеие 
например сходить на сайт. а бинд это если снаружи к нашему процессу ктот должен достучаться
через прокси стучась снаружи на прокси сервер. я эту хрень для простоты рассатривать щас
не буду. если мы хотим сделать конект то CD должен быть равен 1. 
DSTPORT - все понятно это TCP порт на который мы хотим чтобы прокси процесс достучался
DSTIP  - тоже все понятно
USERID - это id юзера под которым мы типа обращаемся на прокси процесс. 
как я понял в теории прокси процесс может обратсят к ident серверу (ident это такой протокол)
и проверить можно ли такому id оббрашаться на такой dest ip и dest port. 
в нашем случае  наш прокси процесс нихрена этого неделает. он разрешает всем id лезть на 
все хрени в интернете. вот такой пакет и посылает наш процесс-А1 к процессу-Д1
а именно


		+----+----+----+----+----+----+----+----+----+----+....+----+
		| 4  | 1  | 443     |      IP-Ж         | 345          |NULL|
		+----+----+----+----+----+----+----+----+----+----+....+----+
 
тоесть получается наш прокси процесс создает новый сокет к компу-Ж на основе 
инфо в этом прокси пакете. от этгого зависят параметры нового создваемого им сокета.
то бишь нового сетевого соедеинененеия. 

если бы мы использовали ipables на компе-Д для модификации сетвого трафика то непонятно
как от приложения передать ядру эту инфо о том какое правило нужно создать ядру для DNAT\SNAT
входящего потока от компа-А

если прокси приложение успешно создает сокет до компа-Ж то оно обратно нашему процессу-А1
шлет прокси пакет вот такого формата


        +----+----+----+----+----+----+----+----+   
		| VN | CD | DSTPORT |      DSTIP        |
		+----+----+----+----+----+----+----+----+
     	   1    1      2              4

где цифры - это число байт в каждом поле.
в стстье что я привел написано что важно только первое поле. а остальные игнориуются
клиентом принимаюшим ответ. а первое поле в ответе содержит уже не номер сокс протокола 
как это было в запросе а содержит код возврата была ли операция успешна. а именно

VN:
    90: request granted
	91: request rejected or failed
	92: request rejected becasue SOCKS server cannot connect to
	    identd on the client
	93: request rejected because the client program and identd
	    report different user-ids


поэтому если все успешно то по идее должно вернуться VN=90

а дальше начинается самое интересное  - теперь наш процесс-А1 просто может пихать любые 
байты (тоесть уже требования к тому чтобы это были байты в формате прокси отпадает) в файл дескриптор который овтечает за сетевой конект до прокси процесса-Д1 , они в рамках тцп конекта
будут долетать до процесса-Д1 тоесть это будет выгляеть вот так

	процесс-А1 " {  [ (любые байты) ] TCP } IP }  ETHERNET " ---------> процесс-Д1

соотвесвтенно ядро на компе-Д вскрывает эту матрешку и в процесс-Д1 передает только пейлоад
от тцп тоесть (любые байты) и процесс-Д1 просто беерет эту мешанину байтов и ему плевать
соотвствуют ли они какотому формату или протоколу и просто тупо сует эти байты в свой второй
сетевой сокет который ведет на комп-Ж !

	процесс-Д1 " {  [ (любые байты) ] TCP } IP }  ETHERNET " ---------> процесс-Ж1


получается процесс-А1 на ушко шепчет процессу-Д1 полезный контент и говооит передай 
этот контент на комп-Ж. и процесс-Д1 берет переданный контент и уже через свой сокет сетевой
передает в комп-Ж
если попробовать осознать как это выглядит то выглядит это так что как бутто у нас байты
которые генерирует процесс-А1 на самом деле генерирует процесс-Д1 и от себя (через свой
сетевой сокет) уже шлет на конечный сервер-Ж на который бы хотел это послать процесс-А1.
это выглядит так что вася хочет позвонить маше но ему нельзя. тогда он звонит пете и говорит
позвони маше и скажи ей "ку". петя звонит и говорит "ку". маша ответчает пете "дза" и петя
звонит васе и говорит ему "дза"

прокси протокол очень простой. в общем то два прокси пакета нужно предать туда и обратно 
а потом уже можно преедаавать голые данные. также получается что легко можно менять на какой
комп мы хотим попасть в итоге. также классно то что прокси процессу в итоге глубоко плевать
в каком формате байты прилетают к нему от клиента. он их просто берет и пересылает (релей) на
конечный комп. получается сокс прокси хорош тем что он непривязывается к байтам от юзер
процесса. поэтому через прокси можно передавать и ssh и HTTP и HTTPS 
суть процесса проксирования получается в том что наш процесс через тцп коннект связывается
с прокси процессом. через простой прокси пакет обьясняет куда хочет попасть. прокси процесс
делает тцп конект с этим дестинейшн компом. для тцп конекта абсолютно все равно какие байты
в него пихают. он просто берет и их пересылает. у нас получается два тцп конекта. 
от клиента до прокси процесса. и от прокси процесса до конечного компа. для простоты скажу
что прокси протокол и прокси процесс исползуются для релея именно тцп конектов. то есть 
прокси процесс вместо нашего исходного процесса  устанавливает тот самый тцп коннект  который
неможет устанвить наш исходный процесс. тоесть ICMP+IP через прокси процеесс неполучится
запроксировать. мы проксируем только тцп конект от имени клиента. по крайней мере в SOCK4
а про пятый я нехочу разбирать. 
получается мой процесс хочет сделать тцп конект с конечным компом. но ему нельзя запрешено.
тогда он делает тцп конект с прокси компом и прокси процессом. и просит его сделать 
ровно тот самый тцп конект который не может сделать мой процесс до конечного компа. 
и в итоге мы имеем два тцп конекта. 

так как прокси протокол такой простой и такой мощный то написать прокси клиент и прокси
сервер очень просто. или например скажем что добавить в программе фукционал прокси клиента
очень просто!

есьт несклько протоколов проксирвания: SOCKS4, SOCKS5. потом яне очень понимаю вроде как 
протоколом проксирования может выступать HTTP протокол. но я это неразбирал. также вроде
как haproxy придумали свой протокол проксирования. а еще есть SOCKS4a, SOCKS5, SOCKS5h прото
колы. 
в чем фишка SOCKS5h - в том что вот нам надо в прокси пакете указать DEST_IP. а теперь
предтсавим что мы обращаемся с браузера на прокси процесс. значит мы хотим попасть на сайт
скажем google.com значит нам нужно в прокси пакете указать DEST_IP сервера накотором крутится
google.com , а для этого наш браузер должен как то САМ зарезолвить домен в IP адрес чтобы
повторюсь его указат в прокси пакете который он будет отсылать прокси процессу. 
ну получается для этого браузер (пусть даже это будет программа curl которая по факту веб клиент
считай что простой браузер) должен либо через NSS зарезолвить домен в айпи либо либо как то
сам это пытасят сделать например как это делает хром которйы резолвить сам через DOH.
так вот представим что мы нехотим чтобы процесс-клиент сам резволил доменные имена в айпи.
тогда можно попросить это сделать сам прокси процесс. и такой функционал ест как раз у 
протокола SOCKS5h
А есть более диковинные прокси процессы когда он не один а их скажем два. причем они 
сидят на разных хостах. таким примером является ssh. на своем компе можно запустить так 
ssh что он на ноутбуке запустить процесс который прокси клиент. мы с нашего процесса например
барузера обращааеимся на этот прокси процесс. а он через шифрорванный тонель пересылает наши
полезные данные не наконеный сервер куда мы хотим попасть а на другой процесс который куртися
на другом сервере. и уже тот процесс ииницирует тцп конект к тому компу к которому хочет
попасть наш исходый браузер.

тоесть вот обычная схема

наш комп               прокси сервер
наш процесс   ------>  прокси процесс --------> конечный сервер


вот более сложная схема как у ssh


наш комп               наш комп                 прокси сервер
наш процесс   ------>  прокси процесс --------> еще один прокси процесс   ------> конечныый сервер

такая схема используется потому что SOCKS протоколы они не шифруют трафик. ну скажем
нам и ненадо ибо мы хотим предавать TLS трафик котоырй уже шифрованный. но вспоминаем что
сам сокс протокол он же тоже требует отослать и принять два прокси пакета. и когда 
мы это делаем в рамках одного компа то оокей - их никто в сети не подсмотрит не прочитает.
а вот когда наш прокси звонит на второй прокси процесс он же тоже передает прокси пакет
незащищенный. и уже по сети. и поэтому нужно чтобы эта передача шла внутри шифрованого тонеля.
поэому между двумя прокси прцоессами натянут ssh тонель. чтоб никто не подсмотрел и непоменял
паратры  пакета в формате прокси протокола socks. 

плюс SOCKS прокси протокола в том что чтобы он заработал нам всего навсего нужно чтобы 
файрвол между нами и удаленным сервером где сидит прокси процесс позволял нам сделать 
TCP конект на IP того компа  на любой TCP порт. тоесть нам нужно чтобы наш файрволл ползволял
нам всего навсего сделать тцп конект до прокси сервера. а тцп конекты это вообще база всех
сетевых конектов. на какой нибудь тцп порт наш файрволл да должен разрешать конект на тот сервер.
тоесть сокс протокол хорош тем что он базируется на тцп. а тцп насктлко сильно используется во
многих конектах что файрволл должен разрешать конект тцп хотя бы на какойто порт на удаленном
компе. тоесть мы хорошо проникаем сквозь файрволл. тоесь запретить на файрволле все тцп содеинеия
это как с Земли откачать весь воздух. сразу все сдохнут. второй плюс SOCKS прокси протокла в том
что ему абсолютно плевать какой L5-L7 протокол (поток байтов) будет через него прокачиваться.
ему на это настолкьо же плевать насколько плевать тому же самому тцп протоколу. для прокси 
процесса это просто поток байтов в которых неужно разбираться. просто возьми эти байты 
из одного тцп конекта и вставь их в другой тцп конект и все. тоесть прокси протокол такой классный
что просто напросто прокси процесс вытаскиват байты из одной коробки и кладет в другую. 
и воще похер какой формат этих даных. 

SOCKS4 протокол хорош тем что он базируется на TCP. а так как тцп конекты лежат в основе
очень многих более высоких протоколов то чтобы с клиента на компе достучаться до прокси
процесса на прокси сервере достаочно чтобы наш файрволл разрешал исходяшее тцп содиениение
хотя  бы на какойто тцп порт. также как пишут заумными словами СОКС4 протокол он типа 
протокол агностик. говоря нормальным языком они имеют ввиду что если мы хотим через прокси
сервер передать любой протокол который лежит выше чем TCP то соскс протокол и сокс прокси
процесс это сделают отлично. потому что им абсолютно плевать какому формату подчиаяются
байты которые прилетают от клиента на прокси процесс через тцп конект. прокси процессу 
это вобще неважно. он прсто беерет эти байты и преереклвдывает из одного тцп пакета в другой
тцп пакет. СОКС4 протокол он подразумевает что мы с клиента стучим на прокси процесс через
именно TCP конект хотя если вот так подумать то ничего немешает чтобы клиент стучал на прокси
процесс через любой транспортный протокол надежный . и сокс4 протокол подарзуметвает что 
прокси процесс он потом когда достукивается до конечного сервера до делает он это тоже ТОЛЬКО
через тцп конект и в прокси пакете который прилетает от клиента укзыавется в графе ПОРТ именно
TCP порт конечного сервера на который нужно постучать. ИМЕННО TCP порт. а не какогото друогого
протокола порт.
прокси процесс принимает от клиента через тцп конект контент (байты) которые передаются 
внутри ТЦП пакетов.  вытаскивает эти байты. и ничего в них неменяя перекладыват их внутрь
других ТЦП пакетов которые он уже отправляет на конечнй сервер. это означает что байты внутри
ТЦП пакетов от клиента могут быть любые. поэтому клиент через СОКС4 прокси может отправтлять 
байты любого протокола который базируется на TCP а именно для прмиера это могут быть байты
формата(протокола) telnet, ftp, finger, whois, gopher, http, tls, ssh  

минусы СОКС4 - неумеет работать с UDP, нет аутентсциикации по паролю, нет шифрования.
поскольку работа с прокси процессом состоит из двух фаз. первая фаза это обмена пакетами
именно в формате ПРОКСИ. и они незашифрваны. то если прокси сервис находится на удаленном
компе то чтобы защитится от злоодея то нужно получается защщать конект до прокси сервера 
через ширфванный тонель. на вторйо фазе уже просто клиент передает свои "голые" данные
на прокси процесс. если эти данные это TLS то на этой фазе получается защита и ненужна потому
что защиту даст сам TLS хотя и он неидален потому что он нешифрует SNI. но на первой фазе защита шированием нужна по любому. 
нет аутентфиикации по паролю - хотя есть контроль доступа через IDENT протокол который поодходу
уже никто неислольует. 
неумеет работать по UDP. это значит что сокс4 протокол предпиывает сокс4 серверу чтобы он 
дозванивался до конечного сервера исклтительно через TCP конект. и больше никак. поэтому и 
через UDP тоже это запрещено делать. это значит что если мы будем  с клиента пихать байты 
в формате протоколов которые базируются на UDP например опенвпн то нас ждет провал. потому что
наш прокси сервис будет дозваниться до опенвпн сереваера исклчтительно по TCP конекту!
тоесть мы должны четко понимать что сокс4 сервис будет звонить на сервер который мы ему укажем
исключительно через TCP конект! а если нам надо позвонить на кончный сервер через UDP конект
то он этого делать НЕБУДЕТ! поэтому СОКС4 прокси сервис подходит только если нам надо достучаться
через него до удаленного сервера через TCP конект и передать внутри этого конекта контент
который базируется на TCP конекте. тоесть протокол который базируется на тцп конекте.
получается можно будет через прокси донести такие протоколы как DNS, IMAP, HTTP, TLS, NTP, POP, 
SMTP, SSH, Telnet. охренеть. круто.

SOKCS5 сервис умеет уже дозванится до удаленного сервера нетолько через TCP но и через UDP,
также умеет аутенцировать клиента через логин пароль. правда толку от этого если он не защищает
себя шифрованием. также SOCKS5h прокси сервис умеет по просьбе клиента делать dns резолвинг. делает он его опираясь на NSS подсистему.

HTTP прокси протокол - это на самом деле всего навсего кусочек от обычного протокола HTTP 1.1
а именно это всего навсего одна команда(в протоколе команды назвыаются диркетивы) в протоколе 1.1, 
а именно команда CONNECT тоесть в протоколе есть команды GET, PUT, POST итд. и вот есть команда
CONNECT. и схема получсется точнот такая же самая как в протоколе SOCKS. тоесть клиент должен
послать команду CONNECT в рамках HTTP запроса на прокси сервер. в формате вот таком
(показываю заголовок HTTP запроса)

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

этот HTTP запрос улетает на прокси сервер. в строке CONNECT мы указываем хост на который 
мы хотим досутчаться в кончном итоге и порт

значит про эту директиву CONNECT расписано вот здесь

  https://www.ietf.org/archive/id/draft-luotonen-web-proxy-tunneling-01.txt


для примепа я покажу как выгляит обычный GET запрос 

> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


я это привел к тому что послденнее поле в строке CONNECT стоит HTTP/1.1 спрашивается зачем
оно там? что оно обозначает? оно обьячняет веб серверу на который прибудет реквест что ээтот
реквест создан на базе протокола HTTP 1.1 тоесть формат входящего сообщения. и также это
намекает веб версерру на каком формате вебсервер должен вернуть ответ на запрос. 
вот смотри

  > CONNECT google.com:443 HTTP/1.1

  > GET / HTTP/1.1

то есть в обоих директивах мы указываем для веб севрера какой формат прилетевшего сообщения.
тоесть поле HTTP/1.1  оно не относится к параметрам GET или CONNECT а эта глобалная настройка
в каком формате ВЦЕЛОМ составлен полный текст запроса. я считаю что это деилиизм вставлять 
глобаяную настройку в частную строку. это путает.
я считаю что это додждно было бы быть лучше вот так

для гет запроса
> GET / 
> HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


для коннект запроса
> CONNECT google.com:443 
> HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

так вот на счет строчки 

  CONNECT google.com:443 HTTP/1.1

итак ее смысл. итак опция HTTP/1.1 это глобальная обьяснялка коорая говорит веб серверу
что весь запорос отформатирован согласно формату HTTP/1.1 
CONNECT google.com:443 означает к какому серверу и на какой порт нужно сделать TCP коннект веб
серверу (или прокси веб серверу) на который прилетел запрос. тоесть клиент просит прокси
сервер устанвитть тцп конект с серврером google.com на тцп порт 443
потворюсь пока что просто тцп конеект. о протоколах более выского уровня речи неидет

в частности из это вытекает что конект может быть нетолько на 443 порт. а на любой тцп порт
например 


  CONNECT google.com:1518 HTTP/1.1


и важно понимать что речь идет о том чтобы прокси сервер сделал всего навсего ТЦП конект. 
о том какой протокол вышего уровня будет передаваться чрез этот конект речи не идет. 
это неоговариваетя неограничивается! а опция HTTP/1.1 гоорит не о том что через порт 1518
пойдет HTTP поток . нет! опция HTTP/1.1 к соажению в этой строке всунута неудачно она неимеет
отношения к опции команды CONNECT. опция HTTP/1.1  сообщает что весь реквест целиком который
прислан на прокси веб сервер отформатрован на основе HTTP/1.1 формата и что клиент ожидает ответ
тоже в формате HTTP/1.1

далее прокси веб сервер делает тцп конект к google.com:443 и остлыает клиенту ответ 
в формате HTTP/1.1

    HTTP/1.1 200 OK  Connection established

ну или полный ответ

  HTTP/1.1 200 Connection established
  HTTP/1.1 200 Connection established

первая строка идет в хидере а вторая это тело соообщения в ответе
в овтете более умно сделано - формат ответа указан с самого начала ответа а не гдето
в конце первой строчки

итак еще раз мы с нашего клиента шлоем на прокси сервер запрос в формате HTTP протокола 
и просим чтобы прокси сервер устанвоил тцп конект с google.com:443

  > CONNECT google.com:443 HTTP/1.1
  > Host: google.com:443
  > User-Agent: curl/8.10.1
  > Proxy-Connection: Keep-Alive

прокси сервер принимает зазпорос. и устанвливает этот тцп конект. 
и нам в ответ сообщает что он это сделал

  < HTTP/1.1 200 Connection established
  HTTP/1.1 200 Connection established

а дальше происходит то что прокси сервер просто сидит ожидает что мы ему пришлем в тцп
пакете. он это вытаскивает и просто тупо кидает в тцп конект к google.com:443

тоесть HTTP прокси сервер ралтботает точно по такому же принципу как и SOCKS сервер. 
асболютно такой же прринцип работы!

и поэтому это значит что HTTP прокси сервер он может форвардить аболютно любой юзер протокол
который базиурется на базе TCP а не только HTTP\HTTPS протоколы!
тоесть я где то встречал что СОКС протокол может форвардить любые юзер протоколы базирующиеся
на ТЦП конектах а якобы HTTP прокси может форврдиь толко HTTP проткоол - ТАК ВОТ это полная 
хуйня! и SOCKS и HTTP прокси могут оба форваодить любые блять юзер протколы бащирующиеся
на TCP.

не вижу никких проблем чтобы такой HTTP прокси сервер форвардил нетолько HTTP поток
но и HTTP+TLS = HTTPS поток. 

прмиером HTTP прокси явялется программа 

  tinyproxy


и вот как можно через curl обраттся в интернте через HTTP прокси

  $ curl -x http://172.16.10.11:2080  -p --dump-header - -v  https://google.com


обращаю внимание что только при сипользовании обоих ключей и -x и -p у нас курл будет
делать реквест используя CONNECT! если ключ -p опустить то курл сгенериурет вот такой
запрос 

  $ curl -x http://172.16.10.11:2080   --dump-header - -v  https://google.com 
> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 Connection established
HTTP/1.1 200 Connection established

кхм.. странно  до этого без ключа -p директиа CONNECT неиспользовалось

на счет поля "Host:" я хотел пояснить. какой его смысл. в пртоколе 1.1 это обязательное поле
для каждого реквеста. здесь так написано

  https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html

и это поле ознаает вот что. вот мы достучались до веб севрера  и туда влеает HTTP текст.
так вот процесс веб сервера может обслуживать несколко сайтов в этом нет никаких проблем.
так вот заголовое "Host:" дебильное название потому что он ообозначает не хост а он обо
значает доменное имя сайта на который мы хотим досутчаться. скажем наш веб сервер процесс
обсивает сайты "vasya.com" и "petya.com" и вот влетает наш реквест и там написнао

    Host: petya.com

и процесс веб сервер понимает какой веб сайт нужно выдать в ответе!
так вот честно говортя в реевесте CONNECT

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive


у нас инфомрация на какой веб сайт мы ломися получается дублиуется ! с одной
стороны она указан в строке CONNECT а вдругой сторне она укаана в строке Host:
ненужная дупликация. хотя это не совсем так.
дело в том что то что указано в строке CONNECT оно используется для установления TCP конекта.
тоесть речи ни о каких HTTP сайтах нихуя не идет! имя сервера в строке CONNECT берется.
дальше оно чрез DNS резолвится в IP. и далее через этот IP и порт делается tcp коннект
а хосту! на этом хосте никаких веб сервров процессов может не быть в поимне! например на этом
хосте может быть SMTP или DNS или FTP сервер! поэтому доменное имя в строке CONNECT необозна
чает никакой веб сайт! он обозначает DNS имя уаленного сервера. который через DNS резволтся
в IP адрес. и с этим IP адресом уснваливет прокси ТЦП коннект!
а вот поле Host: испольщуется в том случае если мы делаем именно реквест к веб серверу
к веб сайту через соотсветеущие директиы например диерктиву GET. и тогда достучавшись до 
процесса веб сервера и передва ему GET реквест 


> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


то веб сервер процесс через поле Host: понимает про какой веб сайт который он обслуживает
из списка несклольких веб сайтоы мы хотим получить обратно ответ!

да! дело в том что сам ХОСТ сам сервер может иметь в системе DNS одно доменное имя.
например 

   cyber.com  A    1.2.3.4

и на этом серверер крутится веб сайт процесс и он соглласно найтрокам в жинкс
обслуживает два веб сайта
  
   vasya.com
   petya.com

у жинкса в это секции server{} опция servername

дапонятно что архитекрутно это дебильно сделано. что выгоднее в DNS сделать вот такие
две записи 

   vasyacom  A    1.2.3.4
   petya.com A    1.2.3.4


так удобнее но это необязательно. у нас доменное имя сервера может быть одним. 
а веб сайты(их доменные имена) которые на нем обслуживает жинкс могут быть совершенно другие.

тогда в веб клиенте мы как должны постуить. мы берем доменное имя срвера cyber.com
и через днс превращаем его в IP. далее мы создаем TCP конект с IP:443
а теперь чрез этот тцп конект мы суем в этот конект HTTP текст в виде

> GET / HTTP/1.1
> Host: vasya.com:443
> User-Agent: curl/8.10.1
> Accept: */*

и нет никаких проблем!

поэтому так как HTTP базируется часто но невсегда на TCP то нам вначале унжно создать тцп
конект. а уже потом через него кидать HTTP запрос. так вот это две большие разницы между
доменным именем сервера который имеет IP. и через этот IP (а значит и это дменное имя ) мы
вначале устанвливаем тцп конект.
а внутри серевера может  быть куча веб процессов кажый из которых обслуживает кучу разных
веб сайтов со своими доменами . и вот эти домены веб сайтов необязаны пересекаться с доменом
самого сервера. поэтому Host: относится к HTTP тексту который приетает на веб сервер процесс
и сообзает этому процессу покажи мне такойто сайт. а домен  в CONNECT он относится не к
HTTP тексту и не для веб серер процесса. он относисятся к той фазе когда мы создает тцп
конект.

тогда спршивается нахер в этом запросе вообще это поле Host:

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

ведь наш прокси сервер это не веб сервер. и он не хостит у себя никкакой веб сайт. 
ну как я понял из документа что привел выше что тупо в протоколе HTTP/1.1 обязательно 
в каждый реквест вставлять поле Host: и там где оно имеет смысл и где смысла в нем нет.

привожу еще раз описание смысла поля Host:

 The Host request-header field specifies the Internet host and port number of the resource being requested, as obtained from the original URI given by the user or referring resource (generally an HTTP URL,

as described in section 3.2.2). The Host field value MUST represent the naming authority of the origin server or gateway given by the original URL. This allows the origin server or gateway to differentiate between internally-ambiguous URLs, such as the root "/" URL of a server for multiple host names on a single IP address.

       Host = "Host" ":" host [ ":" port ] ; Section 3.2.2

A "host" without any trailing port information implies the default port for the service requested (e.g., "80" for an HTTP URL). For example, a request on the origin server for <http://www.w3.org/pub/WWW/> would properly include:

       GET /pub/WWW/ HTTP/1.1
       Host: www.w3.org

A client MUST include a Host header field in all HTTP/1.1 request messages . If the requested URI does not include an Internet host name for the service being requested, then the Host header field MUST be given with an empty value. An HTTP/1.1 proxy MUST ensure that any request message it forwards does contain an appropriate Host header field that identifies the service being requested by the proxy. All Internet-based HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code to any HTTP/1.1 request message which lacks a Host header field. 

тоесть когда мы достучались до веб сервер процесса то он обслуживает несклоько веб сайтов.
а у веб сайта есть доменное имя и есть дальнейший URI который обозначает путь к ресурсу.
так вот у разных веб сайтов может быть одинаковый путь к ресурсу а в запросе GET там укаызвается
именно путь к ресурсу. и из только строки GET непонятно от какго веб сайта этот путь относится.
и тут нам на помощь и приходит поле Host:
я так понимаю этот смысл.

забавно также что вот так деильше повелось что путь к ресурсу в строке GET он раньше совпдаал
с точн таким же путем к этому ресуру на файловой системе линукса. поэтому эти два понятия 
смешиватся. хотя они разные! это четко видно если веб сервер берет ресурсы через php.
потому что если мы возьмем путь к php файлу из GET то мы такой файл на файловой системе
не обнаружим! потому что это разыне вещи которые иногда совпдают.

немножко отойдем в сторону. чем оличается DNAT и прокси. вот расмотрим что мы с компа-А
из процесса пороидили сеетевой поток и он полетел по сети. если поток проходит через прокси
на другом компе-Б то он на прокси машине из сеетвой карты летит в ядро а оттуда 
доходит до юзер спейсе - в процесс в юзер спейсе тоесть поток из машины-А в итоге через ядро
на компе-Б влетает в юзер спейс. и там заканчивает свой путь в рамках одного и тогоже
тцп конекта между компом-А и компом-Б.  что значит заканчивает свой путь. это значит что содер
жимое ТЦП пакета вытаскивается , сам тцп пакет отправляется в мусор. а его контент передается
в процесс. тоесть тцп\ип пакет он полностью уничтожается на этом компе. толлько от него осатется
полезный контент который лежит в пейлоад зоне тцп пакета. а сам пакет уничтожается ядром.
если же сеетвой поток от машины-А влетает в роутер на компе-Б на котором происходит DNAT то в этом случае сетевой поток во первых он недоходит до юзер спейса. он крутится только в районе ядра
этого компа-Б. далее насколько я понимаю нетфильтр\иптейблс в ядре оно не уничтожает прилетевшие
тцп пакеты а просто меняет в них тцп порты и ип адреса в ip части пакета. хотя точно я незнаю
ядро уничтожает тцп\ип заголовки вытаскивает тцп пейлоад и перекладывается в новые тцп\ип 
упоковки или все таки оно только модифицирует имеющиеся тцп\ип заголовки. точно непонятно.
но далее эта хрень сидящая в ядре выплеывается дальше в сеть. повторюсь все обработку делает
толкьо ядро. юзер спейс никак не участвует. правда я не знаб как это вынлядит со стороны таблица
коннтрак. там два потока сетвых в итоге записывается - влетающий и вылетающий или один?
я проверил - поток получается один. с точки зрения записей в таблице коннтрак.
дело в том что коннтрак таблица заполняется на определенном этапе проходждени иптейбл цепочек(ищи картинки в интернете). так вот что я имею. яимею виртуалку у нее IP=172.16.10.11
она сидит на ноутбуке за сетевым бриджем br0
я на виртуалке запускаю команду

    (виртуалка 172.16.10.11) $ nc -v   64.50.236.52 21

смотрю какая запись или записи для этого TCP конекта есть в коннтарк таблице внутри вртуалки

    (виртуалка 172.16.10.11) # cat nf_conntrack | grep 64.50.236.52 | grep EST
    ipv4     2 tcp      6 431993 ESTABLISHED src=172.16.10.11 dst=64.50.236.52 sport=51582 dport=21 src=64.50.236.52 dst=172.16.10.11 sport=21 dport=51582 [ASSURED] mark=0 zone=0 use=2

получается мы имеем пакет 

    TCP  172.16.10.11:51582 -> 64.50.236.52:21

теперь  я  отслеживаю пакет на хосте на ноутбуке. ноутбук имеет карту wlp2s0 IP=192.168.51.1
(удивительный IP потом что гейтвей это 192.168.51.34)
смотрим через коннтрак

  (нотубук хост) $ sudo conntrack -L  | grep 64.50.236.52
  tcp      6 431956 ESTABLISHED src=172.16.10.11 dst=64.50.236.52 sport=51582 dport=21 src=64.50.236.52 dst=192.168.51.1 sport=21 dport=51582 [ASSURED] mark=0 use=1

тоесть пакет уже в стеке хоста выглядит до попадания на SNAT преобразование как

    172.16.10.11:51582 -> 64.50.236.52:21

и после SNAT преобразования как 

    192.168.51.1:51582 -> 64.50.236.52:21

но в любом случае при проходдении стека на хосте компа пакет до преобования NAT
и после преобразования с точки зрения коннтрак принадлежит одному потоку
видно что при преобразовании только меняется SRC IP. а src port неменяется как был 51582
так и остался. потому что этот порт не был занят на хосте. так зачем его лишний раз менять.

таким образо получается что транзитный SNAT поток через комп он во первых обрабвыатется
искочиетльно ядром и не попдаает в юзер спейс это первое отличие от прокси обработки потока.
второе отличие что  с точки зрения ядра и поток котоыйр влетает в комп и поток который из него
вылетает это один и тотже поток только слегка преобрвзанный.  а если поток летит через прокси
приложение то у нас  сточки зрения ядра будет два потока. один который влетает и заканвиается
на юзер процессе. а второй поток который новый он начинается на юзер процессе и улетает в 
интернет и заканчивтся на удаленном хосте. тоесть мы имеем два потока и в таблице коннтрак
будет две записи. в этих двух моментах отличие проксиования сетевого потока от его NAT-ирования.

а вот ксатти как этот поток выглядит ст очки зрения tcpdump. напомню что тисипидамп
ловит поток если исходящий то уже после обработки его НАТ правилами. а если входящий
поток то ДО обработки его НАТ правлами.

    (ноутбук 192.168.51.1) # tcpdump -i wlp2s0 -n tcp port 21
    192.168.51.1.51582 > 64.50.236.52.21

(ну это так чисто для српавлки я првиел)

таким образом я поииследовал чем отличается сетеовй поток котоый влетает  в комп и он 
проксируется на этом компе и высирается наружу либо он натируется на этом компе и высирвется
наружу. в чем разница такой обработки.

получется если поток проксируется то он прям "терминируется"  на юзер процессе(тоесть он умирает заканчивается на юзер процессе. и юзер процесс вытаскивает конеткнт из потока. и далее юзер процесс
создает новый поток и туда перекладаывает карго из певрого потока. и получается два тцп потока
у юзер процесса. и с точки зрения ядра тоже у нас два потока).
а если он натится то он до юзер процессе недолетает  а просто его ядро модифицирует
и высирает наружу и с точки зрения ядра и коннтрак у нас один поток. 

значит есть wireshark но это графическая прога и также пакета pacman в манжаро нет.
есть вместо эттго консольная утилита tshark и можно в ней записать трафик в файл вот так

    # sudo tshark -i eth0 -w test.pcap -F libpcap

правда проблема в том что если я буду трафик как то фильтровать то к сожалению
тогда его в файл прога откажется запмываться.

а после как мы записали трафик в файл то его можно загрузит в другую прогу она есть 
аналог wireshark но работает исклюичетьно сразу в терминале через псевдографику.
это прога

    termshark

в ней фильтры срабатывают точно такие как и в вайршарк

некая неочень хорошая инстрауция от нее 
    
    https://github.com/gcla/termshark/blob/master/docs/UserGuide.md#filtering

пример фильтроов от вайршарк

    https://wiki.wireshark.org/DisplayFilters

как я уже писал пакет tinyproxy который дает функционал HTTP прокси

статья про вайршарк и прочее в арч

  https://wiki.archlinux.org/title/Wireshark



значит вот я хочу потерстировать прокси через curl. для этого мне хочется удобно видеть 
какой трафик сам curl пихиает   в сетевой сокет. без всяких там вайршарков. и дейтсиельно
это можно удобно видеть если заюзать socat.
вот так запускаем socat

  $ sudo socat -v tcp-listen:8080,fork tcp:172.16.10.11:2080

или даже вот так чтобы только ipv4

  $ sudo socat -v tcp4-listen:8080,fork tcp:172.16.10.11:2080

опция fork нужна для того чтобы когда в него прилетает новый конект то он его 
обслуживал в рамках олдльеного нового прцесса. чобы если конект закроет клиент
то процесс погибает но родетлський процесс работает как ни в чем ни бывало. чтобы
его неперезапускать.

тогда он вешается слушать tcp на 0.0.0.0:8080  и работает как прозрачный прокси.
тоесть все что в него влетает через тцп конект имеется ввиу пейлоад от тцп пакетов он 
это вытаскивает и всатвляет в новый тцп пакет который связан с новым тцп конектом который
идет на   172.16.10.11:2080 где у меня работает SOCKS/HTTP прокси.

тоесть socat это реально тоже прокси только он нетребует от приложения разговаривать с ним
по протоколу прокси ему это ненадо потому что он в отличие от протоколов прокси делает
проксирование только на один фиксированный IP:port а так это реальное проксирование.
и плюс его в том что он все то что получает в себя он на экране печатает. а это супер то что
надо. таким макаром я вижу какой контент curl пихает в сетевой сокет.

далее я запускаб curl вот так

  $ curl -4 -x socks4://127.0.0.1:8080 -p   http://google.com 

либо вот так 

  $ curl -4 -x http://127.0.0.1:8080 -p   http://google.com 

тоест курл связыватся с socat а тот прозрачно передает тот что внутри тцп пакетов 
уже далее на прокси сервер используя новый тцп конект и на терминале где висит сокат
я вижу каакой контент протекает через него

ну неуоодобно то что когда курл получает ответ от веб сервера то он берет и сам закрывает
конект и прекращаеьт свою жизнь. чтобы сделать что курл висит бесконечно чтобы
можно было его поииследовать надо еше вот как сделать. 
надо запустить nc

  #  while 1; do nc -l -p 7777; done

он позволяет усатнвить с ним тцп конект и он его незакрывает. и висит так бесконечно долго.
тогда curl надо модифицирвать и запускать вот так

  $ curl -4 -x socks4://127.0.0.1:8080 -p  --noproxy "localhost"   http://google.com   http://localhost:7777

тоесть мы ему говорми что нужно ходить в сеть через прокси. но если доменное равно localhost
то прокси не юзай иди напрямую. далее мы просим его открыть две ссылки
вначале эту

  http://google.com

а потом вот эту 

  http://localhost:7777

первая будет открыта чере прокси. а вторая напрямую. и посколоку от втторого "вебсервера"
небудет ответа но тцп конкт будет то курл будет висеть и ждать ответа и незакрываться.
поэтому его удобно в это время исскледовать



кстати про формат строки в conntrack

  tcp      6 431994 ESTABLISHED src=172.16.10.11 dst=146.190.207.220 sport=33794 dport=443 src=146.190.207.220 dst=192.168.51.1 sport=443 dport=33794 [ASSURED] mark=0 use=1

как видно у нас две пары src+dest и src+dest так вот первая пара

  src=172.16.10.11 dst=146.190.207.220 sport=33794 dport=443

а вторая пара 

  src=146.190.207.220 dst=192.168.51.1 sport=443 dport=33794

так вот если соединенеие ИСХОДЯЩЕЕ то первая пара и src будет IP локальной сетевой карты.
а если оно ВХОДЯЩЕЕ то в первой паре src IP будет какогото компа из интернета.
поэтому в данном случае показано исходящее с компа соединение.
щас покажу на еще одном примере. я сижу на виртуалке и делаю входящее содениенеие по ssh
для виртуалки это входящее содениение а для хоста это исходящее посмотрим как выглядит
конект в двух случаях
смотрим на хосте. ищем сам конект

    #lsof -Pn -p 1515986   | grep TCP
ssh     1515986  TCP 172.16.10.1:56016->172.16.10.11:22 (ESTABLISHED)

ищем конект в conntrack
    # conntrack -L | grep 56016
tcp      6 431994 ESTABLISHED src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22 src=172.16.10.11 dst=172.16.10.1 sport=22 dport=56016 [ASSURED] mark=0 use=1

смотрим чему равно первая пара

    src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22

и вот реально это исходящее соединение потому что в src стоит IP локального компа 172.16.10.1
а в dst стоит IP удаленного компа 172.16.10.11

теперь смотрим как этот поток выглядит  изнутри вируталки

  # echo $$
  584

  # pstree -Asp 584
  systemd(1)---sshd(474)---sshd(560)---sshd(566)---bash(567)---sudo(582)---sudo(583)---bash(584)---pstree(641)

  # lsof -Pn -p 566 | grep TCP
  sshd    566 noroot    4u  IPv4              18466      0t0    TCP 172.16.10.11:22->172.16.10.1:56016 (ESTABLISHED)

  # conntrack -L | grep 56016
  tcp      6 431999 ESTABLISHED src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22 src=172.16.10.11 dst=172.16.10.1 sport=22 dport=56016 [ASSURED] mark=0 use=1

смотрим первую пару 

  src=172.16.10.1 dst=172.16.10.11 sport=56016 dport=22

и смотрим чему равен src 172.16.10.1 это ип адре удаленного компа. и это подтрвеждает что
это входящее содениениение. ксати порт 56016 такойже как порт на коннкете не хосте что
еще раз потдрвеждает что это тот же самый поток.




кстати еще полезный контент про прокси я написал в файле apt.txt ищи по "| proxy"




далее. из коннтрак можно увиетт какие конекты подвергаются NAT преобразованиям.
вот например 

# conntrack -L -n
tcp      6 431990 ESTABLISHED src=172.16.10.11 dst=146.190.207.220 sport=56308 dport=443 src=146.190.207.220 dst=192.168.51.1 sport=443 dport=56308 [ASSURED] mark=0 use=1

смотрим на первую пару 

  src=172.16.10.11 dst=146.190.207.220 sport=56308 dport=443

смотрим на вторую пару 

  src=146.190.207.220 dst=192.168.51.1 sport=443 dport=56308

так вот эти пары покывают вид пакетов котоырые в этом потоке влетают  и вылетают из
компа. если нат преоборазвания нет то они будутт зеркальны. а если есть то небудут зеркаьны.
что и видно.
значит мы видим что в наш комп пакет влетает в виде

  src=172.16.10.11 dst=146.190.207.220

потом он вылетает из компа 
а влетает обратно в наш комп в виде 

  src=146.190.207.220 dst=192.168.51.1

тоесть видно что кода комп из инета стреиться к нам обратно то он имеет dst=192.168.51.1
а не 172.16.10.11 . тоест имеет место NAT. в данном случае SNAT тоесть кода пакет вытает
из компа то у него меняяется src=172.16.10.11 на src=192.168.51.1 поэтому обратный пакет
или летит на 192.168.51.1



далее.
я походу понял смысл настроек в файрфоксе в плане разделения у них  в меню 
на HTTP прокси и HTTPS прокси. дело вот в чем. я пока незнаю на освое какого
документа но походу пьесы HTTP проткол позвоялет  создавать прокси двух видов.
один прокси он для https ссылок и работает через CONNECT. и для такого прокси процесса
запрос от клиента выгдяит как тцп конект через которы будут проукаычиваться байты. 
а что там внутри байтов непонятно. тоесть клиет подлкючается. говоритсоединись  с сайтом
vsya.com , мы дл клиента звоним на vasya.com по тцп. а далее через этот тцп тупо 
релейим форвардим те байты которые лиент шлет по тцп.  если мы даже подозреваем что 
внутри потока байтов както там скрыт http запрос наш прокси процесс никак не может
это разгдяеть потом учто например хттп запрс зашифрован через TLS. так вот минус 
такого прокси подхода в том что мы неможем на прокси кэшировать запросы от клиентов. 
потому что никак нельзя понять что за запросы шлет клиент. поэтому для таких https
запроссов юзается простой кондовый HTTP CONNECT прокси.
а вот если к нам приетает прям HTTP запрос напрокси то мы его можем кэшировать потому что
мы на прокси процессе видим какой HTTP запрос мы пересылаем. ну есствеенно что прокси длжен
понимать как выглдяит формат HTTP запросов. и тогда мы можем на прокси наачть кэшировать ответы
которые приетаюи из интета. и если клиент запрашивает то что у нас уже хранится в кеше 
мы можем без лазииния в инет ему отдать . например такой прокси умыйн это squid. 
поэтому для http запросов можно юзать отдеьный пркси сверер. вот пооэтому в файрвофокс и идет
разделение на http прокси и https прокси.  но это дебилная форулироваька. потому что оба 
этих прокси это HTTP прокси.  было бы правльне в в файрооксе в меню назвать так

   http urls via http proxy
   https urls via http proxy (more smart proxy with cache)

тоест форулироваь в файрфокс меню очень некоретная. 

а вот насчет можно ли через squid кэшировать TLS трафик
Unless a proxy is intercepting the HTTPS traffic (i.e. SSL bump) and thus gets access to the decrypted content, it cannot cache the traffic. When just being a non-intercepting HTTPS proxy squid will just build a tunnel to the final server whenever a client issues a CONNECT request and will forward all traffic without any changes. Even if the client would access the same resource again the proxy will not realize this since the traffic is encrypted differently, i.e. different encryption key and initialization vector. Similar it would not be possible to reply with a cached (encrypted) response since the encryption key used between client and final server would be different from the last access to this resource.

хотя можно все таки затерминировать TLS на сквиде используя SSL_BUMP фичу. 
читаю https://wiki.squid-cache.org/Features/SslBump
  Squid-in-the-middle decryption and encryption of straight CONNECT and transparently redirected SSL traffic, using configurable CA certificates. While decrypted, the traffic can be analyzed, blocked, or adapted using regular Squid features such as ICAP and eCAP.

как я понимаю сквид будет налету гениррвать фейковые сертфиикаты которые он удет подпсиывать
своим CA.  а далее уже сам от себя делать конект на внешние сайты. тут я вижу хрень в том
что ну типа браузеру я доверяю как он проверяет валидность сертфиктаа а вот как это будет 
делать сквид это еще вопрос и как он будет меня информиррвать об этом... 


далее 
HTTP прокси проктолол тоесть по факту CONNECT диркетива она позволяет нам проксировать нетолько
HTTP\S протокол но и любой протокол который базируется на TCP ровно также как это позволяет 
SOCKS протокол
например возьмем curl как пронрамму которая умеет делаь HTTP реквесты 
и в том числе курл умеет работать через HTTP прокси и также курл умеет рабоатть через FTP
протокол который как раз таки работает поверх TCP тоесть я хоу показть доказать что HTTP proxy
позволет проксировать нетлко HTTP\S но и другие TCP based протколы. главное чтобы клиент
который звонит на hTTP прокси умел с ними рабоать.
curl умеет ходиьт на ftp
щас тогда запускаем curl через HTTP прокси и чтобы курл зашел по ftp протоколку пропущенному
через прокси на фтп сервер

запускаем соккат чтобы видеть трафик высираемый курл хотя курл может нам его
и так показать через ключ -v

  # socat -v tcp-listen:8080,fork tcp:172.16.10.11:2080


запускаем curl 

  $ curl -4 -p  -x http://127.0.0.1:8080  ftp://ftp.slackware.com -v


хотя можно и просто вот так без сокат

  $ curl -4 -p  -x http://172.16.10.11:2080  ftp://ftp.slackware.com -v


вот что шлет curl на пркси сервер через HTTP протокол

  CONNECT ftp.slackware.com:21 HTTP/1.1
  Host: ftp.slackware.com:21
  User-Agent: curl/8.10.1
  Proxy-Connection: Keep-Alive

тоесть идет CONNECT запрос 

прокси срвер нам шлет ответ  тоже через HTTP протокол

  HTTP/1.1 200 Connection established

а дальше уже тупо клиент начинает на прокси сервер через тцп конект слать байты в формате
ftp прткола а прокси сервер тупо это переслыает на уденный фтп сервер 

220-\r
220----------------------------------------------------------------------------\r
220-                      R S Y N C . O S U O S L . O R G\r
220-                          Oregon State University\r
220-                              Open Source Lab\r
220-\r
220-       Unauthorized use is prohibited - violators will be prosecuted\r
220----------------------------------------------------------------------------\r
...
...
226 Directory send OK.
QUIT\r
...
...
221 Goodbye.



про ключ -p у курл  . ключ -p   заставялет курл когда мы ему указали юзать HTTP proxy через 
-x http://172.16.10.11:2080 посылать на HTTP прокси сервер именно CONNECT запрос.
потому что я еще не понял почему но если мы указали курл и другим веб клиентам юзать HTTP
прокси и открыает http:// урл то веб клиент вместо CONNECT  послылает GET запрос на прокси
сервер. да он отличатеся от обычного GET запроса но тем неменее! прокси сервер понимает
этот запрос и проксирует его. итак -x http://172.16.10.11:2080 без ключа -p породит
то что курл на прокси пошлет GET запрос. а если -p -x http://172.16.10.11:2080 то курл
пошлет на прокси CONNECT запрос.  ну и плюс такой схемы в том что : вот когда мы послыаем CONNECT
запрос на прокси серввер то он устнвлаие конект с удаелнеемы компом. далее он сообщает клиенту
что тцп конект установлен и только потом кллиент шллет прокси сревреру тот самый запрос на
уаленный срервер. а прокси он уже не анализирует что ему шлет клиент   а тупо эти байты преслыает
на удаленный срврер. если же вместо CONNECT клиент присылвает на прокси уже прям конечный запрос
на удаленный сервер то прокси сразу видит и понимает и конечный запрос клтента на удаленный
сервер и он его  уже от себя зпускает к тому удаленному сервреру и прокси ополуается что знает 
какой был запрос. и может сохранить ответ от сврере. и если потом от лиента прилетит тот же
самйы запрос то пркси саомжет клиенте отдать ответ из кэша. 

тоесть вот так отрабывает схема при 

  -p -x http://172.16.10.11:2080 

первый шаг. клиет шлет на прокси
  > CONNECT google.com:80 HTTP/1.1
  > Host: google.com:80
  > User-Agent: curl/8.10.1
  > Proxy-Connection: Keep-Alive

прокси делает пока просто тцп конект на удаленый сревер  google.com:80 
и клиенту возврашет ответ что тцп конект устанолвен  с гуглом

  <HTTP/1.1 200 Connection established

теперь на втором шаге клиент на прокси шлет свой рельный запрос
  > GET / HTTP/1.1
  > Host: google.com
  > User-Agent: curl/8.10.1
  > Accept: */*

прокси сревер уже не анализирует не парсит что мы шлем. он тупо эти байты
пересылет на удаленный сервер через тца конект.
веб суврер ответчает и прокси нам возварщает овтет

  < HTTP/1.1 301 Moved Permanently
  < Location: http://www.google.com/
  < Content-Type: text/html; charset=UTF-8
  ...
  ...


атеперь как работает схема без ключа -p 
запускаю я вот такой курл

    $ curl -x http://127.0.0.1:9999  http://example.ex:8080/index.html  -v

я ломлюсь на example.ex через прокси  127.0.0.1:9999 (прокси у меня это tinyproxy который
пишет логи в journald). еще раз замечу что ключ -p в курл отсуствует. значит веб клиент
тоесть курл будет обащаться на прокси сервер точно не через CONNECT директиву. а 
как то подругому
и вот что видно как курл звонит на прокси сервер за счет ключа -v

  > GET http://example.ex:8080/index.html HTTP/1.1
  > Host: example.ex:8080
  > User-Agent: curl/8.10.1
  > Accept: */*
  > Proxy-Connection: Keep-Alive


мы еще вернемся к анализу этого запроса
далее tinyproxy в journald фиксирует какой приелет в него запрос

  tinyproxy Request: GET http://example.ex:8080/index.html HTTP/1.1

что полностью совпадает что нам написал curl

далее домен example.ex  в /etc/hosts записано что имеет ip=127.0.0.1 так что тинипрокси
направит свой запрос на 127.0.0.1:8080 но там сидит не веб сервер а сокат вот такой 

    # socat -v tcp4-listen:8080,fork tcp:127.0.0.1:8888


тоесть он слушает 127.0.0.1:8080 и прозрачно передает поток на 127.0.0.1:8888
а уже там у меня сидит жинкс
наличие между тинипрокси и жинксом соката дает то что мы увидим на его экране как выглядит 
трафик который тинипрокси направляет на веб сервер а сокат позволяет визуализировать этот трафик

так вот вот что покажет сокат

    GET / HTTP/1.1\r
    Host: example.ex:8080\r
    Connection: close\r
    Via: 1.1 tinyproxy (tinyproxy/1.11.2)\r
    User-Agent: curl/8.10.1\r
    Accept: */*\r


это тот трафик который тинипрокси из себя высирает

итак еще раз сравним трафик который веб клиент сует на HTTP прокси его самые 
главные строчки

    > GET http://example.ex:8080/index.html HTTP/1.1
    > Host: example.ex:8080

и трафик который тинипрокси шлет на веб сайт

    GET / HTTP/1.1
    Host: example.ex:8080

тоесть разница только в строке GET

так вот я прочитал про метод GET здесь

    https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html

там написано что в нем укзывается так назвыемый Request-URI
а что же такое Request-URI я прочитал вот тут

    https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html

так вот Request-URI это URI от ресурса. 
значит написано что если ури идет в форме аболютного(полного) пути то это всегда означает
что этот реквест предназначен для ПРОКСИ. тоесть если мой веб клиент хочет обратиться на 
веб сайт то он в своем запросе недолжен никогда юзать полный путь. полный путь это когда 
у нас в запросе указан домен. если клиент в запросе формирует полный путь то это значит
что наш клиент собирается сделать свой запрос направить именно на прокси сервер а не на
конечный сервер.  это что касается отправик. если у нас есть процесс и он получил веб 
запрос с полным путем то это значит что тот кто ему этот запрос направил думает что 
наш процесс это прокси. HTTP прокси. и процесс если он реально прокси должен нарпавить
этот реквест на таргет сервер. либо выдать ответ из своего кэша. так написано в хттп стандарте.
также хттп прокси имеет право направить запрос на другой прокси сервер кстати!
также написано что если запрос поступил на веб сервер то веб сервер обязан принять этот 
запрос единственное что я не понял что наш веб сервер приняв такой запрос должен с ним делать.
хорошо если запрос реаьно хочет получить ответ с домена который наш веб сервер обслуживает 
а если наш веб сервер домен из запроса не обслуживает тогда что? надо проверять!

теперь становится понятно почему запрос от курл программы на хттп прокси выглядит так

    > GET http://example.ex:8080/index.html HTTP/1.1
    > Host: example.ex:8080


наш курл должен составить реквест на хттп прокси. вот он и составил. он указал полный 
аболсютный путь к ресурсу в URI. который только и используется в HTTP для того чтобы 
сформировать реквест к хттп прокси серверу! все верно!  реквест означает эй прокси сервер
доставь ка мне ресурс /index.html c веб сайта  example.ex
тут правда такая стремная тема. у нас есть сервер до которого надо по тцп достучаться.
сервер вобще то нам нужен в форме IP. а если нам дали его в форме домена то мы резолвим 
его в IP. а потом добравшись до сервера церез tcp конект (который в себя включает IP) 
мы там уже сообщаем процессу название веб сайта на который мы звоним и адрес ресурса.
так вот доменное имя сервера и домен веб сайта вообще то необязаны совпадать. 
наскоько я понимаю доменное имя сервера задается в строке с GET  

    example.ex:8080

причем тамже указан и tcp порт. именно в этом месте мы узанем доменное имя именно сервера!
и тцп порт. и можем тогда устанвливаеть тцп конект. а имя веб сайта мы узнаем из строки 

    Host: example.ex:8080

правда зачем в этой строке тцп порт непонятно. ибо инфомрация из этой строки она передается
уже внутрь процесса. уже выбор порта позади. ну да ладно.
в нашем конкетно случае доменное имя сервера и домен веб сайта к счастью совпадают.
кстати согласно HTTP/1.1 поле все что ниже первой строчки это назыается header запроса.
тоесть в первой строке запрос. далее хидер. далее уже тело запроса.
так вот в хидере обязательно всегда должна быть строка 

      > Host: ...


итак еще раз смотрим на запрос который курл отослал на хттп прокси

  > GET http://example.ex:8080/index.html HTTP/1.1
  > Host: example.ex:8080



теперь понятно почему запрос так выглядит. потому что мы сказали курлу чтобы он послал запрос
http://example.ex:8080/index.html  ненапрмую а через прокси и не просто прокси а хттп прокси.
а в http стандарте написано что если мы хотим направить запрос на хттп прокси то для этого 
надо взять метод который  бы мы использовали для досутпа на конечный сервер например GET
и там использовать не относительный URI а абсолютный. 

    GET http://example.ex:8080/index.html HTTP/1.1

(HTTP 1.1 на конце обьясняет процессу который примет запрос что он прилетел  в формате HTTP/1.1)

и все такой запрос и является прокси запросом!  или форматом запроса через хттп прокси!

тоесть если мы знаем адрес хттп прокси и хотим через него получить ответ мы берем составляем
обычный HTTP запрос как бутоо мы его ооправяем на обычный веб сервер. 
а в конце когда запрос составили мы просто напросто  в нем добавляем имя сервера и порт 
в первой строке  в URI! и все запрос к http серверу через хттп прокси готов!

сооттсвенно наш прокси сервер этот запрос принимает и в логах journald об этом
отчитывается 

    GET http://example.ex:8080/index.html HTTP/1.1

а сам прокси сервер после  этого уже от себя запускает "обычный" хттп запрос на конечный
сервер


    GET / HTTP/1.1
    Host: example.ex:8080


как мы видим это уже "обычный" хттп запрос. ибо здесь в первой строке мы видим обычный
относительный URI=/


поэтму сумарно получается что HTTP протокол включает в себя прокси субпротокол так скажем.
тоесть в рамках хттп протокола можно делать прокси запросы. и делать это можно двумя
путями. первый путь это когда мы формируем обычный хттп запрос и в реквесте используем абсол
ютный URI. а второй путь это когда мы юзаем метод CONNECT
насколлко я понимаю если мы говорим HTTP прокси то он должен понимать оба формата запросов.
запросы через CONNECT они более универсальные. они позволяют проксировать через прокси любые
пртоколы базирующиеся на TCP. минус в том что роль прокси тольк в том что он создает тцп
конект до конечного сервера а что через этот тцп конект потом передается он не анализирует. 
он это рассматривает просто как поток байтов. если у нас в потоке байтов используется шифрование
тодаже для одинакового запроса из за шфирования поток байтовы каждй раз будет другой поэтому
при таком способе прокисрования у нас прокси сервер не может кэшировать запросы и ответы
потому что он просто нихрена их не понимает так скажем. еще раз подчеркну что такой способ
позволяет приложению проксировать (тоесть прогонять черз прокси срвер)любые протколы базирую
щтеся на TCP тоесть это и HTTP и HTTPS(HTTP over TLS) и SMTP и FTP (правда FTP испоьзует
несколько а именно два тцп конекта но по крайней мере общаться с фтп серером через дата канал).
а вот проксирование через обычные
HTTP запросы с ипользованием аболютных URI дает возможность проксировать только HTTP запросы.
тоесть такой метод подразумевает что хотя и через прокси но мы хотим всегда связаться
именно с веб сервером на той стороне. именно веб сервером и ничем другим. мы конечно можем
в запросе указать порт  за которым сидит скажем FTP сервис например 

    GET http://example.ex:21/index.html HTTP/1.1

но фишка в том что смысла в этом нет. потому что прокси наш создаст тцп конект 
вот сюда  example.ex:21
и потом через этот тцп конект передаст вот такой запрос


    GET / HTTP/1.1
    Host: example.ex:21

такой текст прилетит в фтп сервис и он просто не поймет что за хрень ему прислали

на счет того что можно ли HTTPS прогнать через хттп прокси через обычные HTTP запросы с абсолют
ным URI. ответ нет. потому что : что значит HTTPS это значит что на первом этапе клиент
и удаленный сервер уставливают TLS сеанс через тцп конект. грубо говоря договариваются каким
ключом симеетрчиного шифрвания будут через тцп конект обениваться шифрованной информацией.
а уже на втором шаге наша прога генериует HTTP запрос, потом его шифрует через TLS ключ и потом
уже пихает в тцп конект. так вот проблема в том что установка TLS конекта никак не может 
быть сделана через HTTP реквесты. ну никак! мы неможем послать реквест вида 


    > GET http://example.ex:8080/index.html HTTP/1.1
    > Host: example.ex:8080

тоесть причем здесь этот реквест и TLS сеанс. нипричем.

поэтому итого - с помощью этого метода можно проксировать только самые что ни на есть HTTP 
реквесты и больше никакие! тоесть через этот метод можно прогонять через прокси только HTTP
реквесты и все!

итак я разобрал два метода проксирования через HTTP протокол как протокол проксирования.
поэттому HTTP via TLS aka HTTPS у нас работает только через метод CONNECT.
у нас клиент шлет на прокси сервер HTTP реквест CONNECT


    > CONNECT google.com:443 HTTP/1.1
    > Host: google.com:443
    > User-Agent: curl/8.10.1
    > Proxy-Connection: Keep-Alive

прокси сервер открвает тцп конект  к   google.com:443
и шлет обратно к клиенту ответ о том что это получилось

  < HTTP/1.1 200 Connection established

после этого курл начинает в тцп конект кидать байты по протоколу tls на прокси сервер.
а тот эти байты тупо пререклывдает в тцп конект до веб сервера. 
сервер и клиент обмениваются tls байтами туда и сюда исольузу прокси серер тупо как тцп релей.
когда tls хрень заканчиватеся. то тогда курл уже генерирует обычный HTTP реквест

> GET /index.html HTTP/2
> Host: google.com
> User-Agent: curl/8.10.1
> Accept: */*


потом обоарчивает его в TLS шифрование. и этот поток байтов бросает в тцп конект к прокси.
а тот этот поток байтов прееклдывает в тцп конект до сервера где веб сайт.

вот так эта хрень работает.
ктстати я понял смысл ключа -p  в курл  
он предписывает курлу использовать метод CONNECT при обращении к хттп прокси даже в
случае когда я хочу проксировать HTTP протокол. потому что без этого ключа наш курл
при обрашении к хттп прокси бы использовал обычные хттп методы ( а не CONNECT) плюс абсолют
ные URI

тут надо еще раз подчеркнуть что у нас есть протокол проксирования через котоырй клиент
общается с прокси сервисом.  а есть протокол который мы хотим передавать через протокол
проксирования. вот это надо различать.  также надо понимать что если протокол проксирования HTTP
то в рамках этого протокола ест два метода проксирования. это тоже надо понимать

теперь становится понятно смысл меню в файрфокс где написано 

    HTTP  proxy:
    HTTPS proxy:
    SOCKS host:


первый и второй прокси означает на самом деле один и тот же тип прокси или что тоже самое
один и тотже протокол проксирования HTTP. разница в том что вторая строчка обозначает прокси
сервер к которому файрфокс будет обращатсья через HTTP метод CONNECT. а первая
строчка указывает хттп прокси сервер к которому будет идти обращение через другой метод
проксирования тоже через HTTP проктол но через другие http методы (GET, POST, DELETE) плюс
использовать абсолютные URI.  прикол в том что через первую строчку будут проксироваться только
HTTP запросы. тоесть урлы которые имеют вид http://... будут проксирваться через первую строку.
а урлы вида https:// или ftp:// или все другие урлы кроме урлов http:// будут проксироваться
через вторую строку. 

если я пропишу IP:port только в первой строчке то файрфокс сможет мне октрывать толкьо 
урлы вида http:// и больше никакие в том числе он не сможет открыать урлы вида https://

если я пропишу IP:port только в второй строке то он сможет открывать любые урлы.

если указать их оба то урлы вида http:// будет клиент октрыать через сервер в первой строке
а все остальыне урлы через сервер во второй строке

через третью строчку SOCKS можно открывать любы урлы.
не очень понятно какие строчки будет юзать клиент если я пропишу серверы для всех трех строчек
сразу например 



    HTTP  proxy: 1.2.3.4:1080
    HTTPS proxy: 2.4.5.6:2080
    SOCKS host:  8.9.1.1:3333

вот непоняно какой из этих сервров будет использоваться файрфоксом для конекта.



некокторые вопросы после темы:
>а можно как то curl перенаправить в nc а уже с него в интернет. чтобы через nc наблюдать
дамп того что высирает curl ?

прям такого сделать нельзя.  потому что как программа ходить в инет. она делает запрос
к ядру открыть тцп сокет. ядро возвраещает файл дескрпитор. потом прога в частности
курл пихает данные в этот дескпритор.  каждый раз когда мы идем на другой сайт
то прога открыает новое тцп соедиение а значит получает новый дескприптор по номеру.
ну получется надо чтобы наш nc както прехыватывал трафик котоырый прога пишет в этот 
дескриптор. при том что каждый раз ноер его другой. вот мы привыкли через pipe связывать
проги но это всего навсего дескприор 1 через ядро увызывается с дескрпитором 0 у другого
прцоесса. повторю что курл при конекте на удаленный сайт делает это через тцп конект а значит
для этого ядро выдывает курлу новый файл дескриптор с номером нам заранее неизвстным. поэтому
пайп не прокатит.короче невозможно фигня. 
выход другой. мы говорим проге чтобы она трафик свой пускала через прокси сервер 127.0.0.1:8888
на этот бинд мы сажаем nc или socat и говорим ему чтобы он входящий трафик перенправлял  а точнее
прозарчно проксровал на 127.0.0.1:1080 где уже сидит наш прокси сервер
тогда курл посылает сетевой поток всегда на 127.0.0.1:8888 там его принмиает nc\socat
и рисует на экране что там прилетело. и потом через новый тцп конект передеает контент 
который прилет от клиента по тцп в новый тцп конект до 127.0.0.1:1080 где конент принимает
прокси сервер.  а мы наэкране терминала где запущен nc\socat наблюдаем наш трафик который 
приложение пихает в свой файл дескриптор. тоесть тот поток который лежит выше TCP
вот строчка для этого через socat

  
    # socat -v tcp4-listen:8080,fork tcp:172.16.10.11:2080

сокат слушает порт tcp 8080 и то что получает оттуда вытаскивает пейлоаз тцп пакета
и переклывает в новый тцп пакет который летит в тцп конект к 172.16.10.11:2080 тоесть 
это такой tcp relay. или tcp forward. это аналогично DNAT через iptables. только иптейблс
у нас процессит исключительно ядро. а здесь мы еще юзер процесс привлекаем.  в принципе
запуск сокат требет прав рут поэтому по сути без разницы что добавит DNAT правило в иптейлбс
что запустить сокат. 
fork нужен для ттго что каждй входяший тцп конет будет обслуживаться клоном этого процесса
поэтому он может принимать все новые и нвоые входящие конекты. а закрытие конекта на клеинте
неубивает голвной процесс. удобно

через nc это делаетс вот так. а оказывается nc так делать не умеет. не умеет он делать 
tcp форвадинг. и для этого сокат подходит гораздо лучше

    https://unix.stackexchange.com/questions/293304/using-netcat-for-port-forwarding


предодположим у нас приложение не умеет разговариать на языке прокси протоколов.
нужно это приложение связать с удаленным сервером но напрямую не получится а можно
только через прокси сервер. но как это сделать если приложение не умеет говорить на языке
прокси протоколов. есть прогрммы соксификаторы. они принимают конект как тцп. а потом
сами свызываются с прокси сервером разгоывариют с ним на прокси пртколе.  а потом от клиента
просто пересыалают его байты. так вот socat умеет быть соксификатором. смотри справку.
также есть прграмма redsocks я ее описал в какомто другом месте . она тоже умеет.


> fireofox у него настройки есть HTTP/HTTPS прокси.  - а какая разница?
на это я уже отвеил



> как ssh юзает nc для доступа к серверу через прокси?
поговоим о том что такое nc
как работает что делает nc  в простом обьясеннии.
по дефолту она делает тцп конект к удаленному серверу. а далее вот что. 
если мы чтото пришлем на STDIN этой проги то она это перешлет через тцп конект
удаленному процессу. если удаленнй процесс чтото нам пришлет по тцп конекту то прога
нам это вывдедет на терминал. вот так просто!

  $ nc 127.0.0.1 8888  
GET / HTTP/1.1 <====
Host: localhost <====

HTTP/1.1 301 Moved Permanently
Server: nginx/1.26.2
Date: Sat, 09 Nov 2024 17:06:05 GMT
Content-Type: text/html
Content-Length: 169
Connection: keep-alive
Location: https://vasya.com

<html>
<head><title>301 Moved Permanently</title></head>
<body>
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx/1.26.2</center>
</body>
</html>


стрелочками я показал какие команды я ввел с клавы на STDIN этого процесса.
он их переслал на дуаленный сервер через тцп конект.
и далее вывод на терминал того что приллслалв ответ дуаленный сервер

тоеть еще раз я запускаю nc указывают ему IP +port он до того сервера делает тцп конект.
далее то что я ввожу в терминале тоесть в его fd/0 онже STDIN эту инфо програ передает 
в тцп конект на удаленнй процесс. а то что даленнй процесс через тцп шлет обратно то 
наша програ выдает на экран

ну кончно можено через пайп передать на STDIN в nc то что мы хотим пераедать на удалный 
сервер


$ echo -e "GET / HTTP/1.1\nHost: localhost\n\n\n" | nc 127.0.0.1 8888 | head -n3 
HTTP/1.1 301 Moved Permanently
Server: nginx/1.26.2
Date: Sat, 09 Nov 2024 17:15:27 GMT


теперь о физическом смысле nc (неткат) - когда мы юзаем cat мы читаем 
 файлы с диска. а nc нам позволяет "читать"  
не  файлы на диска а из тцп конекта IP:port
тоесть это типа как tcpcat по смыслу. плюс помимо читать еще можно и писать

спрашивается нахрен эта программа нужна. какой от нее практичккий смысл. 
ну получается что еще раз работает это так - то что мы этой программе кидаем на STDIN
байты и программа их перекидывает через тцп конект на удаленный процесс. например мы указали
nc чтобы она связалась с  1.2.3.4:21 тоесть это FTP сервер . далее мы кидаем "5678" на
STDIN (через терминал или через pipe) и программа эти байты перебрасывает на  1.2.3.4:21
а то что удаленный тцп сервис обратно присылает через тцп конект на nc то оно нам выдает
на STDOUT 
ну а вопрос   а зачем мне "в ручном" режиме через терминал "разговаривать" с тцп сервисом?
обычная программа разговаривет через тцп с удаленным сервисом через свой код. а теперь
мы разговариваем с удаленным сервисом через тцп "руками". так нахрен нам это надо ?

один из случаев когда эта прогармма нам может помочь в чем то практическом?
есть два случая практических. первый это случай это когда прога может достучаться до 
желаемого сервера но ненарпрямую а только через прокси сервер но наша прога при этом
неумеет разговарить на языке прокси протоколов. тогда между нашей прогой и прокси
сервром всталвяется nc который умеет разговаривать на языке прокси протколоов. ровно
вот такой случай когда мы юзаем ssh+nc хотя ssh на самом деле проекрасно знает 
что такое прокси протоколы. я об этом потом ниже подробно освещаю. второй случай исопльзвания
nc на прктике это чтобы перкопировать файл с хоста на хост без шифрования а так чисто по
быстрому. выглядит это примерно так
на одном хосет запускает nc чтобы он сжал в tar архив кучу файлов и передал этот архив
в nc а тот связывается с nc на другом компе и там мы налету этот тар разархививируем

на хосте-А
   tar ...... | nc 1.2.3.4 -p 8890 
на хосте-<
   nc -k -l 1.2.3.4 -p 8890 | tar .....

подходмт для копирования файлов внутри локалки. в отличие от ssh нет шифрвания поэтому 
гораздо быстрее. плюс ssh требует раскалывать ключи по хостам а тут этого ненадо. 
правда понтное дело что нужно зайти на каждый комп видимо по ссш и там запустить эти команды.
вобщем из за отстуивия шифрования скорость будет суе=щественно больше
третий вариант можно через nc пдключиться к какому то процессу через TCP конект и через 
термиал поощаться с этим сервисом "руками". например пообщаться с веб севрером. 
или написать скрипт. и вместо ручного набирания текста в терминале можно скормит nc через pipe 
этот скрипт и посморетть какой будет ответ от удаленного сервиса.
по сути nc позволяет наладить портал или мостик между окном нашего терминала и тем файл
дескриптором который удаленнй процесс юзает для сетевой передачи. тоесть то что он сует 
в тот файл дескприптор у нас полявлется на экране и наорот то что мы пишем на экране высирается
внутрь процесса через тот файл дескрпитор. 


     "1111" ----->----.........................--->---- "1111"  
     "2222" -----<----.........................---<---- "2222"  
    экран компа                                      удаленный процесс



    экран компа <--> nc <--INTERNET-->(сетевой сокет) удаленный процесс


далее про nc. у него есть ключ -w  значит его смысл. если мы тестируем UDP то как
я уже покаал то прога она шлет символ X в юдп конект. и проверяет код возврата. обычно она
шлет через write() три таких посылки она делает. и если все три write() вовзвращают 1 тоесть 
то что ядро сообщет что оно записало успешно 1 байт. то тогда код возврата у проги успешный.
так вот можно через ключ -w указать проге  в течение какого времени нужно посылать посылать 
и посылать write() запросы. скажем

  $ nc -v -n -u -z -w15  127.0.0.54 53

говорит о том чтобы слать символ X в юдп конект в теение 15 секунд. это суммарное время 
тестирования. между кажым write() прога делает паузу. но сколько это мс я незамерял.

 $ sudo strace -e write nc -v -n -u -z -w15  127.0.0.54 53
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(3, "X", 1)                        = 1
write(2, "Connection to 127.0.0.54", 24Connection to 127.0.0.54) = 24
write(2, " 53 port [udp/*] succeeded!\n", 28 53 port [udp/*] succeeded!
) = 28
+++ exited with 0 +++

унас 18 write() значит интервал между ними ~800мс

через этот ключ я так понимаю можно тестировать насколько стабильная связь по интернету
есть с этим юдп конектом. если write хоть какойто вовзратит ошибку в коде возврата
то прога сразу выходит и код возврата будет 1

пример
  $ echo -n "foo" | nc -u -w1 192.168.1.8 5000

вот эта команда nc она принимает на STDIN через пайп поток байтов и переслыает его по udp
на 192.168.1.8 5000 бинд. в течение одной секунды. за это время она успеет послать три
операции write()

надо помнить что если у нас пакет недолете до удаленного хоста то udp пртокол об этом нам
ничего не скажет а наше ядро оно как определяет что write() прошел успешно при записи в 
файл дескрпитор за которым сидит udp конект. он определет так - если в сетевую карту ядро
смогло высрать число заказынных байтов  - значит операция успешная. поэтому если ядро шлет
юдп пакеты в сеть и они тупо там где то терятся то у нас код возвррата во write() будет 
успешный все классно. мы незнаем что пакеты теряются. а если пакет долетает до удаленного компа
и там ядро видит что ни один процесс UDP порт неслушает то ядро то обратно обязано прислать 
icmp пакет о том что операция неуспешная. но по врвемени для нашей системы это будет уже значиеьнл
позже после того как мы "успешно" согласно коду возврата из write() отправили тот юдп пакет.
но ядро наше нам все равно сообщить об этом icmp если конечно он до нас доолетит. наше ядро
сообщить об ошибке вот как - когда мы попробуем послать очередной пакет через write() то
ядро выдаст нам ошибку. так вот надо понимать что это не оошибка отправления этого пакета
это ошибка о том что какойто прошлый пакет реально дошел до удаелнной системы но был отвергунут.
(для сравнеия если ммы далем write() в tcp конект то ядро возвращает нам резульат в зависиомост
от того был ли пакет от той системы о том что пакет туда реально доставлен. наверное пока ядро
неполучит ответ с той стороны оно для процесса не возвращается из выполенения write() )

вобшем команад котоаря выше она в течение минуты бомбит по юдп цель. успет выпустить три 
write(). по идее за это время если первый пакет долетел но был отвернут мы успеем это словить
нашим ядром. и на третьей команде write() мы получим ошибку. вобщем формально мы за 1с
три раза отправим X на уадаленный юдп конект. а! точнее мы отпрваим не X а "foo"
в обем команда полвзяеть тестировать коннективити с уаделнныым юдп процессом. ну если предположить
что у нас надежный интернет и пакеты долетают до конечного хоста. то мы узнаем тогда есть ли
слушаетльна той стоионе или нет. вобще получается вот ткакая ситуация -  у нас с точки зрения 
юдп код отправки юдп пакета всегда  SUCCESS потому что пакетв сетевую карту мы всегда можем
засунуть ПОЭТОМУ если у нас код возварата для write() какйото фейл. в частности ошибка 

    ECONNREFUSED (Connection refused)

то это 100% значит что на той стороне нет процесса котоырйбы слушал этот порт!
тоесть вот мы шлем пакеты одним за одим на удаленный юдп конект. у нас хреноый интернет.
куча пакетов просто недолает пропадает. у нас коды возварта для write() = успешно.
но если мы будем бомбитт пакетами и дальше то в конце концеов кайото пакет долетит до 
конечного хоста! и если там нет слушащего процесса то ядро обратно отарвит icmp отлуп.
опят же - он может недоллетет. мы тогда будем очень долго бомбить ээтот юдп коенкт. и наконец
и обратно icmp пакет долетит. и мы наонец узнаем равду что там никто неслушает. 
поэтому я бы как сказал - заряжаем бмобление этого биндина на 60с.  за это время точно до 
конечного хоста пакет долетит. и если там никто неслушает наш порт то 
ядро пошлет обаррантынй пакет и с какогото раза обратный пакет к нам прорверсятся. и мы 
мы получим отлуп. и мы узнаем что стой сторныихерна никто нелслушает наш порт. если же 
доставка нажедная то мы это узнаем гораздо быстрее. !

вот пример

  $ nc -v -n -u -z -w60  8.8.8.8 556
Connection to 8.8.8.8 556 port [udp/*] succeeded!

я бмобио гул минуту. за это время нц отослал 60-80 паеетов юдп. и мы не поучили обратно 
icmp пакет об ошибке. я поагаю что это доукзаывет что порт "открыт". короткое тестрование
нчего недокажет. нужно обязательно длинное. либо на той сторне так настроено ядро что оно 
неотслыает icmp отлуп. такое тоже может быть

вот пример коприрования файла с хоста на  хост

хост-А
  $ nc -lp 5000 > my.jpg 


хост-Б
  $ nc hostB.com 5000 < my.jpg

ксати вот он делает по tcp. это значит мы 100% можем быть уверены что ни один тцп пакет
не потерялся. за этим проследит ядро. это тот самый реальный юз кейс этой проги

копирование папки

host-A
  $ nc -l 5000 | tar xvf -

host-B
  $ tar cvf - /path/to/dir | nc hostB.com 5000


подчеркну еще ключ -k 
он полезен ели мы юзаем nc с -l тоесть привходящем содениении nc себя форкает
и обсуживает конект в свое клоне. это дает то что когда клиент с той стороны закрывает тцп
конект то умирает клон а гловной прцесс живой. и готов принимать новые коенккты. 
без ключа -k нам придется после одного конекта заново запускать nc -l

а вот как брать просто байты и передатьвать их по сети 

host-A
  $ nc -l -p 5000 | gzip > 1.gz


host-B
  $ dd if=/dev/hda ...| nc  -p 5000 
или 
  $ echo "123" | nc  -p 5000 
или
  $ dd if=/dev/sdb | gzip -c | nc remote_server.com 5000


а вот наоброт берем имадж и по сети его передаем  а там мы его раскрываем и пишем
на диск

хост-А
  $ cat /backup/sdb.img.gz | nc my_local_host.com 5000

хост-Б
  $ nc -lp 5000 | gunzip -c | sudo dd of=/dev/sdb

кстати примеры беру отсюда
  https://gist.github.com/lenciel/8780269

но часьтпримеров хуйня. ошибочные
напрмер он пишет что можно nс использовать как веб срвер отдающий статичекю страницу.
это хуйня. покзываю.

запускаю серверную часть

  $ nc -k -l 5050 < ./index.html

тоесть у нас есть слушающий тцп сокет 5050 порт. и когда клиент установит тцп конект
с этим портом то код этой проги сунет в тцп конект текст из index.html
и ядро этого хоста сунет текст в тцп пакеты и пошлет. а ядро на клиенте эти пакеты в себя
засосет. а далее интеерсно. тцп пакеты с этим текстом сиядя в памяти ядра в области сокета
ядра на компе клиента. так вот клиентская программа получит внутрь себя этот текст только
если клиенсная юзер программа захочет прочитать из файл дескриптора который ведет на тцп сокет.
если клиент не захочет читать не даст команду read() то несмотря на то что текст по сети доставлен
на удаленную машину. он засосан ядром клиенссткой машины. но насилно в юзер программау ядро 
этот текст пихать НЕБУДЕТ!

показываю
запускаб вот это через стрейс

 # strace -f nc -k -l 5050 < ./index.html
socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3
bind(3, {sa_family=AF_INET, sin_port=htons(5050), sin_addr=inet_addr("0.0.0.0")}, 16) = 0
listen(3, 1)                            = 0
accept(...
вот так она висит до подключения от клиента по тцп
когда клиент подключился то уже будет вот так
accept4(3, {sa_family=AF_INET, sin_port=htons(44190), sin_addr=inet_addr("127.0.0.1")}, [128 => 16], SOCK_NONBLOCK) = 4

далее вот так

poll([{fd=0, events=POLLIN}, {fd=4, events=0}, {fd=4, events=POLLIN}, {fd=1, events=0}], 4, -1) = 1 ([{fd=0, revents=POLLIN}])
read(0, "<!DOCTYPE html>\n<html>\n<head>\n<t"..., 16384) = 615
poll([{fd=0, events=POLLIN}, {fd=4, events=POLLOUT}, {fd=4, events=POLLIN}, {fd=1, events=0}], 4, -1) = 2 ([{fd=0, revents=POLLIN}, {fd=4, revents=POLLOUT}])
read(0, "", 15769)                      = 0
write(4, "<!DOCTYPE html>\n<html>\n<head>\n<t"..., 615) = 615

что делает poll()  я незнаю. но видно что сервер читает файл read()
а потом он его пишет в дескриптор закоторым тцп конект write()
все пакты уелетели в сеть

смотрим что на клиенте

   $ sudo strace nc 127.0.0.1 505

socket(AF_INET, SOCK_STREAM|SOCK_NONBLOCK, IPPROTO_TCP) = 3
connect(3, {sa_family=AF_INET, sin_port=htons(5050), sin_addr=inet_addr("127.0.0.1")}, 16) = -1 EINPROGRESS (Operation now in progress)
pselect6(4, NULL, [3], NULL, NULL, NULL) = 1 (out [3])
getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0
poll([{fd=0, events=POLLIN}, {fd=3, events=0}, {fd=3, events=POLLIN}, {fd=1, events=0}], 4, -1) = 1 ([{fd=3, revents=POLLIN}])

вот мы подключаемся прчием неждем пока покдлчюмся а ждем событий через poll()
типа если чтото прилетит на дескпритор который указывает на сокет то ядро нас об этом
извстит

и вот видимо прилетело из сети. и poll() у нас просигнализировал нам. и тода 
программа сама делает read()

read(3, "<!DOCTYPE html>\n<html>\n<head>\n<t"..., 16384) = 615
poll([{fd=0, events=POLLIN}, {fd=3, events=0}, {fd=3, events=POLLIN}, {fd=1, events=POLLOUT}], 4, -1) = 1 ([{fd=1, revents=POLLOUT}])

без этого read() с нашей стороны то что прилетело так и будет лелать в ядре до бесконечнсоти
а потом кода мы счиатли это делоа в память процесса. мы уже делаем write() на экран

write(1, "<!DOCTYPE html>\n<html>\n<head>\n<t"..., 615<!DOCTYPE html>

значит еще раз как сработал схема. наш сервер сидит ждет тцп конект на порт 5050. клиент
сделал этот тцп конект. далее сервер берет файл и пихает его в тцп конект. а он недолжен
этого делать. он должен сидеть и ждать что клиент пришлет через тцп конект HTTP текст с
запросом вида 

GET / HTTP/1.1
Host: 127.0.0.1:5050
User-Agent: curl/8.10.1
Accept: */*

сервер должен этот текст засосать добровольно через read() потом праоанализиоовать и 
только потом в ответ послать непросто файл а вначале он должен сунуть HTTP хидер 
соовствующий и толко после этого сунут уже index.html текст!

так что если мы мыопбнимем вот так сереную часть

    $ nc -k -l 5050 < ./index.html

тоэто соверешенно неделает nc веб сервером! НИХРЕНА!

веб серервер это непросто создать конект по тцп и в него сунуть html текст!
веб сверер это клиент создает тцп конект. потом шлет HTTP текст-запрос. наш сврер его анализирует
правльный ли он и какой ресур он хочет. и потом наш сервер формиурет правлный HTTP хидер. HTTP 
код возврата. и только после этого вставтлвет html текст! вот что такое веб сервер!

если мы хотим сделать жалкое подобие веб сверера то надо измнить файл который он 
высирает из себя на вот такой

HTTP/1.1 200 OK
Server: nginx/1.26.2
Date: Mon, 11 Nov 2024 08:28:29 GMT
Content-Type: text/html
Content-Length: 615
Last-Modified: Fri, 16 Aug 2024 17:52:58 GMT
Connection: keep-alive
ETag: "66bf91fa-267"
Accept-Ranges: bytes

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


тоесть суть какая если к нашему nc подключися настояйщий веб клиент.
то он пошлет нам HTTP запрос. наше ядро его принимает но наш сервер ничего не читает
из дескрпитора. а просто тупо обратно шлет правильные HTTP кодвовзрата + HTTP хидер
+ HTTP текст . и веб клиент это как положено получит считает. и так как это синаткстиески
верно то успешно отрисует в браузере. 
но это пароия на веб сервер. потму что мы не анализируем влетающий HTTP запрс 
а просто вответ шлем правлный HTTP ответ. 

в сттеье указан вот такой пример

$ nc -lp 5000 -e /bin/bash

но у меня в nc нет опции -e. котора бы позволяла выполнять команды. видимо создаетли 
моей версии проги сознательно убрали эту опцию как элемент дыры в безопасности ?

вобщем статья конечно там много ошибок.

далее еще пример

  $ printf "GET / HTTP/1.1\r\nHost: google.com\r\n\r\n" | nc google.com 80 -X connect -x 172.16.10.11:2080
HTTP/1.1 301 Moved Permanently
Location: http://www.google.com/
Content-Type: text/html; charset=UTF-8

я делаю тцп конект к http прокси 172.16.10.11 и прошу его сделать тцп конект к хосту google.com
на tcp port 80
далее на STIN мы присылаем байты в формате HTTP GET реквеста и nc пересылает эти байты используя
http proxy протокол на прокси сервер а он переслыает через тцп конект на google.com:80
там сидит веб сврер. он принимает HTTP реквест анализиурет и прислыает ответ на прокси
сервер тот присылает на nc а тот нам выдает на экран

тоесть данный пример решает вот какую заадчу - как сделать HTTP запрос если на компе нет
браузера или веб клиента и также в интенрнет выход идет только через прокси.

еще пример как копирование файлов сделать
source
    $ tar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso  | pv | nc -l -p 3000 -q 5
dest
    $ nc 192.168.1.4 3000 | pv | tar -zxf -


ключ -q какйото странный
 -q seconds
               after EOF on stdin, wait the specified number of seconds and then quit. If seconds is negative, wait forever (default).  Specifying a non-neg‐
               ative seconds implies -N.

c этим EOF ебала. это хитрая хрень. надо отельно вспоминать

а вот еще вариант создания инвалидского псевдо веб сервера

  $ while : ; do ( echo -ne "HTTP/1.1 200 OK\r\n" ; cat index.html; ) | nc -l -p 8080 ; done



замечу вот еще какую вещь. когда я делаю

  $ nc -l 5050

то таким макаром эта прога создает слюшающий tcp сокет и биндинг. 
и это означает что если ктото хочет связаться с моей прогой по тцп то ОН должен инциироать 
конект то есть он должен "позвонить" ко мне. когда ядра менжду двумя хостами инциируют
конект то сам тцп уже не имеет разделения сервер или клиент. он делает двух участников на 
обоих концах равноправными участниками каджый может пихать байты в конект и они бууудут 
доставлены второму уастнику. вопрос только в том захочет ли участник доавать эти байты из 
ядра через файл дескрпиотор. ибо это совершенно участник делаь не обязан!
так вот nc так сделан что он в при любой форме запуска вот так

  $ nc -l 5050

или вот так

  $ nc 127.0.0.1 5050

он обяательно читает то что приелетло в сокет из сети. обязательно. а если на stdin процесса
прилеетли байты то он обязталеьно их сует в тцп конект. при обоих формах запуска!
я это подчервирукваю потому что  у нас приучили что если участник это "сервер" то он толко 
пассивно ждет запроса снаружи. это полная хуйня.! сервер это всего лишь тот кто не может
первым делать инициацию начального конекта! вот и все! потом они оба совршенно равнопрвыны 
солгласно tcp

так вот если мы запускаем 


  $ nc -l 5050


  $ nc 127.0.0.1 5050


то у нас два процесса могут чиатть писать друг другу используя СЕТЕВОЙ конект.  сточки зрения
памяти в ядре используя сетевые сокеты. связь двух стооронняя. кажый может чаитать и писать
через этот конект. 
так вот помимо формы связи через сетть через сетвой конект процессы если они сидят на одном
хосте то они могут обащсятся друг с другом через локальный сокет через UNIX-domain сокет.
он дает тоже самое. он дает двухсторонний инструмент связи.
выгядит это вот так

процесс-А
  $ nc -l ./un1

процесс-Б
  $ nc ./un1

и вот каждый из участников может читать иписать. если у нас оба процесса заявязаны на терминал
то то что мы пишем в одном терианале будет напевтано и на другом. 
с точки зрения сисколлов все очен похоже как и в случае сети

вот как на клиенте
socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
connect(3, {sa_family=AF_UNIX, sun_path="./un1"}, 8) = 0
poll([{fd=0, events=POLLIN}, {fd=3, events=0}, {fd=3, events=POLLIN}, {fd=1, events=0}], 4, -1^Cstrace: Process 1802059 detached
 <detached ...>

создаем сокет. делаем конект. потом через poll ждем прилета данных на файл дескрипторе

вот как на сеерверер

socket(AF_UNIX, SOCK_STREAM, 0)         = 3
unlink("./un1")                         = 0
bind(3, {sa_family=AF_UNIX, sun_path="./un1"}, 8) = 0
listen(3, 5)                            = 0
accept4(3, ...

зачем он делает unlink незнаю.
хотя..
при вызове nc он создает на диске файл с типом socket

$ stat un1
  File: un1
  Size: 0           Blocks: 0          IO Block: 4096   socket
Device: 254,0 Inode: 548144067   Links: 1

при этом если он находит уже фаайл на диске то он его стиарет
я так понимаю новый файл на диске создает сисколл bind()
ну и получается новый файл будеи иметь новую иноду
   $ stat un1
  File: un1
  Size: 0           Blocks: 0          IO Block: 4096   socket
Device: 254,0 Inode: 548144068   Links: 1

непонятно зачем нужно стирать старый файл сокета

вобщем этот локальный сокет дает нам двухсторонний метод общения для двух 
процессов. ксатти в него также как и в сетевой сокет через команду echo не получится
чтото написать. получатес если хочет чтото записать в такой UNIX domain сокет то надо его
"открывать" например через nc. а уже echo наравлять в nc напрмиер

например беру сокет докер сервера. и пробую туда чегото записать

# echo "123" | nc -U /run/docker.sock
HTTP/1.1 400 Bad Request
Content-Type: text/plain; charset=utf-8
Connection: close

щас к этому вернемся.

окзвыается curl поддерживает кучу протоколов которые выше чем tcp/udp
PROTOCOLS
       curl supports numerous protocols, or put in URL terms: schemes. Your particular build may not support them all.

       DICT   Lets you lookup words using online dictionaries.

       FILE   Read or write local files. curl does not support accessing file:// URL remotely, but when running on Microsoft Windows using the native UNC ap‐
              proach works.

       FTP(S) curl supports the File Transfer Protocol with a lot of tweaks and levers. With or without using TLS.

       GOPHER(S)
              Retrieve files.

       HTTP(S)
              curl  supports  HTTP  with numerous options and variations. It can speak HTTP version 0.9, 1.0, 1.1, 2 and 3 depending on build options and the
              correct command line options.

       IMAP(S)
              Using the mail reading protocol, curl can download emails for you. With or without using TLS.

       LDAP(S)
              curl can do directory lookups for you, with or without TLS.

       MQTT   curl supports MQTT version 3. Downloading over MQTT equals subscribe to a topic while uploading/posting equals publish on a  topic.  MQTT  over
              TLS is not supported (yet).

       POP3(S)
              Downloading from a pop3 server means getting a mail. With or without using TLS.

       RTMP(S)
              The Realtime Messaging Protocol is primarily used to serve streaming media and curl can download it.

       RTSP   curl supports RTSP 1.0 downloads.


       SCP    curl supports SSH version 2 scp transfers.

       SFTP   curl supports SFTP (draft 5) done over SSH version 2.

       SMB(S) curl supports SMB version 1 for upload and download.

       SMTP(S)
              Uploading contents to an SMTP server means sending an email. With or without TLS.

       TELNET Fetching a telnet URL starts an interactive session where it sends what it reads on stdin and outputs what the server sends it.

       TFTP   curl can do TFTP downloads and uploads.

       WS(S)  WebSocket done over HTTP/1. WSS implies that it works over HTTPS.



в курл этот протокол указывтся в виде  

  protocol://...

это пример доступа на сврере google.com по tcp и далее по HTTP и указан явно
какой метод а именно GET мы хотим чтобы курл послал на сервис на серервере 

  $ curl http://google.com --request GET  -x http://172.16.10.11:2080  -v

ну и еще я добавил доступ идет ненпрмярую а через прокси сервер чрез прокси протокол

тоесть оказывается курл это нетольлко HTTP веб клиент как я привык это окзывается еще и
фтп клиент и телнет клиент и вебсокет клиент и scp клиент и боже мой еще какой клиент. 


оказывается что у докер демона ожидаение что ему чрез локальный юникс сокет будут данные
посылать в формате HTTP

подключаиюсь к сокету и шлю HTTP запрос
#  nc -U /run/docker.sock
GET / HTTP/1.1  <=====    
Host: lenovo    <=====

HTTP/1.1 404 Not Found
Content-Type: application/json
Date: Mon, 11 Nov 2024 09:59:09 GMT
Content-Length: 29

{"message":"page not found"}


стрелками это то что я вводил сам. 
а остальное это то что ответил демон мне чрез сокет

так вот окзывается что curl умеет пдключаться через юникс сокет сам без посредников

и вот оказывается вот как можно через юникс сокет  пообщаться с докер сервисом который
разговаривает на HTTP языке протокола

  # curl --unix-socket /run/docker.sock http://v1.47/version  -X GET    -v
*   Trying /run/docker.sock:0...
* Connected to v1.47 (/run/docker.sock) port 0
* using HTTP/1.x
> GET /version HTTP/1.1
> Host: v1.47
> User-Agent: curl/8.10.1
> Accept: */*
> 
* Request completely sent off
< HTTP/1.1 200 OK
< Api-Version: 1.47
< Content-Type: application/json
< Docker-Experimental: false
< Ostype: linux
< Server: Docker/27.2.1 (linux)
< Date: Mon, 11 Nov 2024 10:07:45 GMT
< Content-Length: 790
< 
{"Platform":{"Name":""},"Component... 9-13T21:18:02.000000000+00:00"}



тесть я говорю curl чтобы он открыл ююникс сокет. и наравил HTTP реквест через него. 
реквест имеет метод GET а URI которйы я требую открыть это /version
таже интеерсно что имя веб сайта его домен это v1.47 
также видна важнейщая вещь - что HTTP необязан базиварться на TCP. подойдет любая хрень
которая дотсавляет HTTP текст надежным способом . вот и все! хоть на файл на диске.
хоть пайп. хотть юникс домен сокет. вобще похеру. гавное треование что - если процесс сунул 
в дескрипттор HTTP текст то процесс недолжен парится о том  как он будет досталвен конечному
потребителю. главное чтобы либо был доставлен либо не доставлен. и это было бы в коде 
возврата юзер процессу. и все!

есть версии nc у которых есть оция -e или -c котоаря позволяет запускат команды. 
в моей версии nc таких опций нет

v1.47 это версия api докера

еще рекомедует доабвить кастомный хидер поле  Content-Type: application/json
тогда докеру будет понятно в каком формате отдавать оорбратно ответ


# curl --unix-socket /run/docker.sock http://v1.47/version  -X GET   -H "Content-Type: application/json"  -v

также интеесно то что  тело ответа по протоклу HTTP необязатно быть в формате html!!!
как видно из ответа докер веб сервера выше он тело ответа выдае в формате json!

вот страничка как через curl и юникс сокет докера делать запросы к нему по HTTP

    https://docs.docker.com/reference/api/engine/sdk/examples/


вот как выгдяит в lsof проецесс который слушает юникс сокет

  # lsof -Pn -p 1810483
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1810483 root   3u  unix 0x000000006e748b6f      0t0  20337573 ./un1 type=STREAM (LISTEN)
nc      1810483 root   4u  unix 0x000000003222afc3      0t0  20333495 ./un1 type=STREAM (CONNECTED)

  # lsof -Pn -p 1810543
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1810543 root   3u  unix 0x00000000f18e66ea      0t0  20340707 type=STREAM (CONNECTED)


значит я посмотрел если я пытаютс открыть файл дескрпитор который сокет то 
ядро недает

  openat(AT_FDCWD, "/proc/1810543/fd/3", O_RDONLY) = -1 ENXIO (No such device or address)


в тоже время как мы прекрасно знаем если я пытюаь открыть файл дескрпитор за которым
сидит трминал железка то без проблем

  openat(AT_FDCWD, "/proc/1810543/fd/0", O_RDONLY) = 3

хотя перимншны у дескрипторов одинаковые

# ls -1l /proc/1810543/fd/ 
total 0
lrwx------ 1 root root 64 ноя 11 13:44 0 -> /dev/pts/12
lrwx------ 1 root root 64 ноя 11 13:44 1 -> /dev/pts/12
lrwx------ 1 root root 64 ноя 11 13:44 2 -> /dev/pts/12
lrwx------ 1 root root 64 ноя 11 13:44 3 -> 'socket:[20340707]'

тоесьт получается интеерсая вещь - ядро позволяте без вскяиких прблем пдклчаться (открыват)
чужих проессов файл дескрипторы если за нимии скрываются терминалы или простые файлы с диска
но если за ними сркрвыаются сокеты (сетвые или лкальные) то тут ядро шлет нахер чужой процесс.
зачем было так разделять? только путаница!

а вот еше инеересный пример

$ nc [-C] localhost 25 << EOF
             HELO host.example.com
             MAIL FROM:<user@host.example.com>
             RCPT TO:<user2@host.example.com>
             DATA
             Body of email.
             .
             QUIT
            
насколко я понимаю опция -w она раоает то для исоодящий содениенеие тоесть для -l она не
рабоатте. и означает что если мы куда то стучимтся по сети то ждать масиум столько секугд
сколько в -w указано. а потом если ответа нет то прога выйдет. в мане очень тупо идет 
оьяенеие этой опции

на счет разницы между > и | в bash. 
когда ядро создает процесс то оно открывает для этого процесса обычно три файла.
что значит ядро отрывает файл для процесса. это значит что ядро подготавливает в своей 
памяти структуры через которые ядро будет читать файл. тоесть при чтении файла в памяти 
ядра нужно завести некоторые структуры которые используются ядром при физическом чтении
файла ровно как нужн взять с полки тарелку и ложку для того чтобы поесть суп. и что еще важно
что  ядро выдает процессу некоторые ссылки которые имеют виде чисел 0,1,2 
так вот процесс может используя эти числа обращаться к ядру с просьбой прочитать кусок 
файла которые "скрывается" за этим числом. тоесть используя число 0 или 1 проецсс обьянсяте
ядру какой файл из тех что были до этого открыты процессс просит у ядра прочитать. тоесть
0 1 или 2 это как услонве обозначения для того файла который кроется за этим числом. тоест 
как выглядит работа с файлом вообще - процесс берет полный путь к файлу с которым бы проецсс 
хотел поработать (прочитать или записать в него). но саммоому процессу никак нельзя рабоать
с файлом нарпямуюю. это может делать только ядро. процесс может рабоать с файлом только при
посредничестве ядра. на первом этапе процесс обьявляет ядру  с каким именно файлом процесс
хотел бы поработать при посредничестве ядра. и для этого процесс исползует libc фнукцию

  open()

в которой процесс указвыает "путь" к файлу. ядро выделаяет в соей памяти кусок. заводит там
нужные для себя струкутруры которые ядро будет использвать при доступе к этому файлу. 
а для процесса ядро сообщает число например 15. это число это кодовый идентиацикатор этого файла
который процесс может в дальнешем использовать в функциях read() write() тоесть при чтении
и записи из файла. эти фнкции это просьба процесса к ядру чтобы оно ядро считало кусок из 
файла в память процесса или наоброт взяло кусок из памяти процесса и записало в файл. тоесть
прочиттаь или записать в файл можно только через ядро. оно все делает. мы ему только указываем
что мы хотим а все делат только оно прцоес неимеет нкиакого доступа. так вот в этих read()
write() нужно вставлять вот это чсило выданое ядром 15 . через которое мы обьясняем ядру
с каким файлом мы просим его поработать. тоесть ядро ывдает пцроессу число которое мы потом
же покзываем ядру чтобоы оьбьячнить ядру с каким файлом мы хоим чтобы ядро поработало. 
таким образом когда мы говорим что у процесса открыто три файла. это значит что есть 
три фиксированных файла из которых ядро согласно читать или писать при запросах от данного
процесса. тоесть прежде чем прцоесс получитьб данные из файла он длжен чеерз open() обьявить
ядру тот файл к которому процесс хотел бы получить доступ (ненапрямую а через ядро опять же).
ядро проверяет есть ли у процесса права на доступа в этоот файл. если права есть то ядро 
соглащается и выдает процессу идентийикатор. это называется открыть файл. далее файл предляьвляет
ядру этот идентификаторв в фунциях read() write() таким оразом обьянсяя ядру про какой файл
щас идет речь. и прося ядро чтото считать с файла и предать проецссу иличтото записать в файл
от имени процесса. итак открыть файл для процесса. это значит обьявить ядру со стороны процесса
имя файла с которым процесс хотел бы поработть ( при посреднчиестве ядра) и получть от 
ядра числовой идентиификатор. который далее процесс будет показывать ядру при вызоове других
функций чтобы обтяснить ядру про какой файл идет речь.
так вот как обычно содается процесс. обычно уже есть какйото процесс. и он просит ядра
через фукнцию clone() о том чтобы ядро создало новый процесс. при этом обычно новый роцесс
это копиия процесса который попросил созать новаый процесс. и новый процесс имеет тот же самый 
список открытых файлов ( число 15 что выше назвыается файл десскриптор). так вот каждый файл
дескприптор получается обозначает под собой некоторый файл к которому можно получить доступ
через вызов к ядру. обычно процессы создаются так что дескриптор 0 ведет на файл /dev/pts/1
и тоже самое для десриптора 1 и 2. это не просто файлы это через такие файлы виртуализирован
или абстрагирован доступ на железку "терминал". так вот галочка > это фича баша.
значит вот еще раз если мы запускаем через баш новый процесс то как правило это процесс
получается имеет три файл дескприотора 0 1 2 . тоесть процесс может работаьт с тремя
файлами которые скрываются за этими числами. обычно все три файла это один файл /dev/pts/N
который собой абстрагирует железку терминал. то есть если процесс пишет  в 0 либо 1 либо 2
то в итоге то что он написал мы увидим на окне терминала. обыяно для процесса вот эти откртые
файлы это способ чтото собщить о себе во внешний мир и\или забрать откуда то данные для их
обработки. так вот если мы хотим чтобы баш запустил новый процесс и при этот этот процесс
имел дескрипторы 0 1 2 но которые ведут на файлы отличные от дефолтового /dev/pts/N
то мы можем использовать символ галочки в баше >. галочка это чисто фича баша. это его грубо
говоря команда или опция.  перед галочкой мы ставим номер файлового десприатора 0 либо 1 либо 2
а справа мы ставим путь к файлу в файловой сситеме. например

    $ ps 1>/dev/null

таким макаром мы оьбянсяем бащу что он должен запустиь новый процесс ps через ядро так чтобы
у него дескрпитор 1 в итоге смотрел на файл /dev/null 
в народной литератру это называтся переооюозначить ввод\вывод

ТАК вот важно тут отметить понять что галочкой мы можем пользоваться ТОЛЬКО если мы знаем 
точный путь к файлу на фс на который мы хотим пробросить дескриптор. ЕСЛИ мы точный путь к 
файлу НЕЗНАЕМ то галочкой мы воспользоваться НЕМОЖЕМ!  еще раз галочка это как фунция или оператор
так вот это бинарный оператор. галочка требует двух параметров - номер дескриптор. ну здесь
просто это либо 0 либо 1 либо 2 . это мы всегда знаем. а вот второй параметр это точный путь 
к файлу на ФС. нужно себя спросить знаю ли я этот точный путь к файлу на фс. если я точный 
путь к файлу незнаю то  оператором галочка я воспользоваться НЕСМОГУ!
если номер дскриптора у нас 1  то в цеом его номер можно слева от галочки опутить

  $ ps >/dev/null

итак справ у нас всегда ПУТЬ К ФАЙЛУ. это значит что с помощью галочки нельзя связать 
два процесса! напимер 

  $ ps >grep  

потому что это аналог 

  $ ps 1>grep 

и то что справа отгалочки всопнпринмиается башем не как команда а как имя файла! 
поэтому реузуллтатом будет то что у процесса ps дескриптор 1 будет ссмотреть на файл с 
именем grep. а не то что справа стоит имя команды grep
срав от галочки всегда стоит полный путь к файлу. в данном случае файл grep будет создан
в текущей для роцесса папке!

итак важное про галочку:
  галочка это фича баша
  галочка работает с файл дескрпиторами создаваемого процесса
  галочка это бинарный оператор всегда. нужно два параметра
  параметр слева это число 0 либо 1 либо 2. если число 1 то его можно опустить
  ппараметр справа от галочки это ВСЕГДА ИМЯ ФАЙЛА. А ТЧОНЕЕ ПОЛНЫЙ ПУТЬ К ФАЙЛУ.
            поэтому то что будет стоясть справа баш всегда будет воспринимать как имя файла.
  без знания двух параметров галочкой неполучится воспользоваться. ну хорошо слева мы 
            ничего не поставли и баш считает что там стоит 1. но справа нужно подставить
            имя файла а точне полный путь к файлу. если мы имя файла незнаем то галочкой
            неполуится воспользвоваться!

теперь переходим к pipe. пайп этотоже оператор баша. он тоже требует два параметра.
слева от пайпа стоит имя роцесса. справа от пайпа стоит имя процесса. поэтому почуствуй
разницу уже между галочкой и пайпом! 

       процесс1 | процесс2                 ( пайп)
       процесс1 дескрпитор > имя_файла     ( галочка)

что делает пайп. он тоже работает с дескрипторами у новых создаавемых процессов. 
он делает вот что. баш запускаеьт процесс1 и процесс2 и делает так что дескприор 1 в процессе1
будет указывать на дескрпиттор 0 в процессе 2
галочкой такое никак не сделать. потому что если мы посмотрим на синтксаиси применения 
галочки

     процесс1 дескрпитор > путь_к_файлу/имя_файла 

то нам для того чтобы всползовться галочкой нам нужно справа от галочки указать полный путь
к дескриптору 0 второго процесса. проблема в том что мы запуская процесс1 мы еще не знаем
какой будет id у процесса2. поэтому путь к файлу мы незнаем и не можем узнать. поэтому
то в этом случае и используется pipe !!

еще вот так скажу. когда ядро создает процесс то оно создает процесс нес нуля и не по ини
циативе ядра. обычно заказ на новйы процесс идет от уже какогто сщестующего процесса. 
и новый процесс получсетс путем того что текущий прцесс просто клонируется. при клонровании
файл дескприторы тоже клорируются. тем не менее оюычно как правило у мнжества процессов
у них дескприпторы 0 1 2 они все смотрял на /dev/pts/N тоесть на файлы за которыми скрывается
желещка терминал.  так вот когда мы юзаем галочку (которая явялется просто напросто оператором
именно баша) то мы говорим башу что чтобы когда он будет заказыват у ядра клонирование теку
щего процесса чтобы ядро изменило то на какой файл ведет дескпритор 0 либо 1 либо 2 
дополенение также такое - я думаю там даже вот такая ситуация - при клонировании процесса
ничего неменяется - десрипторы куда ссмотрели туда и смотрят. меняется то что клониорванный баш
он тогда открывает новый файл через open() в новый дескриптор. потом баш юзает dup() чтобы
скопировать дескриптор уже скажем в дескриптор 0. и удалеяет новый дескриптор. вот так работает
галочка.
щас покажу  запускаю вот такую команду

    $  /usr/bin/echo 1>/dev/null

мы сидим в баш его номер процесса   1833372
баш себя клонирует

  clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLDstrace: Process 1833373 attached
, child_tidptr=0x76f993dd5e50) = 1833373

клонированный баш имеет номер 1833373
что интересно при клонировании само тело процесса его облаться памяти юзерская она не клонируется.
а клонируется лишь небольшой кусочек памяти в области ядра. там где сидит описание страктур
процесса. это пхожде на то что при копировании файла у нас тело файла небыло скопировано  а 
только создали новый заговловок. 
потом баш в клонированном процессе ооткрывает файл /dev/null

  [pid 1833373] openat(AT_FDCWD, "/dev/null", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3

получает от ядра дескриптор 3 для этого файла

потом баш в клонрованном процессе копирует дескриптор 3 в дескриптор 1  . тоесть теперь 
дескриптор 1 смотрит ровно на тот же файл что и дескриптор 3

[pid 1833373] dup2(3, 1)                = 1

теперь баш в клонированном процессе закрывает дескриптор 3. удаляется лишь дескриптор 3
а файл на который он смотрел остается открытым

[pid 1833373] close(3)                  = 0

и после этого баш в клонированном процессе просит ядро заменить в этом процессе 
исполняемый код с башевского на /usr/bin/echo

[pid 1833373] execve("/usr/bin/echo", ["/usr/bin/echo"], 0x5839b829e570 /* 66 vars */) = 0


вот как работтает галочка > в баш
тоесть изменение на что смотрит дескриптор идет на на сталии клонирования процесса clone()
а гораздо позже когда уже новый процесс возник!

пайп тоже это оператор баша. он тоже предназначен чтобы перенаправить то на кккакой файл
смотрит дескриптор в новом процессе а точнее уже в двух процессах. пайп тоже реализуется
не на стадии clone() а уже  в склонрррованном процесссе когда там еще сидит код баша 
и этот баш запускает ряд маниупуляций. выстраивает пренарпавление дескрпиров.  а потом 
замещает свой код на код новый команд

так вот разнца еще раз между галочкой и пайпом в том что  у галочки два аргумента. и у пайпа
два аргумента. но галочка в итоге раотате с одним процессом а пайп с двумя. у галочки
аргумент это полный путь к файлу  и один из  дескрипторов нового процесса. а у пайпа это два процесса. 
вот что делает баш на оператор пайп
баш в текущем процессе вызывает два раза пайп

pipe2([3, 4], 0)                        = 0
pipe2([5, 6], 0)                        = 0


потом баш себя клонирует первый раз
clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLDstrace: Process 1836117 attached
, child_tidptr=0x76f993dd5e50) = 1836117

потом бащ себя клонирует второй раз
[pid 1832812] clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLDstrace: Process 1836118 attached
, child_tidptr=0x76f993dd5e50) = 1836118

потом баш из первого клона коприует дескриптор пайпа в 0 этого процесса 
[pid 1836118] dup2(3, 0)                = 0

потом он закрывает дескриптор 3. тут это не покзаано

потом баш во втором клоне копирет дескрипор пайпа в дескрпитор 1
[pid 1836117] dup2(4, 1)                = 1

и закрывает дескпритор 4 тут это не показано

и потом в каждом клоне баш просит ядро заменить код баша на код новых команд

[pid 1836117] execve("/usr/bin/echo", ["/usr/bin/echo", "1"], 0x5839b829e570 /* 66 vars */ <unfinished ...>
[pid 1836118] execve("/usr/bin/sleep", ["sleep", "199"], 0x5839b829e570 /* 66 vars */ <unfinished ...>

зачем баш вызывает пайп два раза. а не один я не понимаю. ведь нам нужен один.
видно что вот этот вызов
    pipe2([3, 4], 0)                        = 0
он возварщает два дескприора 3 и 4. 3 это входной десериптрр куда данные можно писать
а 4 это откуда их можно читать . 
соовстенно наверху видно что у одного процесса копируется 3  в 0 а у другого 4 в 1 

так работает оператор пайп в баше на уровне сисколлов к ядру

итак и галочка и pipe они оба меняют файлы на котоорые будут указывать дескприоры в новом проце
ссе или процессах. но галочка работате с одним процессом а пайп с двумя. для галочки нужно
знать полный путь к файлу на который мы хотим переназначиить дескриптор а пайпу пути ненужны
зато ему надо два процесса! вот такие они похожрие в общем смысла но разные в деталаях
то что делает пайп с помощью глочки не получится сделать. потому что для галчки нужен полный
путь к файлу. а мы незнаем какой будет pid у новго процесса поэтому в галочку подставлять нечего!
если мы путь к файлу незнаем то галочку применить невзоможно!

еще раз про вызов open(). файл с диска читает только ядро. процесс доступа к фаулу не имеет.
у процесса есть доступ только к ядру через ключики.
поэтому процессу нужен какйото ключик от ядра чтобы ядро читало файл с диска а процесс обращаясб к
ключику получали от ядра то что он прочитал от файла! поэтому ест команда open() в ней мы 
укащываем путь на фс к тому файлу котоырй хотим читать. этим мы обясняем ядру что наш процесс
хочет работать с файлом таким то. ядро это пгимает и выдает процессу ключик посредник. этот ключик
является для ядра в дальнешем идентиификатором пути к файлу. и наш процесс может в дальнейшем
этот ключик подставляеть в вызывы read()  которые уже непосредсрвенно просят ядро
прочтать чтото из файла и в этот вызов мыопдставляем этот ключик (это число скажем 4). 
и ядро понимает что 4 это /tmp1/1.txt файл. дале ядро читает этот файл и после этого ядро кладет
кусок данные прочитанных из файла в область памяти процесса. который затем уже может обратиться
к этой области памяти и псмотреть что там лежит.
причем опять же я вот так думаю - что может сам процесс не прибегая к сисикллам ядра (тоест
к коду ядра) сделать сам с теми данными которые лежат в области памяти процесса. 
едиснвтенное что мне приходи в голову это то что процесс может сделать арифметическую операцию
с этим данными своими сиалами . совими силами в том плане что сделаьт чтото не прибегая к 
переключению на код ядра на сисколлы. и больше ничего сам процесс неможет сделать! 
например я считал кусок данных с файла и хочу напечатать на экран. буквально на 99% все делает
ядро! щас покажу

открывам файл  (делает ядро)
  open()

читаем его кусок в память области памятя процесса (делает ядро)
  read() 

напечтать на экране (делает ядро!)
  printf() ---> write ()

тоесть все делал код ядра! единственное каак в этом учасотвовал код процесса код работающий
в юзер спейсе это то что процесс только руководил что делать. и еще какоето время данные
считатнные с диска лежали в области памяти процесса. и все! код процесса это только скрипт
руковрждеие что делать! а самое делание это 99% только ядро! проецессу ничего делать нельзя
сомоаомому!  ниего неопзволяется! я вот говорю пусть у нас в побласти памяти процесса 
ллежат данные. впорос что с ними можт сделать сам роцесс не приебгая к помощи ядра? 
мне кажется что почти нихрена! вот пример 
   

   int i = 1;

вот у нас данные в области памяти процесса. что мы можем с ним сделать сами. без ядра?
ну только если сделать ариеяеметичекую операцию

  int j = i+10;

все!

тоесть даже над данным которые лежат вобласти памяти самого процесса сам порцесс мало что
может сделать. чтобы чтото сделат нужно постоянно просить делать ядро! напрмиер вывести
данные на экран это нужно просить сделать это ядро итд!

это пиздец!

вопрос нахер нужен open() почппу бы сразу не было сделать чтобы пользоваться read() 
без open() .дело в том что ядро разделило задачу доступа к файла. на две задачти.
когда мы делаем open() то ядро проверяет етть ли у процесса права на доступ к файлу. есть
ли сам сам на дсике. если этого всег онет то ядро шлет нахер в ответ  с ошибкой! также
в open() мы указываем что мы будем делать с файлом. будем ли мы его чиать или писать или то
и другое. права на это дело проерверябтся ядром тут. поэтому ядро тут все проверяет и только
если все окей выдает дескриптор. поэтому далее команде read() уже ненужно это ничего проверяеть.
она видит дескрпитор и ядро пнимает что все проверки для файла были пройдены успешно и можно
сосредодточться на самопроцессе чтения! вот почему для чтения из файла нужна связвака двух
функций

    open()
    read()

важно понимать что когда мыдаелмем open() то как результат ядро нетолько нам прсто выдает
дескриптор как ключ дальнейшего доступа на файл. но и то что ядро у себя в памяти создает
структуры лично для себя которые будут помогат ядру рабоатьт с файлом фактиечкски. дело
в том что несколько процессов могут запросить open() от одного и тогоже файла. и декрипторы
то выдать кажду проецессу это дело нехитрое. но нужно же как то потом одновременно обрабатьвыа
запросы от двух процессов к одному файлу (пусть комп многпроцессорный и обра процесса сейчас
пряс работают на компе). например для каждого дескприора ядро создает счетчик-курсов он указывает
на каком байте от начала файла щас стоит этот курсор. тоесть скжаем при команде чтения у нас
будет читаться тот байт на котором стоит курсор. создание курсора это именно протисходит
на стадии open() для файла. тоесть при open() со стороны ядра помимо проверки прав ядром 
прав доступа процесса к файлу ядро еще создает для себя в памяти ядра струрктуры всопомнательные
которые нужны ядру для того чтобы физчиески осуществлять раоту с файлом при запрсах от процесса!

когда мы юзаем open() мы указываем файл с которым мы хотим рабоатт через его адрес (путь)
на такой абстрации как файловая система. так вот этот путь на фс это абстракция. 

ладео с этим ужасом вроде покончил. теперь слудщий вопрос

вопрос
вот мы делаем

   $ cat $path/file

по факту при этом проиходит 

    open($path/file)

так вот почему open() успешно отрабатывает для:
  обычный файла на фс на диске 
  файл ведущий на терминал
  файл ведущий на пайп 
  файл на proc является дескрпитором от  другого процесса 
а вот неуспешно отрабатывает для:
  файл ведущий на сокет сетевой или локальный юникс 

почему так?

значит вот так  прога подключатеся к юникс сокету который в файле ./un2

    socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
    connect(3, {sa_family=AF_UNIX, sun_path="./un2"}, 8) = 0

тоесть заметим то что наша прога не использует open() с ./un2  а испольщует socket+connect
это уже наводит на мысль почему open() на сокет файлах неработает

немного в сторону
тут кстати кое что интерсное вслпыло. беру процесс

    # lsof -Pn -p  1863607 2>/dev/null
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1863607 root cwd    DIR              254,0      125 549482395 /home/noroot/temp/nc
nc      1863607 root rtd    DIR              254,0      234       128 /
nc      1863607 root txt    REG              254,0    39352 805576410 /usr/bin/nc
nc      1863607 root mem    REG              254,0    55408   1739074 /usr/lib/libmd.so.0.1.0
nc      1863607 root mem    REG              254,0  2014520     55931 /usr/lib/libc.so.6
nc      1863607 root mem    REG              254,0    63736   1594801 /usr/lib/libresolv.so.2
nc      1863607 root mem    REG              254,0    76664   1745618 /usr/lib/libbsd.so.0.12.2
nc      1863607 root mem    REG              254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
nc      1863607 root   0u   CHR              136,5      0t0         8 /dev/pts/5
nc      1863607 root   1u   CHR              136,5      0t0         8 /dev/pts/5
nc      1863607 root   2u   CHR              136,5      0t0         8 /dev/pts/5
nc      1863607 root   3u  unix 0x000000008a6ce284      0t0  21131671 ./un2 type=STREAM (LISTEN)
nc      1863607 root   4u  unix 0x00000000cf7cf408      0t0  21131672 ./un2 type=STREAM (CONNECTED)

так вот смотрим на колонку FD . это колонка с файл дескприторами у процесса. 
так вот видно что помимо 0  1 2 3 4  имена которых прекрасно знакомы есть имена дескрипторов
которые вобще никакгда не встречаются в /proc/pid/... а  именно

  cwd  rtd  txt mem 

тоесь поучается в пмяти ядра в таблице описывающей процесс в таблице файл дескрпиоторов
там горадо боьле дескриторов чем нам показывают в /proc/ ! но процессу эти дескрипторы 
ведущие на файлы для работы недоступны!  получается lsof показывает в плане открытх файлов
процессом или для процесса гораздо больше чем это покзывает /proc/pid/fd !

cwd - это открытый файл который папка ( получается папку тоже можно через open открыть )
      который явялтся "текущей папкой" для процесса

rtd -незаю

txt - это файл в котором код который на цпу пихается

mem - это файл который открыт не через open() а через mmap()
по поводу mmap() вот его формат

  pa=mmap(addr, len, prot, flags, fildes, off);

так вот для запуска mmap для файла сдиска надо прежде всего этот файл открыть обычным 
способом через open() получит файл дескриптор. потом его подставть в mmap в позицию flides
и только тогда срабатвыает mmap! после того как mmap() успешно сработал то файл дескпрттор
от open() можно сразу закыть он больше ненужен. и далее уже можно работать с файл десприктором
который дал mmap()
пример проги которая далает mmap() это 232.c
а вот как выгдяитит этот запущенный процесс 232.exe

    $ lsof -Pn -p $(pidof 232.exe)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
232.exe 1870174 noroot cwd    DIR  254,0      140 549482395 /home/noroot/temp/nc
232.exe 1870174 noroot rtd    DIR  254,0      234       128 /
232.exe 1870174 noroot txt    REG  254,0    16064 549482372 /home/noroot/temp/nc/232.exe
232.exe 1870174 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
232.exe 1870174 noroot mem    REG  254,0       42 549482391 /home/noroot/temp/nc/3.tx
232.exe 1870174 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
232.exe 1870174 noroot   0u   CHR 136,17      0t0        20 /dev/pts/17
232.exe 1870174 noroot   1u   CHR 136,17      0t0        20 /dev/pts/17
232.exe 1870174 noroot   2u   CHR 136,17      0t0        20 /dev/pts/17
232.exe 1870174 noroot  10w  FIFO   0,15      0t0  21173021 pipe
232.exe 1870174 noroot  12w  FIFO   0,15      0t0  21173022 pipe

нас интереуует вот эта строка

COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
232.exe 1870174 noroot mem    REG  254,0       42 549482391 /home/noroot/temp/nc/3.tx


nтоесть мы видим что FD=mem это значит имя файлдескрпитора mem и то что он был создан
в резултате откратия файла через mmap() сисколл! а не через open()

мы видим что TYPE=REG это значит что файл который "открыт" это "обычный " файл с диска

там же в программе видно я вначале через open() открываю файл обычным споособом.
потом я беру полученынй дсекспритор и подслвяю его в mmap() 
он дает нам новый дескприр. а тот я сразу закываю чеерез close() и все окей. дальше успешно
работаю с файлом через новый дескрпитор. !

если мы через lsof ссотрим на процесс то FD=cwd означает откртый файл  тип папка который
явлется для процесса "current working directory" и мы узнаем текущую рабочую папку для 
процесса. что она дает на практике процессу. хрен знает

что такое FD=rtd непонятно. понято что d это значит directory. но что за директори
какой ее смысл?

вот мы беерм проесс файрфокс . смотрим на него через lsof

$ lsof -Pn -p 1645824
COMMAND     FD   TYPE             DEVICE  SIZE/OFF      NODE NAME
Isolated    cwd  DIR               0,23         0  18566136 /proc/1645831/fdinfo
Isolated    rtd  DIR               0,23         0  18566136 /proc/1645831/fdinfo
Isolated    txt  REG              254,0    928656 537573059 /usr/lib/firefox/firefox
Isolated    DEL  REG                0,1             5877432 /memfd:mozilla-ipc
Isolated    mem  REG              254,0   3838696   3425909 /usr/share/fonts/droid/DroidS.ttf
...
...


итак lsof по своей сути покызвает ВСЕ открыте ядром для этого процесса файлы!

на счет mmap интерсно. получается надо вначале файл открыть чрез open() а только потом
можно запустть mmap()

посмотрим когда процес стартует то какие он файл отыркывает через open() а какие через
open()+mmap()



    $ strace -e openat,mmap,mmap2,close   sleep 600
openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
mmap(NULL, 141231, PROT_READ, MAP_PRIVATE, 3, 0) = 0x73e0c27ff000
close(3)                                = 0

openat(AT_FDCWD, "/usr/lib/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
mmap(NULL, 2034616, PROT_READ, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x73e0c260c000
close(3)                                = 0

openat(AT_FDCWD, "/usr/lib/locale/locale-archive", O_RDONLY|O_CLOEXEC) = 3
mmap(NULL, 3060384, PROT_READ, MAP_PRIVATE, 3, 0) = 0x73e0c2200000
close(3)                                = 0

openat(AT_FDCWD, "/usr/share/locale/locale.alias", O_RDONLY|O_CLOEXEC) = 3
close(3)                                = 0

тоесть это делает прям юзер код самого процесса а не ос перед тем как выполнить execve()
также вот видно что вначале файл откоывается через open() потом запускатя mmap() а
после этого уже ненужен дескрпитр от open() и делатеся close() ненужного дескрпиттора.
поэтому я ожидаю что у этого процесса файлы 

  ld.so.cache
  libc.so.6
  locale-archive


буудут в lsof выглядеть как открыты через mmap !
проверяем:


      $ lsof -Pn -p $(pidof sleep)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
sleep   1874964 noroot cwd    DIR  254,0      140 549482395 /home/noroot/temp/nc
sleep   1874964 noroot rtd    DIR  254,0      234       128 /
sleep   1874964 noroot txt    REG  254,0    35040 805307925 /usr/bin/sleep
sleep   1874964 noroot mem    REG  254,0  3060384 671112721 /usr/lib/locale/locale-archive
sleep   1874964 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
sleep   1874964 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
sleep   1874964 noroot   0u   CHR 136,24      0t0        27 /dev/pts/24
sleep   1874964 noroot   1u   CHR 136,24      0t0        27 /dev/pts/24
sleep   1874964 noroot   2u   CHR 136,24      0t0        27 /dev/pts/24

и действительно! видим что 

COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
sleep   1874964 noroot mem    REG  254,0  3060384 671112721 /usr/lib/locale/locale-archive
sleep   1874964 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
sleep   1874964 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2

вот эти три файла открыты для процесса через mmmap() !

значит я в итоге понял о том что все шаред либрари подключаются к процессу именно 
и только через mmap! шаред либрари это файл на диске. значит этот файл нужно так или
иначе открыть изнутри процесса. шерд либрари это набор готовы фкнций юзер кода
которые мы хотели бы включить в состав нашей программы. open() для файлов юзается
в случае если мыхотим из файла читать или писать байты! а если у нас в файле лежать готоые
комплированные функции то их чрез open() "подключить" не получится. их подкчают чрез

  open()+mmap()+close()

значит вопрос какие хрени ядро закрывает в процессе при вызове execve() нужно 
читать в 

  man 2 execve

из того что я прочитал то все файл открыте через mmap() закрыватся.
все открытые папки закрываются
а открытые файлы через open() по деолфту остаются открытыим. 
цитиурую

 •  By  default,  file  descriptors  remain open across an execve().  File descriptors that are marked close-on-exec are closed; see the description of
          FD_CLOEXEC in fcntl(2).  (If a file descriptor is closed, this will cause the release of all record locks obtained on the underlying file  by  this
          process.   See fcntl(2) for details.)  POSIX.1 says that if file descriptors 0, 1, and 2 would otherwise be closed after a successful execve(), and
          the process would gain privilege because the set-user-ID or set-group-ID mode bit was set on the executed file, then the system may open an unspec‐
          ified file for each of these file descriptors.  As a general principle, no portable program, whether privileged or not, can assume that these three
          file descriptors will remain closed across an execve().


это значит все файлы которые мы вдим у процесса который был запущен через execve() от родитеь
ского это все файлы которые достались не от родтелсвкого а были открыты именно кодом это 
бинарника который мы запустили

у нас подключение функции обычно идет в прогрмме через 

#include *.h

которая штука ялется директивой для комплиятора както это срабатывает и он в итоге 
как я понимаю опрееделяет в итоге какой *.so шаред либрарри нужно прикрутить к нашему
файлу.

но можно и напрму в тексте программы сказать что мы хотим подгрузить вот такой то .so
либрарри. 
для начала ничего не грузим берм такую программу

$ cat 235.c
#include <unistd.h>

    int main(void) {

     sleep(60);

    }
    
компилируем смотрим какише либрари покажет ldd

       $ ldd 235.exe
  linux-vdso.so.1 (0x00007195771f2000)
  libc.so.6 => /usr/lib/libc.so.6 (0x0000719576fd3000)
  /lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007195771f4000)
 

значит эту будт точно погружены.
запустим посмоирим что на практике

 n Т  $ lsof -Pn -p $(pidof 235.exe)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
235.exe 1885204 noroot cwd    DIR  254,0    12288 403710483 /home/noroot/git/docs/C
235.exe 1885204 noroot rtd    DIR  254,0      234       128 /
235.exe 1885204 noroot txt    REG  254,0    15440 405389062 /home/noroot/git/docs/C/235.exe
235.exe 1885204 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
235.exe 1885204 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
235.exe 1885204 noroot   0u   CHR 136,27      0t0        30 /dev/pts/27
235.exe 1885204 noroot   1u   CHR 136,27      0t0        30 /dev/pts/27
235.exe 1885204 noroot   2u   CHR 136,27      0t0        30 /dev/pts/27
235.exe 1885204 noroot  10w  FIFO   0,15      0t0  21319617 pipe
235.exe 1885204 noroot  12w  FIFO   0,15      0t0  21319618 pipe
 H 跏  $ 


нас инетерусуют эти строки
 n Т  $ lsof -Pn -p $(pidof 235.exe)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
235.exe 1885204 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
235.exe 1885204 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2

ну что два из трех файлов реально были подгружены через mmap в процесс

а теперь я в явной формет подгружу .so либрари


 . 㟒  $ cat 234.c
#include <stdlib.h>
#include <dlfcn.h>
#include <unistd.h>

    int main(void) {
        void *handle;

        handle = dlopen ("/usr/lib/libcap.so.2", RTLD_LAZY);
        if (!handle) {
            exit(1);
        }

     sleep(60);

    dlclose(handle);
    exit(0);
    }
    

вот этот вызов dlopen ()

смтрим покажет ли эту шаред либари ldd
 P 䯨  $ ldd 234.exe
  linux-vdso.so.1 (0x000077c55cf24000)
  libc.so.6 => /usr/lib/libc.so.6 (0x000077c55cd05000)
  /lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x000077c55cf26000)
 E ✨  $ 


нет не видит!

запускаем смотрим что на практкие

 M 댧  $ lsof -Pn -p $(pidof 234.exe)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
234.exe 1885976 noroot cwd    DIR  254,0    12288 403710483 /home/noroot/git/docs/C
234.exe 1885976 noroot rtd    DIR  254,0      234       128 /
234.exe 1885976 noroot txt    REG  254,0    15592 405389100 /home/noroot/git/docs/C/234.exe
234.exe 1885976 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
234.exe 1885976 noroot mem    REG  254,0    43064     46152 /usr/lib/libcap.so.2.70
234.exe 1885976 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
234.exe 1885976 noroot   0u   CHR 136,27      0t0        30 /dev/pts/27
234.exe 1885976 noroot   1u   CHR 136,27      0t0        30 /dev/pts/27
234.exe 1885976 noroot   2u   CHR 136,27      0t0        30 /dev/pts/27
234.exe 1885976 noroot  10w  FIFO   0,15      0t0  21319617 pipe
234.exe 1885976 noroot  12w  FIFO   0,15      0t0  21319618 pipe
 W /  $ 

и видим да - заканная либрари загружнеана через mmap()

COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
234.exe 1885976 noroot mem    REG  254,0    43064     46152 /usr/lib/libcap.so.2.70


а еще прикол такой что даная шаред либрари она имеет зависимости от других либарари.
и ты уазываеь подгрузить один файл а по факту будет загржуена тьма разных шаред либрари 
файлов!

вобщем разобрались что открытие файла через dlopen() котоырй скоерй всего в своей работе
использует mmap() щас проверим

 ⛉  $ sudo strace -e openat,mmap,mmap2,close ./234.exe
openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
mmap(NULL, 141459, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7cb76df54000
close(3)                                = 0

openat(AT_FDCWD, "/usr/lib/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
mmap(NULL, 2034616, PROT_READ, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7cb76dd61000
close(3)                                = 0

openat(AT_FDCWD, "/usr/lib/libcap.so.2", O_RDONLY|O_CLOEXEC) = 3
mmap(NULL, 45128, PROT_READ, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7cb76df6b000
close(3)                                = 0


тоесть так и есть libc dlopen() исполщует по факту open()+mmap()+close()

так вот либо через mmap() напрямую мы загржуаем файл то 
это дает то что именно через такое у нас шаред либбрарли поключаются
к процессу. и то что в lsof такой файл будет попказан в строке с FD=mem


COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
234.exe 1885976 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
234.exe 1885976 noroot mem    REG  254,0    43064     46152 /usr/lib/libcap.so.2.70
234.exe 1885976 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2

тоесть кода мы видим такие строки то надо понимать что это файлы которые были открыты
через mmap() 
как правило бОльшая чась таким способом открытых файло у процесса это файлы шаред либбари
которые так октырыт чтобы можно было из кода получить досутп к ихним откомпилировнным
функциям!


возврашается к файл дескрипторму cwd  (current working direcotry)
берем такую рограмму

 | 銱  $ 
 x 蒈  $ cat 235.c
#include <unistd.h>

    int main(void) {

     sleep(60);

    }
    

запускаем

смотрим лсоф

 s 艁  $ 
 v 䂽  $ lsof -Pn -p $(pidof 235.exe)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
235.exe 1887188 noroot cwd    DIR  254,0    12288 403710483 /home/noroot/git/docs/C
235.exe 1887188 noroot rtd    DIR  254,0      234       128 /
235.exe 1887188 noroot txt    REG  254,0    15440 405389062 /home/noroot/git/docs/C/235.exe
235.exe 1887188 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
235.exe 1887188 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
235.exe 1887188 noroot   0u   CHR 136,27      0t0        30 /dev/pts/27
235.exe 1887188 noroot   1u   CHR 136,27      0t0        30 /dev/pts/27
235.exe 1887188 noroot   2u   CHR 136,27      0t0        30 /dev/pts/27
235.exe 1887188 noroot  10w  FIFO   0,15      0t0  21319617 pipe
235.exe 1887188 noroot  12w  FIFO   0,15      0t0  21319618 pipe
 V 頋  $ 


вот виим что упроцесса есть такой дескрпитор

COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
235.exe 1887188 noroot cwd    DIR  254,0    12288 403710483 /home/noroot/git/docs/C

вопрос - он из кода программы както доступен? ответ - НЕТ! напрямую нет!
чтобы его узнать надо юзать  getcwd()

NAME
       getcwd, getwd, get_current_dir_name - get current working directory
SYNOPSIS
       #include <unistd.h>

       char *getcwd(char buf[.size], size_t size);
       char *get_current_dir_name(void);


а как его можно поменять для процеса? через chdir()


NAME
       chdir, fchdir - change working directory
SYNOPSIS
       #include <unistd.h>

       int chdir(const char *path);
       int fchdir(int fd);

тоесть мы его можем менять ненрпямую а опосдервванно. 
окей. а что он дает процесу?
а вот что есть функция openat()

       #include <fcntl.h>
       int openat(int dirfd, const char *pathname, int flags, ...
                  /* mode_t mode */ );


она позволяет открвыать файлы. и при этом использваоть не абсолюрый путь а относиельный.
а относительно какой папки? а вот как раз относительно той папки которая лежит в cwd дескрипторе
при условии что первая опция равна AT_FDCWD

   openat()
       The openat() system call operates in exactly the same way as open(), 
       except for the differences described here.

       The dirfd argument is used in conjunction with the pathname argument as follows:

       •  If the pathname given in pathname is absolute, then dirfd is ignored.

       •  If the pathname given in pathname is relative and dirfd is the special 
          value AT_FDCWD, then pathname is interpreted relative to the current working
          directory of the calling process (like open()).

       •  If  the  pathname  given in pathname is relative, then it is interpreted 
          relative to the directory referred to by the file descriptor dirfd (rather
          than relative to the current working directory of the calling process, as is 
          done by open() for a relative pathname).  In this case, dirfd must  be
          a directory that was opened for reading (O_RDONLY) or using the O_PATH flag.

       If  the  pathname  given in pathname is relative, and dirfd is not a valid 
       file descriptor, an error (EBADF) results.  (Specifying an invalid file de‐
       scriptor number in dirfd can be used as a means to ensure that pathname is absolute.)

вот! где пригождается cwd





что важно - то что рисует нам /proc это сугубо уведомительная хрень. если мы работмаем
внутри кода процесса. то он все свои параметры про себя может узнать у ядра через запросы
libc функций. процессе для этого лазить и отркывать файлы  в /proc ненужно! поэтому мы когда
юзаем read(1,...) мы не указваем /proc/pid/fd/1 ! мы указываем сразу 1  
/proc может вообще быть отключен на фс. 

тоесть когда процесс пишет read(1,..) он говори ядру. эй ядро прочитай там кусок из файла
который у тебя там в твоей памяти ядра в таблице моего процесса в графе файл дескрипторы 
за цифрой 0  у тебя помечен.  тоесть путь в /proc вообще тут никак не участвует!

интересна и колока TYPE в ней указан тип файлы который скрывается за этим файл дескрпиотором

DIR   значит что дескриптор ведет на папку
REG   значит что дескриптор ведет на обычный файл на диске
CHR   значит что дескриптор ведет на файл за которым кроется  CHAR устройство 
unix  значит что дескриптор ведет на файл за которым кроется  unix сокет

получатеся не все файл дескрипторы у процесса покаызватся потом в /proc/pid/fd и получается
не ковсем октрытым для процесса файлам он имеет доступ !!!


а вот как выглядит спмсок отыртых файлой у проги sleep

 # 霱  $ 
 " 爩  $ lsof -Pn -p $(pidof sleep)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
sleep   1888459 noroot cwd    DIR  254,0    12288 403710483 /home/noroot/git/docs/C
sleep   1888459 noroot rtd    DIR  254,0      234       128 /
sleep   1888459 noroot txt    REG  254,0    35040 805307925 /usr/bin/sleep
sleep   1888459 noroot mem    REG  254,0  3060384 671112721 /usr/lib/locale/locale-archive
sleep   1888459 noroot mem    REG  254,0  2014520     55931 /usr/lib/libc.so.6
sleep   1888459 noroot mem    REG  254,0   228376     46492 /usr/lib/ld-linux-x86-64.so.2
sleep   1888459 noroot   0u   CHR 136,27      0t0        30 /dev/pts/27
sleep   1888459 noroot   1u   CHR 136,27      0t0        30 /dev/pts/27
sleep   1888459 noroot   2u   CHR 136,27      0t0        30 /dev/pts/27
sleep   1888459 noroot  10w  FIFO   0,15      0t0  21319617 pipe
sleep   1888459 noroot  12w  FIFO   0,15      0t0  21319618 pipe


тоесть получается что мы имеем открытыим файлы:
  папка которая ткущий каталог
  rtd  я незнаю что это такое
  txt это файл с коодом бинарным ктторый исполняется
  три файа mem открыте через mmap() это шаред либрари подкючане со совими функиями
  и последние 5 файлов с дескрпиторами 0 1 2 10 12 это файлы доставшиеся от головного
  процесса . ибо exevve() незакрывает те дескпрторы которые были у процесса до вызова execve()
  а они были  у процесса потому что до этого в этом процессе крутится bash бинаркник!


таким макараом с смыслом лсоф и то что выводим стало блоее мнее попнятно!

тажкже стало понятно что не все файл дескрпторы котоыре у процесса есть они ему видны
и досутпны! также стало понятно что те файл дескрпиторы с которыми можно рабтать через
read() write() то это дескприторы в виде натуральных чисел 1  2 14 



теперь я вовзаращаюсь к юникс локальным сокетам.
создаю локальный юникс сокет. которая я биндю к папке ./un2

  # nc -l -k -U  ./un2

socket(AF_UNIX, SOCK_STREAM, 0)         = 3
unlink("./un2")                         = 0
bind(3, {sa_family=AF_UNIX, sun_path="./un2"}, 8) = 0
listen(3, 5)                            = 0
accept4(3, ...)

тоесть создаем сокет как структура в памяти. 
биндю его сажаю на файл ./un2 (аналог когдмы биндим сетевой сокет на тцп конект ip:port
тоест если в ядро прилетит тцп пакет на ip и тцп порт port то ядро знает что этот пакет пред
назначен для сокета такого то процесса) . значит ядро знает что если особым образом
поступит конект на папку ./un2 тоядро будет знать что этот конект эти байты они предна
значены для сокета от процесса nc

смотирим кааие фалы окртыты у этого процсса

# lsof -Pn -p $(pidof nc) 2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1890027 root   3u  unix 0x000000000c67d2a7      0t0  21464365 ./un2 type=STREAM (LISTEN)

подключаюсь клиентом и вот что вижу у серерного процесса наконец то 
ядро вернулось из вызова accept()

accept4(3, {sa_family=AF_UNIX}, [128 => 2], SOCK_NONBLOCK) = 4

почму? потому что наконец то в слушающий  сокет прилетабет от клиента первый конект.
и вызов акцепт он берет оттуда данные ядро имеет берет и создает новый сокет в серерном 
процессе уже через который этот конект может обслуживаться . смотрим что стало 
у серверного процесса

    # lsof -Pn -p 1890027  2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1890027 root   3u  unix 0x000000000c67d2a7      0t0  21464365 ./un2 type=STREAM (LISTEN)
nc      1890027 root   4u  unix 0x000000006d963ab4      0t0  21464366 ./un2 type=STREAM (CONNECTED)


как и ожидалось в дескрипторе 4


посмотрим как это вындяит у клиента 

  $ nc -U ./un2 
socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
connect(3, {sa_family=AF_UNIX, sun_path="./un2"}, 8) = 0

тоесть мы создаем сокет. и потом исоальзуя файл un2 как адрес (аналог ip:port)
мы "коннектимся" к удаленному процессу

смотрм на открыте файлы

# lsof -Pn -p 1890705  2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1890705 root   3u  unix 0x000000002aab75b6      0t0  21468682 type=STREAM (CONNECTED)


далее   ябуду пытаться чрез open() открывать все десрпитторы на сервере 
и также на клиенте 

# strace -e openat cat /proc/1890027/fd/3 2>&1 | grep proc
openat(AT_FDCWD, "/proc/1890027/fd/3", O_RDONLY) = -1 ENXIO (No such device or address)

# strace -e openat cat /proc/1890027/fd/4 2>&1 | grep proc
openat(AT_FDCWD, "/proc/1890027/fd/4", O_RDONLY) = -1 ENXIO (No such device or address)

# strace -e openat cat /proc/1890705/fd/3 2>&1 | grep proc
openat(AT_FDCWD, "/proc/1890705/fd/3", O_RDONLY) = -1 ENXIO (No such device or address)

итак видно что через open() нельзя "подключиться" ни к одному файлу в проц за которым
кроется юникс сокет!

проббуем через тот файл который на диске лежит который тоже сокет

    # strace -e openat cat ./un2  2>&1  | grep un2
openat(AT_FDCWD, "./un2", O_RDONLY)     = -1 ENXIO (No such device or address)

тоже пошел нахер

а еще пробуем вот так

  # nc -U /proc/1890027/fd/3

socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
connect(3, {sa_family=AF_UNIX, sun_path="/proc/1890027/fd/3"}, 21) = -1 ECONNREFUSED (Connection refused)

вобщем это очень неваравльая правика что можно пдключться через open() к 
простым файлам на диске через /proc файл дескрипторы! надо было это все 
запреить! а так они запретили только к дескприторам которые ведут на сокеты!

значит я тут неожиданно понял то что ключ -k у  nc то это не как у соката форк!
ниухя! это просто ключ о том что если клиент завершит конект то nc небудет выходить.
а продоолжит работу ! а никакого форка нет! nc приходящий конект в рамках своего
голвного процесса!


так вобщем чт я  хочу дальне сказать про юникс сокет.программа nc 
если ее запустить 

  $  nc -l -U ./un1 -k

она создает слушающий сокет.
socket(AF_UNIX, SOCK_STREAM, 0)         = 3
unlink("./un1")                         = 0
bind(3, {sa_family=AF_UNIX, sun_path="./un1"}, 8) = 0
listen(3, 5)                            = 0
accept4(3, 

и запускает accept() ожидая кода прилетит в слушащий сокетпервый конект
и ядро на основе него создать новый сокет и вернет его процессу

o nc]# lsof -Pn -p   1899772   2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1899772 root   3u  unix 0x00000000933e8605      0t0  21545692 ./un1 type=STREAM (LISTEN)


и вот  я делаю конект
и на серврее accept() возврашает дескрпитор 4 для этого коенкта

accept4(3, {sa_family=AF_UNIX}, [128 => 2], SOCK_NONBLOCK) = 4


]# lsof -Pn -p   1899772   2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1899772 root   3u  unix 0x00000000933e8605      0t0  21545692 ./un1 type=STREAM (LISTEN)
nc      1899772 root   4u  unix 0x00000000d9145d98      0t0  21546686 ./un1 type=STREAM (CONNECTED)


и далее nc севрер начинает сидеть и слушать только этот 4-ый десскрпиттор

poll([{fd=0, events=POLLIN}, {fd=4, events=0}, {fd=4, events=POLLIN}, {fd=1, events=0}], 4, -1

а новые акцепты он не делает!

это приводу к тому что можно меняться данными между клиентом и свререром
но если я пбуду подключаться новым клиентом , вторым клиентом то будет вот такое
у кдтиента 

socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
connect(3, {sa_family=AF_UNIX, sun_path="./un1"}, 8) = 0
poll([{fd=0, events=POLLIN}, {fd=3, events=0}, {fd=3, events=POLLIN}, {fd=1, events=0}], 4, -1

тоесть конект вроде как бы есть до сервера.
мой клиент он слушает через poll два дсекрпитра fd/0 которы STDIN и fd/3 который
сокет от конекта удаленного.

если я на клаве набью то он это из fd/0 прочитает и в fd/3 перешлет

write(3, "12121\n", 6) 

но! так как на сервере  нет исополняется accept то на сервере это в приожение не прилетит!

ядро на сверере принимает новые конекты в слушающий сокет.
но юзер приложение оно  сделало accept() только первому клиенту.
и сидит и только слушает сокет от превого клиента !

]# lsof -Pn -p   1901398   2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
nc      1901398 root   3u  unix 0x0000000088ddb1b8      0t0  21563614 ./un1 type=STREAM (LISTEN)
nc      1901398 root   4u  unix 0x00000000ff65618c      0t0  21557159 ./un1 type=STREAM (CONNECTED)

тот который 4 !

а новые конекты наше приджоение не проверяе! потому что apccept оно не выполняет!

поэтму это вынляит так - наш првый клиент подключился к срверу. и они друг с дуом
обмениваься сообщеняими. а вторйо клиент вроде как бы и покчлюися и шлет туда данные
и они туда оулалетают. но они там только в ядро прилетают. а в юзер приложении срверера 
ничего обб этом незнают. точнее знать не хотят.  в юзер приложение сервере ничегоне
повялвется!

ключ -k дает только то что при завенрешии сеанса от превого клиента срвер обратно
запустить accept()


socat раоботает както странно. 
если его заупстть без fork

    $ socat  unix-listen:./un3  EXEC:"sleep 120"

то вначлае все выгядит прилично
# lsof -Pn -p   1902086   2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID   USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
socat   1902086 noroot   5u  unix 0x0000000067d9a163      0t0  21563009 ./un3 type=STREAM (LISTEN)


а вот как выглядит после подключания клииента

# 
# lsof -Pn -p   1902086   2>/dev/null | grep -E "PID|STREAM"
COMMAND     PID   USER  FD   TYPE             DEVICE SIZE/OFF      NODE NAME
socat   1902086 noroot   5u  unix 0x0000000067d9a163      0t0  21563140 type=STREAM (CONNECTED)
socat   1902086 noroot   6u  unix 0x00000000a010e271      0t0  21563139 ./un3 type=STREAM (CONNECTED)
socat   1902086 noroot   8u  unix 0x000000006906660b      0t0  21563142 type=STREAM (CONNECTED)

как бы что это за хуйня
у нас исчез слушащий сокет. и появилось еще два. что за нахер?
тоесть начинается у соката все вот так

socket(AF_UNIX, SOCK_STREAM, 0)         = 5
bind(5, {sa_family=AF_UNIX, sun_path="./un3"}, 7) = 0
listen(5, 5)                            = 0
getsockname(5, {sa_family=AF_UNIX, sun_path="./un3"}, [7 => 8]) = 0
pselect6(6, [3 5], NULL, NULL, NULL, NULL

а когда прилететае клиенсткое подклчение то вот так

accept(5, {sa_family=AF_UNIX}, [16 => 2]) = 6
getpeername(6, {sa_family=AF_UNIX}, [112 => 2]) = 0
getsockname(6, {sa_family=AF_UNIX, sun_path="./un3"}, [112 => 8]) = 0
close(5)                                = 0
getpid()                                = 1902349
socketpair(AF_UNIX, SOCK_STREAM, 0, [5, 7]) = 0
socketpair(AF_UNIX, SOCK_STREAM, 0, [8, 9]) = 0
clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x71666fd8ca10) = 1902448
close(9)                                = 0
close(7)                                = 0
poll([{fd=8, events=POLLIN|POLLHUP}], 1, -1) = 1 ([{fd=8, revents=POLLIN|POLLHUP}])
pselect6(7, [5 6], [5 6], [], NULL, NULL) = 2 (out [5 6])
pselect6(7, [5 6], [], [], NULL, NULL



тоест он делает accept()
это понятно. потом идет какойто мудеж с сокетами.
потом он себя клонирует и в новом процессе запускает  команду что яуказал 

    sleep 120

ну и так как в исходном процессе у нас уже нет слушающего сокета
то новые конекты не могут прилетать

а еси я вот так сделаю


    $ socat  unix-listen:./un3,fork  EXEC:"sleep 120"

тогда будет тоже самое. при постулпении конекта сокат будет себя клонрироватьь
и уже в проессе клоне будет обслуживать конект. но что хороо что теперь в голвном
процессе он будет продолжать LISTEN то есть принимать новые конекрты!
в общем на этом пока все


тепер когда достаточно поговили про nc взвращаюсь к основному вопоросу про ssh и nc. 
тоесть соственно основной вопрос который  я поднял выше.
их связь. получается что nc это такая прогармма которая умеет форвардит поток байтов
который ей кинули на stdin на tcp conect тоесть в сеть.

ssh(fd/8) | STDIN(fd/0) -- nc ---  (fd/20)127.0.0.1:5656 -> 127.0.0.1:1080  --- (fd/21) proxy proga ---  (fd/22)127.0.0.1:9090 -> 1.2.3.4.5:22

получается nc создает tcp конект (127.0.0.1:5656 -> 127.0.0.1:1080) между собой и прокси программой. и чрез прокст протокол nc просит прокси программу сделать у себя тцп конект
до удаленного серера (127.0.0.1:9090 -> 1.2.3.4.5:22)
значит ssh через pipe кидает свойи байты на STDIN у nc. а nc через тцп конект эти байты по сети
пересылает на прокси сервер используя прокси протокол (но таккак прокси протоколы очень простые
то чаще всего это тупо выгдяит так что nc берет байты от ssh и просто их вкалдыает в тцп пакеты
котоыре он отсылает на прокси сервер). а тот вытаскивает эти байты и тупо их перекодывает в тцп 
конект до удаленного сервера. 
таким образом nc это просто прокладака межу ssh и прокси сервером. и эта прокладка все что
оригинального она умеет делать это разговаривать с прокси сервером на языке прокси протокола.
и все. так вои дебилно и смешно то что на самом деле ssh он сам понимает что такое прокси протоклы
потому что можно запустит так ssh что это будет связка из двух прокси процессов один на нашем
компе и этот прокси процесс понимает все прокси протколы. потом вторйо прокси на удаленном серврере
и между ними шифрованный тонель. поэтому я не понимаю почему в ссш тае сделано дебильно то что
если мы хотим с ссш путстиь трафик на прокси сервер то  мы конектиимся к прокси свреру не силами
самого ссш а через программу прокладку. это было бы логично если бы ссш бинарнки не умет разговари
вать на язке прокси проткооколов. но он умеет.  зачем тогда привлекать доп программу которая
вместо ссш разговариет с спрокси сервисом на язяке прокси протокола НЕПОНЯТНО
на счет того как ssh подлключает pipe от себя на stdin у nc. да как обычно. он создает через
ядро пару дескприров которые обесечивают пайп. потом себя клониурует. новый процесс имеет
теже дескприооры. потом он клонирует дескприоры пайпа в fd/0 и fd/1  а исходные убирает.
а потом он в этом процессе запускает nc чрез execve. и готово. потворюсь что nc нужен ssh 
потому что он якобы неумеет разговаривать с прокси сервером на языке прокси пртокола. и эту
задачу на себя берет nc. хотя ссш прекраснознает прокси протоколы и по идее ему этот nc
нахрен ненужен. а nc в целом работает так. он устаналивает tcp конект с удаленным сервером
и сервисом там.  а потом те байты которые ему поступают на fd/0 перепралвяет на удаленный сервис
через этот тцп конект. 

                                                           TCP конект
некая программа -->   STDIN-nc-(127.0.0.1:9999)  ~~~~~~~~~~~INTERNET~~~~~~~~~~> 1.2.3.4.5:22

таким образом программа кторая по каким то причинам не может сама соамтотельно достучаться
до по сети до удаленного сверра делает это черз посредник nc


                     посредник
некая программа --> ~~~~~~~~~~~~~~  1.2.3.4.5:22


особенно nc полезен если нужно достучаться до удаленного срвера через прокси сервис. 
потому что nc знает формат прокси протколов через которые нужно общаться с прокси сервисом

тогда получается между "некоей прогарммой" и конечным удаленным сервром будет два посредника.
вначале это nc потом прокси сервис. и только потом данные уже достиаагают удаленного сервиса
                 
                 pipe      TCP+прокси протокол                  TCP
некая программа ------> nc ~~~~~~~~~~~~~~~~~~~~> прокси сервис ~~~~~~> удаленный сервер конечный



вот пример.
вот у нас дерево процессов

 $ pstree -Asp 1742035 
  ...---ssh(1742035)---nc(1742036)

вот у меня процесс ssh 
и его пайпы
$ lsof -Pn -p 1742035 | grep pipe
ssh     FIFO    0,15      0t0  19432408 pipe
ssh     FIFO    0,15      0t0  19432409 pipe


вот у нас nc
у котрого теже самые пайпы и tcp конект ведущий на прокси
$ lsof -Pn -p 1742036 | grep -E "pipe|TCP"
nc      FIFO     0,15      0t0  19432408 pipe
nc      FIFO     0,15      0t0  19432409 pipe
nc      IPv4 19432410      0t0       TCP 172.16.10.1:35508->172.16.10.11:2080 (ESTABLISHED)


вот наш прокси
# lsof -Pn -p  484 | grep 35508
proxy IPv4  21346      0t0    TCP 172.16.10.11:2080->172.16.10.1:35508 (ESTABLISHED)

на счет того как коненкретно присобачивается nc к ssh
а вот идем в config файл


Host vasya  10.20.0.5
HostName 10.20.0.5
Port 22
ProxyCommand nc -X 5 -x 172.16.10.11:2080 %h %p
IdentityFile ~/.ssh/id_ed25519
User vasya
ForwardAgent yes

тоесть вот этой строкой

  ProxyCommand nc -X 5 -x 172.16.10.11:2080 %h %p

мы обьясняем ssh что когда  я буду ломиться на 

   $ ssh vasya
либо
   $ ssh 10.20.0.5

то он должен запутсить nc и через пайп его к себе присобачить и через него пускать
сетевой трафик чтобы достигнуть конечный сервер 

теперь про эту строку

    nc -X 5 -x 172.16.10.11:2080 %h %p

-x 172.16.10.11:2080 =  адрес прокси сервиса
-X 5 =  c прокси сервисом nc должен общаться по протоколу SOCKS5
%h %p  = встроенные перменные ssh. первая обрзначает коненый сервер который мы хотим достичь
и второе это порт. значения этих перменых задается строками
  HostName 10.20.0.5
  Port 22

можно конечно и вставтиь все напрмую

  ProxyCommand nc -X 5 -x 172.16.10.11:2080 10.20.0.5 22

но так неудобно потому что в файле config может быть дохрена хостов на которые мы 
пойдем через тот же прокси и строку вида

  ProxyCommand nc -X 5 -x 172.16.10.11:2080 %h %p

удобнее копировать через copy-paste

вместо прогармммы nc можно заюзать socat тока надо разбраться с какими ключами.
так то socat более мощная прога чем nc


далее 
что интеерсно что "присосаться" к дескрпитору чужого процесса который ведет на ПАЙП
нет никаких прблем!

    $   sleep 120 | cat

    $ lsof  -Pn -p $(pidof sleep)
COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
sleep   1919549 noroot   0u   CHR  136,2      0t0         5 /dev/pts/2
sleep   1919549 noroot   1w  FIFO   0,15      0t0  21677213 pipe
sleep   1919549 noroot   2u   CHR  136,2      0t0         5 /dev/pts/2
 | 綿  $ lsof  -Pn -p $(pidof cat)


COMMAND     PID   USER  FD   TYPE DEVICE SIZE/OFF      NODE NAME
cat     1919550 noroot   0r  FIFO   0,15      0t0  21677213 pipe
cat     1919550 noroot   1u   CHR  136,2      0t0         5 /dev/pts/2
cat     1919550 noroot   2u   CHR  136,2      0t0         5 /dev/pts/2

  $ echo 123 > /proc/$(pidof sleep)/fd/1

  $ echo 456 > /proc/$(pidof sleep)/fd/1
 


далее.
как через nc влетающий поток  пустить в шелл

  $ mkfifi /tmp/f
  $ /bin/sh -i 2>&1  < /tmp/f  | nc -l 127.0.0.1 1234 > /tmp/f

что мы имеем .у нас будет запущено два процесса 

    sh
    nc


у nc будет скажем три потока. 
    будет поток влетающий из интенрнета. 
    будет поток STDIN  
    будет поток STDOUT


у sh будет два потока
    будет поток STDIN  
    будет поток STDOUT


у nc 

  STDOUT ====> /tmp/f
  STDIN <===== STDOUT sh
  socket <===== internet


у sh 

  STDOUT ====> STDIN nc
  STDIN <===== /tmp/f

и получается будет так.
из интентерета прилетают байты и nc их сует в /tmp/f
sh оттуда из /tmp/f читает обрабатывает. и высирает реузулттат на stdout который летит
на stdin nc а он их пересылает в inernet

еще можно через локлаьный сокет

 $ /bin/sh -i 2>&1  < ./fif1  | nc -l -U ./un4  >  ./fif1

когда запускаем клиент 

  $ nc -U ./un4

то может показася что ничего не сработало. на самом деле это мы уже подклюачлисли
и провалились в новый шелл!


далее задача - суперважная с которой вобще все начинается
чтобы nc слушал принимал тарфик по сети по порт 1234
и перенаправлял его на порт 8888 . на порту 8888 сидит жинкс

    $ mkfifo ./fifo1
    $ nc -k -l 1234 < ./fifo1  | nc 127.0.0.1 8888 > ./fifo1

проверяею

    $ curl http://localhost:1234
сработало!


алгоритм работы такой nc(1234) он принимает поток из сети (из сетевого сокета) от клиент аи высирает
его на STDOUT который в свою очередь смотрит на STDIN от nc(8888) тот его пересылает в сетевой
сокет который ведете на хосте 127.0.0.1:8888 там сидит жинкс. он отвечает обратно в сеть
и через тот же сетевой сокет nc(8888) принимает этот поток и пеересылает его на свой STDOUT 
который смотрит на именоваый пайп fifo1.  далее nc(1234) принимает на свой STDIN который смотоит
на fifo1 ответ от жинкса и пересылает его в сеть через сетевой сокет на 127.0.0.1:1234 откуда
у нас позвонил клиент 

вот какая схема полета для запроса от клиента до жинкса

клиент                    nc1                      nc2                          жинкс
(cет сокет)  --------> (сет сокет) ---       --> (сет сокет) --------->  (сет сокет 127.0.0.1:8888)
                                     |       |
                         STDOUT   <---       ----- STDIN 
                           |                        |
                          \|/                      /|\
                           |                        |
                           --------------------------


обратный полет лень рисовать заморочно!



далее.
спрашивается а какой смысл этой команды?
что будет?

  $  nc -k -l 8181 

ну сидит nc и слушает порт 8181. и что? что будет если на него постучать?
а будет то что между клиентом и nc установится tcp конект.
и что дальше? 
а дальше то что - то что поступиит на STDIN у nc он перешлет в тцп конект тоесть по сети
к клиенту. а то что клиент перешлет по сети на nc то nc перешлет на свой STDOUT
в данном случае STDIN и STDOUT у nc смотрят на терминал. поэтому это будет отослано на 
экран терминала! вот какой смысл и эффект.
если мы перенарпавиим STDIN и STDOUT у nc на какито проги через галочки и пайпы то тогда
наш клиент сможет общаться с какйото программой через посредника nc

вот пример я запускаю nc

  $ nc -k -l -p 8181 

потом на него стучуть через curl

  $ curl http://localhost:8181

он устанвливает тцп конект с nc и шлет через него HTTP байты. 
nc принимает этот поток и переслыает его на STDOUT а тот смотртит на терминал поэтому все 
вылетеает на окно  терминала. и мы на терминале видим

    $ nc -k -l -p 8181 
GET / HTTP/1.1
Host: localhost:8181
User-Agent: curl/8.10.1
Accept: */*


вот у нас как с файлами открытыми у nc

    $ lsof -nP -p 1928419
COMMAND     PID   USER  FD   TYPE   DEVICE SIZE/OFF      NODE NAME
nc      1928419 noroot   0u   CHR    136,3      0t0         6 /dev/pts/3
nc      1928419 noroot   1u   CHR    136,3      0t0         6 /dev/pts/3
nc      1928419 noroot   2u   CHR    136,3      0t0         6 /dev/pts/3
nc      1928419 noroot   3u  IPv4 21730218      0t0       TCP *:8181 (LISTEN)
nc      1928419 noroot   4u  IPv4 21730219      0t0       TCP 127.0.0.1:8181->127.0.0.1:55280 (ESTABLISHED)


тоесть мы видим что 0 1 2 смотрят на терминал
также есть слушающий tcp сокет
и есть сокет уже обрабатыващий наш конкнетный конект с клиентом с curl

кстати я замечу то что все файловыые дскрипторы у прцесса которые имеют вид числа - это
те файлы с которым процессу реално разрешается самоу работыть - как то маниуелировать.
чиатть закрывать писать. менять режим работы. 

а вот я курл закрыл и как выгдяит nc

 > 乜  $ lsof -nP -p 1928419
COMMAND     PID   USER  FD   TYPE   DEVICE SIZE/OFF      NODE NAME
nc      1928419 noroot   0u   CHR    136,3      0t0         6 /dev/pts/3
nc      1928419 noroot   1u   CHR    136,3      0t0         6 /dev/pts/3
nc      1928419 noroot   2u   CHR    136,3      0t0         6 /dev/pts/3
nc      1928419 noroot   3u  IPv4 21730218      0t0       TCP *:8181 (LISTEN)

тоесть курл при закрытии себя он закрывает tcp конект. поэтому тут все чисто.
не висит тцп конект потерянный



далее.
далее вот появился такой вопрос
  
  https://stackoverflow.com/questions/70925679/curl-command-line-keep-connection-open/79184749#79184749


cуть его такова. человек делеает чеерз curl в качестве веб клиента запрос на веб 
сервер. и когда курл получает ответ то он закрывает TCP конект к серверу и полносью
заканчивает свою работу и мы возвращаемся в терминал 
и чуак спрашивает а можно как то уломать курл чтобы он незакрвыать тцп конект откртый к веб
серверу и просто вот так висел и ничего неделал.
значит тут основа всего вот какая - кода клиент обрщается к веб сверур то он создает TCP 
конект получает дескрпитор от этого конекта и в него пихает HTTP текст. так вот 
протокол HTTP/1.0 с дефолтовыми настройками он предписывал веб сервер что как только он сунул 
ответ в формате HTTP текста в тцп сокет то сразу звонит на ядро и просит ядро закрыть тцп 
конект.  там потом люди  вобход протоокола ( в обход rfc ) писали кастонмые веб свервры и 
кастонмые веб клиенты которые добавляли в http хидер поле 

    Connection: keep-alive

таким образом клиент сообщал сеерру чтобы он незкарывать тцп конектп после ответа.
а сервер если понимал эту фичу он в ответ в подрверждеие посылал ту же хрень.
спрашивается а зачем это надо? а чтобы клиент следущий хттп запрос отправил через тот же 
тцп конект. а зачем это надо? а потому что инициировать хттп конект это долгая операция
она треует переысылку тех тцп пакетов. это может занять до 300мс.
так вот в протоколе HTTP/1.1 уже по дефолту  подразумевается что всегда теперь новый тцп 
конект он будет использоваться для нескольких хттп реквестов. поэтому уже теперь всгегда
веб сервер не закрывает тцп конект  и для этого ненадо всталвят болше ниакие спец поля
в хттп хидер. но теперь зато если мы нехотим чтобы конект оставлся то мы можем вставть вот
такое поле в хидере

      Connection: close

оно гворит веб свереур что он может спокойно закрывать тцп конект когда нам отошлет ответ.

теперь возварращаемся к курл. он получает ответ от веб сервера и сам по своей воле инициирует
закрытие тцп конекта. а потом и вовсе заканчивает свое существование. веб сверер здесь 
ни при чем. показываю

    $  sudo strace curl -4 --noproxy "*" http://example.ex


socket(AF_INET, SOCK_STREAM|SOCK_NONBLOCK, IPPROTO_TCP) = 5
connect(5, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr("127.0.0.1")}, 16) = -1 EINPROGRESS (Operation now in progress)
poll([{fd=5, events=POLLOUT}, {fd=3, events=POLLIN}], 2, 0) = 1 ([{fd=5, revents=POLLOUT|POLLERR|POLLHUP}])
close(5)          

я вот не очень понимаю. по идее poll() он как раобтает. это следилка за дескприторами.
мы говорим ядру  - эй ядро вот спмсок дескприоров ты мне скажи когда какото из них будет
иметь инфо внутри. и ядро сует процесс в слип стейт. до тех пор пока на кайото десприктор
не прилетят данные. тогда ядро просыпает процесс и он както там понимает какой дескриттор 
имеет данные. и далее по идее должна идти команда read() которая бы читала данные в процесс
из дескприра на котором появились данные. так вот видно из трейса что я привел что никакого 
read() не происходит. хотя вот код другой программы там вместо poll() идет select() 
и там четко все как я говорю

  pselect6(1, [0], NULL, NULL, NULL, {sigmask=[], sigsetsize=8}) = 1 (in [0])
  read(0, "2", 1)                         = 1


но тем не менее озваращаюсь к трейсу от курл. 
зато четко видно то что курл делает close(5) где 5 это дескриптор сетевого сокета
правда может возникнуть подозрение о том что может быть все таки веб сервер с той стороны
инциировал тцп конект разрыв? а я щас проверю через дамп сети!

  $ sudo tcpdump -n -i lo tcp port 8888
    IP 127.0.0.1.51158 > 127.0.0.1.8888: Flags [F.]
    IP 127.0.0.1.8888 > 127.0.0.1.51158: Flags [F.]
    IP 127.0.0.1.51158 > 127.0.0.1.8888: Flags [.]


так вот видно что первым инициатором закончитть тцп конект является  127.0.0.1.51158
а это точно не веб сверрер это клиент!

а что касается дампа HTTP потока между клиентом и сервером

  $ curl -4 --noproxy "*" http://example.ex:8888 -v

то там четко видно что с сервера не прилает ниаких сообщений что он собирается тущить конект.
да и в HTTP протоколе насколько я знаю нет таких сообщений диркетив и команд - что мол 
эй я собираюсь тушить конект.
более того сервер когда нам ответчает он нам намекает и обозначает что он со своей стороны
тушить тцп конект не собирается!

< HTTP/1.1 200 OK
< Server: nginx/1.26.2
< Date: Wed, 13 Nov 2024 13:22:17 GMT
< Content-Type: text/html
< Content-Length: 615
< Last-Modified: Fri, 16 Aug 2024 17:52:58 GMT
< Connection: keep-alive


об этом вот эта строчка говорит с его стороны

  < Connection: keep-alive


итак выяснено то что - если мы шлем HTTP/1.1 запрос на веб сверер. то он со своей
стороны тцп конект не убивает после того как отослал нам хттп ответ. а это делает
наш сам курл который выстпает в роли веб клиента.  более того сервер со совей стороны
прям явно говорит эй чувак я со своей стороны тцп конект не закрывай имей ввиду!

  < Connection: keep-alive

значит можно попробовать заставить курл не закрывать тцп конект к веб сереверу путем вот
какой хитрости - в курл можно втсвить нескоько веб ссылок. первую ту которую хотим открыть
а вторую типа левую ссылку. левая ссылка будет указывать на такой биндинг  за которым сидит
nc. курл по дефолту он ссылки открывает поочереди. делает веб запрос на первую ссылку. 
получает ответ. и потом уже пееходит на вторую ссылку. и дело в том что у нас же nc 
только создаст тцп конект. курл туда кинет ХТТП запрос. но nc в ответ конечно же ничего не 
отошлет и курл будет висеть вечно и ждать ответа от nc.  и фишка втом что когда курл получает
ответ от певрого веб сервера по тцп он зная что у него впереди вторая ссылка которую надо 
межд прочим открвыть через новый тцп конект то он первый тцп конект незакрывать. он осталвляет
его как есть. и просто открывает второй тцп конект и шлет туда хттп запрос. здесь нужно
подчернуть что вторая ссылка она ведет на другой биндинг. именно поэтому курл хочет он или нет
он вынужден и обязан отркывать новый тцп конект 

    $ curl -4 --noproxy "*"  http://localhost:8181   http://example.ex:8888 -v 

тоесть первый тцп конект будет к 127.0.0.1:8181 
а второй тцп конект будет к 127.0.0.1:8888
поэтому это потрубет от курл однозначно два тцп конекта создавать. 
так вот конечно вопрос как поступает курл когда он чрез первйый тцп конект получет ответ.
далее у нее выбор лиобо закрыть этот тцп конект ибо он ему уже не будет нужен. либо его
оставить открытым и переходит ко второму урлу и соддвавать второй тцп конект. в целом есть
логика не зкрывать уже откртый первйы тцп конект потому что можеттак случиться что вторая
ссылка она ведет на тот же биндинг например вот такк

    $ curl -4 --noproxy "*"  http://localhost:8181/index1.html   http://localhost:8181/index2.html

и поэтому оба урла можно запросить через один и тот же тцп конект на 127.0.0.1:8181
поэтому если мы курлу дали несколько урлов то есть смысла незакрвать никакие тцп конекты
которые он уже открыл на случай если они еще пригодятся. я так считаю.

можно еще вот так сделать. можно курлу сказать чтобы он все урлы пытался открыть паралельно.


  $ curl --parallel --parallel-immediate --parallel-max 5 https://httpbin.dev/headers https://httpbin.dev/headers https://httpbin.dev/headers

подсмотрел тут 
    https://scrapfly.io/blog/how-to-send-multiple-curl-requests-in-parallel/


опция --parallel-immediate говорит курлу о том чтобы он на каждый урл создавал новый тцп
конект что бы было точно бытсрее а если просто использовать --parallel то он будет якоббы
стараться урлы которые можно просунуть в один тцп конект совать в один. 

так или иначе вроде как да - эскпримент показыает что если курлу скармилваем несколько 
урлов сразу то он закрывает тцп соединения только уже в самом конце когда он от всех 
урлов получил ответ и собирается выходить в баш уже. причем опятьже я четко роверил если
мы неиммполуем прараельную орбрабтку от курл обрабвыает урлы строго слева направо.
первым он запрашиваыет самый левый урл.  поэтому  в итоге я делаю вот так.

я запускаю nc

    $ nc -k -l 127.0.0.1  8181

это у нас будет такой "веб сервер" который тцп конект принимает. а на хттп запрос никогда 
не ответчает

и делаю реквест через курл

  $ curl -4 --noproxy "*"   http://example.ex:8888  http://localhost:8181   -v 

первая ссылка это наш реальный жинкс с которым я хочу чтобы конект тцп осталвся открытым
а вторая ссылка это сслыка заглушка на nc
курл полчает ответ по первой ссылке. преходит на вторую. отркывает туда тцп конект. 
послыает туда хттп реквест ничего в ответ н получает и висит и ждет ответ вечно. 
а вот подтврждение что все работает

вот смотри на curl

 $ lsof -nP -p $(pidof curl) | grep -E "FD|TCP"
COMMAND     PID   USER  FD   TYPE   DEVICE SIZE/OFF      NODE NAME
curl    1955228 noroot   5u  IPv4 21922040      0t0       TCP 127.0.0.1:41068->127.0.0.1:8888 (ESTABLISHED)
curl    1955228 noroot   6u  IPv4 21922042      0t0       TCP 127.0.0.1:38940->127.0.0.1:8181 (ESTABLISHED)

четко видно что у него ESTABLISHED оба тцп конекта. что он ни один не закрыл!

а вот ситуация на жинкс

$ sudo lsof -Pn -p  1792527   2>/dev/null | grep -E "FD|TCP"
COMMAND     PID USER  FD      TYPE             DEVICE SIZE/OFF      NODE NAME
nginx   1792527 http   3u     IPv4           21920352      0t0       TCP 127.0.0.1:8888->127.0.0.1:41068 (ESTABLISHED)
nginx   1792527 http   5u     IPv4           12351541      0t0       TCP 127.0.0.1:8888 (LISTEN)

и вот виден точно аткойже тцп конект как  у курла

тоест мы доились чего хотели!

мы сходили на веб сервер курлом. получили ответ. курл его получил.  и он после этого 
незакрытл тцп конект на этот веб сверер

КСАТАТИ! на жинксе ест переменная keepalive_timeout она задает то что вот у жинкса есть 
тцп конкт от клиента. он клиету на запрос отвеил. тцп конект остался открыым. далее 
болше хттп реквестов от клиента не прилетает. так вот жинкс ждет 60с и если так еще больше
хттп ревесет ноывый не прилетит то жинкс тогда закроет этот тцп конект через 60с!

ксатти можно заставит веб сервер самого закрыть тцп конект когда он нам ответил. 
всего нвсего надо ему от клиентпа передать поле

  Connection: close


  $ curl -4 --noproxy "*" -H "Connection: Close"  http://example.ex:8888  http://localhost:8181   -v 

вот проверяем. на жиксе тцп конект закрыт

$ sudo lsof -Pn -p  1792527   2>/dev/null | grep -E "FD|TCP"
COMMAND     PID USER  FD      TYPE             DEVICE SIZE/OFF      NODE NAME
nginx   1792527 http   5u     IPv4           12351541      0t0       TCP 127.0.0.1:8888 (LISTEN)

прокрим что на curl
$ lsof -nP -p $(pidof curl) | grep -E "FD|TCP"
COMMAND     PID   USER  FD   TYPE   DEVICE SIZE/OFF      NODE NAME
curl    1955976 noroot   5u  IPv4 21925352      0t0       TCP 127.0.0.1:59650->127.0.0.1:8181 (ESTABLISHED)

тоже все поплану - толко тцп конект на второй урл  которы не ответчает!

так вот можно вместо курл и танца с бубнами заюзать nс как веб клиент

берем делает файл в который заносим хттп реквест

 ` ້  $ cat http.txt 
GET / HTTP/1.1
Host: example.ex
Connection: keep-alive

и делает запрос через nc

 $ nc 127.0.0.1 8888 < http.txt 
HTTP/1.1 200 OK
Server: nginx/1.26.2
...


конект тцп осается открытым. потому что срвер его незкарывает так как мы юзаем HTTP/1.1
и более того мы еще перстархываеися и доблавяем необзяательное поле в наш реквест

    Connection: keep-alive

и клиент наш тоже тцп конет не закрвыает. все мы получили что хтетели!

проеряем что там на счет тцп конкутов

смотрим на nc
    $ lsof -nP -p $(pidof nc) | grep -E "FD|TCP"
COMMAND     PID   USER  FD   TYPE   DEVICE SIZE/OFF      NODE NAME
nc      1956673 noroot   3u  IPv4 21927351      0t0       TCP 127.0.0.1:59370->127.0.0.1:8888 (ESTABLISHED)
  


смотрим на жинкс
    $ sudo lsof -Pn -p  1792527   2>/dev/null | grep -E "FD|TCP"
COMMAND     PID USER  FD      TYPE             DEVICE SIZE/OFF      NODE NAME
nginx   1792527 http   3u     IPv4           21928269      0t0       TCP 127.0.0.1:8888->127.0.0.1:59370 (ESTABLISHED)
nginx   1792527 http   5u     IPv4           12351541      0t0       TCP 127.0.0.1:8888 (LISTEN)


повторюсь это яращбирал вот этот вопрос

    https://stackoverflow.com/questions/70925679/curl-command-line-keep-connection-open/79184749#79184749


потом я встретил вот этот вопрос

  https://stackoverflow.com/questions/37306068/do-https-connections-require-https-proxies-or-can-i-use-http-proxies/79187847#79187847

вопрос был про HTTPS прокси. прикол в том что это неверный термин.
нет такой хрени как HTTPS прокси.  когда гоорвят XXX прокси то имеют ввиду тот язык
проткол на котором  прокси клиент и прокси свреер говорят друг с другом.
есть SOCKS прокси прооокотокл. есть HTTP протоокол в котором есть  хрени которые позволяют
часть этого протокола исползовать как прокси протокол.  так вот у хттп протокола есть 
два подхода его использования как прокси протокола. первый подход это кода прокси клиент
шлет на прокси севрер оббычные хттп запросы с чуть изменным заголовком. такой подход
позволяет нам через такой прокси  "проксирвовать" чистые HTTP запросы (без TLS).
такой подход позволякт нам свяаться с удаленным вебсайтом на связвке tcp+HTTP
мы обясняем прокси свреру на хттп языке чтобы он связался от нашего имени с удаленным
веб свервером на tcp+HTTP языке. как следсвтие при таком подходе мы можем в бразуерре
открывать только ссылки имеющие вид http://.. .
тоест такйо подход позволяет нам "проксирвать" толко HTTP протокол и больше нкиакой.

второй подход наш прокси клиент шлет на прокси сверер на языке HTTP метод CONNECT.
это говорит прокси севреру чтобы он по тцп связался с удаленным сервером. а что дальше
делать он незнает. а дальше мы шлем на прокси сервер уже просто тупо байты. он 
их вытасвивает и вставлет в тцп конект до удаленного сврера. прокси сервер уже непонимает
чем мы там занимается с удаленым сервером. он просто принимает байты и пересылает байты
на удаленный сервер. 
при таком подходе мы можем "проксирвать" любой портокол котоырй базируется на TCP. - FTP, DNS,
SMTP, POP3, HTTPS (TLS с HTTP внутри зашифрованным)

так вот второй подход это тоже HTTP прокси . потому что юзает HTTP как прокси протокол.
а в файрфокс эти деилы называют его "HTTPS прокси" это полня брехня. это такой же HTTP 
прокси. разница в том что через сервер указанный в файрфокс в строке "HTTP прокси" файрфокс
может "проксироовать" или открыть урл вида http://....
а через сервер указанный в файрфокс в строке "HTTPS прокси" файрфокс
может "проксироовать" или открыть урл любого вида http://....  httpS://....  ftp://....
scp://....   sftp://....  
при досутпе к серверу из первой строчки файрфокс использует хттп запросы с модифицированными
заголовками а при досутпе к свреру из вторйо строчки файрфокс юзает CONNECT


так вот есть еще такие прокс которые работает через "HTTP proxy via TLS" 
наколько я понимаю файрфокс с аткими раотать неможет. а курл якобы может.
схема выгдядит так - прокси клиент на первом шаге устаналивает с прокси сверером конект 
через TLS. а когда между ними шифрованный тонель поднять то пускает HTTP proxy реквесты
через этот шифранный тонель. эта схема дает то что клиент может 100% быт уверен что он 
шлет и ему отвечает именно ИСТИННЫЙ прокси сервер (подлнность которого обесеивается TLS в 
частсрности сертфикатом 0). это дает вот что. положим у нас обычнчй HTTP прокси. тогда
злодей между нами и нами и настояимщим прокси сервером перехвает наш трафик. положим что
мы юзаем CONNECT для досутпа к прокси. и через "проксификацию" шлем TLS трафик (HTTPS)
тогда злодей неможе расшфировать наш трафик. но! он знает dest_IP уаденного серервера куда
все летит (удаленного именно а не прокси серера!) и через SNI он знает домен веб сайта 
на котоый мы лезем. а это неприятно.   тоесть злодей незнает что мы шлем на удадленный сайт
но знает куда мы шаримся. если же у нас между прокси клиентмо и прокси среером ест TLS
то ничего этого злодей неузнает!

вот здесь написно про HTTP proxy via TLS

  https://stackoverflow.com/questions/45874515/what-is-https-proxy


насколько я понимаю если мы хотим через курл достчаться именно до токого прокси серера
то мы это указвыаем в опции -x

  -x httpS://......

тоеесть 

  -x socks4://...    (это мыломимся на сокс прокси)

  -x http://  (это мы ломимся на HTTP прокси)

  -x https://.  (это мы ломимся на HTTP прокси via TLS)


если мы ломиися на обычный HTTP прокси то ключ -p в добавок к -x

 -p  -x http://  

говорит курл чтобы он юзал CONNECT метод

если мы ключ -p невставляем то лика курла вот какая если мы ломимся на http url
то он неиспользует CONNECT а исползует модифицированный обычне http реквесты в прокси
проткоооле


  -x http://...  http://google.com


а если у нас ссылка https то  для связи с прокси будет заюзан CONNECT

  -x http://...  httpS://google.com



ебануться можно.


далее.
курл умеет рыбоать нетолько с HTTP но и сдруим выскоими аппликейшн проктолками.например 
с ftp. об этом сказано в мане

    $ curl ftp://ftp.slackware.com/welcome.msg


далее 
еше раз пример про  --parallel

  $ curl --parallel --parallel-immediate --parallel-max 5 https://httpbin.dev/headers https://httpbin.dev/headers https://httpbin.dev/headers



далее
про nc
я обнаружил что если я юзаю nc в связке с прокси сервером то nc к сожалению рапортует
что tcp порт успешно открыт на удаленном сервере хотя прокси сервис несмог успешно установить
tcp конект с удаленным сервисом! это пиздец
и код возврата будет 0.
пример


      $ nc -4vnz   -X 5  -x 172.16.10.11:2080    74.6.231.21  448; echo $?
      0

опции   -X 5    -x 172.16.10.11:2080   говорят что нужно через SOCKS5 протокол
связаться с прокси сервером  172.16.10.11:2080

опция -z говорит в случае tcp протокола nc тольлко смотрит ядро успешно сделало ли 
трехшаговый хендшейк (проверяется это через код возврата от connect() ) и все. тоесть
в устанолвенное tcp соединение nc непихает никакие байты

таким образом к моему ужасу nc нельзя проверять на счет "открыт ли tcp порт" если мы хотим
это сделать через прокси сервис!

как это сделать через socat я толком ненашел и нестал заморачиваться






>>>> неотвеченые вопросы

nginx не арботает как forward proxy для https трафика потому что 
он не поддреживает метод CONNECT?


ксатти пимомо SOCSK и HTTP прокси протколоков есть еще прокси проткол от haproxy


curl как SCP клиент. или другой диковыиинны клиент
https://www.filestash.app/ftp-with-curl.html
socat examples
https://www.filestash.app/ftp-with-curl.html
??? на теелефоне текстоый файл обрабоать в зап книжке 


у жжнкса один воркер может иметь неолкко открытых тцп конектов.
попробвать использовать жинкс как хттп прокси
оказывается почемуто можно помлать прсто HTTP запрос на прокси типа GET и он зафоровадит
наш запрос дальhttps://www.w3.org/Protocols/rfc2616/rfc2616-sec5.htmlше. это рабоатет только для HTTP запроса не HTTPS
окзывается еще прокси бывают такие что у нас между клиентом и прокси связь идет
через HTTPS!

как жинкс можно использовать как прокси сервер?

как происходит преерключение с HTTP/1.1 на HTTP/2 и самое интересное на HTTP/3
и в свете этого QUIC через xray может работать ?
https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html

выше я распиал примеры прокси протоколов. теперь надо скзаать названия программ которые умеют
работать с прокси протоколами. это :  ssh, squid, dante, haproxy, nginx(это неточно)


реверс прокси и форвард прокси ???

кстати при TCP конекте мы через 3 пакетный хендшейк мы убеждаемся что на той стороне реально
есть ктото живой! в отличие от UDP когда мы нихрена не знаем есть ли  там ктот живой. мы
просто отослаhttps://www.w3.org/Protocols/rfc2616/rfc2616-sec5.htmlли и хер знает живой там ктото или нет
 
netcat (nc)    vs   socat?


поробвать все таки squid попровать потавить с фичей ssl_bump ?

как вот так сделатть чтоы создать неймспейс сетвой привзяь его к tap2 даеть ему IP и
чтобы он был за br0
и на его базе https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.htmlзапустит процесс sing-box. тоесть чтобы без виртуалки?



ссылки в том числе непроработанные:
https://www.filestash.app/ftp-with-curl.html
https://www.w3.org/Protocols/rfc2616/rfc2616.html
https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html
https://en.wikipedia.org/wiki/SOCKS
https://www.linuxbabe.com/nginx/difference-between-npn-and-alpn-plus-how-to-enable-alpn-on-your-site
https://github.com/httpwg/http2-spec/wiki/Implementations
https://en.wikipedia.org/wiki/Application-Layer_Protocol_Negotiation
https://everything.curl.dev/cmdline/urls/parallel.html
https://everything.curl.dev/http/versions/http2.html
https://everything.curl.dev/libcurl-http/multiplexing.html
https://everything.curl.dev/cmdline/urls/parallel.html
https://stackoverflow.com/questions/67432199/how-to-send-multiple-http2-requests-over-the-same-connection-with-libcurl
https://stackoverflow.com/questions/36517829/what-does-multiplexing-mean-in-http-2
https://www.baeldung.com/cs/http-versions
https://www.ietf.org/rfc/rfc2068.txt
https://stackoverflow.com/questions/7577917/how-does-a-http-proxy-utilize-the-http-protocol-a-proxy-rfc
https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt
https://seriousben.com/posts/2020-02-exploring-the-proxy-protocol/
https://dev.to/somadevtoo/difference-between-forward-proxy-and-reverse-proxy-in-system-design-54g5
https://superuser.com/questions/604352/nginx-as-forward-proxy-for-https
https://github.com/chobits/ngx_http_proxy_connect_module
https://ubuntuhandbook.org/index.php/2024/06/lightweight-socks5-ubuntu-debian/
https://bbs.archlinux.org/viewtopic.php?id=127133
https://www.alibabacloud.com/blog/how-to-use-nginx-as-an-https-forward-proxy-server_595799
https://stackoverflow.com/questions/46060028/how-to-use-nginx-as-forward-proxy-for-any-requested-location
https://www.baeldung.com/nginx-forward-proxy
https://crosp.net/blog/administration/install-configure-redsocks-proxy-centos-linux/
https://medium.com/@jogarcia/breaking-the-proxy-walls-with-redsocks-in-linux-f4c1bfb6fb6a
https://dev.to/somadevtoo/difference-between-forward-proxy-and-reverse-proxy-in-system-design-54g5
https://dev.to/iaadidev/step-by-step-instructions-for-forward-proxy-setup-c22
https://wiki.archlinux.org/title/Proxy_server
https://www.tecmint.com/open-source-reverse-proxy-servers-for-linux/
https://proxiesapi.com/articles/how-do-i-make-curl-ignore-the-proxy
https://superuser.com/questions/484265/use-netcat-as-a-proxy-to-log-traffic
https://www.rutschle.net/2007/05/05/Port_redirection_and_transparent_proxies.html
https://askubuntu.com/questions/11709/how-can-i-capture-network-traffic-of-a-single-process
https://stackoverflow.com/questions/4777042/can-i-use-tcpdump-to-get-http-requests-response-header-and-response-body
https://oxylabs.io/blog/curl-with-proxy
https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/CONNECT
https://www.ietf.org/archive/id/draft-luotonen-web-proxy-tunneling-01.txt
https://datatracker.ietf.org/doc/html/rfc2616#section-9.9
https://en.wikipedia.org/wiki/HTTP_tunnel
https://developer.mozilla.org/en-US/docs/Web/HTTP/Proxy_servers_and_tunneling
https://oxylabs.io/blog/socks-vs-http-proxy
https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt
https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/
https://en.wikipedia.org/wiki/SOCKS
https://stackoverflow.com/questions/67463216/how-does-a-browser-know-if-a-site-supports-http-3

