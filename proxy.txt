| proxy

значит как юзер приложение (процесс) связывается с удаленным
компом ? 
юзер приложение делает с ядру запрос чтобы ядро создало tcp конект с удаленным
процессом. ядро это делает. а нашему процессу возвращает указатель (файл дескриптор)
в который мы можем писать байты. любые байты. мы суем в указатель байты а ядро их тогда
подхватыывает упаковыавает в TCP+IP+ETHERNET и сует в сеть. это все летит на удаленный комп
там обратный процесс и удаленный процесс получает ровно те байты котоыре мы сунули в
наш указатель.  итак с точки зрения нашего процесса запись в сеть это всего навсего 
запись байтов в указатель. вот так просто. и не более того. для процесса как бы сети и нет
нет всех этих сложных подробностей сетевых протоколов. для процесса это просто процесс
записи тех байтов которые процесс хочет передать удаленному процессу в указатель.
для процесса это все выглдяит вцелом ровно также как если бы процесс писал в указатель
за котррым сидит pipe ядра. тоесть если два процесса связать пайпом то для процессов
все будет вылядеть вцелом также. что в пайп писать что в сеть. это все остается прозрачно
невидимо с точки зрения как это выглядит и запаривает процесс.
при записи в файл на диске для процесса все вяглядит также. мы просим ядро открыть файл.
ядро его както там открывает. а процессу возвращает указатель. тогда процесс точно также 
сует байты в этот указатель а ядро их уже сует на диск. тоесть для процесса скрыты все подро
бности сети. процессу всего навсего нужно совать байты в указатель. а все остальные подробности
которые нужны для преедачи по сети делает ядро совершенно невиимо прозрачно для процесса.

итак у нас есть юзер программа. она в ос выглядит как процесс.  вобласти памяти ядра есть кусок
памяти в которой находятся разные структуры в которых ядро записывает разные хрени которые
ядро использует при обслуживании этого процесса. там же лежат файл дескрипторы этого процесса.
в /proc мы из юзер спейса можем посмотреть часть этих служебных переменных и параметров храня
щихся в памяти ядра про наш процесс. тамже можно увидеть номера всех файл дескрпиторов которые 
ядро создало для нашего процесса. можно увиедеть ряд подробностей что там кроется на бекенде
от этих файл дескрипторов. напрмиер можно увидеть что данный файл дескриптор ведет на файл
на физ диске. или ведет на сетевое соединение.  так вот lsof умеет вытаскивать прикольную
информацию о том что скрывается за файл дескриптором. 

у нас за каждоый файл дескриптом скрывается какйото бекенд - либо файл на диске, либо пайп (буфер в
ядре), либо сетевое соедиенние. 
соовтесвтенно как работает запись в указатель. юзер процесс запускает glibc функцию с параметрами ,
эта функция запускает сисколл. сисколл переключает цпу на исполненение ядерного кода. 
он запускается и он видит вот что - что юзер процесс его вызывал для того чтобы скопировать 
байты которые лежат в памяти начиная с такого то адреса в файл дескриптор. для ядра файл 
дескриптор это фуфло ширма прикрытие. ядро смотрит за что отвечает данный файл дескприптор.
например он указывает на сетевое соединение. тогда ядро берет байты из памяти. упакоывает их 
в TCP+IP+ETHER пакет и сует в сетевую карту. и оно вылетает в сеть. если за файл дескпритптором
кроется файл на диске то он эти байты сует на диск. 

итак если процессу нужно передать на удаленный процесс какойто поток байтов(пусть это будет
текст в формате HTTP) по сети. то процесс решает через какой стек протоколов он хочет передать
этот поток байтов. например мы решили что мы будем передавать через связку TCP+IPv4
то мы узнаем dest_IP и dest_port. далее наш процесс делает ряд вызоовов функций glibc в 
через которые мы обьясняем ядру что мы хотим чтобы ядро установило TCP конект с удаленным
компом который имеет IP=dest_IP и TCP_port=dest_port . ядро делает этот конект и в таблице
памяти нашего процесса создает +1 файл дескриптор номер которого он и возвращает нашему процессу.
теперь наш процесс можем используя glibc фнукции пихать любые байты в этот указаетель и тогда
ядро и прочие хрени доставят эти байты тому удаленному процессу который на удаленном компе 
"привязан" удаленным ядром к тому dest_IP+dest_port.

таким образом если я знаю что у меня процесс пишет данные в сеть по TCP то это значит что у него
есть файл дескриптор у которого в свойствах будет указаны параметры сетевого соединенеия 
котроое ядро установило с удаленой системой а именно 
src_IP+src_port и dest_IP+dest_port и это всегда можно посмотреть используя lsof
итак ядро обеспечивает нам конект. и выдает процессу ссылку. а процесс со своей стороны сует
в ссылку полезные данные. которое потом ядро через конект сует в сеть.
конект у нас определяется ip адресом и портом. адрес задает другой хост. порт задает другое
прилоожение на том хосте. ядро изготовлено так что оно привязывает конкретное прилоожоение
к порту tcp. 

предположим что с нашего компа-А мы хотим переслать пакет с dest_TCP_port=443 на комп-Е.
и у нас на файрволле нашего компа-А или какогтто компа-Б в сети на пути пакета к 
конечному компу-Е закрыт проход. зато файрволл разрешает пролетание пакетов например 
на порт 4444
и у нас есть  доступный комп-Д у которого если с него слать пакеты на dest_TCP_port=443
то такой tcp пакет пройдет успешно. значит было бы прикольно установить конект от
нашего компа-А до компа-Д чтобы мы стучались с нашего компа-А на IP-Д на порт 4444
а тот комп пересылал наши пакеты на комп-Е на 443

один из методов это можно на том транзитном компе-Д настроить iptables правила что если пакет
прилетает  с IP-А нашего компа то делать DNAT тоесть менять dest_IP c IP-Д на IP-E и менять
dest_port c 4444 на 443

эта хрень будет работать на уровне ядра компа-Д и никакие юзер процессы для этого на том
компе ненужны. 

однако минус в том что это негибко! программа на компе-А он сейчас хочет достучаться до компа-Е
а потом до компа-Ж а потом до компа-З и на компе-Д через iptables ненастроить так правила
чтобы можно как то было переключаться. тоесть нет ниакой гибокость чтобы выбирать налету
и налету менять иптейбл правила на какой комп переправлять пакеты от нашего компа. 
iptbales отлично подходит для устанолвения "статических" перенаправлений. 

поэтому придумали прокси. поэтому на сцену выходит новый инстурмеент прокси. 
если iptables это набор правил в ядре. и обрабатывает эти привила ядро то прокси это всегда
юзер программа. сетевой поток если мы говорим о прокси всегда обрабатываеся юезер программой
тоесть юзер процессом. юзер кодом. тоесть унас обязательно в списке процессов должен виесть
какойто процесс. который ообрабатывает сетевой поток. 
помимо процесса также был придуман новый проотокол. это протокол выше транспортного уровня.
они так придумали. поэтому он обрабатывается не ядром а исклюительно юзер спейс кодом. 
так повелось что протоколы уровня транспортного L4 и ниже они обычно вмонтрованы в ядро. 
а юзер процесс работает с ними через  функции glibc. условно говоря что мы решаем какой
транспортный l4 протокол мы хотим заюзать. вызываем socket() там указываем этот транспортный 
протокол и его параметры. ядро нам возвращает указатель. и дальше мы просто пихаем данные
полезные именно нашей программы. незаботясь  о подроностях протокола L4 и все что ниже ,так
как этим занимется ядро.  а протоколы уровня L5 и выше уже проблема юзер кода. юзер
программа должна в своем коде их сама реализовывать. так вот они решили что прокси протокол
это будет протоокол выше чем   L4.  наверно при желании прокси протокол можно было бы вкорячить
в само ядро. и юзер программа бы работала с ним через новые функции glibc. но они так не 
захотели. поэтому поток данные байты от прокси протокола их формиорвание и оббаботка это 
искючиетльно проблема юзер спейс программы. 
прокси протокол нам что дает - он нам дает то что мы берем нашу программу на компе-А которая
хочет достучаться до компа-Ж на порт 444 и мы звоним комп-Д где работает ююзер процесс
который понимает прокси протокол и мы туда обращаемся через прокси протокол и в нем мы
обьясняем или задаем тот комп-Ж его dest_IP и dest_port 443 на котрый мы хотим постучать.
и прокси программа на компе-Д нам пробросит наш трафик туда куда мы заказываем. 
если мы хотим попасть на комп-З то мы меняем параметры в прокси протоколе . формируем новый
прокси пакет. звоним на комп-Д. и он читает наш прокси пакет. и перенаправляет трафик легко
и непринужедденно на комп-З. таким образом в отичие от iptables мы можем используя прокси
протокол и прокси процесс легко и динамично выбирать тот конечнйы комп на который мы 
хотим обратиться. в этом и есть главная фишка придумки прокси протокола в отличие от инстурмента
iptables который  я привел вначале. вот ответ на вопрос нхрен был придуман прокси протокол
еслои у нас уже был iptables.
потом еще такой момент. вот нашей программе нужно куда то попасть. если юзать iptables то 
нужно получать рут доступ на тот комп-Д и там под рутом менять правила иптейблс.
если же мы юзаем прокси хрень то нам на компе-Д ненужен никакой доступ в плане к командной 
строке. мы через сеетевой поток через сетевой прокси протокол спокойно сами без посредников
задаем на какой комп мы хотим в итоге достучаться. мы управляем потоком сами прям с нашего компа.
а в случае иптейбл нужно когто просить причем постоянно. плюс все те минусы что я наверху
расписал.

ткперь более конекретно как работаем передача через прокси сервер.
у нас комп-А у него IP-А на нем крутится наше приложение процесс-А1 на удаленном компе-Д
крутится процесс-Д1 он сидит на биндинге IP-Д и TCP_port-Д=4444

наш процесс-А1 обращается к ядру устнвить TCP конект с IP-Д:4444
ядро устаналивает конект с параметрами пакетов сететвого тарфика который будет обслуживаться
этим конектом вот с такими параметрами

  локальный IP=IP-А   локальыйн TCP_port=50345
  удаленный IP=IP-Д   удаленный TCP порт = 4444

соовтсвтнненно в рамках данного TCP конекта наше ядро будет обарабывать пакеты с параметрами

  src_IP = IP-А  src_port = 50345     dst_IP = IP-Д  dst_port = 4444
либо 
  src_IP = IP-Д  src_port = 4444      dst_IP = IP-А  dst_port = 50345

и нашему процессу-А выдается указатель fd/4 
далее теперь наш процесс если он сует байты в этот указатель то ядро будет паковать 
эти байты в TCP+IP+ETHER пакет с параметрами

  src_IP = IP-А  src_port = 50345     dst_IP = IP-Д  dst_port = 4444

и высирать через сетвую карту в сеть.

а если из сети будет приетать пакет с паараметрами 

  src_IP = IP-Д  src_port = 4444      dst_IP = IP-А  dst_port = 50345

то ядро будет принимать этот пакет вскрыать его. вытаскивать содержимое и передавать
процессу

если мы посмотрим через lsof на свойства этого процесса-А1 и своства его файл дескриптора fd/3
там будет вот такое на экране написано


	TCP IP-А:50345->IP-Д:4444 (ESTABLISHED)

оно нам покзывает на каком TCP+IP биндиге сидит через этот указатель наш процесс
и на каком биндинге сидит удаленный процесс до которого это указатель позволяет достучаться 
таким образом мы наданный момент пока что только достучались до компа-Д 
до процесса-Д1

кстати это означает что на удаленном компе-Д тоже работает процесс. у него тоже 
есть указатель. и этот указатель покзывает тоже самое только наоброт

	TCP IP-Д:4444->IP-А:50345 (ESTABLISHED)



ИНТЕРЕСНАЯ ВСТАВКА=НАЧАЛО
яхочу этим сказать что если мы со совего компа инииицировали TCP конект то это значит
что обязательно где то там далеко на какомто компе обязательно живет еще один процесс
что на той стороне конекта обязательно сидит именно процесс. код исполняемый в юзер 
спейсе как и уменя. а не ядро. что tcp конект это всегда именно способ связать два процесса.
обязательно будет два процесса. в отичие от случая например когда мы делаем ICMP конект(кстаи
icmp он неюзает ни tcp ни udp. он просто сразу оборачивается снаружи IP пакетом). так
вот когда мы на нашем компе  с юзер процесса шлем icmp в сеть и нам прилетает ответ то 
на той стороне ниакой юзер процесс не принимает и не обрабатывает наш поток! в этом случае
на том стороне это делает ядро! то удаленное ядро! вот что важно понимать про tcp конект.
если он  у нас есть то с  обоих сторон обязательно есть два процесса. то хрени кртящиеся
в юзер спейсе. чьи то частные програмки.
тут есть нюанс - в конечном итоге это верно. на той стороне в конечном итоге точно сидит 
гдетто юзер проесс на том конце конекта. но между нами могут быть компы которые тоже участвуют
в передаче трафика и там вместо юзер процесса будет ядро. щас оббьсню. положим мы шлем 
пакет с нашего компа на dest_IP=1.2.3.4 tcp_dest_port=443
трафик прилетает на тот комп. а там его принимает ядро и там настроен iptables который
делает DNAT и он меняет dst_IP на какойто другой скажем IP=7.8.9.10
пакет от нас влетает в комп 1.2.3.4. его обрабатывает ядро через iptables без участия
юзер процессов. и пакет летит дальше на комп 7.8.9.10 и там уже его ждет юзер процесс.
поэтому да - на нашей стороне сидит запущеннй юзер процесс который явялется одним концом
тцп конекта и на 7.8.9.10 тоже сдит юзер процесс который принимает в конечном итоге наш пакет.
но между ними могут сидеть куча промежуточных компов с iptables которые пропускают 
через себя этот трафик и меняют TCP\IP параметры этих пакетов используя код ядра. у нас
конечно изначльный dest_ip был 1.2.3.4 и на нем нет юзер процесса. но где то потом в конце
цепочки юзер процесс обязательно есть получаетс просто необязательно что этот юзер процесс
сидит именно на том хосте который имеет ip=1.2.3.4 . вот это интеересный вывод про tcp 
конект. тоесть тцп конект всегда служит для связи двух процессов. их обязательно два. а
не один например. 

еще интеерснейший момент в том что мы привыкли смотреть текущие конекты на компе через 
скажем утилиту ss
а она в свою очередь смотрит это в /proc

так вот в /proc она это смотрит как я понимаю анализирую свойства файл дескрипторов 
у всех процессов на данный момент которые показаны в /proc

но фишка в том что на компе есть сетевые конекты которые никак не привязаны ни к одному
файл дескриптору.  а значит в /proc о них нет никкакой инфомрации! значит
утиита ss частично слепа!

речь идет например о tcp конектах которые транзитно редиректятся через наш комп.
тоесть в наш комп влетает сеетевой поток который был порожден на процессами нашего компа
и этот сетевой поток не предназначен для процессов нашего компа.  самый простой пример 
это ктото пингует наш комп. влетающий icmp поток он не будетпередан ни одному прцессу нашего
компа. поэтому не будет ни одного файл дескриптора где бы инфомрации об этом потоке была
была бы отображена. однако поток влетает. и он забираетв ядре всякие буферы куски памяти.
тоесть он напрягает систему. он составляет некий конект. но ss об этом потоке не покажет 
нихерна. это вот если мы будем со совего компа пиновать удаленный комп. то так как поток
породается одимиз процессов нашего компа. то будет файл дескрпиотор и ss покажет инфо 
об этом потоке. это да. 

другой пример такого невидимого потока это трафик который просто роутится на нашем компе 
тоесть влетает в одну сет карту и тут же вылетает через другую куда то дальше в сеть
без измнения IP\TCP парамтеров потока. хотя все таки и в таком потоке наше ядро будет
нагружаться потому что нужно  в этом потоке менять TTL у IP заголовка. а это циклы цпу. 
нагрузка на память. 

так вот увидеть полную карину всех конектов на компе можно через команду 

		# conntrack -L

для справки читаем 

        $ man conntrack

и тут я понял одну вещь - что команда ss она конечно же не лазиит в /proc и не парсит
файл декрипторы. это было бы охренено долго. 
щас я это докажу

	# strace -e openat ss -4n 2>&1| grep proc
	пусто

вместо ss юзает совершенно другой механмзм получения информации. а именно юзер процесс может
к ядру сделать сисколл чтобы создать сокет. но это не просто локальный UNIX_DOMAIN сокет
и не сетевой сокет. это AF_NETLINK сокет. это получается такой канал связи между ядром
и юзер процессом. тоесть часть информации о своей ядерной требухе ядро выставляет в /sys
и /proc папке но это же далеко не все. так вот ядро позволяет получит канал мостик 
связи между собой и юзер процессом. через спец сокет. 

# strace -e socket ss -4n | grep socket
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_SOCK_DIAG) = 3
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_SOCK_DIAG) = 3
socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 4

вот каким образом ss получает инфомрацию о сетевых конектах!

также как в случае классичесокго сетевеого сокета первый параетр AF_NETLINK задает более общий
класс сокетов. а трретий парметр задает уже более конкртный вид сокета внутри класса.

описание этих типов сокета

	$ cat /usr/include/linux/netlink.h | grep NETLINK_SOCK_DIAG
	#define NETLINK_SOCK_DIAG	4	/* socket monitoring				*/

	$ cat /usr/include/linux/netlink.h | grep NETLINK_ROUTE
	#define NETLINK_ROUTE		0	/* Routing/device hook				*/

справка по этим сокетам
		
		 $ man  7 netlink

NETLINK_ROUTE
              Receives routing and link updates and may be used to modify the routing tables (both IPv4 and IPv6), IP addresses,  link  parameters,  neighbor
              setups, queueing disciplines, traffic classes, and packet classifiers (see rtnetlink(7)).

 NETLINK_SOCK_DIAG (since Linux 3.3)
              Query information about sockets of various protocol families from the kernel (see sock_diag(7)).

посмотрим man sock_diag
и там видим что 

		sock_diag - obtaining information about sockets

	   #include <sys/socket.h>
       #include <linux/sock_diag.h>
       #include <linux/unix_diag.h> /* for UNIX domain sockets */
       #include <linux/inet_diag.h> /* for IPv4 and IPv6 sockets */

       diag_socket = socket(AF_NETLINK, socket_type, NETLINK_SOCK_DIAG);

походу под sock_diag они понимают  NETLINK_SOCK_DIAG
и с помощью такого сокета можно из ядра зарпщиапть информацию о сокетах в ядре.
насколько я понял можно посмотреть инфо о двух больших классах сокетов это UNIX сокеты
и  IPV4\IPV6 сокеты.

как я уже писал в файле про СИ то сокет это не имеется ввиду парочка чисел IP:port
это биндинг. а сокет это стурктура данных в памяти ядра которую ядро создает для процесса
при исользвании глибс фнкции   

		socket()

в памяти ядра создаетая стркутара котоаря будет оббслуживать сетевой поток а процессу возвращается
дескиптор на эту струтуру. 

так вот ВАЖНО насколько я понял вот эта хрень

       diag_socket = socket(AF_NETLINK, socket_type, NETLINK_SOCK_DIAG);


она позволяет посмореть инфомрацию исключительно о сокетах которые юзер процессы создали в ядре.
но как  я уже говорил у нас есть например сквозоной трафик на хосте к которму либо прмиенятеся
SNAT\DNAT либо нет. но все равно такой трафик все равно обррабатывается(как минмиум в пакетах менятеся TTL и менятеся ETHER оболочка) . под это нужны струкотуры в памяти ядра так вот я вот
не знаю для таких потоков трафика создает ли само ядро для себя сокеты или нет. или это уже не
сокеты а какието друие струкрутуыры. 

в люом случае из моих эксприментов я дела вывод что для сквзного трафика (наприме унас на хосте
ест виртуаока и я с втуалки пингую гугл и ос стек хоста при этом работает как ротуер плюс к 
потому применяется SNAT). так вот для такого трафика я не вижу чтобы ss показал инфо что такойй
поток есть!

информацию о таком потоке покзывает утилита 

   # conntrack -L


я посмотрел чем она пользуется

# strace -e openat,socket conntrack -L
openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libmnl.so.0", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libnetfilter_conntrack.so.3", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libc.so.6", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, "/usr/lib/libnfnetlink.so.0", O_RDONLY|O_CLOEXEC) = 3
socket(AF_NETLINK, SOCK_RAW, NETLINK_NETFILTER) = 3

соовтесвтенно она получается юзает тот же способ получения информации как AF_NETLINK
тоесть через спец сокет  с ядром. НО! эта прога юзает другой субкласс сокетов 

	NETLINK_NETFILTER


в man 7 netlink про него сказано

     NETLINK_NETFILTER (since Linux 2.6.14)
              Netfilter subsystem.

и не более того. но походу именно иза него conntrack показывает расшиеную инфомрацию
по когектам внутри ядра!

также  я даже без этой утилиты нашел на саомом деле инфо о конектах в /proc/net
там прям она разбита по типаом конектов

/proc/net/udp/tcmp/icmp/raw
также 
/proc/net/protocol
/proc/net/nf_conntrack

например то что выводит на экран команда conntrack это можно руками увидеть в nf_conntrack

таким образом какя понимаю ss она показывает инфо об сетевых сокетах тоесть об сущностях
которые сидят в памяти. но сетевой сокет это еще не сетевой поток. да ессли создан сетевой
сокет то в его свойствах есть сетевой поток. но как я понимаю сетевой сокет через socket() 
создается тлоько по запросу юзер процесса. а если у нас трафик транзитный в которомо не принимает участия юзер процесс то и сокет не создается а сетевой поток пакетов при этом все равно есть.
поэтому я бы предположил что ss покзывает сетевы потоки порожденные юзер процессами. 
но это не весь сетеовй поток проходящий через комп. и полный сетеовой поток надо смотреть
либо в /proc/net/... либо через conntrack

тут я еще узнал что у линукса есть сокеты типа packet. они предназначены чтобы юзер приложение
могло само форимровать все уровни сетевого пакета выше L2. тоесть мы пишем в такой сокет
и далее ОС надевает на наши байты сверху  L2 (ETHERNET) оболочку. а наша задача получается
сформирорвать в наших байтах все что выше тоесть L3,L4, ... L7
для сравнения AF_INET сокеты они уровня L4. тоесть юзер приложение должно само содавать
через байты которые оно пшет в сокет уровни L7-L7
в ss можно посмтреть такие сокеты через 

   # ss -o

а еще можно почитать man 7 packet

я так и непонял как через ss посмтреть статистике по icmp потокам которые мы сами
запустили с хоста!

а вот я нашел почему был придуман нетлинк:
	Why do the above features use netlink instead of system calls, ioctls or proc filesystems for communication between user and kernel worlds? It is a nontrivial task to add system calls, ioctls or proc files for new features; we risk polluting the kernel and damaging the stability of the system. 

тоесть вместо того чтоы придумывать кучу новых сисколлоов для получения статисткии придумали
просто один сокет. а уже через него будут херачит всякие статисткиики. нето чтобы супер
но навервно чуть получше. ведь эту стаистику все равно нужно будет создавать внутри ядра.
просто несколько другой метод получения доступа к ней. фрнтенд метод

а вот еше нашел:
	When you are writing a linux application that needs either kernel to userspace communications or userspace to kernel communications, the typical answer is to use ioctl and sockets.

я так и непонял раницу между сокетом нетлине и ioctl. какой из методов выглядит проще
сточки зорения испольования в приложении. в каком методе нужно вводить болше данных и мудить

нетлинк позволяет нетолько читать из ядра его требуху но наоброт - чтото менять внутри
ядра ! подкручивать там настройки.

значит статистика по icmp потокам можно руками посмтреть  в /proc/net/icmp (а не в raw)
выглядит она вот так 

 sl  local_address rem_address   st tx_queue rx_queue tr tm->when retrnsmt   uid  timeout inode ref pointer drops             
   19: 00000000:002E 00000000:0000 07 00000000:00000000 00:00000000 00000000  1000        0 14128362 2 0000000002d5d055 0      

странно при этом то что невозможно определить на какой IP мы посылаем пинги.
также важно сказать что в этом файле показан icmp трафик который обязательно на нашем компе
приходит на юзер приложение. тоесть трафик генерируется нашим юзер приложением.если
через комп идет транфизтный трафик icmp то здесь такой поток не отражается!

на счет команды conntrack я не могу грантировать точно но вроде бы в ядре есть две связанные
но вроде бы все такие разные системы одна из них это netfilter это ровно как раз те
самые правила которые можно редатировать через строчную утилиту iptables а вторая хрень
в ядре это conntrack. это подсистема которая занмается отслеживанием поток трафика. 
и согласно вот этой картинке 
	https://en.wikipedia.org/wiki/Netfilter#/media/File:Netfilter-packet-flow.svg
подсистема netfilter заглядывает в подсистему conntrack для принятия решений об потоке сетевом.
ищи на картинке черный овал  с надписью conntrack.
в простом смысле коннтрак  в ядре держит таблицу о сетевых соединениях. тоесть грубо 
говоря вот отправили мы первый TCP пакет с флагом SYN тогда коннтрак в своей таблице 
создает новую строчку и помечает что статус этого соедиенеия [NEW ] и возможно еще как [SYN-SENT]
прилетел нам ответ он там модифицирует статус этого соединения. 

в iptables комнаде можно указывать так называемые модули например модуль tcp.
модули это такие расширения которые нам позволяют все более изошренно анализировать 
и обрабатывать трафик. например 
вот например пример анализа тарфик на основе модуль -m udp

	-A FORWARD -i br+ -o wlp2s0 -p udp -m udp --dport 123 -j ACCEPT

у этого модуля есть доп опции
	
	-m udp --dport 123

инфорацию о модуле иптейблc можно найти в мануале 

	$ man  iptables-extensions

например там про модуль udp написано 

udp
       These extensions can be used if `--protocol udp' is specified. It provides  the  following
       options:

       [!] --source-port,--sport port[:port]
              Source  port or port range specification.  See the description of the --source-port
              option of the TCP extension for details.

       [!] --destination-port,--dport port[:port]
              Destination  port  or  port  range  specification.   See  the  description  of  the
              --destination-port option of the TCP extension for details.


так вот тамже есть и модуль conntrack
у эттого модуля есть доп опция

 [!] --ctstate statelist
              statelist  is  a  comma separated list of the connection states to match.  Possible
              states are listed below.

там же нарисано какие стейты есть у сетевого потока с точки зрения коннтрак

	   INVALID
              The packet is associated with no known connection.

       NEW    The packet has started a new connection or otherwise associated with  a  connection
              which has not seen packets in both directions.

       ESTABLISHED
              The  packet  is  associated  with  a  connection  which  has  seen  packets in both
              directions.

       RELATED
              The packet is starting a  new  connection,  but  is  associated  with  an  existing
              connection, such as an FTP data transfer or an ICMP error.

       UNTRACKED
              The  packet  is  not  tracked at all, which happens if you explicitly untrack it by
              using -j CT --notrack in the raw table.

       SNAT   A virtual state, matching if the original source address  differs  from  the  reply
              destination.

       DNAT   A  virtual  state,  matching  if  the  original  destination differs from the reply
              source.


вот пример такого конекта и его статуса с точки зрения коннтрак

	# conntrack -L | head -n 3
	tcp      6 136571 ESTABLISHED src=192.168.220.1 dst=1.1.1.1 sport=60916 dport=443 src=146.190.207.220 dst=192.168.220.1 sport=443 dport=60916 [ASSURED] mark=0 use=1

а вот кстати как видит коннтрак icmp поток

	# conntrack -L |   grep 8.8.8.8
	icmp     1 29 src=192.168.51.1 dst=8.8.8.8 type=8 code=0 id=48 src=8.8.8.8 dst=192.168.51.1 type=0 code=0 id=48 mark=0 use=1

понятное дело что у icmp нет нкиакого статуса конекта. статус конекта есть только в tcp
потоков

а вот самое известное правило модуля conntrack которое обычно можно найти в любой таблице 
правил iptables

	-A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT

тость мы юзаем модуль иптейблс conntrack и его фичу ctstate
таким образом мы разрешаем проходить пакетам которые принадлежат TCP потокам у которых 
в таблице коннтрак статус потока с точки зрения коннтрак равен RELATED или ESTABLISHED

conntrackd это юзер демон - он нужен только для того если пытаться построить откзауо
стойчивый кластер на основе иптейбл. ибо он будет копировать таблицу конекшенов на другую
ноду - ровно также работает pfsense кластер
вот тут очень много полезного я узнал про иптейбл и коннтрак
	
	https://serverfault.com/questions/1030236/when-does-iptables-conntrack-module-track-states-of-packets

еще раз справка по модулям iptables

	https://manpages.ubuntu.com/manpages/oracular/en/man8/iptables-extensions.8.html


получается такой момент - если трафик инициируется на юзер процессе на нашем компе
или трафик снаружи прилетает на юзер процесс нашего компа то информация об этом сетевом
пакете будет отражена в ss. потому что трафик породит файл дескриптор который будет привязан
к процессу. если трафик транзитный никак не связанный с юзер процессом тоггда в ss небудет
о нем инфо. но в любом случае инфомрация об трафике этом будет отражена в conntrack таблице.
тоесть если заюзать команду conntrack то там инфо обо всем трафике на компе будет отображена


сcылки
https://github.com/mwarning/netlink-examples/blob/master/articles/Why_and_How_to_Use_Netlink_Socket.md



ИНТЕРЕСНАЯ ВСТАВКА=КОНЕЦ



вовзращаюсь к основному вопросу про прокси
мы составляем  в нашем процессе-А1 поток байтов  согласго формату прокси протокола
(назовем этот поток байтов которые сует процесс как пакет. пакет имеет формат прокси протокола) и суем их в указатель.

далее ядро берет наш прокси пакет оборачивает его в тцп пакет потом оборачивает 
в ип пакет потом обрачиваем в эзернет пакет. сует в сетвую карту и эта матрешка полетела в сеть.

прилетела на комп-Д . там ядро разобрало эту матрешку и в процесс-Д1 поступает только вкусное
содржимое а именно прокси пакет. все остальное это была лишь транспортная упаковка. 

так вот в этом прокси пакете будет написано на какой IP и на какой порт наш процесс-А1 
на самом деле хочет попасть. например нам там напсиано 

   IP = IP-Ж  TCP порт = 443

процесс-Д1 понимает формат пакетов прокси протокола поэтому он все понимает что от него хотят.
тогда процесс-Д1 обращается к ядру и создает тцп коннект к компу-Ж

на данный момент тогда мы имеет следующую ситуацию в плане коннектов:

   комп-А 
   процесс-А1 
      IP-А:45345 -> IP-Д:4444


   комп-Д
   процесс-Д1 
      IP-Д:4444 -> IP-А:45345


но также процесс-Д1 имеет и второй коннект

   комп-Д
   процесс-Д1 
      IP-Д:4444  -> IP-А:45345
      IP-Д:56345 -> IP-Ж:443


тоесть у процесса-А1 один сетевой сокет а у процесса-Д1 два сетевых сокета

	процесс-А1 <----------->  процесс-Д1 <---------------------> процесс-Ж1
IP-А:45345 -> IP-Д:4444       IP-Д:4444  -> IP-А:45345           IP-Ж:443 -> IP-Д:56345
                              IP-Д:56345 -> IP-Ж:443

так значит статья которая обьясняет как выглядит пакет в формате SOCKS4 вот она

 	  https://ftp.icm.edu.pl/packages/socks/socks4/SOCKS4.protocol
еще вот тут можно посмотреть
	 https://en.wikipedia.org/wiki/SOCKS


значит когда наш процесс шлет прокси пакет на комп-Д он шлет пакет в виде 

		+----+----+----+----+----+----+----+----+----+----+....+----+
		| VN | CD | DSTPORT |      DSTIP        | USERID       |NULL|
		+----+----+----+----+----+----+----+----+----+----+....+----+
     	   1    1      2              4           variable       1


цифры внизу это число байт - сколлько байт занимает то или иное поле в пакете.
VN - это номер сокс протокола. в текущем случае это версия 4
CD - это команда которую просим выполнить у прокси процесса. может быть всего две 
команды это CONNECT и BIND. конект это если мы хотим сделать исходящее соедиенеие 
например сходить на сайт. а бинд это если снаружи к нашему процессу ктот должен достучаться
через прокси стучась снаружи на прокси сервер. я эту хрень для простоты рассатривать щас
не буду. если мы хотим сделать конект то CD должен быть равен 1. 
DSTPORT - все понятно это TCP порт на который мы хотим чтобы прокси процесс достучался
DSTIP  - тоже все понятно
USERID - это id юзера под которым мы типа обращаемся на прокси процесс. 
как я понял в теории прокси процесс может обратсят к ident серверу (ident это такой протокол)
и проверить можно ли такому id оббрашаться на такой dest ip и dest port. 
в нашем случае  наш прокси процесс нихрена этого неделает. он разрешает всем id лезть на 
все хрени в интернете. вот такой пакет и посылает наш процесс-А1 к процессу-Д1
а именно


		+----+----+----+----+----+----+----+----+----+----+....+----+
		| 4  | 1  | 443     |      IP-Ж         | 345          |NULL|
		+----+----+----+----+----+----+----+----+----+----+....+----+
 
тоесть получается наш прокси процесс создает новый сокет к компу-Ж на основе 
инфо в этом прокси пакете. от этгого зависят параметры нового создваемого им сокета.
то бишь нового сетевого соедеинененеия. 

если бы мы использовали ipables на компе-Д для модификации сетвого трафика то непонятно
как от приложения передать ядру эту инфо о том какое правило нужно создать ядру для DNAT\SNAT
входящего потока от компа-А

если прокси приложение успешно создает сокет до компа-Ж то оно обратно нашему процессу-А1
шлет прокси пакет вот такого формата


        +----+----+----+----+----+----+----+----+   
		| VN | CD | DSTPORT |      DSTIP        |
		+----+----+----+----+----+----+----+----+
     	   1    1      2              4

где цифры - это число байт в каждом поле.
в стстье что я привел написано что важно только первое поле. а остальные игнориуются
клиентом принимаюшим ответ. а первое поле в ответе содержит уже не номер сокс протокола 
как это было в запросе а содержит код возврата была ли операция успешна. а именно

VN:
    90: request granted
	91: request rejected or failed
	92: request rejected becasue SOCKS server cannot connect to
	    identd on the client
	93: request rejected because the client program and identd
	    report different user-ids


поэтому если все успешно то по идее должно вернуться VN=90

а дальше начинается самое интересное  - теперь наш процесс-А1 просто может пихать любые 
байты (тоесть уже требования к тому чтобы это были байты в формате прокси отпадает) в файл дескриптор который овтечает за сетевой конект до прокси процесса-Д1 , они в рамках тцп конекта
будут долетать до процесса-Д1 тоесть это будет выгляеть вот так

	процесс-А1 " {  [ (любые байты) ] TCP } IP }  ETHERNET " ---------> процесс-Д1

соотвесвтенно ядро на компе-Д вскрывает эту матрешку и в процесс-Д1 передает только пейлоад
от тцп тоесть (любые байты) и процесс-Д1 просто беерет эту мешанину байтов и ему плевать
соотвствуют ли они какотому формату или протоколу и просто тупо сует эти байты в свой второй
сетевой сокет который ведет на комп-Ж !

	процесс-Д1 " {  [ (любые байты) ] TCP } IP }  ETHERNET " ---------> процесс-Ж1


получается процесс-А1 на ушко шепчет процессу-Д1 полезный контент и говооит передай 
этот контент на комп-Ж. и процесс-Д1 берет переданный контент и уже через свой сокет сетевой
передает в комп-Ж
если попробовать осознать как это выглядит то выглядит это так что как бутто у нас байты
которые генерирует процесс-А1 на самом деле генерирует процесс-Д1 и от себя (через свой
сетевой сокет) уже шлет на конечный сервер-Ж на который бы хотел это послать процесс-А1.
это выглядит так что вася хочет позвонить маше но ему нельзя. тогда он звонит пете и говорит
позвони маше и скажи ей "ку". петя звонит и говорит "ку". маша ответчает пете "дза" и петя
звонит васе и говорит ему "дза"

прокси протокол очень простой. в общем то два прокси пакета нужно предать туда и обратно 
а потом уже можно преедаавать голые данные. также получается что легко можно менять на какой
комп мы хотим попасть в итоге. также классно то что прокси процессу в итоге глубоко плевать
в каком формате байты прилетают к нему от клиента. он их просто берет и пересылает (релей) на
конечный комп. получается сокс прокси хорош тем что он непривязывается к байтам от юзер
процесса. поэтому через прокси можно передавать и ssh и HTTP и HTTPS 
суть процесса проксирования получается в том что наш процесс через тцп коннект связывается
с прокси процессом. через простой прокси пакет обьясняет куда хочет попасть. прокси процесс
делает тцп конект с этим дестинейшн компом. для тцп конекта абсолютно все равно какие байты
в него пихают. он просто берет и их пересылает. у нас получается два тцп конекта. 
от клиента до прокси процесса. и от прокси процесса до конечного компа. для простоты скажу
что прокси протокол и прокси процесс исползуются для релея именно тцп конектов. то есть 
прокси процесс вместо нашего исходного процесса  устанавливает тот самый тцп коннект  который
неможет устанвить наш исходный процесс. тоесть ICMP+IP через прокси процеесс неполучится
запроксировать. мы проксируем только тцп конект от имени клиента. по крайней мере в SOCK4
а про пятый я нехочу разбирать. 
получается мой процесс хочет сделать тцп конект с конечным компом. но ему нельзя запрешено.
тогда он делает тцп конект с прокси компом и прокси процессом. и просит его сделать 
ровно тот самый тцп конект который не может сделать мой процесс до конечного компа. 
и в итоге мы имеем два тцп конекта. 

так как прокси протокол такой простой и такой мощный то написать прокси клиент и прокси
сервер очень просто. или например скажем что добавить в программе фукционал прокси клиента
очень просто!

есьт несклько протоколов проксирвания: SOCKS4, SOCKS5. потом яне очень понимаю вроде как 
протоколом проксирования может выступать HTTP протокол. но я это неразбирал. также вроде
как haproxy придумали свой протокол проксирования. а еще есть SOCKS4a, SOCKS5, SOCKS5h прото
колы. 
в чем фишка SOCKS5h - в том что вот нам надо в прокси пакете указать DEST_IP. а теперь
предтсавим что мы обращаемся с браузера на прокси процесс. значит мы хотим попасть на сайт
скажем google.com значит нам нужно в прокси пакете указать DEST_IP сервера накотором крутится
google.com , а для этого наш браузер должен как то САМ зарезолвить домен в IP адрес чтобы
повторюсь его указат в прокси пакете который он будет отсылать прокси процессу. 
ну получается для этого браузер (пусть даже это будет программа curl которая по факту веб клиент
считай что простой браузер) должен либо через NSS зарезолвить домен в айпи либо либо как то
сам это пытасят сделать например как это делает хром которйы резолвить сам через DOH.
так вот представим что мы нехотим чтобы процесс-клиент сам резволил доменные имена в айпи.
тогда можно попросить это сделать сам прокси процесс. и такой функционал ест как раз у 
протокола SOCKS5h
А есть более диковинные прокси процессы когда он не один а их скажем два. причем они 
сидят на разных хостах. таким примером является ssh. на своем компе можно запустить так 
ssh что он на ноутбуке запустить процесс который прокси клиент. мы с нашего процесса например
барузера обращааеимся на этот прокси процесс. а он через шифрорванный тонель пересылает наши
полезные данные не наконеный сервер куда мы хотим попасть а на другой процесс который куртися
на другом сервере. и уже тот процесс ииницирует тцп конект к тому компу к которому хочет
попасть наш исходый браузер.

тоесть вот обычная схема

наш комп               прокси сервер
наш процесс   ------>  прокси процесс --------> конечный сервер


вот более сложная схема как у ssh


наш комп               наш комп                 прокси сервер
наш процесс   ------>  прокси процесс --------> еще один прокси процесс   ------> конечныый сервер

такая схема используется потому что SOCKS протоколы они не шифруют трафик. ну скажем
нам и ненадо ибо мы хотим предавать TLS трафик котоырй уже шифрованный. но вспоминаем что
сам сокс протокол он же тоже требует отослать и принять два прокси пакета. и когда 
мы это делаем в рамках одного компа то оокей - их никто в сети не подсмотрит не прочитает.
а вот когда наш прокси звонит на второй прокси процесс он же тоже передает прокси пакет
незащищенный. и уже по сети. и поэтому нужно чтобы эта передача шла внутри шифрованого тонеля.
поэому между двумя прокси прцоессами натянут ssh тонель. чтоб никто не подсмотрел и непоменял
паратры  пакета в формате прокси протокола socks. 

плюс SOCKS прокси протокола в том что чтобы он заработал нам всего навсего нужно чтобы 
файрвол между нами и удаленным сервером где сидит прокси процесс позволял нам сделать 
TCP конект на IP того компа  на любой TCP порт. тоесть нам нужно чтобы наш файрволл ползволял
нам всего навсего сделать тцп конект до прокси сервера. а тцп конекты это вообще база всех
сетевых конектов. на какой нибудь тцп порт наш файрволл да должен разрешать конект на тот сервер.
тоесть сокс протокол хорош тем что он базируется на тцп. а тцп насктлко сильно используется во
многих конектах что файрволл должен разрешать конект тцп хотя бы на какойто порт на удаленном
компе. тоесть мы хорошо проникаем сквозь файрволл. тоесь запретить на файрволле все тцп содеинеия
это как с Земли откачать весь воздух. сразу все сдохнут. второй плюс SOCKS прокси протокла в том
что ему абсолютно плевать какой L5-L7 протокол (поток байтов) будет через него прокачиваться.
ему на это настолкьо же плевать насколько плевать тому же самому тцп протоколу. для прокси 
процесса это просто поток байтов в которых неужно разбираться. просто возьми эти байты 
из одного тцп конекта и вставь их в другой тцп конект и все. тоесть прокси протокол такой классный
что просто напросто прокси процесс вытаскиват байты из одной коробки и кладет в другую. 
и воще похер какой формат этих даных. 

SOCKS4 протокол хорош тем что он базируется на TCP. а так как тцп конекты лежат в основе
очень многих более высоких протоколов то чтобы с клиента на компе достучаться до прокси
процесса на прокси сервере достаочно чтобы наш файрволл разрешал исходяшее тцп содиениение
хотя  бы на какойто тцп порт. также как пишут заумными словами СОКС4 протокол он типа 
протокол агностик. говоря нормальным языком они имеют ввиду что если мы хотим через прокси
сервер передать любой протокол который лежит выше чем TCP то соскс протокол и сокс прокси
процесс это сделают отлично. потому что им абсолютно плевать какому формату подчиаяются
байты которые прилетают от клиента на прокси процесс через тцп конект. прокси процессу 
это вобще неважно. он прсто беерет эти байты и преереклвдывает из одного тцп пакета в другой
тцп пакет. СОКС4 протокол он подразумевает что мы с клиента стучим на прокси процесс через
именно TCP конект хотя если вот так подумать то ничего немешает чтобы клиент стучал на прокси
процесс через любой транспортный протокол надежный . и сокс4 протокол подарзуметвает что 
прокси процесс он потом когда достукивается до конечного сервера до делает он это тоже ТОЛЬКО
через тцп конект и в прокси пакете который прилетает от клиента укзыавется в графе ПОРТ именно
TCP порт конечного сервера на который нужно постучать. ИМЕННО TCP порт. а не какогото друогого
протокола порт.
прокси процесс принимает от клиента через тцп конект контент (байты) которые передаются 
внутри ТЦП пакетов.  вытаскивает эти байты. и ничего в них неменяя перекладыват их внутрь
других ТЦП пакетов которые он уже отправляет на конечнй сервер. это означает что байты внутри
ТЦП пакетов от клиента могут быть любые. поэтому клиент через СОКС4 прокси может отправтлять 
байты любого протокола который базируется на TCP а именно для прмиера это могут быть байты
формата(протокола) telnet, ftp, finger, whois, gopher, http, tls, ssh  

минусы СОКС4 - неумеет работать с UDP, нет аутентсциикации по паролю, нет шифрования.
поскольку работа с прокси процессом состоит из двух фаз. первая фаза это обмена пакетами
именно в формате ПРОКСИ. и они незашифрваны. то если прокси сервис находится на удаленном
компе то чтобы защитится от злоодея то нужно получается защщать конект до прокси сервера 
через ширфванный тонель. на вторйо фазе уже просто клиент передает свои "голые" данные
на прокси процесс. если эти данные это TLS то на этой фазе получается защита и ненужна потому
что защиту даст сам TLS хотя и он неидален потому что он нешифрует SNI. но на первой фазе защита шированием нужна по любому. 
нет аутентфиикации по паролю - хотя есть контроль доступа через IDENT протокол который поодходу
уже никто неислольует. 
неумеет работать по UDP. это значит что сокс4 протокол предпиывает сокс4 серверу чтобы он 
дозванивался до конечного сервера исклтительно через TCP конект. и больше никак. поэтому и 
через UDP тоже это запрещено делать. это значит что если мы будем  с клиента пихать байты 
в формате протоколов которые базируются на UDP например опенвпн то нас ждет провал. потому что
наш прокси сервис будет дозваниться до опенвпн сереваера исклчтительно по TCP конекту!
тоесть мы должны четко понимать что сокс4 сервис будет звонить на сервер который мы ему укажем
исключительно через TCP конект! а если нам надо позвонить на кончный сервер через UDP конект
то он этого делать НЕБУДЕТ! поэтому СОКС4 прокси сервис подходит только если нам надо достучаться
через него до удаленного сервера через TCP конект и передать внутри этого конекта контент
который базируется на TCP конекте. тоесть протокол который базируется на тцп конекте.
получается можно будет через прокси донести такие протоколы как DNS, IMAP, HTTP, TLS, NTP, POP, 
SMTP, SSH, Telnet. охренеть. круто.

SOKCS5 сервис умеет уже дозванится до удаленного сервера нетолько через TCP но и через UDP,
также умеет аутенцировать клиента через логин пароль. правда толку от этого если он не защищает
себя шифрованием. также SOCKS5h прокси сервис умеет по просьбе клиента делать dns резолвинг. делает он его опираясь на NSS подсистему.

HTTP прокси протокол - это на самом деле всего навсего кусочек от обычного протокола HTTP 1.1
а именно это всего навсего одна команда(в протоколе команды назвыаются диркетивы) в протоколе 1.1, 
а именно команда CONNECT тоесть в протоколе есть команды GET, PUT, POST итд. и вот есть команда
CONNECT. и схема получсется точнот такая же самая как в протоколе SOCKS. тоесть клиент должен
послать команду CONNECT в рамках HTTP запроса на прокси сервер. в формате вот таком
(показываю заголовок HTTP запроса)

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

этот HTTP запрос улетает на прокси сервер. в строке CONNECT мы указываем хост на который 
мы хотим досутчаться в кончном итоге и порт

значит про эту директиву CONNECT расписано вот здесь

  https://www.ietf.org/archive/id/draft-luotonen-web-proxy-tunneling-01.txt


для примепа я покажу как выгляит обычный GET запрос 

> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


я это привел к тому что послденнее поле в строке CONNECT стоит HTTP/1.1 спрашивается зачем
оно там? что оно обозначает? оно обьячняет веб серверу на который прибудет реквест что ээтот
реквест создан на базе протокола HTTP 1.1 тоесть формат входящего сообщения. и также это
намекает веб версерру на каком формате вебсервер должен вернуть ответ на запрос. 
вот смотри

  > CONNECT google.com:443 HTTP/1.1

  > GET / HTTP/1.1

то есть в обоих директивах мы указываем для веб севрера какой формат прилетевшего сообщения.
тоесть поле HTTP/1.1  оно не относится к параметрам GET или CONNECT а эта глобалная настройка
в каком формате ВЦЕЛОМ составлен полный текст запроса. я считаю что это деилиизм вставлять 
глобаяную настройку в частную строку. это путает.
я считаю что это додждно было бы быть лучше вот так

для гет запроса
> GET / 
> HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


для коннект запроса
> CONNECT google.com:443 
> HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

так вот на счет строчки 

  CONNECT google.com:443 HTTP/1.1

итак ее смысл. итак опция HTTP/1.1 это глобальная обьяснялка коорая говорит веб серверу
что весь запорос отформатирован согласно формату HTTP/1.1 
CONNECT google.com:443 означает к какому серверу и на какой порт нужно сделать TCP коннект веб
серверу (или прокси веб серверу) на который прилетел запрос. тоесть клиент просит прокси
сервер устанвитть тцп конект с серврером google.com на тцп порт 443
потворюсь пока что просто тцп конеект. о протоколах более выского уровня речи неидет

в частности из это вытекает что конект может быть нетолько на 443 порт. а на любой тцп порт
например 


  CONNECT google.com:1518 HTTP/1.1


и важно понимать что речь идет о том чтобы прокси сервер сделал всего навсего ТЦП конект. 
о том какой протокол вышего уровня будет передаваться чрез этот конект речи не идет. 
это неоговариваетя неограничивается! а опция HTTP/1.1 гоорит не о том что через порт 1518
пойдет HTTP поток . нет! опция HTTP/1.1 к соажению в этой строке всунута неудачно она неимеет
отношения к опции команды CONNECT. опция HTTP/1.1  сообщает что весь реквест целиком который
прислан на прокси веб сервер отформатрован на основе HTTP/1.1 формата и что клиент ожидает ответ
тоже в формате HTTP/1.1

далее прокси веб сервер делает тцп конект к google.com:443 и остлыает клиенту ответ 
в формате HTTP/1.1

    HTTP/1.1 200 OK  Connection established

ну или полный ответ

  HTTP/1.1 200 Connection established
  HTTP/1.1 200 Connection established

первая строка идет в хидере а вторая это тело соообщения в ответе
в овтете более умно сделано - формат ответа указан с самого начала ответа а не гдето
в конце первой строчки

итак еще раз мы с нашего клиента шлоем на прокси сервер запрос в формате HTTP протокола 
и просим чтобы прокси сервер устанвоил тцп конект с google.com:443

  > CONNECT google.com:443 HTTP/1.1
  > Host: google.com:443
  > User-Agent: curl/8.10.1
  > Proxy-Connection: Keep-Alive

прокси сервер принимает зазпорос. и устанвливает этот тцп конект. 
и нам в ответ сообщает что он это сделал

  < HTTP/1.1 200 Connection established
  HTTP/1.1 200 Connection established

а дальше происходит то что прокси сервер просто сидит ожидает что мы ему пришлем в тцп
пакете. он это вытаскивает и просто тупо кидает в тцп конект к google.com:443

тоесть HTTP прокси сервер ралтботает точно по такому же принципу как и SOCKS сервер. 
асболютно такой же прринцип работы!

и поэтому это значит что HTTP прокси сервер он может форвардить аболютно любой юзер протокол
который базиурется на базе TCP а не только HTTP\HTTPS протоколы!
тоесть я где то встречал что СОКС протокол может форвардить любые юзер протоколы базирующиеся
на ТЦП конектах а якобы HTTP прокси может форврдиь толко HTTP проткоол - ТАК ВОТ это полная 
хуйня! и SOCKS и HTTP прокси могут оба форваодить любые блять юзер протколы бащирующиеся
на TCP.

не вижу никких проблем чтобы такой HTTP прокси сервер форвардил нетолько HTTP поток
но и HTTP+TLS = HTTPS поток. 

прмиером HTTP прокси явялется программа 

  tinyproxy


и вот как можно через curl обраттся в интернте через HTTP прокси

  $ curl -x http://172.16.10.11:2080  -p --dump-header - -v  https://google.com


обращаю внимание что только при сипользовании обоих ключей и -x и -p у нас курл будет
делать реквест используя CONNECT! если ключ -p опустить то курл сгенериурет вот такой
запрос 

  $ curl -x http://172.16.10.11:2080   --dump-header - -v  https://google.com 
> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 Connection established
HTTP/1.1 200 Connection established

кхм.. странно  до этого без ключа -p директиа CONNECT неиспользовалось

на счет поля "Host:" я хотел пояснить. какой его смысл. в пртоколе 1.1 это обязательное поле
для каждого реквеста. здесь так написано

  https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html

и это поле ознаает вот что. вот мы достучались до веб севрера  и туда влеает HTTP текст.
так вот процесс веб сервера может обслуживать несколко сайтов в этом нет никаких проблем.
так вот заголовое "Host:" дебильное название потому что он ообозначает не хост а он обо
значает доменное имя сайта на который мы хотим досутчаться. скажем наш веб сервер процесс
обсивает сайты "vasya.com" и "petya.com" и вот влетает наш реквест и там написнао

    Host: petya.com

и процесс веб сервер понимает какой веб сайт нужно выдать в ответе!
так вот честно говортя в реевесте CONNECT

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive


у нас инфомрация на какой веб сайт мы ломися получается дублиуется ! с одной
стороны она указан в строке CONNECT а вдругой сторне она укаана в строке Host:
ненужная дупликация. хотя это не совсем так.
дело в том что то что указано в строке CONNECT оно используется для установления TCP конекта.
тоесть речи ни о каких HTTP сайтах нихуя не идет! имя сервера в строке CONNECT берется.
дальше оно чрез DNS резолвится в IP. и далее через этот IP и порт делается tcp коннект
а хосту! на этом хосте никаких веб сервров процессов может не быть в поимне! например на этом
хосте может быть SMTP или DNS или FTP сервер! поэтому доменное имя в строке CONNECT необозна
чает никакой веб сайт! он обозначает DNS имя уаленного сервера. который через DNS резволтся
в IP адрес. и с этим IP адресом уснваливет прокси ТЦП коннект!
а вот поле Host: испольщуется в том случае если мы делаем именно реквест к веб серверу
к веб сайту через соотсветеущие директиы например диерктиву GET. и тогда достучавшись до 
процесса веб сервера и передва ему GET реквест 


> GET / HTTP/1.1
> Host: localhost:8888
> User-Agent: curl/8.10.1
> Accept: */*


то веб сервер процесс через поле Host: понимает про какой веб сайт который он обслуживает
из списка несклольких веб сайтоы мы хотим получить обратно ответ!

да! дело в том что сам ХОСТ сам сервер может иметь в системе DNS одно доменное имя.
например 

   cyber.com  A    1.2.3.4

и на этом серверер крутится веб сайт процесс и он соглласно найтрокам в жинкс
обслуживает два веб сайта
  
   vasya.com
   petya.com

у жинкса в это секции server{} опция servername

дапонятно что архитекрутно это дебильно сделано. что выгоднее в DNS сделать вот такие
две записи 

   vasyacom  A    1.2.3.4
   petya.com A    1.2.3.4


так удобнее но это необязательно. у нас доменное имя сервера может быть одним. 
а веб сайты(их доменные имена) которые на нем обслуживает жинкс могут быть совершенно другие.

тогда в веб клиенте мы как должны постуить. мы берем доменное имя срвера cyber.com
и через днс превращаем его в IP. далее мы создаем TCP конект с IP:443
а теперь чрез этот тцп конект мы суем в этот конект HTTP текст в виде

> GET / HTTP/1.1
> Host: vasya.com:443
> User-Agent: curl/8.10.1
> Accept: */*

и нет никаких проблем!

поэтому так как HTTP базируется часто но невсегда на TCP то нам вначале унжно создать тцп
конект. а уже потом через него кидать HTTP запрос. так вот это две большие разницы между
доменным именем сервера который имеет IP. и через этот IP (а значит и это дменное имя ) мы
вначале устанвливаем тцп конект.
а внутри серевера может  быть куча веб процессов кажый из которых обслуживает кучу разных
веб сайтов со своими доменами . и вот эти домены веб сайтов необязаны пересекаться с доменом
самого сервера. поэтому Host: относится к HTTP тексту который приетает на веб сервер процесс
и сообзает этому процессу покажи мне такойто сайт. а домен  в CONNECT он относится не к
HTTP тексту и не для веб серер процесса. он относисятся к той фазе когда мы создает тцп
конект.

тогда спршивается нахер в этом запросе вообще это поле Host:

> CONNECT google.com:443 HTTP/1.1
> Host: google.com:443
> User-Agent: curl/8.10.1
> Proxy-Connection: Keep-Alive

ведь наш прокси сервер это не веб сервер. и он не хостит у себя никкакой веб сайт. 
ну как я понял из документа что привел выше что тупо в протоколе HTTP/1.1 обязательно 
в каждый реквест вставлять поле Host: и там где оно имеет смысл и где смысла в нем нет.

привожу еще раз описание смысла поля Host:

 The Host request-header field specifies the Internet host and port number of the resource being requested, as obtained from the original URI given by the user or referring resource (generally an HTTP URL,

as described in section 3.2.2). The Host field value MUST represent the naming authority of the origin server or gateway given by the original URL. This allows the origin server or gateway to differentiate between internally-ambiguous URLs, such as the root "/" URL of a server for multiple host names on a single IP address.

       Host = "Host" ":" host [ ":" port ] ; Section 3.2.2

A "host" without any trailing port information implies the default port for the service requested (e.g., "80" for an HTTP URL). For example, a request on the origin server for <http://www.w3.org/pub/WWW/> would properly include:

       GET /pub/WWW/ HTTP/1.1
       Host: www.w3.org

A client MUST include a Host header field in all HTTP/1.1 request messages . If the requested URI does not include an Internet host name for the service being requested, then the Host header field MUST be given with an empty value. An HTTP/1.1 proxy MUST ensure that any request message it forwards does contain an appropriate Host header field that identifies the service being requested by the proxy. All Internet-based HTTP/1.1 servers MUST respond with a 400 (Bad Request) status code to any HTTP/1.1 request message which lacks a Host header field. 

тоесть когда мы достучались до веб сервер процесса то он обслуживает несклоько веб сайтов.
а у веб сайта есть доменное имя и есть дальнейший URI который обозначает путь к ресурсу.
так вот у разных веб сайтов может быть одинаковый путь к ресурсу а в запросе GET там укаызвается
именно путь к ресурсу. и из только строки GET непонятно от какго веб сайта этот путь относится.
и тут нам на помощь и приходит поле Host:
я так понимаю этот смысл.

забавно также что вот так деильше повелось что путь к ресурсу в строке GET он раньше совпдаал
с точн таким же путем к этому ресуру на файловой системе линукса. поэтому эти два понятия 
смешиватся. хотя они разные! это четко видно если веб сервер берет ресурсы через php.
потому что если мы возьмем путь к php файлу из GET то мы такой файл на файловой системе
не обнаружим! потому что это разыне вещи которые иногда совпдают.

немножко отойдем в сторону. чем оличается DNAT и прокси. вот расмотрим что мы с компа-А
из процесса пороидили сеетевой поток и он полетел по сети. если поток проходит через прокси
на другом компе-Б то он на прокси машине из сеетвой карты летит в ядро а оттуда 
доходит до юзер спейсе - в процесс в юзер спейсе тоесть поток из машины-А в итоге через ядро
на компе-Б влетает в юзер спейс. и там заканчивает свой путь в рамках одного и тогоже
тцп конекта между компом-А и компом-Б.  что значит заканчивает свой путь. это значит что содер
жимое ТЦП пакета вытаскивается , сам тцп пакет отправляется в мусор. а его контент передается
в процесс. тоесть тцп\ип пакет он полностью уничтожается на этом компе. толлько от него осатется
полезный контент который лежит в пейлоад зоне тцп пакета. а сам пакет уничтожается ядром.
если же сеетвой поток от машины-А влетает в роутер на компе-Б на котором происходит DNAT то в этом случае сетевой поток во первых он недоходит до юзер спейса. он крутится только в районе ядра
этого компа-Б. далее насколько я понимаю нетфильтр\иптейблс в ядре оно не уничтожает прилетевшие
тцп пакеты а просто меняет в них тцп порты и ип адреса в ip части пакета. хотя точно я незнаю
ядро уничтожает тцп\ип заголовки вытаскивает тцп пейлоад и перекладывается в новые тцп\ип 
упоковки или все таки оно только модифицирует имеющиеся тцп\ип заголовки. точно непонятно.
но далее эта хрень сидящая в ядре выплеывается дальше в сеть. повторюсь все обработку делает
толкьо ядро. юзер спейс никак не участвует. правда я не знаб как это вынлядит со стороны таблица
коннтрак. там два потока сетвых в итоге записывается - влетающий и вылетающий или один?
я проверил - поток получается один. с точки зрения записей в таблице коннтрак.
дело в том что коннтрак таблица заполняется на определенном этапе проходждени иптейбл цепочек(ищи картинки в интернете). так вот что я имею. яимею виртуалку у нее IP=172.16.10.11
она сидит на ноутбуке за сетевым бриджем br0
я на виртуалке запускаю команду

    (виртуалка 172.16.10.11) $ nc -v   64.50.236.52 21

смотрю какая запись или записи для этого TCP конекта есть в коннтарк таблице внутри вртуалки

    (виртуалка 172.16.10.11) # cat nf_conntrack | grep 64.50.236.52 | grep EST
    ipv4     2 tcp      6 431993 ESTABLISHED src=172.16.10.11 dst=64.50.236.52 sport=51582 dport=21 src=64.50.236.52 dst=172.16.10.11 sport=21 dport=51582 [ASSURED] mark=0 zone=0 use=2

получается мы имеем пакет 

    TCP  172.16.10.11:51582 -> 64.50.236.52:21

теперь  я  отслеживаю пакет на хосте на ноутбуке. ноутбук имеет карту wlp2s0 IP=192.168.51.1
(удивительный IP потом что гейтвей это 192.168.51.34)
смотрим через коннтрак

  (нотубук хост) $ sudo conntrack -L  | grep 64.50.236.52
  tcp      6 431956 ESTABLISHED src=172.16.10.11 dst=64.50.236.52 sport=51582 dport=21 src=64.50.236.52 dst=192.168.51.1 sport=21 dport=51582 [ASSURED] mark=0 use=1

тоесть пакет уже в стеке хоста выглядит до попадания на SNAT преобразование как

    172.16.10.11:51582 -> 64.50.236.52:21

и после SNAT преобразования как 

    192.168.51.1:51582 -> 64.50.236.52:21

но в любом случае при проходдении стека на хосте компа пакет до преобования NAT
и после преобразования с точки зрения коннтрак принадлежит одному потоку
видно что при преобразовании только меняется SRC IP. а src port неменяется как был 51582
так и остался. потому что этот порт не был занят на хосте. так зачем его лишний раз менять.

таким образо получается что транзитный SNAT поток через комп он во первых обрабвыатется
искочиетльно ядром и не попдаает в юзер спейс это первое отличие от прокси обработки потока.
второе отличие что  с точки зрения ядра и поток котоыйр влетает в комп и поток который из него
вылетает это один и тотже поток только слегка преобрвзанный.  а если поток летит через прокси
приложение то у нас  сточки зрения ядра будет два потока. один который влетает и заканвиается
на юзер процессе. а второй поток который новый он начинается на юзер процессе и улетает в 
интернет и заканчивтся на удаленном хосте. тоесть мы имеем два потока и в таблице коннтрак
будет две записи. в этих двух моментах отличие проксиования сетевого потока от его NAT-ирования.

а вот ксатти как этот поток выглядит ст очки зрения tcpdump. напомню что тисипидамп
ловит поток если исходящий то уже после обработки его НАТ правилами. а если входящий
поток то ДО обработки его НАТ правлами.

    (ноутбук 192.168.51.1) # tcpdump -i wlp2s0 -n tcp port 21
    192.168.51.1.51582 > 64.50.236.52.21

(ну это так чисто для српавлки я првиел)

таким образом я поииследовал чем отличается сетеовй поток котоый влетает  в комп и он 
проксируется на этом компе и высирается наружу либо он натируется на этом компе и высирвется
наружу. в чем разница такой обработки.

получется если поток проксируется то он прям "терминируется"  на юзер процессе(тоесть он умирает заканчивается на юзер процессе. и юзер процесс вытаскивает конеткнт из потока. и далее юзер процесс
создает новый поток и туда перекладаывает карго из певрого потока. и получается два тцп потока
у юзер процесса. и с точки зрения ядра тоже у нас два потока).
а если он натится то он до юзер процессе недолетает  а просто его ядро модифицирует
и высирает наружу и с точки зрения ядра и коннтрак у нас один поток. 

значит есть wireshark но это графическая прога и также пакета pacman в манжаро нет.
есть вместо эттго консольная утилита tshark и можно в ней записать трафик в файл вот так

    # sudo tshark -i eth0 -w test.pcap -F libpcap

правда проблема в том что если я буду трафик как то фильтровать то к сожалению
тогда его в файл прога откажется запмываться.

а после как мы записали трафик в файл то его можно загрузит в другую прогу она есть 
аналог wireshark но работает исклюичетьно сразу в терминале через псевдографику.
это прога

    termshark

в ней фильтры срабатывают точно такие как и в вайршарк

некая неочень хорошая инстрауция от нее 
    
    https://github.com/gcla/termshark/blob/master/docs/UserGuide.md#filtering

пример фильтроов от вайршарк

    https://wiki.wireshark.org/DisplayFilters

как я уже писал пакет tinyproxy который дает функционал HTTP прокси

статья про вайршарк и прочее в арч

  https://wiki.archlinux.org/title/Wireshark



значит вот я хочу потерстировать прокси через curl. для этого мне хочется удобно видеть 
какой трафик сам curl пихиает   в сетевой сокет. без всяких там вайршарков. и дейтсиельно
это можно удобно видеть если заюзать socat.
вот так запускаем socat

  $ sudo socat -v tcp-listen:8080,fork tcp:172.16.10.11:2080

опция fork нужна для того чтобы когда в него прилетает новый конект то он его 
обслуживал в рамках олдльеного нового прцесса. чобы если конект закроет клиент
то процесс погибает но родетлський процесс работает как ни в чем ни бывало. чтобы
его неперезапускать.

тогда он вешается слушать tcp на 0.0.0.0:8080  и работает как прозрачный прокси.
тоесть все что в него влетает через тцп конект имеется ввиу пейлоад от тцп пакетов он 
это вытаскивает и всатвляет в новый тцп пакет который связан с новым тцп конектом который
идет на   172.16.10.11:2080 где у меня работает SOCKS/HTTP прокси.

тоесть socat это реально тоже прокси только он нетребует от приложения разговаривать с ним
по протоколу прокси ему это ненадо потому что он в отличие от протоколов прокси делает
проксирование только на один фиксированный IP:port а так это реальное проксирование.
и плюс его в том что он все то что получает в себя он на экране печатает. а это супер то что
надо. таким макаром я вижу какой контент curl пихает в сетевой сокет.

далее я запускаб curl вот так

  $ curl -4 -x socks4://127.0.0.1:8080 -p   http://google.com 

либо вот так 

  $ curl -4 -x http://127.0.0.1:8080 -p   http://google.com 

тоест курл связыватся с socat а тот прозрачно передает тот что внутри тцп пакетов 
уже далее на прокси сервер используя новый тцп конект и на терминале где висит сокат
я вижу каакой контент протекает через него

ну неуоодобно то что когда курл получает ответ от веб сервера то он берет и сам закрывает
конект и прекращаеьт свою жизнь. чтобы сделать что курл висит бесконечно чтобы
можно было его поииследовать надо еше вот как сделать. 
надо запустить nc

  #  while 1; do nc -l -p 7777; done

он позволяет усатнвить с ним тцп конект и он его незакрывает. и висит так бесконечно долго.
тогда curl надо модифицирвать и запускать вот так

  $ curl -4 -x socks4://127.0.0.1:8080 -p  --noproxy "localhost"   http://google.com   http://localhost:7777

тоесть мы ему говорми что нужно ходить в сеть через прокси. но если доменное равно localhost
то прокси не юзай иди напрямую. далее мы просим его открыть две ссылки
вначале эту

  http://google.com

а потом вот эту 

  http://localhost:7777

первая будет открыта чере прокси. а вторая напрямую. и посколоку от втторого "вебсервера"
небудет ответа но тцп конкт будет то курл будет висеть и ждать ответа и незакрываться.
поэтому его удобно в это время исскледовать





а можно как то curl перенаправить в nc а уже с него в интернет. чтобы через nc наблюдать
дамп того что высирает curl ?
как ssh юзает nc для доступа к прокси?
у жжнкса один воркер может иметь неолкко открытых тцп конектов.
попробвать использовать жинкс как хттп прокси
оказывается почемуто можно помлать прсто HTTP запрос на прокси типа GET и он зафоровадит
наш запрос дальше. это рабоатет только для HTTP запроса не HTTPS
окзывается еще прокси бывают такие что у нас между клиентом и прокси связь идет
через HTTPS!

выше я распиал примеры прокси протоколов. теперь надо скзаать названия программ которые умеют
работать с прокси протоколами. это :  ssh, squid, dante, haproxy, nginx(это неточно)


реверс прокси и форвард прокси ???

кстати при TCP конекте мы через 3 пакетный хендшейк мы убеждаемся что на той стороне реально
есть ктото живой! в отличие от UDP когда мы нихрена не знаем есть ли  там ктот живой. мы
просто отослали и хер знает живой там ктото или нет


ссылки:
https://en.wikipedia.org/wiki/SOCKS

