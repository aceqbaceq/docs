как работает keepalived

вначале упрощенное описание

есть нода котрая мастер 
а есть которые бэкап

ноды которые бэкап они слушают по сети приходящие 
пакеты vrrp от мастера. пока они их получают то они нерыпаются. они остаются
в состоянии бэкап. тоесть они непытаются отжать кластерный IP.

если бэкап нода в течение более 3 * advert time неполучает vrrp
то она считает что мастер сдох и сама себя назначает мастером. 

сразу покажу конфиг мастера

! Configuration File for keepalived
vrrp_script check_haproxy {
  script       "/usr/local/bin/check-haproxy.sh"
  interval 2
  fall 2
  rise 2
}

vrrp_instance test_instance {
   interface ens160

   track_interface {
     ens160
   }

   state MASTER
   virtual_router_id 50
   priority 255 
   advert_int 1
!   nopreempt

   unicast_src_ip 172.16.102.34
   unicast_peer {
     172.16.102.35
   }

   virtual_ipaddress {
      172.16.102.100/24 dev ens160
   }

   track_script {
      check_haproxy
   }

   notify /usr/local/bin/keepalived.state.sh
}

и будем его коментироовать.
значит advert в конфиге это advert_int.
как я понимаю если в конфиге неуказать advert_int то он подефолту равен 1 секунда.

так вот если грубо говоря за определный интервал бэкап нода неполучит от мастера
vrrp пакет то она в течение какого то времени назначает себя мастером 
и выставяляет у себя cluster-IP.
проблема в том что если у нас несклько нод неважно какая у каждый ноды роль
мастер иили бэкап то  если каждая из них окажется изолирваной то мастер
нода останется мастер нодой а бэкавп нода из за того что неполучает vrrp
подумает что мастер нода сдохла и тоже себя назначит мастер нодой.
получим сплит брейн.

есть еще одна проблема условно говоря vrrp механизм который обеспечивает кипэлайвдэ 
демон ии который условно говоря он сам реализует без нашего участия он решает
только задачу что там с мастером. что там с удаленной нодой. но есть еще 
одна проблема это собсвтеное здоровье ноды. что толку нам захватывать роль мастера
если сервис ради которого мы настраиваем кластерный IP неработает. например
на хосте сидит хапрокси. и на том хосте сидит хапрокси. мы на обоих настраиваем
кипэлайвдэ для того чтобы дать его хапрокси.  нода недолжна захватвыать роль мастера
даже если удаленный мастер сдох если на нашей ноде сдох сервис хапрокси.
вот для чекинга здоровья того сервиса ради которого настраивается кипэлайвдэ
использется настройка track_script (в конфиге).
там мы прописываем любой наш скрипт который проверяет здоровье некипэлайвдэ.нет
а того второго сервиса (например хапрокси) на нашей ноде , здоровье хапрокси.
если скрипт возвращает ошибку то наща нода добровольно себя обьявляет в состояние
FAIL. и наша ноде небудет пытаться стать мастером даже если удаленный мастер
сдох ибо нет смысла.

таким оюразлом в кипэлайв есть два совершенно разных мехаизма проверки здоровья
которые проверяю соверещшенно разные вещи.

встроенный механизм который мы ненастраиваем заисклочением указания 
пременной advert_int  - этот механизм ждет VRRP пакеты от мастера в течение advert
интервала. получая их наща нода понимает что удаленная нода мастер в порядке.
это один механизм. мы проверяем здоровье удаленной ноды мастера.

и второй механизм коорый мы уже конкрено сами настрваием через опцию vrrp_script
и которая служит соверещнно другой цели. этот скрипт дает понять службе кипэлайв дэ
о здоровье нашей ноды. но в не всмысле здоровья сервиса кипэлайв. нет.
о здоровье той второй службы коорая крутится на нашем хосте ради которой
устаналвливлася кипэалай. например это служба хапрокси.  еси служба хапрокси
жива (на основе return code от нащего самописного скрипта который делает 
проверку хдоровья хапрокси) тогда кипэлайв знает что хапрокси жив на нащшем
хосте и что есть смысла захватывать роль мастера если удаленный мастер сдох.
если наш скрипт говрорит что хапрокси мерты то тогда нет смысла 
становится мастером ни при какихз условиях. и  на нащшей ноде кипэлайв обьявляет
нашу ноду как FAIL.
имя проверочного скрипта указвыается в секции 
 track_script  
 а детали вызова скрипта уже прописываются в секции
 vrrp_script

есть еще срипт который прописывется в секции notify.
нахуй он нужен.
он выполнятся на хосте каждый раз когда на хосте сменяется state.
state может принимать значения:
MASTER
BACKUP
FAIL

вот каждый раз когда на ноде меняется state кипэлайв будет запаусктаь 
этот скрипт. который может делать что угодно.
МОЖНО СКАЗАТЬ ЧТО notify скрипт это уже action скрипт.
тоесть если track_script это просто проверялка то notify это уже делалка.
дело в том что если у нас обычный комп тоесть не тот ккоторый на амазоне
то наш комп будет в конфиге кипэлайв иметь строчку  virtual_ipaddress
она указвыает тот самый кластерный Ip который наша нода АВТОМАТОМ без каких 
бы то нибыло с нашей стороны настроек наша нода себе назначит. так вот скажем на амазоне
там себе назначить адрес неполучится. на счет локального незнаю.  а вот так назвыаемый
elastic ip ты его никак себе на ноду в секцию virtual_ipaddress невставишь.
поэтому именно notify скрипт будет через утилиту aws дават команду амазону 
при смене state на то чтобы амазон перекинул elastic ip с одной ноды на другую.
тоесть я это говорб к тому что notify скрипт является action скриптом а не проверочным.


что важно что этот скрипт будет получать ТРИ параметра

$1 = "GROUP" or "INSTANCE"
$2 = name of group or instance
$3 = target state of transition ("MASTER", "BACKUP", "FAULT")


у меня этот скрипт в случае когда кипэлайв крутился на хосте
где работал хапрокси вынглядел так

# cat /usr/local/bin/keepalived.state.sh

#!/bin/bash

TYPE=$1
NAME=$2
STATE=$3

echo $STATE > /var/run/keepalived.state


тоесть он просто фиксировал состояние ноды в файл чисто для справки.
чтобы я мог зайти на ноду заглянуть в файл и всегда
знать что там думает про себя кипэлайв. 
какого либо влияния на рабту кипэлайва этот сприкт у меня неделал.

еще раз скажу что хотя notify скрипт это скрипт делаетль но он неделает ничего
в плане влияния на сам кипэлайв. кто там будет мастером и так далее. 
он делает чтото с бэкендом ноды. например стопит или стартует хапрокси.
или просто пишет в лог запись. на состяние master\backup\fail он напрямую невлияет.


вот вариант этого скрипта у дргого чувака

#!/bin/bash

TYPE=$1
NAME=$2
STATE=$3

case $STATE in
        "MASTER") /etc/init.d/apache2 start
                  exit 0
                  ;;
        "BACKUP") /etc/init.d/apache2 stop
                  exit 0
                  ;;
        "FAULT")  /etc/init.d/apache2 stop
                  exit 0
                  ;;
        *)        echo "unknown state"
                  exit 1
                  ;;
esac

вот такой вот множественный мудеж с кипэлайвом. вот у него столько 
разных сущностей которые делают соверщенно разные задачи.


вот оно самое главное про связь между advert(VRRP) пакетами,
track_script\vrrp_script и notify

дальше ниже описаны уже размусоленные подробности.

краткая хорошая статья о всех этих скриптах для чего они 
вот здесь - https://tobru.ch/keepalived-check-and-notify-scripts/

=======================================
установка:



помечаем входящий на порты 80 и 443 трафик одной меткой 80

iptables -t mangle -A PREROUTING -p tcp -d 192.168.16.100/32 -m multiport --dport 80,443 -j MARK --set-
mark 80
-

ставим пакет который позволит загружать правила iptables при загрузке

apt-get install iptables-persistent
--

разрешаем входить протоколу VRRP

iptables -I INPUT -p vrrp -m comment --comment "VRRP" -j ACCEPT

сохраняем это правило iptables чтоб сохранилось при перезагрузке
iptables-save > /etc/iptables/rules.v4
--

разрешаем проходить насквозь пакетам через хапрокс хост к бэкенд серверам
/etc/sysctl.conf
net.ipv4.ip_forward = 1

--
включаем на хосте настройку которая позволяет как я понял некоторым процессам на хосте
работать с IP адресами которые по факту не имеются в системе ни на одной сетевой карте.(кластерный IP)

net.ipv4.ip_nonlocal_bind = 1
---

ставим haproxy как на исходной системе

sudo apt-get install software-properties-common
sudo add-apt-repository ppa:vbernat/haproxy-1.8
apt-get update
apt-cache policy haproxy
apt-get install haproxy
--

делаем конфиг хапрокси

--


ставим кипэлайвдэ

yum install -y keepalived

---

в keepalived

interval - это как часто запускать проверяющий скрипт
timeout - сколько ждать его выполнение. а если скрипт за это время неответит считать что скрипт выполнился
неудачно

advert - это как часто кипэлайв на ноде принимает решение менять состояние ноды или нет
fall 1 - сколько раз проверочный скрипт должен вернуть fail состояние чтобы ноду считать сломаной
rise 1 - тоже самое только починеной
weight 50 - вообще эта опция она походу взаимосключающая с fall и rise
потому суммарно работает так. если скрипт вернул fail то приоритет ноды понижается на weight. если
скрипт вернул success то скрипт повышает приоритет ноды на weight.
если weight убрать а оставить fall и rise то при возврате скриптом fail столько раз сколько
указано в fail он поставит ноду в fail состояние. успешное выполнение скрипта сколько указано в
rise вернет ноду в success состояние



чтобы в логах небыло ошибки нужно доставить пакет

apt-get install -y libipset-dev
==





как работает:



замечена ошибка в логах
keepalived.service: PID file /var/run/keepalived.pid not readable (yet?) after start: No such file or directory

но походу она невлияет.
------------------------------
у меня стоит

fall 1

значит что когда скрипт 1 раз вернет fail exit code
то нода перейдет в состояние failed ( а вообще состояния бывают
failed, transitonal,master\backup )

НО! перейдет она в него немгновенно а через <= 60 секунд.

вот как в syslog это выглядит

Jun 27 15:55:49 ... VRRP_Script(check_haproxy) failed
Jun 27 15:56:33 ... VRRP_Instance(LVS_HAP) Entering FAULT STATE
Jun 27 15:56:33 ... VRRP_Instance(LVS_HAP) Now in FAULT state

видно что когда скрипт зафейлился то только через 44 секунды
нода перешла в fault состояние


----------------------------
по идее как работает любая нода с кипэлайвдэ ( мастер бэкап неважно)
( условие скрипт чекер показыает все окей поэтому его влияние
пока мы не рассматриваем)
стартует сервис.
он слушает эфир.



нода мастер выключена.
включаем ноду бэкап.
стартует сервис keeplaived. он видит что данная нода типа бэкап
(а не мастер)

и пишет в логах

VRRP_Instance(LVS_HAP) Entering BACKUP STATE

это сайту не дает ничего. так как вииртуальный кластерный IP не присва
ивается. это просто значит что с кипэлайвом все в порядке и он
ПОТЕНЦИАЛЬНО готов если нужно к переведению в мастер стейт.

далее в конфиге у меня указано

---

interval 30
rise 2

advert 120 ( по идее когда на ноде меняется ее состояние
то она выбрасывает наружу
--

даже когда мастер написал

Entering MASTER STATE

то кластерный ip адрес создается в системе немгновенно а через 30 секунд

===

после старта кипэла1вдэ на бэкап ноде
она переходит в состояние бэкап - мгновенно

если в сети нет мастера то через 3*advert бэкап нода переходит в транизшнл состояние
Transition to MASTER STATE

и еще через advert уже становится мастером
Entering MASTER STATE

итого бэкап нода при исчезновении мастера через 4*advert становится мастером
( это в случае пропадания связи или падением кипэлайв самого. то есть не связано
со скриптом проверки сайта)

если на мастере мы гасим кипэлайв через стопинг службы то мастер при этом выплевывает в сеть пакет.
при этом на бэкапе тот час же загорается
Transition to MASTER STATE

и через 1 advert время бэкап становится мастером.



===

если хапрокси вырубился то для кипэлайа это выглядит так

он делает первую проверку через interval получает succeeed
делает вторую еще через таймаут получает fail

===
когда стартанули кипэлавдэ на мастер ноде.
то через advert время она пишет

Transition to MASTER STATE

причем этот интервал никак не связан с interval

пакет в сеть при этом он шлет и на бэкапе получаем запись в сислог
Received higher prio advert 100
(и при этом бэкап сервер который если был мастером мгновенно переходит в бэкап
состояние - Entering BACKUP STATE)

получается к этому моменту все бэкап сервера получили пакет.
тот бэкап сервер который был мастером заткнулся и стал бэкапом.

далее мастер еще ждет advert время ( в этот проемежтоу времени получается
мастера нет вообще зато нет и сплитбрейна) и переходиит в мастер состояние

Entering MASTER STATE


====
итого кластер переключается на мастер бэкап
по двум совсем разным причинам

1. кипэалайв умер сервер умер сеть умерла.
2. кипэлайв жив но скрипт проверки показывает fail


==================================
==================================
чтобы схема работала нужно в конфиге хапрокси в качестве ВСЕХ IP указать не IP ноды а

0.0.0.0

это нужно чтобы хапрокси слушал сокеты вида

0.0.0.0:81
0.0.0.0:443

чтобы когда добавлялся кластерный IP то на его сокете 192.168.16.100:443 хапрокси автоматом
слушал и принимал запросы , обрабатывал.

если указать в конфиге хапрокси IP ноды. то при добавлении кластерного IP хапрокси на его сокете ничего
не слушает .

----

всякий раз когда мастер выдает сообщение в лог

Transition to MASTER STATE

он выплевает в сеть пакет.и все бэкапы при этом если были мастерами тотчас же становятся бэкапами.
далее мастер ждет 1 advert время и берет на себя роль мастера.

при этом в течение 1 advert времени сервис становится недоступен

---
всякий раз когда мастер штатно ( service keepalived stop ) гасится то он плюет в сеть пакет.
и бэкапы тут же пишут
Transition to MASTER STATE

и через 1 advert время один из бэкапов берет на себя роль мастера

при этом 1 advert время сервис недоступен

---
немного остается открытвм вопрос сколько время проходит между стартом сервиса кипэлайв

VRRP_Script(check_haproxy) succeeded

и моментом когда он начинает претендовать на мастера рассылая пакет в сеть
VRRP_Instance(LVS_HAP) Transition to MASTER STATE

обычно 1 advert time, хотя бывает и быстрее

--

если сервис кипэлайвдэ работает на мастере. потом на нем отключаем сеть
потом когда он опять получит сеть то мастер через 1 advert время кидает пакет в сеть
и инициирует
forcing a new MASTER election который аналогичен ( Transition to MASTER STATE )
бэкапы при этом мгновенно переходят в Entering BACKUP STATE
далее через 1 advert на мастере переходит в
Transition to MASTER STATE
и далее через 1 advert на мастере переходит в
Entering MASTER STATE

получается если изза проблем с сетевой картой мастер отвалится то через 1 advert сайт становится недоступен
в течение 2 advert и потом мастер опять становится мастером.

-----
очень важный момент.
видно что всякий раз когда происходит смена кто тянет роль мастера при этом возникает даунтайм
в доступности сайта. это для нас плохо.

также. как мы знаем роли у нод бывают мастер и бэкап.
так вот когда мастер возврашается в строй то он всегда тянет на себя роль мастера.

это вызывает то что происходит много ненужных даунтаймов когда нода с ролью мастер то доступна
то недоустнупна.

этого можно избежать если

1. все ноды имеют в конфиге роль бэкап. ( так можно)
2. в конфиге указываем опцию nopreempt

тогда это всеработает так.

запускаем ноды. все они роли бэкап. они выбирают кто мастер на основе у кого больше priority
==

вот как сейчас по схеме бэкап+бэкап+нопримт работает система при потере связи с текущим мастером.

14:15:12 - потеря связи
14:17:42 VRRP_Instance(LVS_HAP) Transition to MASTER STATE (через 2,5 advert)
14:18:42 VRRP_Instance(LVS_HAP) Entering MASTER STATE ( через 1 advert )

вот еще

14:27:31
14:29:43 VRRP_Instance(LVS_HAP) Transition to MASTER STATE ( через 2 advert )
14:30:43 VRRP_Instance(LVS_HAP) Entering MASTER STATE ( через 1 advert )


получается что при потере связи с текущим мастером даунтайм длится 3-3.5 advert времени
( и это совсем независит от того все ли в порядке с проверочным скриптом , от него независит )
==

нужно через logrotate ограничтиь лог от haproxy health checker сделать ему ротацию

===
после надписи

12:16:36 VRRP_Script(check_haproxy) succeeded

может пройти и не 2 advert времени а даже и 3 advert времени до надписи

Transition to MASTER STATE

====
когда скрипт обламывается то нода переходит в fault стейт.

а потом обратный путь fault->transition state->backup(master)


12:34:06 VRRP_Script(check_haproxy) failed
12:34:11 VRRP_Instance(LVS_HAP) Entering FAULT STATE
12:34:11 VRRP_Instance(LVS_HAP) Now in FAULT state
12:34:26 VRRP_Script(check_haproxy) succeeded
12:35:12 VRRP_Instance(LVS_HAP) Entering BACKUP STATE

=====

вот запись в логе

Jun 30 12:53:01 haproxy-t2 systemd[1]: Stopping HAProxy Load Balancer...
Jun 30 12:53:01 haproxy-t2 systemd[1]: Stopped HAProxy Load Balancer.
Jun 30 12:53:12 haproxy-t2 systemd[1]: Stopped HAProxy Load Balancer. ( вот это уже начал работаь скрипт по
перезагрузке. так как там стоит service restart то он его вначале стопит а потом уже стартит )
Jun 30 12:53:12 haproxy-t2 systemd[1]: Starting HAProxy Load Balancer...
Jun 30 12:53:12 haproxy-t2 systemd[1]: Started HAProxy Load Balancer.

=====

запись в логе

12:56:37 VRRP_Script(check_haproxy) failed
12:56:57 VRRP_Script(check_haproxy) succeeded

запись о том что скрипт отработал сюксесс либо фейлд пишется в лог не для каждой проверки
работы скрипта а только для ситуаций когда меняется состояние ноды и другими словами
когда fall выполняется либо rise

у меня

fall 1
rise 4
interval 5

поэтому при однократном фейл скрипта идет запись

12:56:37 VRRP_Script(check_haproxy) failed

и через 4 * interval = 4 * 5 = 20 , через 20 секунд через 4 успешных отработки скрипта идет запись

12:56:57 VRRP_Script(check_haproxy) succeeded


то есть на самом деле она некоректная. это не скрипт отработал успешно как написано в логе.
это событие rise 4 отработало в сумме успешно. а скрипт при этом отработал аж 4 раза.

то естть еще раз эти строки показыают на самом деле что fall или rise отработал успешно целиком.

то ест в логе должно бы чтобы коректно быть написано

12:56:37 VRRP (fall 1) failed
12:56:57 VRRP (rise 4) succeeded


и самое главное !!!!!! оказывается теперь это стало понятно что если события fall rise
происходят внутри интервала advert , а у меня

fall 1
rise 4
interval 5
advert 60

тогда состояние ноды никак !!!!! не меняется . никак. как бутто ничего и не было.
не меняется ни на самой ноде состояние. ни в сеть не пуляется никакой пакет.


======

при старте двух нод когда нет никакиху уже работаютщих нод
одна из нода станет мастером через 3 advert

====
думаю что при старте сервис кипэлайв рассылает пакет.
но если все ноды бэкап то в итоге чтобы ктото стал мастером надо 3 адверт тайма.
а если одна из нод мастер в конфиге то я думаю она станет мастером быстрее не проверял

в одном из эксперимнтов прошло не 2 а целых 3 адверта когда появилась надпись

Transition to MASTER STATE

а потом уже (как положено ) через +1 адверт надпись
Entering MASTER STATE

в итоге появление мастера заняло не 3адверта суммарно а 4 адверта
( это при старте с нуля двух бэкапов )

===

еще раз провеил при старт двух бэкапов
надпись

Transition to MASTER STATE
появляется через 3 адверта

ну и Entering MASTER STATE еще +1 адверт

в итоге сервис становтся доступен через 4 адверта ( а раньше вроде через три адверта)
====

вот из этого видно

Jun 30 14:06:16 haproxy-t1 Keepalived_vrrp[8398]: VRRP_Script(check_haproxy) failed
Jun 30 14:06:36 haproxy-t1 Keepalived_vrrp[8398]: VRRP_Script(check_haproxy) succeeded

что состояние fall не приводит к тому что сервис обьявляет ноду как FAILED.
изменение состояния ноды происходит(либо не происходит) только в момент когда нужно пакет
слать в сеть ( в конце адверт интервала)
====



ATTENTION !!!



разберем какие состояния есть происходят проходят на ноде в сервисе кипэлайвдэ и как он их отражает в логах.

предположим на ноде окей и она находится в состоянии MASTER

в логах это выглядит так

14:10:15 VRRP_Instance(LVS_HAP) Entering MASTER STATE



далее в конфиге указыается параметр



advert = 120



это секунды. через каждые 120 секунд кипэлайв  принимает решение остается ли нода в MASTER состоянии или у нее чтото сломалось и ее нужно перевести в другое состояние.

помимо состояния MASTER есть еще состояние BACKUP, FAULT 



в логах это выглядит так

14:12:15 : VRRP_Instance(LVS_HAP) Now in FAULT state



14:13:16 : VRRP_Instance(LVS_HAP) Entering BACKUP STATE



итак нода глобально может быть в трех состояниях MASTER, BACKUP, FAULT



важно отметить из логов что привел что нода УЖЕ находится в состоянии BACKUP или MASTER хотя даже написано что entering как бутто она тока входит в это

состояние. на самом деле она как тока написала так она уже в нем находится



: VRRP_Instance(LVS_HAP) Entering BACKUP STATE

: VRRP_Instance(LVS_HAP) Entering MASTER STATE



а вот состояние FAULT обозначается как Now

14:12:15 : VRRP_Instance(LVS_HAP) Now in FAULT state



почему на этом внимание заострено. потому что переход из одного состояние в другое происходит поэтапно зачастую немгновенно например переход в мастер

состояние выглядит обычно так

Jun 30 14:42:19 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Instance(LVS_HAP) Transition to MASTER STATE
Jun 30 14:43:19 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Instance(LVS_HAP) Entering MASTER STATE



про переход из состояния в состояние наверное еще нужно дописать будет.

в любом случае процесс перехода происходит когда истекает таймаут advert .  в нашем примере это после 120секунд.

но внутри этого интервала в кипэлайв также проходят процессы и сменяются внутренние статусы которые также отражаются в логах.

если MASTER, BACKUP, FAIL это статусы кипэлайв внешние. для внешних запросов от других нод.

то статус fall и rise это внутренние статусы которые невидны другим нодам . разберем сейчас их.

внутренни статусы нужны для того чтобы когда наступает конец advert то кипэлайв смоорит какой сейчас внутрений статус .

если fall то значит нода поломана. если она и до этого была поломана то есть ее внешний статус при этом FAIL то внешний статус неменяется. если же внешний статус есть MASTER\BACKUP то внещний статус меняется на FAIL. об этом рассылается пакет

если внутренний статус rise из внешний статус MASTER то внещний статус неменяется. если внешний FAIL то начинает меняться. это тоже нужно отдельно расписать.



за время advert внутренние статусы fall и rise могут успеть несколько раз сменить друг друга.  это очень важно 

потому что при этом никак не меняется внешний статус ноды ( как это изначально интуитично казалось бы должно быть).

итак подробно остановимся и рассмотрим внутренний статусы в кипэлайв и как они записываются в логах.



пусть

advert = 60

interval = 10

fall 1

rise 4



это значит что  каждые 120 секунд (но не раньше) кипэлаайв  принимаеет решение менять ли ему внешний статус

каждый 10 секунд запускается проверочный скрипт

при однократном запуске проверчоного скрипта и если он вернул ошибку внутрений статус переходит в fall

если проверочный скрипт 4 раза подряд возвращает success то внутренний статус переходит в rise



далее еще раз подчеркну

keepalived в логах не указывает событие когда скрипт проверочный вернул ошибку. он этого не отражает. ( как это интуитивно казалосьбы должно быть)

он отражает вместо этого более глобально событие это событие fall. и в логах оно выглядит так

14:33:44  VRRP_Script(check_haproxy) failed



если fall 1 то событие наступает при однократной ошибке кода возврата проверочного скрипта

если fall 20 то событие fall наступает ТОЛЬКО аж при двадцатикратном подряд проблеме с проверочным скриптом



далее

разберем конкретно тайминги какое время ожидать в логах между моментом поломки \ остановки хапрокси и наступлением события fall

здесь еще важно уточненеи  как работает наш проверочный скрипт ( по тех заданию).

первый раз запускается скрипт - он не проверяет как там хапрокси себя чуствует а автоматом возвращает success

второй раз запускается проверочный скрипт - он проверяет здоровье хапрокси но если даже он сломан то все равно возвращает success

третий раз запускается проверочный скрипт - проверятеся здороьв хапрокси и и если и на этот раз он все еще сломан тогда возвращается код bad



итак 

минимальное время наступления fall

таймлайн:

|0|--------------стопим хапрокси--|10|-----------(первая неудачная проверка но код возврата sucess)---|20|-(fall, вторая неудачная проверка код возврата bad, перезапуск хапрокси)

тогда получается что минимально когда возникает событие fall это 1 interval с малюсенкьим хвостиком.
скажем стоп хапрокси на 9 секунде.
на 10-ой первая нудачаная проверка
на 20-ой fall , перезапуск хапрокси

в итоге в логе будет такой тайминг и такие события

14:05:09 Stopped HAProxy Load Balancer. (здест остановили хапрокси)

вторая неудачаня проверка , код возврата скрипта = bad , так как fall 1 то наступает событие fall, и также у нас запрограммировано если fall то перезапускаем хапрокси.

поэтому ниже залогирован перезапуск хапрокси и настпление события fall

14:06:20 Stopped HAProxy Load Balancer. .(перезапуск хапрокси три строки)
14:06:20 Starting HAProxy Load Balancer..
14:06:20 Started HAProxy Load Balancer.
14:06:20 VRRP_Script(check_haproxy) failed (тутже кипэлайв залогировал наступления события fall 1 )

видим что прошло 11 секунд. то есть 1 interval плюс маленький хвостик. хвостик потому что стоп хапрокси должен произойти прямо перед 10-ой секундой например девятая. между девятой

и десятой в итоге создается секунда она и есть хвостик.

максимальное срабатываение fall

|10|-первая проверка удачная ---стоп хапрокси---|20|-|0|-----не проверяет--------|10|-----------(первая неудачная проверка)---|20|-(fall, перезапуск хапрокси)

получается максимальное время срабатыавния fall будет через почти 3 interval

получается суммарно после поломки хапрокси событие fall возникнет через 1интервал( с хвостиком)-3интервал(без малого хвостика)

скажем если interval =10 то fall будет через 11-31 секунду

после этого переход в ноды в FAULT стейт будет через 1с-1advert интервал времени (при условии что за это время
неуспеет rise отработать).
если адверт 60с то тогда будет через 1-60 секунд.

Jun 30 14:33:44 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Script(check_haproxy) failed
Jun 30 14:34:17 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Instance(LVS_HAP) Entering FAULT STATE

в этом примере через 28 секунд

при этом на второй ноде будет переход на мастера через 1 advert

Jun 30 14:34:18 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Instance(LVS_HAP) Transition to MASTER STATE
Jun 30 14:35:18 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Instance(LVS_HAP) Entering MASTER STATE

итого сколько же пройдет времени когда хапрокси упал и вторая нода стала мастером.

максимум это будет через 3 interval + 1 advert + 1 advert = 2 advert + 3 interval
в моем случае адверт 60 ,интервал 10 получаем макс = 2*60 + 3*10= 150с

миниум будет (1 интервал + 1с) + (1сек) + (1 advert) = 1 advert + 1 interval + 2сек
в моеем случае мин = 1*60 + 1*10 + 2 = 72с

итого от 72 до 150 сек



итого общая формула время наступления внутреннего события fall
fall ( 11-31 ) + FAULT ( 1-60 ) + Entering MASTER STATE ( 60 ) = 72 - 151

эксперимент #1

fall (22)

Jun 30 14:42:08 haproxy-t2 systemd[1]: Stopped HAProxy Load Balancer.
Jun 30 14:42:08 haproxy-t2 systemd[1]: Starting HAProxy Load Balancer...
Jun 30 14:42:08 haproxy-t2 systemd[1]: Started HAProxy Load Balancer.
Jun 30 14:42:08 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Script(check_haproxy) failed


FAULT (10)

Jun 30 14:42:08 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Script(check_haproxy) failed
Jun 30 14:42:18 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Instance(LVS_HAP) Entering FAULT STATE


Entering MASTER STATE (60)

Jun 30 14:42:19 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Instance(LVS_HAP) Transition to MASTER STATE
Jun 30 14:43:19 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Instance(LVS_HAP) Entering MASTER STATE

итого 22 + 10 + 60 = 92сек

эксперимент №2

fall (14)

Jun 30 14:53:00 haproxy-t1 systemd[1]: Stopped HAProxy Load Balancer.
Jun 30 14:53:14 haproxy-t1 systemd[1]: Stopped HAProxy Load Balancer.
Jun 30 14:53:14 haproxy-t1 systemd[1]: Starting HAProxy Load Balancer...
Jun 30 14:53:14 haproxy-t1 systemd[1]: Started HAProxy Load Balancer.
Jun 30 14:53:14 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Script(check_haproxy) failed

FAULT (5)

Jun 30 14:53:14 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Script(check_haproxy) failed
Jun 30 14:53:19 haproxy-t1 Keepalived_vrrp[10785]: VRRP_Instance(LVS_HAP) Entering FAULT STATE

Entering MASTER STATE (60)

Jun 30 14:53:19 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Instance(LVS_HAP) Transition to MASTER STATE
Jun 30 14:54:19 haproxy-t2 Keepalived_vrrp[11292]: VRRP_Instance(LVS_HAP) Entering MASTER STATE

итого 14 + 5 + 60 = 79с

--



проблема со скриптом. sed некоректно делает замену в файле

вопрос - что делать с rise. нужно ли делать чтобы он был маленький. чтобы он успевал отработать мы перезагрузили
на текущей нода хапрокси и неделалаи переключение на другую ноду
или это плохо.
также понять что про публикацию.
---


////////// секция - конкретные команды как  установить куб HA кластер

примечание.
	внизу приведены конфиги при установке keepalived на контрол плейн.
	 конфиги для установки на ДАТА НОДЫ см в файле "keepalived для data нод куба.txt" 
	а установка такая же как внизу описано только haproxy ставить ненужно.
	 и скриптом будем проверять неналичие процесса haproxy в ps aux а будем проверять наличие процесса kube-proxy
	если ставим на контрол плейны то надо будет запрограммировать один кластерный ip. кластерный ip будет относится к хапрокси
	 а если ставим на дата ноды то надо будет запрограммировать столько кластерных ip сколько дата нод. кластерный ip каждый это будет ip для обращения внешних клиентов на проброенные порты. и мы хотим раскидать весь сетевой трафик по всем дата нодам
	

$ sudo  apt-get install -y ipset keepalived haproxy
$ sudo systemctl enable keepalived haproxy

edit confs

/etc/keepalived/keepalived.conf

конфиг на первой ноде

! Configuration File for keepalived
vrrp_script check_haproxy {
  script       "/usr/local/bin/check-haproxy.sh"
  interval 2
  fall 2
  rise 2
}

vrrp_instance test_instance {
   interface ens160

   track_interface {
     ens160
   }

   state MASTER
   virtual_router_id 50
   priority 255
!   nopreempt

   unicast_src_ip 172.16.102.34
   unicast_peer {
     172.16.102.35
   }

   virtual_ipaddress {
      172.16.102.100/24 dev ens160
   }

   track_script {
      check_haproxy
   }

   notify /usr/local/bin/keepalived.state.sh
}

конфиг на второй ноде

! Configuration File for keepalived
vrrp_script check_haproxy {
  script       "/usr/local/bin/check-haproxy.sh"
  interval 2
  fall 2
  rise 2
}

vrrp_instance test_instance {
   interface ens160

   track_interface {
     ens160
   }

   state BACKUP
   virtual_router_id 50
   priority 240
#   nopreempt

   unicast_src_ip 172.16.102.35
   unicast_peer {
     172.16.102.34
   }

   virtual_ipaddress {
        172.16.102.100/24 dev ens160
   }

   track_script {
      check_haproxy
   }

   notify /usr/local/bin/keepalived.state.sh
}

СРАЗУ замечу что на соседней кипэлайвдэ ноде priority должен отличаться минимум на 15 очков иначе небудет надежного переключения (выяснил на опыте)


кстати видно как прописать чтобы было не один а несколько кластерных
ip адресов



создаем скрипты для кипэлайвдэ на обоих нодах они одинаковые будут
делаем на них 700 и exec perm 

# touch /usr/local/bin/check-haproxy.sh; chmod +x /usr/local/bin/check-haproxy.sh; chmod 700 /usr/local/bin/check-haproxy.sh


# touch /usr/local/bin/keepalived.state.sh; chmod +x /usr/local/bin/keepalived.state.sh; chmod 700 /usr/local/bin/keepalived.state.sh





~# cat /usr/local/bin/check-haproxy.sh

#!/bin/bash
# Check if haproxy is running, return 1 if not.
# Used by keepalived to initiate a failover in case haproxy is down

HAPROXY_STATUS=$(/bin/ps ax | grep -w [h]aproxy)

if [ "$HAPROXY_STATUS" != "" ]
then
  exit 0
else
  logger "HAProxy is NOT running. Setting keepalived state to FAULT."
  exit 1
fi

еще скрипт

~# cat /usr/local/bin/keepalived.state.sh

#!/bin/bash

TYPE=$1
NAME=$2
STATE=$3

echo $STATE > /var/run/keepalived.state


# systemctl enable keepalived haproxy

важные моменты:
virtual-router_id одинаковый на всех нодах
priority 255 а на втором (-20) 235 можно еще меньше. я поставил на второй 250 и оба лезли в мастеры тоесть небыло надежного переключения
nopreempt - закоментировано это значит что "мастер" нода кипэлайведэ
когда вернется в строй заберет обратно кластерный ip
еще важный момент
virtual-router_id если у нас в одном вилане нахроится несколько незавиисимых кипэлавдэ кластеров то важно чтобы их id был разный. чтобы
неполучилось что он один и тот же в разных кластерах иначе начнется
каша


теперь конфиг 
(хапрокси нужен на контрол плейн. 
на дата ноду он ненужен)

/etc/haproxy/haproxy.cfg


#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    log /dev/log local0
    log /dev/log local1 notice
    daemon
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
#    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 1
    timeout http-request    10s
    timeout queue           20s
    timeout connect         5s
    timeout client          20s
    timeout server          20s
    timeout http-keep-alive 10s
    timeout check           10s

#---------------------------------------------------------------------
# apiserver frontend which proxys to the masters
#---------------------------------------------------------------------
frontend apiserver
    bind *:6440
    mode tcp
    option tcplog
    default_backend apiserver




#---------------------------------------------------------------------
# round robin balancing for apiserver
#---------------------------------------------------------------------
backend apiserver
    option httpchk GET /healthz
    http-check expect status 200
    mode tcp
    option ssl-hello-chk
    balance     roundrobin
        server test-kub-04 127.0.0.1:6443 check
        server test-kub-05 172.16.102.35:6443 check
        server test-kub-06 172.16.102.37:6443 check


проверяем что конфиг валиден

# haproxy -c -f /etc/haproxy/haproxy.cfg

$ sudo systemctl enable haproxy
$ sudo systemctl start haproxy
$ sudo systemctl status haproxy

когда мы его стартанем то он напишет что ни один бекенд сервер
нероступен и это так и есть. он станет доступен только 
после установки первой контро панели

$ sudo systemctl  enable keepalived
$ sudo systemctl  start keepalived
$ sudo systemctl  status keepalived

далее перезагружаем комп и увбеждаемся
что haproxy+keepalived запускаются на автомате

!ВНИМАНИЕ  с keeaplived есть проблемка такая. я столкнулся. если мы перезапустим сеть на хосте через
	# systemctl restart networking
то при этом ip адреса которые на этом хосте породил keeaplived они исчезнут невосстановятся. поэтому нужно будет еще перезапустить kepalived
на этом хосте.
	# systemctl restart keepalived
точнее вроде бы эта проблема всплывает если на интерфейсе есть secondary ip адреса. если секондари нет то перезапуск сети неудаляет keepalived ip адреса.

////////// конец секции

