C - programming language
=============================

обьявление переменных
int vasya

это обьявляет переменную vasya типа integer

далее можно присвоить знаение
vasya = 15

можно сразу сделать два дела и обьвявить перееменную и присвоить ей значение
int vasya = 15

int = это целоый тип
float = это с точкой
char = это литеры ascii я так понял





====

// my first program on C
#include <stdio.h>
#include <unistd.h>

int main ()
{

int vasya = 15;
int *pvasya = &vasya;

printf ( "$vasya = %d\n", vasya  );
printf ( "$vasya = %d\n", *pvasya );

vasya = 16;
printf ( "new $vasya = %d\n", vasya );

*pvasya = 17;
printf ( "new $vasya = %d\n", *pvasya );


return 0;
}

===
printf
%x - позволяет распечатать только 32 битное hex число максимум, а
%lx - уже позволяет распечатать 64 битное hex число !

===

тип переменной определяет для компилятора и проги сколько байт надо выделять под нее 
в памяти и в каком формате туда писать и  как интерпретировать  байты при считывании.
условно говоря если переменная целого типа значит при считывании и печати числа и нее
просто печатать число на экране а если переменная типа char то считанное число 
интерпретировать в литеру.  переменная типа поинтер хранит не значение а адрес памяти по которому
лежит значение тоесть в переменной мы храним незначение а ссылку в памяти где лежит значение 
этой переменной. почему то принято считать что поинтер указывать на память где хранится
значение другой переменной. а на самом деле в поинтере хранится значение самой переменного поинтера.
пример

int a = 1;
int *pA = &a;

получсется мы имеем две переменные которые имеют одинаковый бекенд. и как тут сказать 
какая из них на самом деле владее значением.   походу поинтер это как хардлинк на фс на файл.
несколко хардлинков указвают на один бекенд. 

когда программа напарывается на поинтер то она понимает что в этой переменной pA хранится незначение
переменной а ссылка на то где искать это значение. а значит некоторые операции доступные для обычных
переменных недоступны для переменной типа поинтер а ннекоторые новые операции зато доступны
для переменной поинтер которые недоступны для обычных переменных.

==================

printf

коогда мыхотим напечатать переменную то нужно в явном виде сказать принтф 
какой тип у переменной хотя ведь мы при созании перееменной это явно прописываем.

теме неменее это так  увы


#include <stdio.h>

int main () {

   int  var1 = 500;


printf( "var1=%d\n", var1 );
printf( "octal form=%#o\n", var1 );
printf( "hex form=%#x\n", var1 );
printf( "HEX form=%#X\n", var1 );

printf ( "\n"  );


float var2 = 600.5;
printf( "var2=%f\n", var2 );
printf( "var2=%#A\n", var2 );
printf( "var2=%#a\n", var2 );
printf( "var2=%#g\n", var2 );



   return 0;
}



в этом примере обьявляется переменная var1 с типом int
так вот что интересно что принтф позволяет при распечатке ее конвертировать ее вывод
из дефолтового (десятичного)  в другие виды. что существенно и в чем подьебка что конвертация
доступны не во все виды а только в некоторые

вот такой будет вывод на экран

var1=500
octal form=0764
hex form=0x1f4
HEX form=0X1F4

var2=600.500000
var2=0X1.2C4P+9
var2=0x1.2c4p+9
var2=600.5


таким образом int переменную можно при распечатке вывести в восьемричном виде, hex виде и 
на этом и все.  тоесть например нельзя переменную int типа вывести в виде с плавающей точкой
почему непонятно в чем проблема.


тоже самое касается переменной var2 типа float. 
ее можно при печати вывести в hex виде, но попытка выести ее в виде целочисленном
выдаст ошибку. 

если дефолтовая форма как обьяснить принтф какой тип имеет печатаемая переменная это 
%d
%x
%f

итп, то алтернативный вида задается в виде %#x. вобщем я бы сказал бы что просто напросто
 целочисленныую переменную можно ввывести в dec,hex,oct формате тоесть это блять никакое 
 не преобразование как об этом пишут в man это всего навсего меняется система счисления.
 а переменную с точкой можно вывести в dec,hex форматах. вот и все что касается %# в принтф.

 итак при выводе с помощью %# целое остаетс целым а с точкой осается с точкой 
 просто вывод идет по разным система счисления.

 а если мы хотим целое вывести как с точки или с точкой вывести как целое то для этого
 надо  на переменную натраваить функцию которая ее преобразует. пример

float var2 = 600.5;
printf( "var2=%i\n", ( int ) var2 );

вывод на экране
var2=600.500000
var2=600

вообще я непонима смысл этой решетки %# потому что - тип данных имеет скажем целый тип.
тоесть число без точки. от того что мы печаатем его в hex или dec или oct или bin виде 
от этого тип данных неменяется. поэтому на мой взгляд 

%x = %#x
%X = %#X

непойму нахуя вобще этот оператор # .

потому что и без него все рабоатет.

еть у нас var1 с типом int. окей. теперьмы для себя определяемся в какой системе исчисления
мы хотим распечатать переменную. еси в dec виде то 

printf ("%i", var1);

если в hex виде то 

printf ("%x", var1);

если в HEX виде то 

printf ("%X", var1);

нахуя нам тогда этот #  ??????????? он ничего непреобрзует. и никаких новых возможностей недает!




 =============


еще интерснейщие пример
он про указатели. 
про принтф


#include <stdio.h>

int main () {

   int  var1 = 500;
   printf( "var1 = %i\n", var1  );


   int *pVar1;
   pVar1 = &var1;
   printf( "Address of var1 variable in memory(hex) =  %p bytes\n", pVar1  );
   printf( "Address of var1 variable in memory(dec) =  %li bytes\n", ( long int ) pVar1  );
   printf( "Address of var1 variable in memory(dec) =  %li TB\n",( ( long int ) pVar1 ) /1024 /1024 /1024 /1024);


   printf( "var1 = %i\n", *pVar1 );


   char var2 = 'a';
   printf ( "var2=%c\n", var2 );
   printf ( "var2=%d\n", ( int ) var2 );






   return 0;
}


вывод на экран:
var1 = 500
Address of var1 variable in memory(hex) =  0x7ffe4bbe0c1c bytes
Address of var1 variable in memory(dec) =  140730169166876 bytes
Address of var1 variable in memory(dec) =  127 TB
var1 = 500
var2=a
var2=97



разберем


   int  var1 = 500;
   printf( "var1 = %i\n", var1  );

  наэкране
  var1 = 500

  обьявляет целую переменную и печатает ее обьясняя принтф что мы собираемся печатать 
  перменуую целого типа. все таки непонятно если мы уже указали что перменная целого типа зачем
  еще раз это прописывать в принтф





   int *pVar1;

   обявляем поинтер переменную которая укащывает на ячеку памяти в которой лежит значение
   целого типа, 


   pVar1 = &var1;

   записываем в pVar1 адрес ячейки памяти от переменной var1




   printf( "Address of var1 variable in memory(hex) =  %p bytes\n", pVar1  );

   на экране
   Address of var1 variable in memory(hex) =  0x7ffe4bbe0c1c bytes


   печатаем адрес ячейки памяти который хранится в поинтере. получается в принтф переменная
   типа поинтер указывается как %p.
   заметим что pVar1 указывается без всяких *
   таким образом если мы хотим заглянуть в сам поинтер то просто указвыаем его имя pVar1
   никаких звезд ненадо. как видно по дефолту значение поинтер переменной тоесть адрес ячейки памяти  печается на экране в виде hex.


   printf( "var1 = %i\n", *pVar1 );

   на экране
   var1 = 500


   а тут мы печатаем не поинтер а значение ячейки на которую он указывает. поскольку значение
   в ячейке имеет целоый тип то для принтф указываем %i
   поскольку мы хотим посмотреть не в значение поинтера а по адресу который в нем хранится 
   то импользуем звезду *pVar1

   вот такая большая разница в смысле pVar1 и *pVar1

   pVar1 = 0x7789798798dc  тоесть адрес ячейки памяти
   *pVar1 = 500  значение которое лежит по адресу втой ячейке памяти

   соотвестенно для принтф
   %p = указывает что печатаем переменную поинтер типа
   %i = указываем что печаатем переенную целого типа

прикольно то что нет просто переменной понинтер типа. помимо поинтер типа этот тип еще 
дожен иметь некий классический тип

	int * pVar1 = переменная поинтер типа + целого типа





   printf( "Address of var1 variable in memory(dec) =  %li bytes\n", ( long int ) pVar1  );
   printf( "Address of var1 variable in memory(dec) =  %li TB\n",( ( long int ) pVar1 ) /1024 /1024 /1024 /1024);

   на экране
	Address of var1 variable in memory(dec) =  140730169166876 bytes
	Address of var1 variable in memory(dec) =  127 TB

 здесь интересно то что я хотел распечатать поинтер на экране не в дефолтовом виде hex 
 а в десятичном виде. если переводить адрес памяти в из hex в dec то он очень большой и влезает 
 только в тип данных long int  поэтому я преобразовал hex в long int для этого я заюзал функцию  ( long int )  которую натрваил на pVar1. в английских книгах преобразование одного типа данных в 
 другой называется type casting  или просто casting. 
 в принтф я тоже указал что мы печатаем на экране тип данных %li.
 во второй строке  я применили арифмтические операции и перевоел байты в терабайты при этом получается число которое влезает в тип данных integer поэтому для принтф я указал %i

получается про принтф. в нем мы укаваем какой тип данных имеет переменная а потом указываем
эту переменную

printf ( "%тип_данных_у_переменной", имя_переменной)

вот так эта хрень работает.


или в более сложном вариенте принтф вместо переменной мы можем указать функцию которая берет пременную ее значение и тип и пребразует
ее значение вдругой тип

printf ( "%тип_данных_у_переменной",  (функция ) имя_переменной)

на выходе функции должны полчаться данные ровно того типа что мы указали в %.

теперь вспомним что написано в man 3 printf. там написано что если мы в принтф юзаем %p
то принтф ожидает увидеть переменну с типом void *. тоеть переменную котора была обьявлена 
вот так 

void * pVasya

тоесть перменная должна быть поинтером да непросто какого то там типа а воид типа.
если же мы посмотрим как унас вверху заюзано

	int *pVar1;
	pVar1 = &var1;

	printf( "Address of var1 variable in memory(hex) =  %p bytes\n", pVar1  );

   	на экране
   	Address of var1 variable in memory(hex) =  0x7ffe4bbe0c1c byte


то мы увидим что в принтф мы подставили перменную pVar1 которая да поинтер 
но она не воид типа а int типа. хм... почему же компилятор незаругался и у нас все сработало?
тоесть моя практика показывает что принтф если указать %p то в качестве переменной он принимает
переменую типа поинтер абсолютно любого типа ему пострать это int * или void * или long * 
абсолютно похер. главное чтобы был тип поинтер неважно какого типа.
ключ %p печаатает адрес который хранится в поинтере. поэтому ему как бы похер на какой 
тип данных этот адрес ссылается.
 
 я считаю вот эта мудота с обозначением поиинтера как int * или void * это полный идиотизм
 вместо этого надо было ввести обозначениее для создания поинтера чтото типа  такого 

 int * var1  ----> pointer_i  var1
 void * var1 ----> pointer_v  var2

 вместо этих дурацкий звезд. а то сиди иломай мозг разгадывая смысл этих звезд.



=================

потихоньку начинает вырсиываотся смысл

ssize_t read(int fd, void *buf, size_t count);

получается функция read резульаттом является данные с типом ssize_t ( об этом потом) там отличие 
ssize_t от size_t  в том что может вернуться -1 

int fd = тип переменной целое, смысл переменной это файловый дескриптор (файл) из которого читаем
void *buf =  тип переменной поинтер, тип у поинтера void. смысла ее это буфер в который пишем что прочитали
size_t count = тип переменной size_t . смысл ее это размер читаемого в байтах


какой смысл у поинтера который указыаем на ячейку в котрой хранятся данные типа void.
воид это значит что тип данных неизвестен. я так поимаю что запись в поинтер который указывет 
на ячейку с неизвестным типмо данных дает нам взможность типа того что писать туда данные в raw формате. 

но void * var1 поинтер будем подрообнее анализивать и обсуждать ниже.

===========


соатновился на задаче как счиатть 1 символ с клавы
а потом выести на экран

вот этот код считыает с клавы 1 символ

#include <stdio.h>
#include <unistd.h>

int main () {

    int klava = 0;

    void * bufer;
    char k[1];
    bufer = &k;

    printf ( "void address = %p\n", bufer );

    size_t count;
    count = 1;

    read( klava, bufer, count);

   return 0;
}


что здесь существенно. 
если просто обьявить void поинтер 

void * bufer;

то как показал практика в реале он неиницилизируется и ни на какую ячейку памяти
непоказывает. поэтому нужна вот такя комбианация чтобы bufer начал релаьно укзатьвыа на
ячеку в памяти

void * bufer;
char k[1];
bufer = &k;


ну а эта строчка

printf ( "void address = %p\n", bufer );

это просто проверка что ячейка понинтер кудато реально указывает.

поопутно как я понимаю тип char это 1 байт в памяти под каждый char обьект. по факту 
это число от 0 до 255. целое.
поэтому наша воид поинтер по факту указыает на ячейку 1 байт длинной.



правлная инциализация void - то что он сам по себе никуда незукывает.
его плюс в том что его можно директить на соврешенно разные типы бекенд ячеек

int a = 10;
char b = 'x';

void *p 
p = &a;  
p = &b; 


минус в том что с этой поинтера нельзя напрямую печатать

int main()
{
    int a = 10;
    void *ptr = &a;
    printf("%d", *ptr);
    return 0;
}

если даже на бекенде сидит у нас int то получаетс что по определению void поинтер указывает 
на ячейку памяти в которой сидит занчение перменной у котроой нет типа!!!! а раз нет типа
то ее нельзя распечатать.

поэтому воид поинтер надо конвертить ( и он конвертирится в любой тип без проблем) в тот 
тип данных что мы хоим печатать

int main()
{
    int a = 10;
    void *ptr = &a;
    printf("%d", *(int *)ptr);
    return 0;
}

на счет вот этой хрени  *(int *)ptr


(int *)ptr  = создает поинтер нового типа. адрес в нем такой же а вот тип данных который лежит в
 ячейке по адреу поинтера утверждоаетс что новый тоесть

 int a = 1;
 void * vasya;
 vasya =  &a;

 хотя мы знаем что в ячеке пойинтереа лежит целое но C этого незнает.
 пожтому что мы заявили что по адресу поинтера vasya лежат какието данные но их тип определить
 невозможно.

 поэтому

 int * petya;
 petya = ( int *) vasya;

 тоест мы сконевертировали воид поинтер в int поинтер и подсунули адрес а petya

 теперь petya смотрит тудаже куда и вася но для C уже понятрно что бекенд пети это интеджер.

 поэтому его можно печатать.

 printf ( "%c", *petya  );



так вот я про вотэту команду

*(int *)ptr

она преобразует воид поинтер и вызывает значение по его адресу.

пример

#include <stdio.h>
#include <unistd.h>

int main () {

    void * pVasya;
    char a = 'a';
    pVasya = &a;

    printf ( "pointer pVasya = %p\n", pVasya   );

    char * pa;
    pa = (char *) pVasya;

    printf ( "pointer pa = %p\n", pa);

    printf ( "char = %c\n", *pa  );

   return 0;
}

вывод на экран:
pointer pVasya = 0x7fff5d948237
pointer pa = 0x7fff5d948237
char = a



видно что поинтеры указывают на одну ячейку.

так ну пока я невижу какогот то полезного смысла в воид поинтер
кроме такого что  его указатель можно менять на любые типы препеменных.


int a = 10;
char b = 'x';

void *p;
p = &a;  
p = &b; 

прикольно. но какой в этом болшой смысла на практкие хер его знает


ясно одно кога мы создаем воид поинтер то мы четко заявляен стстеме что тип данных у ячейки остуствует. его нет. мы неможем его определит. данные есть но их тип неопреелим. несущетсвует.
поэтому принтф для воид поинтера неработает. 
для самого адреса %p конечно работает а вот для занчения перменной в ячейке нет.



==========
cast
type cast
type conversion

это все слова о том что можно натравливать на переменную одного типа функцию
которая превратит данные этой переменной в другой тип и эти новыве данные нового типа
можно будет присвоить новой переменной

(type_name) expression

из этого получается что в выражении void * vasya кусок void * является одним целым
и означает тип данных.

тоесть 

int
double 
float
char
int *
void *

это все так обозначаются типы данных. но меня больше всего тут заинтересовало int * и void *

==

#include <stdio.h>


int main () {

    int a;
    printf ("%p\n", &a );

    int * pVasya;
    printf ( "%p\n", pVasya );


    void * pVasya2;
    printf ( "%p\n", pVasya2 );


   return 0;
}

vasya@vasya-Lenovo-IdeaPad-L340-15IWL:~/C$ gcc -o 15 15.c; ./15
0x7fff55d56584
0x5570162325a0
0x7fff55d56680


странно.
переменным неприсовено никакого значения но под них 
уже зарезервированы ячейки в памяти.
===

вот еще пример


#include <stdio.h>


int main () {

    int a;
    printf ("variable \"a\" memory address = %p\n", &a );
    printf ("var a = %i\n", a );


    int * pVasya;
    printf ( "variable pVasya points to memory address = %p\n", pVasya );
    printf ( "*pVasya = %i\n", *pVasya );



    void * pVasya2;
    printf ( "pointer pVasya2 points to memory address = %p\n", pVasya2 );


   return 0;
}


$ gcc -o 15 15.c; ./15
variable "a" memory address = 0x7ffe1165e0c4
var a = 21881
variable pVasya points to memory address = 0x5579416f95a0
*pVasya = -1991643855
pointer pVasya2 points to memory address = 0x7ffe1165e1c0


видно что мы обьявили пермеменные но не присвоили значения
однако значения у них уже есть! это чтото !

=========

пример cast преоббразования одних типпов данных в другие


#include <stdio.h>


int main () {

int a = 10;
int b = 2;

double count;


count = (double ) a / b;

printf ( "%i / %i = %f\n", a, b, count    );


   return 0;
}


vasya@vasya-Lenovo-IdeaPad-L340-15IWL:~/C$ ./15
10 / 2 = 5.000000

======

поговорим про ascii
станадартк кодирования символов. типа есть текст с байтами их надо 
как то транслировать в символы. man ascii
кодирование сосотоит из 128 символов. на него хватвает 7 бит.
часть кодов это несимволы а упправляющие сигналы. кодировка уходит
во времена печаатающих маштнок ( typewriter). и электрических печатных машин ( teletype writer)
так вот хочу про два спец символа сказать

CR - carriage return это когда на печатной машинке мы беремся за железную такую ручку
и тащим налево это приводит к тому что печатная гооловка оказывается опять в левом крайнем
положении. что важно отметить что при этом мы остаемся в тойже строке тоесть бумага при этом
на прокручивается вверх вертикально. ( хотя как я помню из своего опыта печатания на печатной
машинке то при перенеосе головки влево и бумага автоматом прокручивается вверх на одну линию)

LF - line feed. прокрутка листа бумаги на одну линию вверх. при этом с головкой ничего непросиходит
так что печать начнется с тойже позициив строке а не с левой крайней.

поэтому получается что на классическом матричном принтере если закончилась строка то чтобы
начать печатать с новой строки надо подать два сигнала оба тоесть CR + LF либо LF + CR
более того я прочитал что якобы для матричных принтеров нужно было дать несколько таких
сигналов типа CR + CR +CR +LF потому что якобы они неусмевали дотащить головку влево 
за время одного CR что по мне бред ведь они печатают из буфера а не стой скоростью с которой
 в них прилетают сигналы.

 тем не менее теперь понятна разница между CR и LF.
 LF часто называют "new line". а так тепер стало понянто. CR это перметсить печатную голову влево
 а LF это прокруттить рулон бумаги вверх.

 итак вся эта хрень относилась к элетрическим печатным машикам по которым по проводам телеграммы
 летели.  а когда перешли к  теримналам с электронными экранами то ( как я понял ) необходимость
 для переноса строки юзать два символа типа пропала. поэтому в файлах начали изать для этой цели
 обычно один символ либо CR либо LF и  терминалу стало понятно что надо сделать CR + LF.

 поэтому в линуксе в текстовых файлах знаком конца строки и начла новой строки является символ
 LF тоесть байт который в себе несет значение 10 (dec) или 0A (hex)

 тут еще раз важно отметить что символ LF формально должен только пркручиваь бумагу на 1 строку 
 вверх а голвка при этом остается на том же месте. но в линуксе в его терминале это приводит к тому
 что экран прркучиватеся на 1 строку вверх и также при этом и гооловка печаатающая возвращается
 в левое крайнее положение

 вот программа

 $ cat 1.bash 
#!/bin/bash

sleep 1; 

echo -n "v"; 
sleep 1; 

echo -n  "a"; 
sleep 1; 

echo -n "s"; 
sleep 1; 

echo -n "y"; 
sleep 1; 

echo  -n "a"; 
sleep 1;

echo -n -e "\x0D"; 
sleep 2; 



echo -n "*"
sleep 1;

echo -n "*"
sleep 1;

echo -n -e  "\x0A"
sleep 1;


echo -n "!"
sleep 1;

echo -n -e  "\x0A"
sleep 5;

вот ее вывод

**sya
!

теперь разберем.
вначале програ печаате по буквам слово vasya

vasya

печатется оно буквам в одной строке. ключ -n  в echo дает то что при выводе буквы после 
нее непроисходит прокрутки бумаги и возврата головки(курсора) в левое положение ( условно говоря
не вставляется enter)

потом печатная головка (курсор мигающий) возвращаетмя в начало строки (без прокрутки бумаги).
за это отвечает команда echo -n -e "\x0D".  0D это символ CR (возврат головки влево) в hex 
формате. здесь важно отметить еще раз что при возврате каретки бумага непрокручивается. так что 
печать остается в тойже строке.
и далее начинает затираться строка печатая две звездочки. так что мы увидим вот такое

**sya

этим я хотел еще раз подчеркуть что при поступлении в терминал ( котрорый можно сравнить с матричным принтером) символа CR ( 0D hex ) происходит возрат головки вначало строки и не происходит
прокрутка бумаги. так что печать начинает затирать ранее набранные символы.

далее в
 бумага прокручивается вверх  потому что на терминал поступает байт 0A (hex ).
это делается командой echo -n -e  "\x0A"
0A (hex) этот байт это  символ LF - line feed - прокрутка  бумаги наверх. но помимо покрутки бумаги также происходит и возврат головки
 вначало строки. здесь уже 0A ни причем это чисто отсебятина linux терминала.

терминал можно сравнить с матричным принтером. а байты которые в него влетают с управляющими командами. если прилетает байт 76(hex) то принтер печаатает головкой литеру "v"
если прилетает байт 61 (hex) то принтер печатает литеру "a"
а если прилетает байт 0D (hex) то принтер ничего непечаатает он возвращает головку в левое положение
в начало строки.
а если прилетает байт 0A (hex) то принтер пррокручивает бумагу на одну строку вверх. и еще добавляет  от себя возврат головки в начало строки.



так вот когда напечаатся **sya то происходит пркрутка бумаги и  возврат в начало строки 
и печается !

**sya
!

далее происходит опять пркоуркртка бумаги и возврат в начало строки.

ключ -e позволяет через echo передвать байты в hex виде.
ключ -n засталвяет echo невставлять автоматом LF в конце команды. так что мы имеем полный контроль
над выводом в терминал.

такми мккаром мы изучили влияние и работу CR и LF.

также полчается если у нас есть файл и мы запустим его вывод на терминал 

$ cat vasya.txt

то файл содержит байты. так вот терминал (ядро линукса) он как матричный принтер рассматривает каждй байт в файле как 
литеру ascii таблицы и либо печатает сооветвующую литеру или если это нелитера а управляющая команда
такая как CR или LF то прокручивает бумагу (то бишь экран) или возвращает головку (курсор) в начало
строки. таким образом голые байты в файле превращаются  в набор литер на экране. (ну или на бумаге) есл бы это был принтер.

таким образом в файле никакие литеры нехранятся ( как это могло бы интуитивно показаться). 
файл это не бумага. в файле литер нет. там только голые байты.


далее. что инересно.
если мы откроем скажем в mcedit файл в котлром есть байты которые в таблице asccii отвечают 
за упраляющие команды то на экране  в определенных местах будут стремные символы например для 
байта CR на экране в тексте будет что то типа ^M

вообще это интересно. потому что при просто выводе на экран такого ничего нет.
курсор терминала просто делает что ему предписано прокручивает бумагу или возвращает курсор 
в начало строки. так что при просмотре файла никаких стремных символов нет. потому что просмотр файла  это аналогично печати на принтер. где каждый байт это либо литера либо управляющая команда
для головки принтера (курсор)

вобще полчучется файл это поток байтов которй по проводам предавался от одгной электро печатной машинки на другую и использоваолся чтобы набрать текст в автоматическом режиме.


типа это как рецепт изготовления пирога.

принтер является поваром а файл рецептом.

терминал выступает в роли принтера.

когда мы открваем файл на редактирование то получается в определеннымх местах нам надо 
как то показывать что здесь сохранен непросто литера а управляющая команда. 
это как открыть файл ворда. там помимо голого текста еще напихана куча управляющей информации.
таким образом это власть текстового редактора показывать каким либо одразом управляющие символы
ascii или их скрывать.

mcedit например байт CR показывает как ^M

===

ascii

рассмотрим следущий байт

00 (hex)

он означает то что при получении этого байта терминал ничего неделает. ни печатает и с головой
ничего неделает.


пример

$ echo -n -e "\x00v\x00a\x00s\x00y\x00a"

здесь получается слово vasya и между буквами вставлены байты 00 (hex) 
на экране мы увидим просто слово 

vasya

между буквами ничего небудет никаких лишних символов.
возможно в teletype writeer машиных этот символ вставляли для того чтобы у принимащей
машины были паузы при печати незнаю для чего эти паузы. чтобы машины не перегревались.

этот как в процессоре команда nope которая ничего неделает 1 цикл.


что при этом существенно что если мы направим вывод в файл 

echo -n -e "\x00v\x00a\x00s\x00y\x00a" > ./vasya2.txt

то конечно внутри него попимо байтов отвечающих за литеры vasya еще будут байты 00 (hex)

делаем следущий экспримент

$ cat /dev/zero

на экране терминала небудет отображаться ничего. потому что в темринал поступают байты у которых
значение 00 (hex) поэтому драйвер терминала  ( ака виртуальный матричный принтер) интерпретурует
данную команду как ничего непечатать и неделать.

(а совсем не так как казалось бы интуитивно раз поступают байты с нуля то на экране побегут нули. хаха)

в тоже время если направить эту штуку в файл то конечно файл забьется байтами 00 (hex)
и это можно отлично посмотрет в hex редакторе файлов.


итак с 00 (hex) байтом в ascii разоборались
===

ascii

написано что в бинарном виде заглавная буква отличается от маленькой буквы только одним битом.
я проверил это так.

например

dec 87 = 'W'
dec 119 = 'w'

$ echo 'obase=2;87' | bc
1010111

$ echo 'obase=2;119' | bc
1110111

напписано что на мехаических teletypewriter такая схема облегачала 
их програмирование их контсрукции

===
ascii

вначае был морзе. потом бодо(ita1). потом ita2

в морзе буква от буквы при передаче отделяются опреденной паузой размером как три точки.
слово от слова отделяется еще более длинной паузой как семь точек.

морзе можно передавать  через один провод.
кодирование идет ну типа двоичное но на каждый симовол уходит много бит потому что еще идут и стоп биты. условно говоря если точка это 0
а тире это 1 , то буква A = 01, буква B = 1000 что касается букв то максимум 4 бита используется, 
например H = 0000 
но прикол в том что чтобы передать слово из одной буквы уходит слишком много битов еще и на стопы.
передача слова из одной буквы выглядит так:
стоп-(от одного до  четырех бит)-стоп-стоп-стоп

при наборе даже одной буквы между точкаи и тире делается пауза в размере одной точки по длине.
получается берем букву A: A = 01, тогда передача выглядит по времени как

0-пауза-1-пауза-пауза-пауза

вобщем да очень много стоп битов так скажем при передаче чтобы уметь отделять на слух буквы друг от друга и слова от слов.



далее был бодо. его код уже работает только через ПЯТЬ проводов ! зато уже нет этих многочисленных
стоп пауз.

у бодо уже чисто двоичная кодировка символов. как я понял схема работала на практике так.
с нашей стороны аппарат с клавой на 5 кнопок потом в поле уходит пять проводов,
на той стороне принимающий аппарат и принтер. принтер это полоска бумаги.


на нашей стороне мы жмем комбинацию из 5 кнопок. например A = 10000 
этот сигнал по пяти проводам полетел туда. там на ленте пробивается столбик. в котором одна дырка
и четыре места где нет дырок. соотвесвтенно на пяти битах можно запрграмировать 32 символа.

тоесть на той стороне будет лента на которой будет вот такая лента. каждая буква
это отдельный символ

--------
1 1 0 0
0 0 0 0
0 0 1 1
0 0 1 1
0 0 0 0
---------

на этой ленте запрограмировано AABB сообщение.
при этой схеме человек сидит печаатет и сообщение улетает в режиме онлайн.

как я понял бодо изобрел и метод кодировки и сам аппарат по отправке сообщений.
на аппарате небыло как я понимаю клавы с символами  а было просто реально пять кнопок
каждая из которых была привязана к одному проводов которые шли через леса и поля на ту сторону
приемника.

потом мюррей придумал такой аппарат что можно на нашей стороне набрать собщение которое
у нас же отпечаается в форме ленты с дырками. потом взять эту ленту засунуть в другой апарат
и он уже автоматом по этой ленте отправит собщение. так что набор сообщение стал отделен от 
процесса отправки сообщения. также как я понимаю мюррей уже привязал печатную машинку с полноценной
клавой к этой кодировке бодо. тоесть человек нажимал на кнопки на которых были нарисованы уже буквы. что сущесвтеннно улучшает удобство отправки. как на печатной машинке. и также как уже сказал
мюррей сделал асинхронным процесс набора текста и его отправку. 
также как я понимаю что касается приема сообщений все оставалось неудобным тоесть на 
принимающей стороне люди получали ленту с дырками а не печатный текст. ленту получается
надо было еще руками дешифровать. но хотя бы отправка удобная стала.



как я примерно понял кодировка бодо на пяти битах называется также ita1
ita это чтото типа телеграфный стандарт алафита.


потом сделали ita2
он тоже базируется на 5 проводах.но! в нем сделали хитрую штуку. добавили спец символ 11011
и таким макаром получили двойную емкость символов. выглядело так.
есть закодированный набор букв от 00000 до 11111 ( за исчключением некоторых комбиацний  в частности спец символ 11011). и есть закодированный набор уже доп символов всякие там цифры
знак доллара, знак вопроса, знак фунта, кавычки запятые условно говоря всю эту группу назовем
группа цифр. так вот если мы хотим отправить цифру то мы вначале шлем на ту сторону спецсимвол а уже за ним код символа. спец символ 11011 это что типа прообраза кнопки Shift на клаве.
для того чтобы показать что мы обратно вернулись к отправке букв был тоже спецсимвол 11111

тогда отправка выгляди так

11011 - цифра-цифра-цифра-11111-буква-буква-буква-буква

как видно если подряд идет несколько бука или цифр то между ними спецсимвол каждый раз неставиться
он ставиться только вначале группы.

пример 

буква A = 11000
цифра 1 = 11101

тогда передаем AA11 сообщение:


11111(спецсимвол группы букв)-11000-11000-11011(спецсимвол цифр)-11101-11101


и вся эта хрень на той стороне выбиывалась на ленте в форме дырок вот так:


					-------------	
					|o o o o o o
					|o o o o o o
начало ленты=>		|o       o o        <== конец ленты
					|o     o  
					|o     o o o
					|-------------

буквами 'o'  я обозначил дырки на ленте в бумаге.

в этой кодировке также был символ для пробела. символ прокрутки бумаги на одну строку (line feed), символ переноса каретки влево вначало строки.

как бы спрашивается какая хрен каретка. каретка она же у печатной машинки только есть.
тоесть зачем пеересылать на ту сторону знак прокрутки бумаги или знак возврата печати символа 
с левой позиции. я так понимаю что эти симфолы форматирования текста отсылались на ту сторону
в форме дырок на ленте чтобы человек который дешифрует сообщение мог понимать как ему форматировать  текст уже на бумаге на печатной машинке чтобы текст невыглядел как сплошная длинная одна строчка. может так ? и видимо на каком то этапе все таки сделали автоматический аппарат который вместо человека считывал дырки и печатал литеры уже на печатной машинке автоматом с соблю
дением форматирования. тем более что на печатной машинке нужно по любому переносить строку 
потому что лист то имеет конечную ширину (чото я непомню если печатть на печатной машинке  и дошел до конца строки она что делает продолжает долбит в последнюю позицию в строке или что ?)

 с точки зрения печатных машинок непонятно какой смысл имеет CR без функции LF.
 какой смысла на печатной машине вернуть головку вначало строки и начать запечатыывать а именно
 поверх старых символов херачить сверху новые символы. мы же наавоз получим на бумаге.
 это непонятно. по мне CR всегда должен быть с автоматичеким LF. непонятно

 однако имеенно на этом этапе ita2 появились управлящие спец символы CR и LF.

 эти симолв предназанчались для тех усторйств которые декодировали ленты с дырками 
  и печатали текст на печатных машинках на бумаге (типа проорбразы принтеров) чтобы текст получался
  полноценно форматированным. типа такого


  ------------------------------
  |мама мыла                   |
  |раму.                       |
  ------------------------------

тоесть CR и LF нам позволяет начать печатать новое слово с новой строки в любой момент.


таким образом уже видно что CR и LF символы были придуманы для управления принтерами печатными 
устройствами но не электронно-лучевыми мониторами терминалов.

это важно понять откуда зачем почему для чего появились символы CR и LF.
короткий ответ - для автоматического управления принтерами. для форматирования текста при печати.
ровно как это есть при печати на матричном принтере!

единственное мне пока остается непонятным как же они ухитирились создать аппарат котоырй автоматом
умел расшифроввыать дырки на ленте и потом печаать правильные буквы на печатной машине.
как это можно было сделать без компов и микросхем с процессорами.

так вот забегая вперед скажем про ASCII.
ascii хотя я неочень понял это стандарт кодирования символов в телекоммуникациоонном оборудовании
и в компах. походу поскольку телеграфные устройства это тоже телекомуникационное оборудование
то типа того что ascii это апгреженный вариант кодирования бодо. только уже на 7 проводах.
потому что используется 7 бит для этого. так вот получается что управляющие символы в кодировке этой опять же предназначены для того чтобы на принмающей поток стороне  принтеры (телепринтеры так вроде их звали) правильно форматировали текст. тоесть еще раз в ascii кодировке управляющие символы это символы управления форматироваиния печати на принтерах. тоесть эти символы придумывались изначально для принтеров ! а не элт экранов терминалов.


тоесть принимает телепринтер телеграмму и поступают к нему буквы он их печаатет. потом в потоке
поступает символ CR+LF и телепринтер прокручивает бумагу пеерводит печатную головку вначало строки
и дальнейшая печать идет с новой строки. таким образом передаваемый поток содержал нетолько инфо 
что печатать но и как печатать как форматировать.
====

ascii

получсется код бодэ был придуман чтобы по шине 5бит передавать буквы.
где хранилась данная инфо в цифровом виде. ответ нигде.
кроме как в виде дырок на ленте. лента была как файл и жесткий диск одновременно.
данное цифрая инфо только летела по шине (проводам) и все больше нигде не хранилась.

 важный вопрос что такое кодировка? =  получается кодировка бодэ это отправка от источника к приемнику информации о букве  в виде двоичного кода длинной 5 бит.
за что отвечает кодировка на что она влияет? = если у нас есть цифровой канал от источника
к получателю и мы отсылаем символ из предоопределенной таблицы ( букву "a") через этот канал 
то согласно кодировке этот символ будет иметь конкретный вид битовый. 00100. кодировка этот 
метод преобразования литеры в 5битовое слово.  в разных кодировка литера будет иметь другой вид
этого слова. получается если у нас есть поток буквенной информации и мы этот поток шлем по 
цифровому канаду в виде битов то кодировка определяет конкоетный битовый вид для литеры.

поток литер может идти как с клавы так и из файла. в файле поток сразу выглядит в форме потока
битов. если поток идет  с клавы то вначале коды нажатия клавиш надо преобразовать в коды литер
согласно кодировке. а потом отправлять получателю. получаетель должен получить при приеме поток 
битов и байтов составленный согласно кодировке. 

кодировка позволяет набор литер преобразовать в битовый\байтовый массив. в цифровой вид. нули и единицы. эта же кодировка позволяет преобразовать битовый массив в человеческие литеры. 

получается 


 литеры на бумаге  --> кодировка -->  массив байтов 
 литеры на бумаге  <-- кодировка <--  массив байтов


применительно к компьюетеру а не телеграфу = есть файл на диске в виде набора байтов.
нам его надо вывести на монитор или принтер в виде букв литер. для этого надо их преобразовать.
преобразование , поиск какой байт кодирует какую литеру идет через таблицу кодировки.

в случае телеграфа кодировка позовляла задать для буквы на бумаге или кнопки на клаве 
конкретный цифровой сигнал битовый который уже можно отправит по шине. по каналу связи. через
электричество. 

кодировка позволяет имея битовый сигнал узнать что за буквы за ним кроются.
или имея буквы трансформировать их в битовое представление.

получается еще раз что позволяет кодировка

aABjqwdkqwdkd  --> кодировка --> 01010101001010100101
или
0101010101011  --> кодировка --> Abbdbdbdbwdjewbfjwfb


пуолчается кодировка нам позволяет информацию из литерного вида перевести в битовый вид
и наоборот из битового вида в литерный вид.

битовый вид можно передавать по каналам связи. а литерный нет. поэтому то и нужна кодировка
чтобы как преобразовать литерный вид в битовый.

остается вопрос где этот битовый поток информации живет и существует. что является его носителем.
где байты рождаются и где умирают и где хранится что их носитель.
для телеграфа это  канал связи. и походу только в нем и все. ну еще на перфорированной ленте.
человек тыкает кнопку на телетайпе , аппарат создает битовый поток на этой стороне провода. это место рождения этого потока этих бит. потом аппарат его пихает в канал связи. канал связи носитель этого потока бит.поток прилетает на приемник и там как цифровой вид он умирает. остается в форме дырок на бумажной ленте. лента это носитель. но это уже не цифровой вид потока. хотя...наверное цифровой все таки. просто немагнитный носитель а так цифровой.

для компьютера поток хранится во первых в файлах. в файлах записаны биты. биты имеют опреденный порядок как раз на основе кодировки. биты текстовой информации естественно потому что ascii это 
таблица кодировки символов литер букв. где еще.. еслы мы говорим о кодировке значит
должен быть где то в компе поток либо битов которые надо перевести в литеры на бумаге или экране
либо поток чегото еще (например кодов нажатия кнопок на клаве) в поток битов. когда мы говорим
 о кодировке значит сейчас у нас есть поток литер букв которые нужно перевести в поток битов
 либо у нас есть поток битов которые нужно перевести в литеры.  там где кодировка там обязательно 
 должен быть поток либо либо битов либо литер. потому что кодировка это трансляция букв в биты 
 либо в битов в буквы. это процесс. как горение. а как же клава. при чем здесь кодировка.
 мы нажали кнопку и программа обработчик берет код нажатия кнопки  и понимает что это литера 
 "A" и ей нужно преобразовать эту литеру этот код нажатия в биты в битовый вид согласно 
 правилу преобразования на основе кодировки и эти биты куда то потом отправить какому то получаетелю. если говорим о кодировке значит есть отправитель и получаетель.  вцелом получается
 кодировка позволяет взять литеры на одном конце преобразовать их в биты и послать на другой конец.
 или наоборот. взять биты на одном конце полать в другой и там их преобразовать в литеры.

 кодировка это получаеся всегда два процесса преобразование и передача.

а то получается непонятно было . кодировка это про буквы и биты. а где эти буквы где 
эти биты. все висело в воздухе.

для компа в самом простом случае если мы говорим кодировка то вспоминаем файл внутри которого
лежат биты и байты и кодировка нам позволяет получить из битов и байтов литеры буквы текст.
позоволяет полученные нули и единицы преобразовать в понятный текст. на заре интернета часто 
при заходе на страницу надо было выбирать кодировку для страницы. потому что - страница имеет 
вид битов и байтов и типа тогда почемуто браузер непонимал какой кодировкой закодирован текст
внутри этих битов и байтов.  получаетс в одном из случаев если мы получили некий кусок байтов
некий набор байтов то применив на них кодировку мы можем получить набор букв. текст. превратить
набор битов и байтов в текст. 

получается когда слышим слова кодировка то на ум должно прходить сразу два ее спутника : некий поток набор байтов и  набор текста.  кодировка позволяет превратить одно в другое.
если мы хотим бумажный текст на листе засунуть в комп то нужно составить массив байтов согласно 
кодировке и поместить в комп. если у нас на компе есть файл из байтов а мы хотим его прочитать 
то нужно на читать массив байт за байтом искать в таблице кодировки за какую литеру отвечает 
текущий байт и писать на бумажку эту литеру. таким макаром мы из байтов получим текст

в чем прикол кодировки. это как слово горение. оно лишь часть картины. горение это 
процесс. значит если есть горение значит есть дрова и продукты отходов горения и  продукт полезный от горения. и это надо все вместе значть понимать и рассматривать. это как полет.
полет это процесс. есть обьект полета - самолет. есть куда он летит откуда. и другие атрибуты полета они все неотделимы от полета. полет лишь часть картинки. надо всю картинку представлять.

тоже самое про кодировку. есть обьекты кодирровки - это литеры и биты. есть процесс пееревода 
одно в другое. есть носители обьектов - бумага, жесткий диск. так что  одного слова кодировка
мало чтобы оно говорило обо всей картине в целом.  а важна именно картинка  в целом.

если мы скажем полет. то ничего непоняно. что летит. куда летит откуда летит. как быстро летит.
итд итп.  слово полет это лишь часть картины. это процесс. но одного слова полет мало чтобы 
было все понятно.


так и кодировка. что кодиируется . во что кодируется. что является носителем источника
и результата. 

согласно man ascii кодируются литеры  ( а что такое литеры в целом говоря ?) в байтовый вид.
но этого мало.  что является носителем литер. кто делает преобразование. что является носиелем
байтов. 

ясно одно если мы слышим слово кодировка ascii то значит где то там мы будем иметь дело с литерами
и с байтами  и преобразовывать одно в другое.

это как слово костер. значит гдето там мы будем иметь дрова и высокую температуру. 
неотьемилимые свойства костра.

так и с кодировкой. буквы и байты это ее неотьемлимный свойства

=======

OCT, HEX, DEC

вопрос захера нам oct вид в компьютерах. заодно и hex.
где это все полезно на практике?



в файле все записано битам 01010101010101101010101010
но по факту они на диске разбиты на байты

байт байт байт байт байт байт байт байт



распечатаем на экране содержимое файла в виде байтов. причем в трех вариантах.
вначале байт будет представлять в десятичном виде, в шестнадцарритриицном и в восьемирочном.

(dec) 118  97 115 121  97  13  42  42  10 118  97 115 121  97  10
(hex)  76  61  73  79  61  0d  2a  2a  0a  76  61  73  79  61  0a
(oct) 166 141 163 171 141 015 052 052 012 166 141 163 171 141 012


возникает какой из этих видов проше всего позволяет налету глазами конвертировать байт в битовый 
двоичны вид 0101010101 чтоб неприбегать к калькулятору. ответ восьмеричный вид! щас разберем так. поэтому то он и существует!


в восьемричном исчислении знаков восемь 0,1,2,3..7
семерка  в битовом представлении это 7 = 111

получается один символ собой кодирует три бита. 
в самом деле

0 = 000
1 = 001
2 = 010
3 = 011
4 = 100
5 = 101
6 = 110
7 = 111

как видно если у нас есть восьмеричное число на данный момент состоящее из одной цифры
то можно точно сказать что за ней кроется три бита.


поэтому скажем за двухзначным oct числом кроется всегда шесть бит. неболее шести бит

57 = 101 111

проверим (oct) 57 = ( dec (8^1)*5 + (8^0)*7  ) = (dec 47) = 101 111

так вот у oct чисел ест два охуенных свойства.  первое это то что если перевести oct число в битовый вид то число бит будет небольше чем число oct цифр * 3. тоесть

(oct) x = 3 бита
(oct) xу = 6 bit
(oct) xyz = 9 bit итд

пример

(oct) 7 = 111
(oct) 77 =  111 111
(oct) 777 = 111 111 111

7 - это макс число из одной цифры в oct счислении
77 - это макс число из двух цифр в oct счислении
777 - это макс число из трех цифр в oct счислении

поэтому тут я доказал эту щтуку что если у нас есть oct число 3457676 то в битовом виде
оно будет занимать 7цифр * 3бита = 21 бит

и второе суперкрутое свойство это то что для того чтобы перевести oct число в битовый вид
надо взять каждую цифру в числе и представить в битовом виде
тоесть

57 = 5 и 7 
5 это 101
7 это 111

значит 57 = 101 111

и это суперкруто! потому что такое правило совершенно неработает в десятичном представлнии байта.

еще пример

(oct ) 456 
4 это 100
5 это 101
6 это 110

значит (oct) 456 = 100 101 110

это совершенно фантастичкское свойство oct чисел.
ничего подобного у dec чисел абсолютно нет!
чтобы пеервести dec число в битовый вид это нужно поебаться конкретно.

приведу пример
вот битовое представление некоторых dec чисел

dec
 9 								1001
 99 						110 0011
 999 					11 1110 0111
 9999 			   10 0111 0000 1111
 99999 		   1 1000 0110 1001 1111

как видно даже и близко тут неработает правило что битовый вид является суммой
отдельных цифр.

единственное что верно для dec чисел это то что если dec число состоит из n цифр 
то его битовый вид займет не более чем n*4 бит. получается за каждой dec цифрой скрывается 
четыре бита. или до четырех бит точнее. но поскольку инфо на дисках хранися в виде байтов то есть
в виде неменее 8 бит то получается что 

пример есть у нас число 

(dec)  1212338898898234980

оно состоит из 19 цифр. значит оно занимает 19*4 бит = 76 бит
дополняем до полных байтов итого 80 бит или 8 байт.

таким макаром мы умеем представлять сколько бит надо надо на диске в зависимости от того 
сколько цифр в dec числе.  за 1 dec цифрой кроется 4 бита.это хорошо. но как уже сказал dec числа плохи тем что зная цифровой вид dec числа его совершенно непросто переввести в битовый вид.


а вот oct числа переводятся просто прекрасно. за одной oct цифрой кроется 3 бита.
и битовый вид можно получить записав битовый вид отдельных цифр.

как уже говорил вот пример

(oct)  1237701 = 

берем табличку для цифр

0 = 000
1 = 001
2 = 010
3 = 011
4 = 100
5 = 101
6 = 110
7 = 111

получаем
1237701 = 001 010 011 111 111 000 001

проверим на компе 
$ echo "obase=2; ibase=8; 1237701" | bc
1 010 011 111 111 000 001

полное совпадение!

тут сразу скажу про obase и ibase их нужно вставлять в echo именно в порядке что 
obase первый а ibase второй всегда. если наоброт то будет полная хуйня. 
obase это счисление которое хотим получить а ibase исходное.


чтоб было еще лучше видно то что 
цифры в dec числе  никак не вяжутся с ббитами вот картинка



dec
 9 								1001
 99 						110 0011
 999 					11 1110 0111
 9999 			   10 0111 0000 1111
 99999 		   1 1000 0110 1001 1111

в тоже время oct и hex числа отлично переводятся в биты

oct
 7 				111
 77 		111 111
 777 	111 111 111


hex
 F 			 	    1111
 FF			   1111 1111
 FFF 	  1111 1111 1111


это потому что последняя цифра и в oct и в hex выражается через биты у которых 
все единицы


тоесть 7 (oct) = 111
       F (hex) = 1111


hex счисление отличается от oct только тем что hex цифра за собой скрывает четыре бита
а в oct цифра скрывает три бита вот и вся разница.


тоесть если мы видим 

hex ABC4567F 

то мы можем смело понимать что в битах это число занимает n*4 бит = 8 * 4 = 32бита
тоесть каждая цифра в hex отбражает половину байта. таким образом если наше hex число 
можно отбразить в байтовом виде как

(AB)(C4)(56)(7F)

кстати говоря в oct виде байт занимает три цифры.
поскольку байт в dec максимально это 255 посмотрим чему оно равно в oct.
dec 255 = bin 1111 1111 = 11 111 111 = oct 377

правда тут несовем понятно ведь 3 = 011 тоесть 377 = 011 111 111 это 9 бит а не 8 бит
так что неочень понятно в точности как это байт можно в oct представить. мы типа 0 откинули в 
oct 3.

а вот у hex все впорядке одна цифра это 4 бит. так что две цифры это 8 бит или байт.
так что любой байт  в hex это две цифры.

таким образом в целом говоря кода мы смотрим на oct или hex мы сразу понимаем сколько бит
занимает данное цисло и можем достаточно быстро на лету перевести в битовый вид.
ну для hex это сложнее потмоу что надо помнить в битовом виде  16 значений ( для 0, 1, 2,... F)
а вот для oct это существенно проще так как там нужно помнить всего 7 битовых значений
для 0,1, 2, ... 7

причем как уже сказал чтобы перевести из hex,oct в битовый вид надо просто взять 
перевести в биты каждую цифру и просто соединить. эьто очень круто и очень просто дает 
вохможность налету переводить окты и хексы в битовый вид.


возвращаемся к нашему исхоному примеру.
у нас есть файл в котором есть байты. я их 
распечатал в трех разных методах отображения байтов


(dec) 118  97 115 121  97  13  42  42  10 118  97 115 121  97  10
(hex)  76  61  73  79  61  0d  2a  2a  0a  76  61  73  79  61  0a
(oct) 166 141 163 171 141 015 052 052 012 166 141 163 171 141 012

dec вобщн бесполезен для трансформации в 010011 вид
hex сложноват ибо за одним символм кроется аж четыре бита
oct самый простой для визуальной трансформацмии  в 010 вид ибо за одним символом кроется всего три бита.

берем строчку

(oct) 166 141 163 171 141 015 052 052 012 166 141 163 171 141 012

уже находу я могу много ее кусков перевести на лету в битовый вид

(oct) 001хххххх 001ххх001 001хххххх 001111001 001ххх001 000001ххх 000хххххх ...


потмому что в oct виде 1= 001, 7= 111, 0= 000

тоесть даже помня всего две цифры в битовом виде ( один и семь) я уже сразу 
существенно понимаю как в битовом виде выглядят байты в файле. это огонь.

еще раз в чем прикол oct и hex счислений.
инфо на диске ханится в виде байтов.  байт состоит из битов.
в конечно итоге на диске хранятся биты.
0101010100101101010101010101010110101011010101001

тоесть инфн на диске хранися в двоичной системе
отображать инфо в форме битов (двоичная систе) очень громоздко накладно.

далее можно конечно всмопнмить что все таки в прграмму инфо считывается в форме байтов тоесть
в группах по 8 бит. 

далее мы можем отобразить каждый байт в dec системе  тогда соедержимое файла это будуц
цифры то 0 до 255

123 111 000 007  012 017 итп

в чем минус такого отображения что налету из dec числа трудно его преобразить в битовый вид.
а нам порой это хочется. также если у нас есть небайтовый вид а битовый

010101001010101010101010 

то его вобще  в dec отобразить сложно

толидело когда мы юзаем oct

для того чтобы байт отобразить в oct надо просто напросто разбить его на группу по три бита

aabbbccc  правда у нас нехваетает одного бита но вместо него добавляем незначащий 0
тоесть

0aabbccc

а далее мы ищем для каждой группы просто цифру по таблице. пример

11010110 = 11 010 110 = 011 010 110

таблица


0 = 000
1 = 001
2 = 010
3 = 011
4 = 100
5 = 101
6 = 110
7 = 111

получаем

011 010 110 = (oct) 326 

вот так мгновенно и легко мы биты превратили в oct
более того нам похеру сколько у нас бит в потоке. 

берем поток 
0101010110101010110101010110100110

разбиваем его на группы по три. применяем таблицу и получаем это число  в oct виде.

тоже самое для hex счисления. только там группы по 4 бита и таблица на 16 символов.

таким образом бинанрный вид биты можно буквально на лету трансофирировать в oct поток
ну и если чуть более постараться то и в hex поток.


верна и обратная задача . если есть oct число  то его супер легко трансформироваь в битовый вид

пример

(oct) 24577453562 = 010 100 101 111 111 100 101 011 101 110 010

все по тойже таблице
 

таблица


0 = 000
1 = 001
2 = 010
3 = 011
4 = 100
5 = 101
6 = 110
7 = 111

просто берем цифру и ее превращаем в три бита.

это все просто супер. и в тоже время для dec преобразований вида dec -> bin , bin -> dec
ничего подобного неработает. в этом и фишка hex и oct счислений.

в следущей части я разобрал почему 377 (oct) в bin виде имеет четко 8 бит а не 9бит
как это нам казалось.
============
bit, byte
octal 
bin

прежде всего байт это группа битов в размере 8 штук. (как кучка семечек которых 8 штук)
а что такое бит. бит это некая хрень которая может принимать два значения. условно
 говоря либо значение "a" либо значение "b". бит это чисто математическое понятие
 тоесть абстрактное. физическим носителем бита может быть что угодно. например кружка воды.
 она может быть полная или пустая. далее нужна некая система которая будет считывать 
 физический носитель бита и выдаваь нам такое: бит (состояние ) = полный\пустой. 
 или бит(состояние) = жив\мертв
 далее нам же похеру каким физ свойством закодировано состояние бита нам главное 
 понимать суть состояния. поэтому далее некая машина нам преобразует физ свойство в
 мат величину. например. полный = a, пусто = b
 получаем 
 bit = a
 или
 bit = b

 далее вспоминаем что в байте битов у нас 8 
 значит мы имеем группу сущностей вида abbababa
 вместо a и b можно заюзать 0 и 1
 получаем 

 байт это множество вида 0100110
 причем байт это непросто множество битов а оно еще и упорядоченное то есть
 у битов есть порядковые номер тоесть биты нельзя переставлять местами

 ну хорошо у нас куча байтов

 01010101
 01010110
 01001010
 01010101

 тоесть некая система нам генерирует эти байты.

 далее можно этому обьекту сопоставить число. потому что по случайности 
 ровно такой же вид имеют числа в бинарной записи

 таким образом у нас абстрактный мат обьект как бы превратился в число.
 ну или мы можем всяегда для байта найти однозначное сооовествие в форме числа которое 
 имеет такой же вид в бинарной системе счисления.

дален интересно. если у нас есть bin число то мы всегда можем его 
преобразовать по алгоритму в dec число

01001011 =  (2^7)*0+ (2^6)*1+ (2^5)*0+ (2^4)*0+ (2^3)*1+ (2^2)*0+ (2^1)*1+ (2^0)*1= 75 (dec)  

(oct)64*x+8*y+1*z=(dec) 75
x=1

8*y+z=11
y=1

z=3

113 (oct) = (dec) 75
из верхнего мы уже знаем что (dec) 75 = (bin) 01001011

полуаетчся что 113(oct) = (bin) 01001011
далее мы берем цифры 113 и смотрим что
1(oct) = (bin) 001
3(oct) = (bin) 011

и мы замечаем что 113 (oct) = 010 010 011 где каждые три бита представляют собой его же цифры.
010 = 1(oct)
010 = 1(oct)
011 = 3(oct)

берем 255 dec

255(dec) = 11111111 
255(dec) = 64*x+8*y+z
x=3
8*y+z=63
y=7
z=7

значит

255(dec) = 377(oct)

значит 377(oct) = 255(dec) =  11111111
значит
377(oct) = 1111 1111

теперь посмотрим а какой бинарный вид имеют цифры в числе 377(oct)

3(oct) = 3(dec)=(bin) 011
7(oct) = 7(dec) = 111

выпишем рядышком 3(oct)7(oct)7(oct)= 011 111 111 = 0 1111 1111

сравниваем 377(oct) =   1111 1111
и 3(oct)7(oct)7(oct)= 0 1111 1111

видно что из второго числа можно получть первое если отбросить лидирующий ноль.

получаем такое правило: если мы хотим преобразоать oct в bin то прямой способ точный 
это преобразовать oct в dec а уже dec в bin. но есть быстрый способ - берем каждую цифру 
в oct числе. преботазауем ее в bin. потом записываем вместе все биты и отрасываем головной бит "0"

посмотрим а может ли быть такой вариант что головной бит будет "1"
мин число в бинарном было бы вида 100 000 000 = 1 0000 0000 = 1 0000000 . тоесть это число
уже больше чем 1 байт. 
также посмотрим из каких oct цифр 100 000 000 оно бы состояло = 4(oct)0(oct)0(oct)
(oct) 400 > (oct) 377
тоесть если мы говорим о преобразовании oct чисел не превышающих байт то 
вышеописанное правило точно работает.

прикольно что правило oct -> dec -> bin это реальное правило преобразования
а правило (oct) abc -> a(bin)b(bin)c(bin) минус лидирующий бит "0" это некая удачная эмпирическая формула. поэтому то мы ноль и убираем чтобы подогнать эмпирическую формулу под строгий мат результат. поэтому то 377 (oct) и занимает 8 бит а никак не 9 бит. 9бит это чисто погрешность
эмпирической формулы.

еще раз почеркну что полноцеееный способ перевода октал в бин это октал перевести в дек
а дек перевестив бин. а способ кодга мы еберем цифры в окт и каждую раскладываем в три бита
а потом собираем вместе это эмпирическая формула поэтому то она в случае с (oct)377 и дает
девять бит. тоесть эмпирическуб формулу надо использовать с поправкой в итоге. никаких девяти
бит там нет. также стоит заметиь что для бин числа что  11 что 00000000011 это одно и тоже 
поэтому вобщем и целом эмпирическя формула дает верный результат даже без поправки

============
откуда 
скачал intel книжки по cpu 

https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html


============
octal 2 byte

если записать в файл символы "1 " (один  и пробел)

$ echo -n "1 " > ./text1

далее мы читаем этот файл через od в режиме oct и в режиме когда поток байтов в файле
разбивается на биты и далее обьединяется в группы размером 1 байт

$ od -t o1 ./text1 
0000000 061 040

0000000 это просто адрес офсета смещения относительно начала файла.
(oct) 061 отвечает за "1"
(oct) 040 отвечает за " "

пока все нормально все ожидаемо.

далее мы читаем этот же файл в режиме когда считыаемый поток разбивается на биты а потом
эти биты обьединяется в группу по 2 байта.

$ od -t o2 ./text1
0000000 020061


 пеерведем байты в биты

(oct) 061  = 00 110 001 = 00110001  
(oct) 040  = 00 100 000 = 00100000  

(oct) 020061 = 0 010 000 000 110 001 = 00100000 00110001

ожидалось что мы будем иметь 
00110001 00100000 = 0 011 000 100 100 000  ( oct 030440 )


а программа показала нам что на диске
00100000 00110001 = 0 010 000 000 110 001  ( oct 020061 )

тоесть байты поменяны местами.


на самом деле вот в чем дело. на диске биты байтов записаны в порядке
little endian. это значит что первым на диск пишется LSB (least siginificant bit тот который
2^0) а последним пишется MSB (most significant bit тот который 2^7)

берем (oct) 061 = 0011 0001 он будет записан на диске вот в таком виде 
(начало файла) --> 1000 1100

после него (oct) 040  = 0010 0000 будет записан на диске в виде
0000 0100

и таким образом оба байта будут записаны на диске виде битовой последовательности
(начало файла) --> 1000 1100  0000 0100 --> (конец файла)

при этом еще раз бит с 2^0 он находится несправа как мы привыкли а слева в каждом байте

еще раз битовый вид как мы ожидали

00110001 00100000

а вот как по факту 

10001100 00000100

как видно сами байты расположены в томже порядке. но каждй байт зеркально отражен 
относительно своей середины. потому что (еще раз) комп пишет биты байта в файл начиная с младшего
бита а не старшего. потому что intel цпу они хранят байты в оперативке а значит и на диске
согласно "little endian" правила. мы же обычно на бумаге записывем биты в порядке "big endian" 
чего нет на цпу интел. точнее в регистрах цпу интел тоже биты хранятся в режиме big endian но в
оперативке и походу значит на диске биты хранятся в порядке little endian.

еще раз покажу где находится 2^0 бит в файле как мы полагали 

    (начало файла)-->  (2^7)00110001(2^0)    (2^7)00100000(2^0)

а вот как по факту они записаны в файле

    (начало файла)--> (2^0)10001100(2^7)     (2^0)00000100(2^7)



собственно далее как происходит расшифровка битов в случае файла с big endian.

есть поток битов
01010101 10101010 1011010 1010101...

мы говорим что мы хотим его расммотреть в разрезе скажем по три байта. выбираем первые 
три байта
01010101 10101010 10110101

если кернел знает что биты записаны как big-endian то комп их расшифроваывает вот так:
(2^23)--> 01010101 10101010 10110101 <--(2^0)

если же кернел знает  что биты записаны как little endian то он эти же биты расшифроывает как
(2^0)--> 01010101 10101010 10110101 <--(2^23)


покажу на примере как выглядит разбивка потока битов в файле скажем когда мы хотим 
посмотреть разбивку по 2 байта

(2^0)01010101 01010101(2^15) (2^0)010101010 00101010(2^15) (2^0)10101010 10101010(2^15) (2^0)10101010 10101010(2^15) (2^0)10101010 10101010(2^15) (2^0)10101.. 


получаетс чтобы правильно понимать как записаны биты байтов в файл надо знать какой endian
юзатеся на компе. тоже самое для обратной задачи. если мы читаем биты с файла то чтобы их 
интерпретровать в байты надо знать как endian юзается на компе потмоу что одна и таже последова
тельность битов можно интерптретировать двояко


(2^0)01010101 01010101(2^15)

либо

(2^15)01010101 01010101(2^0)

 биты одни и теже. а числа будут разныве в итоге.




еще раз возьмем исходный пример

(oct) 061  = 00 110 001 = 00110001  
(oct) 040  = 00 100 000 = 00100000

пишем 061 на диск. пишем как bigendian. первым пишем старший бит в байте.
00110001

далее пишем 040 получаем 

00110001 00100000

далее читаем биты и  интеретируем помня что это bigendian

(2^15)00110001 00100000(2^0) = 0 011 000 100 100 000 = (oct) 030440


теперь пишем на диск тот же 061 и 040 но используем little endian тоесть первым на диск
идет младщий бит байта

10001100 00000100

читаем эти биты помня что это litle endian.

(2^0)10001100 00000100(2^15)

переписываем эти биты в стандарной бинарной нотации

00100000 00110001 =  0 010 000 000 110 001 = (oct) 020061


теперь понятно почему мы получили 020061

$ od -t o2 ./text1
0000000 020061



далее. запишем в файл "1 1"
$ echo -n "1 1" > ./text1

посмотрим побайтно
$ od -t o1 ./text1 
0000000 061 040 061

попробуем предсказать как будеь выглядеть ответ если мы посмотрим потрехбайтно.


(oct) 061  = 00 110 001 = 00110001  
(oct) 040  = 00 100 000 = 00100000

на диске "1 1" в битовом виде с учетом того что комп их туда пихает в виде little endian
будет вылядеть как

10001100 00000100 10001100

далее мы вспоминаем каков порядок битов в little endian

(2^0)10001100 00000100 10001100(2^23)

перепишем биты в стандартной нотации бинарной

00110001 00100000  00110001 = 001 100 010 010 000 000 110 001 = (oct) 14220061

проверяем на компе

$ od -t o3 ./text1
od: invalid type string ‘o3’;
this system doesn't provide a 3-byte integral type

хахаха! od неподдерживает трехбайтовый вид.

прикольно что и hex трехбайтовый неподдерживается утилитой od
$ od -t x3 ./text1 
od: invalid type string ‘x3’;
this system doesn't provide a 3-byte integral type








endiness зависит от типа цпу.

little ending
big ending

процесс когда хочет писать в память он переклчается в режим ядра  а если нет как же 
утвержлоение что доступ к железу от процесса идет тлоько через ядро?

paging - разееление памяти на 4к.  а как тогда обратится к байту отедьнуму?

paging vs segmantaion ?

кто пишет данные в tlb ? сам цпу или кернел участвует

как цпу знает где лежит page table


mmu и os. как os управляет mmu


пока что я понял вот что. кернел для процесса в памяти создает page table в которой записан 
маппинг между вирт адресации памяти ( с которой оперирует программа в юзер спейсе) и физ памятью.
между цпу и шиной памяти стоит mmu железка. цпу всегда оперирует внутри себя тоесть знает
только вирт адресное пространство. как он он физ адресацию незнает невидит не вкурсе.
цпу обращается к вирт адресу. запрос от цпу приходит на mmu. как я понял в регистр CR3 цпу кернел
записывает адрес в физ памяти с которого начинается page table для текущего процесса. так что mmu читая тот регистр знает где ей в физ памяти искать page table. таким образом mmu для вирт адреса
в оперативке в page tabe находит физ адрес. этот адрес mmu и выставляет на шину адреса памяти
для запроса. 

таким образом работа кернела в том чтобы в памяти для процесса создать page table
заполнить ее а в цпу в CR3 регистр пихнуть адрес page table. после этого работа ядра по настройке
mmu закончена. а mmu имеет все для того чттбы сделать свою работу -> преборазовать вирт адрес
в физ адрес.

context switch. это коогда мы из цпу выдавливаем все инфо от одного процесса
и загружаем в него инфо от друого процесса. как я понял это на данный момент истории происходит
так - шедулер ядра копирует значения регистров цпу в память в область ядра. в ту структуру 
в памяти которая описывает процесс. далее шедулер загружает в цпу в регистры значения регистров
того процесса который будет следущим запущен на цпу. когда все регистры загружены то 
шедулер копирует как я помнимаю в спец регистр адрес начала кода нового процесса на который
мы будем пеереклчаться. и далее как я понимаю происходит (неким образом)возможно чеерз int 
возврат цпу из режим ринг 0 в ринг 3. и процессор получетя имеет все регистры с данными от процесса
и также начинает иполнять ту команду ссылка на которую у него хранится тоже в спец регистре.
так что вуаля - новый процесс начнет выполняться в тайм слайсе. фишка главная в том что context
switch это не какая то там особая секретаня загадочная процедура а просто тупо баналоьно упрощенно
говоря шедулер(кусок кода ядра) копирует регистры от процесса в память в таблицу информации
 о процессе или наоборот из памяти в цпу.  руками как говорится. есть еще и процесс
 когда сохраннеиео инфо о регистрах можно делать неруками и типа одной командой цпу (тоесть hardware context switch) но типа он якобы менее гибкий ибо сохраняет все регистры а это может 
 быть и ненадо и типа и медленней. ведь каждый регистр сохранить это обращение к памяти поэтому 
 ручное сохранение происходит  типа в кончено итоге быстрее.













===========
утилита od

предыдущая тема теерь здорово поможет при расммотрении утилиты od
она позволяет выводит на экран содержимое файла в виде байтов . причем 
моэно выбирать в каком счислении эти байты будут отображены. ведь байт это просто число.
вопрос только в том   в какой системе счисения мы это число выводим. dec, hex, oct

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


=============
???????????????????????
понять как поменят кнопку и литеру в которую она резолвится. key remapping.
приколльно есть scan коды клавы, есть ascii, есть escape коды терминала
====
linux locale. связь с ascii

====
ascii
escape симолв и терминал escape коды и ASCII взаимосвязь ?






