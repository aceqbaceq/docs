nmcli-NetworkManager-Networkd-netplan

есть целый рой таких херней как NetworkManager, Networkd(он же
systemd-networkd), netplan
в чем разница. и все такое.


значит netplan это фронтенд штука по управлению сетью в линуксе.
а бекендом для него может быть либо NetworkManager либо networkd
(еще есть херня netctl но ее я не стал разбирать, походу это типа еще один нетворк
менеджер. тоесть получаеся их как минмиум три: NetworkManager, systemd-networkd и 
netctl)

вот в чем разгадка.

networkd как пишут больше подходит для настроек карточек для серверов.
там где больше статического в конифгах картчоки.

networkmanager болше подходит лэптопов где больше динамического в 
настройке карточки.

как я понял запускается netplan и он уже обращается к networkd либо к networkmanager
передает им конфиги и они уже руками делают настройки карточек.

если нет нетплана то networkd свои нативные конфиги  ищет 
в /etc/systemd/network
где лежат нативные конфиги networkmanager я не знаю

как проверить что нетплан есть на компе
$ sudo dpkg -l | grep -i netplan | awk '{print $2}'
libnetplan0:amd64
netplan.io


как понять нетплан что использует для бекенда networkd или networkmanager
идем в /etc/netplan/*.yaml
и если там строка 
	renderer: networkd
значит бекендом явлется networkd
а если там строка 
	renderer: NetworkManager
значит бкендом явлется networkmanager


у меня на компе это выглядит так
$ cat /etc/netplan/01-network-manager-all.yaml  | grep -v "#"
network:
  version: 2
  renderer: NetworkManager

очевидно что у меня нетплан использует на бекенде Networkmanager



далее. нетплан он может использовать только один бекенд. чтото одно.
 либо networkd
либо networkmanager но не оба сразу.

насколкьо я понял если в конфиге нетплан нет строки "renderer:"
значит бекендом явялется networkd
например если поставить lxd виртуалку с ubuntu то 
там у нетплана будет вот такой конфиг

# cat /etc/netplan/50-cloud-init.yaml | grep -v '#'
network:
    version: 2
    ethernets:
        enp5s0:
            dhcp4: true

из чего я делаю вывод что нетплан использует на бекенде networkd



далее и networkd и networkmanger это юниты (сервисы)  в systemd

# systemctl list-units | grep networkd | awk '{print $1}'
networkd-dispatcher.service
systemd-networkd-wait-online.service
systemd-networkd.service                                                                     


# systemctl list-units | grep -i networkmanager | awk '{print $1}'
NetworkManager.service


на моем компе networkd отключен, работает только NetworkManager
$ systemctl list-units --type=service --all | grep -E "NetworkM|networkd"
NetworkManager.service     loaded    active    running  Network Manager                                                                 
systemd-networkd.service   loaded    inactive  dead     Network Service      

а на виртуалке lxd ubuntu имеем такое

# systemctl list-units  --type=service | grep  -E "NetworkM|networkd"
  networkd-dispatcher.service                    loaded active running Dispatcher daemon for systemd-networkd
  systemd-networkd-wait-online.service           loaded active exited  Wait for Network to be Configured
  systemd-networkd.service                       loaded active running Network Configuration

тоесть  NetworkManager его нет а есть юниты networkd


также интересно то что NetworkManager ставится из пакета network-manager
# apt-get install NetworkManager
а вот networkd (он же systemd-networkd) как я понимаю он не имеет
отдельного пакета а входит в пакет systemd


далее прикол в тмо что обе эти системы управления сетвыми картчоками 
они могут одновременно работать на компе. 
главое чтобы они управлялли разными картчоками. 
и тогда не буде никакого конфликта

далее
как по быстрому узнать какие сет карточки управляются networkd

# networkctl
IDX LINK             TYPE               OPERATIONAL SETUP     
  1 lo               loopback           routable    configured
  2 enp3s0           ether              off         unmanaged 
  3 wlp2s0           wlan               routable    unmanaged 
  4 virbr0           ether              no-carrier  unmanaged 
  5 virbr0-nic       ether              off         unmanaged 
  7 docker_gwbridge  ether              routable    unmanaged 
  8 docker0          ether              routable    unmanaged 
641 vethfcf5465      ether              degraded    unmanaged 
1708 vboxnet0         ether              off         unmanaged 
1709 vboxnet1         ether              off         unmanaged 
1710 vboxnet2         ether              off         unmanaged 
1711 vboxnet3         ether              off         unmanaged 
1712 vboxnet4         ether              off         unmanaged 
1713 vboxnet5         ether              off         unmanaged 
1714 vboxnet6         ether              off         unmanaged 
1715 vboxnet7         ether              no-carrier  unmanaged 
264091 veth81ffe49      ether              degraded    unmanaged 
465285 br-f3471869c7a4  ether              routable    unmanaged 
465313 veth31c3f0d      ether              degraded    unmanaged 
465329 veth9d07c3b      ether              degraded    unmanaged 
465335 veth7eb6575      ether              degraded    unmanaged 
465343 vethbdadd5b      ether              degraded    unmanaged 
465345 veth4452000      ether              degraded    unmanaged 
465351 veth730a10d      ether              degraded    unmanaged 
465374 lxdbr0           ether              no-carrier  unmanaged 
465375 lxdbr1           ether              no-carrier  unmanaged 


соовтсвтенно все картчоки которые "unmanaged" значит ими
 networkd неуправляет
а вот посмотрим что при этоп происано в нетплан

# cat /etc/netplan/01-network-manager-all.yaml 
network:
   version: 2
   renderer: networkd
   ethernets:
      lo:
        match:
           name: lo
        addresses: [ 172.16.10.10/32 ]

видим полное совпдаение. в нетплане указан remderer:networkd
только для карточки lo.
а все остальыне картчоки управлятся получается через networkmanger.
тоесть все совпало.

вот по идее конфиг нетплана когда одна карточка управляется
через нетворкманагер а вторая через нетворкд. (но я не проверял
рабочий ли этот конфиг)

network:
  version: 2
  renderer: NetworkManager
  ethernets:
      enp3s0:
        optional: true

network:
   version: 2
   renderer: networkd
   ethernets:
      lo:
        match:
           name: lo
        addresses: [ 172.16.10.10/32 ]


вот пример с моего компа
$ networkctl 
WARNING: systemd-networkd is not running, output will be incomplete.

IDX LINK             TYPE               OPERATIONAL SETUP     
  1 lo               loopback           n/a         unmanaged 
  2 enp3s0           ether              n/a         unmanaged 
  3 wlp2s0           wlan               n/a         unmanaged 
  4 virbr0           ether              n/a         unmanaged 

вверху даже висит предупреждение что systemd-networkd просто 
напросто нахрен даже не работает



вот другой пример

# networkctl
IDX LINK   TYPE     OPERATIONAL SETUP     
  1 lo     loopback carrier     unmanaged
  2 enp5s0 ether    routable    configured


втдим что enp5s0 упрвлятеся через netowrkd
посмотрим что при этом написано в нетплан

# cat /etc/netplan/50-cloud-init.yaml 
network:
    version: 2
    ethernets:
        enp5s0:
            dhcp4: true

видим что тут нет строкаи "renderer" что как я уже сказал значит 
что нетплан  исползует networkd.
тоесть все совпало.


а как узнать какие картчоки управляются через netoworkmanager? 
я думаю метдом исключения - узнаем через networkctl какие картчоки 
управятся через networkd а остальные карточки оставшися управлятся 
через netoworkmanger. а полный список картчоек можно помотреть 
через ip -c a

хотя наверное есть способ узнат напрямую какие картчоки управляются 
через networkmanger:
# nmcli   device
DEVICE           TYPE      STATE      CONNECTION 
wlp2s0           wifi      connected  bbb12       
br-f3471869c7a4  bridge    unmanaged  --         
docker0          bridge    unmanaged  --         
docker_gwbridge  bridge    unmanaged  --         
lxdbr0           bridge    unmanaged  --         
lxdbr1           bridge    unmanaged  --         
virbr0           bridge    unmanaged  --         
enp3s0           ethernet  unmanaged  --         
vboxnet0         ethernet  unmanaged  --                 
lo               loopback  unmanaged  --         
tapd7309131      tun       unmanaged  --         
virbr0-nic       tun       unmanaged  --         

соотвевтенно в колонке STATE указано какие сет карточки уравляются 
через netowwork manager, в колонке DEVICE указана карточка
(nmcli это утилита NetworkManager, nmcli это переводится
как NetworkManager CLI)

а вот как это выглядит на моем компе
$ nmcli device | awk '{print $1 "\t\t\t\t "  $3}'
DEVICE               STATE
wlp2s0               connected
br-1bdf05b37fcc      connected
docker0              connected
lxdbr1               connected
virbr0               connected
enp3s0               unavailable
lo                   unmanaged
virbr0-nic           unmanaged
wg0                  unmanaged

что странно. показано что часть карточек обслужиывается не 
NetworkManager. а кем тогда? если нетворкд вообще остановлен на компе




и вот еще полезная команда. котоаря покажет нетолько сет карточки фищические
упраляемые через networkmanger но и сет конекты (например VPN коннекты) котоыре 
уарвляются через networkmanger

# nmcli   connection
NAME                        UUID                                  TYPE  DEVICE 
bbb1                         ec06da1e-7503-4aa1-a96d-8d6e5da05667 wifi  wlp2s0 
cartos-ovh                  28302ee0-1ac0-4861-827b-ad56899dbcb8  vpn   wlp2s0 


вот видно что есть два конекта котоыре урвляются через networkmannger 
оба проходят через сет карточку wlp2s0





есть еще одна утилита настройки карточек чеерз networkmanger это "nmtui"
она работает в виде псевдографического интерфейса.


немного отойдем в сторону и поговрим про 
dnsmasq
systemd-resolvd


есть две программы похожие по целям. и обе хер понятные

# netstat -tnlp | grep -E  "dnsmasq|reso"
127.0.0.53:53           0.0.0.0:*               LISTEN      949822/systemd-reso 
10.113.151.1:53         0.0.0.0:*               LISTEN      627658/dnsmasq      
10.78.50.1:53           0.0.0.0:*               LISTEN      627605/dnsmasq      
192.168.122.1:53        0.0.0.0:*               LISTEN      2085/dnsmasq        

из чего видно что systemd-reolvd отвечает в убунту 18 за резолвинг.

разберем о чем адреса где dnsmasq

10.113.151.1 это адрес lxdbr1 это адрес lxd шлюза

# ip -c a | grep 10.113.151.1 -B3
       valid_lft forever preferred_lft forever
465375: lxdbr1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 00:16:3e:1b:1e:55 brd ff:ff:ff:ff:ff:ff
    inet 10.113.151.1/24 scope global lxdbr1

10.78.50.1 это тоже шлюз в lxd сеть
# ip -c a | grep 10.78.50.1 -B2
465374: lxdbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 00:16:3e:bb:37:ee brd ff:ff:ff:ff:ff:ff
    inet 10.78.50.1/24 scope global lxdbr0

192.168.122.1 это сетвая карта от виртуалбокс
# ip -c a | grep  192.168.122.1  -B2
4: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:a5:01:fd brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0



возврашаемся обратно.
вовзращаесся к networkmanger
это для лэпотопа та самая програмаа для настрйоки VPN.
если у нас есть граф интерфейс на лэпотпе то мы небудем настраивать networkmanger
через netplan. мы будем его настрваить через граф интерфейс.

чтоб можно было это делать надо поставит ряд пакетов


# dpkg -l | grep -i network-manager | awk '{print $2}'
network-manager
network-manager-config-connectivity-ubuntu
network-manager-gnome
network-manager-l2tp
network-manager-l2tp-gnome
network-manager-openconnect
network-manager-openconnect-gnome
network-manager-openvpn
network-manager-openvpn-gnome
network-manager-pptp
network-manager-pptp-gnome



что классно . это то что если у нас есть текст файл апример от OpenVPN конекта
то мы можем его через граф интерйфейс просто импортировать и руками не настрваить конект
это оень круто. и тут важно сказать что чтобы networkmanger понял о каком впн идет 
речь в конфиг файле котоырй мы импортруем надо  ОБЯЗАТЕЛЬНО чтобы текст конфиг имел
на конец расширение *.ovpn иначе при имопрте граф утилита напишет ошибка мол не могу 
испортирвать впн. а делов всего навсего в том что файл не имеет расширения.

далее важно - по дефолту опенвпн конект если мы его активиовали то он весь трафик
погонит через впн тоесть и тот трафик котоырй ютуб и трафик который в сервреам в локалке
за впн. при том что впн клиент засосет роуты локалок за впн автоматом и добавить 
их в таблицу маршруутизации на лиукнсе. проблема тут в том что при активации впн
конекта у нас создается +1 маршрут по дефолуту котоырй веет в впн сеть и он имеет 
очень неблольшую мертрику скажем 50 . а маршрут по дефолту который был на компе он имеет 
обычно метрику скажем 600. поэтому несмотяря на то что впн клиент добавил ротуты локалок
за впн и по идее конект к компам которые не лежат  в сетиях за впн не дожлен идти 
через впн но это просходит из за выскойо метрики  в маршуртует по дефолту у компа который 
был до запуска внп - ведь он никуда не исчезает. так вот всего навсего чтоы этот кошмар
исправти надо в маршурет по дфету поставить низку метрку. показываю на примере


вот какая табиоца маршуртиацзиии до конекта к впн

# ip  route sh | grep wlp2s0
default via 192.168.0.1 dev wlp2s0 proto dhcp metric 600 
169.254.0.0/16 dev wlp2s0 scope link metric 1000 
192.168.0.0/24 dev wlp2s0 proto kernel scope link src 192.168.0.104 metric 10 

видно что 
у нас сетвая картчока wlp2s0
и метрика маршурта по дефолту равна 600
пока все окей.

запускаем впн

# ip  route sh | grep -E "wlp2s0|tun0"
default via 192.168.0.1 dev wlp2s0 proto dhcp metric 600

default dev tun0 proto static scope link metric 50 
10.0.0.0/16 via 10.200.201.1 dev tun0 proto static metric 50 
10.90.0.0/24 via 10.200.201.1 dev tun0 proto static metric 50 
10.96.0.0/22 via 10.200.201.1 dev tun0 proto static metric 50 
10.100.0.0/16 via 10.200.201.1 dev tun0 proto static metric 50 
10.101.0.0/24 via 10.200.201.1 dev tun0 proto static metric 50 
10.200.201.0/24 dev tun0 proto kernel scope link src 10.200.201.49 metric 50 

157.90.147.104 via 192.168.0.1 dev wlp2s0 proto static metric 10 
169.254.0.0/16 dev wlp2s0 scope link metric 1000 
192.168.0.0/24 dev wlp2s0 proto kernel scope link src 192.168.0.104 metric 10 
192.168.0.1 dev wlp2s0 proto static scope link metric 10 

мы видим что здесь добавились маршуруты через tun0 интерфейс.
и то что добавился вторйо маршурут по умолчанию

default via 192.168.0.1 dev wlp2s0 proto dhcp metric 600
default dev tun0 proto static scope link metric 50 

и что мы видим что с однйо стооны у нас впн клиент нам добавил правла 
для локалных сетей котоыре лежат за tun0. и это хорошо. потому что нам надо как раз таки
конктится к серверам за впн через tuno0 тоесть собвственно через впн конект.
но! у нас два маршурута по умоалчанию и будет исползован ровно тот маршуррут 
у котороо метрика меньше. в даннмо сулчае это маргуршрут с метрикой 50 тоесь
через tun0 тоесть через впн. а вот это уже жопа. нам ненадо чтобы пакеты к ютубу
летели через впн сеть. 

выход такой - надо у маршурута по умолчанию исходного уменьшить метрику с 600 на 10.
как это сделать а вот так


# nmcli connection modify <connection-name> ipv4.route-metric 10

как узнать <coonection-name> 
а вот так

# nmcli connection
NAME                        UUID                                  TYPE  DEVICE 
bsh                         ec06da1e-7503-4aa1-a96d-8d6e5da05667  wifi  wlp2s0 

тогда

# nmcli connection modify bsh ipv4.route-metric 10

кстати возоможно еще надо пеерзаустить службу (но это неточно)
# systemctl restart NetworkManager

и тогда у нас будет в итоге вот такая картина

# ip  route sh | grep -E "wlp2s0|tun0" | grep default
default via 192.168.0.1 dev wlp2s0 proto dhcp metric 10 
default dev tun0 proto static scope link metric 50 

красота! все пакеты для которых нет явного маршурурта будут идти через
обычный гейтвей. 
а все пакеты  для которых есть явыне маргрурты в таблице будут идти через впн
конект через tun0


олчно!


слкдущий момент - положим у нас в впн сети есть dns сервер чтобы резолвить компы сидящие
за впн по их днс именам. тогда в граф утилите к networkanger надо снять галочку
что DNS автоматом и руками прописать адрес dns сервера сидящего в впн сети. тогда 
при попытке пинга и нейм резолвинга видим будет делться попытка сделать резовлинг 
через исозный днс и будем получени отлуп скажем от 8ю8ю8ю8 и потом будет делться 
попытка сделать резовлинг чеерз dns от впн. и резолвинг будет успеным. я так понимаю 
этоработает.

далее. вот зариосвка
как настривать wifi карточку не через netowkmanger его граф утиллиту
а чрезе netplan (коотоырй уже сам будет что нужно посдсывать это к netowek manger)

network:
  version: 2
  renderer: networkd
  wifis:
    wlo1:
      dhcp4: true
      dhcp6: true
      access-points:
        "network_ssid_name":
          password: "**********"

  я не провеляер работае т ли этот пример.

 также непнятно в каких файлах в итоге харнятся конфииги netowrk mamger

 далее. что еще прикольно. это то что можно запускать 
 сразу много тоннелей впн (конектов) и они друг дргуу не мешают.
 это очень удобно. ненадо постоянно выключать один чтобы выклюить дргой!
 
 далее . есть однахрень.
 если мы заустилли через граф утилиту netowrkanger несколько впн конектов
 то даже если перевести полузнок впн влево(off) то как нис транно часть впн
 конектов будет оставнлена .а часть не т.дебилиизм.поэтому
 остальые впн конеткты надо стопить руками.  а именно

 # nmcli connect
 ...
 ,
 ,
 смотрим как нызваеся наш впн конект уотоырй мы хотми погасить и гасим его руками

 # nmcli connect  down cartos-htz

 
далее я столкнулся вот с чем.
я выше писал что при запуске впн конекта у нас образуется +1 default
маршрут. из за этого надо четко смотреть и приавить чтоббы метрика изначлаьного
дефолт маршуртуа была самая мелкая. тоесть идет мудота с маруршрутамт по умаолчанию.
если мы запускаем сразу 10 тонелей впн то у нас будет +10 маршутов по умолчанию хотя
они нам нахер не встарлись. однако в этом случае нет проблем с dns резволингом
для компов которые сиядят за впн. причем мне неадо даже прописывать какие домены 
у нас находтся за впн. такое оущщенеи что он опаршвает все днс  подряд.
так вот если в граф утилите актиировать галчку "use this connection only for the resources of ots network" то тогда при активации впн конекта у нас +1 дефолт маршрут не создается что круто 
ибо он нам нахер не всрался. и получается что пакеты будут лететь через этот tunX только 
те для которых прописаны маршурты которые посадсываются при конекте по впн. тоесть
вот я актиировал впн коекнт у меня в таблице маруризации повялтся маршурты для сет картчоки
tun0

# ip r s | grep tun0
10.0.0.0/16 via 10.200.201.1 dev tun0 proto static metric 50 
10.90.0.0/24 via 10.200.201.1 dev tun0 proto static metric 50 
10.96.0.0/22 via 10.200.201.1 dev tun0 proto static metric 50 
10.100.0.0/16 via 10.200.201.1 dev tun0 proto static metric 50 
10.101.0.0/24 via 10.200.201.1 dev tun0 proto static metric 50 
10.200.201.0/24 dev tun0 proto kernel scope link src 10.200.201.49 metric 50 

и не плятется новый дефолт маршрут. эти маршурты получены путем подсоса с впн серверра.
значит пакет ыдля 10.96.0.0 будут лететь через этот впн коенект. все красивао все хорошо. но!
при этом абсолютно перестают раотать кастомные dns серврера которые я указывают в граф 
утилите в этом впн конекте. по каойто причине линукс больше к ним не обращается. (напомню что 
в /etc/resolv.conf котоырй сгенеиован через networkmanger стоит nameserver 127.0.0.1 )
и тогда выход какой - нужно руками указть для каждого впн конекта те домены которые сидят 
за этим внп. пример

# nmcli connect  modify   VPN1        ipv4.dns-search 'consul'

это значит что для vpn  конекта с именем "VPN1" мы говорим что за ним сидит доменая зона
"*.consul" и что чтобы ее резволить надо обращаться к dns сервреру который  я прописал 
через граф уттилтут для этго впн коекта.  непоняно почему через граф утлиииту можно 
пропсиать dns сервер для этого конкта но нельзя указать доменные зоны которые на нем 
сидят.

кстати как посмотреть в комадной строке все настройки для какого конекта. 
а вот так

# nmcli conn show  VPN1 

очень удобно 

хрошая статья со спмском гоярчикх команд nmcli 
 https://www.golinuxcloud.com/nmcli-command-examples-cheatsheet-centos-rhel/#4_List_all_the_configuration_of_interface)


====
значит выше я усирался описыал как же нам заставить при активации впн тонеля чтобы
хосты за этим тонелем резолвились через dns сервер лежащий   в этом тонеле.
ну и я гоорил о том что надо помудить с настройками NetworkManager. ксати эти
настройки хранятся в 

/etc/NetworkManager/system-connections

вот в часотности пример конфига впн конекшена


# cat /etc/NetworkManager/system-connections/vpn.io 
[connection]
id=
uuid=
type=vpn
permissions=user:vasya:;
timestamp=1690512760

[vpn]
ca=/home/vasya/.cert/
cert=/home/vasya/.cert/
cert-pass-flags=0
cipher=AES-256-CBC
comp-lzo=adaptive
connection-type=password-tls
dev=tun
key=/home/vasya/.cert/
password-flags=1
remote=xxx.io:4910
remote-cert-tls=server
remote-random=yes
reneg-seconds=0
username=kuku
service-type=org.freedesktop.NetworkManager.openvpn

[ipv4]
dns=10.200.201.1;
dns-search=consul;
ignore-auto-dns=true
method=auto
never-default=true

[ipv6]
addr-gen-mode=stable-privacy
dns-search=
method=auto


в этом конфиге нам вот что интересно 

dns=10.200.201.1;
dns-search=consul;


эта хрень гооворит что за этим впн тонелем лежит домен *.consul
и что его зона лежит на сервере 10.200.201.1

так вот - я узнал как нам решить туже задачу но без мудежа с впн настройками:


для начала нужно на компе задиейблить systemd-resolvd  (клиент dns)

кстати как увидеть что именно systemd-resolvd у нас яуляетс клиентом dns
# netstat -tnlp | grep 53
tcp  0  0 127.0.0.53:53        0.0.0.0:*               LISTEN      384/systemd-resolve 

вот мы видим что на 127.0.0.53 сидит systemd-resolve 

итак дизйблим его нахер
# systemctl stop    systemd-resolved.service
# systemctl disable systemd-resolved.service


теперь нам надо активировать dnsmasq. это кэширующий dns сервер  - что нам и надо.
а также он dhcp сревер. но это уже нам похер.

и тут два вариант - либо нам его надо поставить из пакетов.
либо его ставить ненадо а можно заюзать dnsmasq плагин входящий в состав NetworkManager.
поскольку на моем лэпотпе щас за все сетевые карточки отвечает NetworkManager 
то я буду юзать его плагин.

выглядит это так - нужно создать два файла:

првый файл говорит нетворкменеджеру чтобы он актиивировал плагин dnsmasq
# cat /etc/NetworkManager/conf.d/00-dnsmask-enable.conf 
[main]
dns=dnsmasq



второй файл  говоррит на какой сет карту вешать этот плагин
чтобы он принимал там запросы клиентов и какие зоны доменные лежат на каких dns 
серверах

# cat /etc/NetworkManager/dnsmasq.d/dnsmasq.conf 
interface=lo
cache-size=0
server=/vasya.co/10.200.201.1
server=/petya.ru/10.200.201.1
server=/#/8.8.8.8

этот файл говорит что зона *.vasya.co лежит на dns серверер 10.200.201.1
также он говорит что все остальные запросы направлть на 8.8.8.8

перезапускаем netowrkmamger

# systemctl restart NetworkManager

если все ок то мы увидим

# pstree  -A  -s  -p    8802
systemd(1)---NetworkManager(8380)---dnsmasq(8802)

тоесть мы видим что процесс dnsmasq(8802) являтся дочерним от NetworkManager

также мы уивдим

# systemctl status NetworkManager
● NetworkManager.service - Network Manager
   Loaded: loaded (/lib/systemd/system/NetworkManager.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2023-08-02 00:00:08 +06; 26min ago
     Docs: man:NetworkManager(8)
 Main PID: 8380 (NetworkManager)
    Tasks: 13 (limit: 4915)
   CGroup: /system.slice/NetworkManager.service
           ├─8380 /usr/sbin/NetworkManager --no-daemon
           ├─8802 /usr/sbin/dnsmasq --no-resolv --keep-in-foreground --no-hosts --bind-interfaces --pid-file=/run/NetworkManager/dnsmasq.pid --listen-address=127.0.1.1 --cache-size=0 --clear

нас здесь интерсует вот эта строка


 ├─8802 /usr/sbin/dnsmasq --no-resolv --keep-in-foreground --no-hosts --bind-interfaces --pid-file=/run/NetworkManager/dnsmasq.pid --listen-address=127.0.1.1 --cache-size=0 --clear


также можно посмтреть 

# cat /proc/8802/cmdline 
/usr/sbin/dnsmasq--no-resolv--keep-in-foreground--no-hosts--bind-interfaces--pid-file=/run/NetworkManager/dnsmasq.pid--listen-address=127.0.1.1--cache-size=0--clear-on-reload--conf-file=/dev/null--proxy-dnssec--enable-dbus=org.freedesktop.NetworkManager.dnsmasq--conf-dir=/etc/NetworkManager/dnsmasq.d


в resolv.conf увидим
# cat /etc/resolv.conf
# Generated by NetworkManager
search vasya.co petya.ru
nameserver 127.0.1.1


домены  в строке "search" vasya.co petya.ru 
появятся не сразу а когда мы активиурем впн конекты.

по дефолту resolv.conf это симлинк на какойто файл. если есть сомненеия в это файле
то просто удаляем симлинк. создаем пустой resolv.conf
и перезапускаем NetworkManager и он отредаетикрует resolv.conf как надо.


если ставит dnsmasq не как плагин а как отделный пакет то по мне  с ним
будет поболше мудежа. поэтому  я останвоился на варианте плагина от NetworkMamnager.

вот и все мы без всякого муллежа получили ту фишку что у нас теперь мы можем 
указать любую dns зону и с какого dns серверра ее читать. ура.

мне очень помогла статья вот эта при подготовке https://fedoramagazine.org/using-the-networkmanagers-dnsmasq-plugin/

что еще добавлю. итак по дефолту в убунту по крайней мере в убунту18 нет dnsmasq
но если  у нас установлен lxd через snap то он ставит и юзает dnsmasq тоже видимо снаповую версию но биндит он dnsmasq только на свои вирт устройства. остальные он не трогает. пример

# netstat -tnlp | grep 53
tcp  0      0 10.113.151.1:53         0.0.0.0:*               LISTEN      3404/dnsmasq        
tcp  0      0 10.78.50.1:53           0.0.0.0:*               LISTEN      3306/dnsmasq        
tcp  0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1992/dnsmasq     

все эти dnsmasq это lxd забиндил для "своих " интерфейсов

# ip -c a | grep 10.113.151
    inet 10.113.151.1/24 scope global lxdbr1

# ip -c a | grep  10.78.50
    inet 10.78.50.1/24 scope global lxdbr0

# ip -c a | grep    192.168.122
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0

(lxddbr* это для контейнеров а virbr* для виртуалок)


====

| ping.manjaro.com

значит если взять процесс NetworkManager 


        # lsof -Pn -p $(which NetworkManager)


то можно заметить что у него естт файл
дескрпитор котрый явялется TCP конектом к ping.manajaro.com
что это нахер это нужно я непонял. об этом написано тут

    https://man.archlinux.org/man/NetworkManager.conf.5#CONNECTIVITY_SECTION


так вот я просто задизейбили эту фичу 

/etc/NetworkManager/conf.d/20-connectivity.conf

[connectivity]
enabled=false


как написано тут

    https://wiki.archlinux.org/title/NetworkManager#Checking_connectivity



