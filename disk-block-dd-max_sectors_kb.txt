значит о чем речь

у нас есть диск. скажем sda

ксатии замечу что /dev/sda это не прямое окно  к железке диск. нихуя.
это окно заказок в ДРАЙВЕРУ который обслуживает диск.

                             |
программа ------> /dev/sda --|--->  драйвер диска ------> диск
                             |

палкой я показал что слева от палки юзерсепейс . справа от ядра прорстранство ядра

но всетаки акцент не наэтом а на том что /dev/sda это не окно доступа к диску. 
ни к какому диску доступа у нас нет. это окно заказов к драйверу в ядре который обслуживает диск
а уже он приняв наш заказ думает что он будет делать из нашего заказа а что нет и как он 
будет это делать. 

в этом свете забавны такая штука что /dev/sda1 это доступ к столку заказов к тому же драйверу
просто таким макаром нам драйвер показывает что в этом окне принимаются заказы по досутпу
к партишену 1 на диске. тоесть это тоже доступ к диску просто начиная с некоторого сектора от 
начала. тоесть если sda это доступ к диску с LBA=0 то sda1 это доступ к диску начиная
с какогто LBA=100500 
с точки зрения юзер спейс программы /dev/sda это доступ к файлу. который характиеризуется
тем что нам надо открыть файл через open() потом туда можно начать писать через write()
и еще можно ядру указать через lseek() с какого оффсета надо начать писать в этот файл
вот стрейс от dd. 
 $ sudo strace dd  if=/dev/sda of=/dev/null bs=120k count=1

openat(AT_FDCWD, "/dev/sda", O_RDONLY)  = 3
dup2(3, 0)                              = 0
close(3)                                = 0
lseek(0, 0, SEEK_CUR)                   = 0
openat(AT_FDCWD, "/dev/null", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3
dup2(3, 1)                              = 1
close(3)                                = 0
read(0, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 122880) = 122880
write(1, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 122880) = 122880
close(0)                                = 0
close(1)                                = 0

значит мы открываем файл , открвыает второй файл , читаем из  одного и пишем в другой.


вместо dd можно было бы легко заюзать 

  # echo "123" > /dev/sda 

это без разницы

параметр bs=120k у команды dd ей говорит о том какой размер буфера указать для read() write()
тоесть dd в частнотси запульнет запрос read() из файла /dev/sda размером 120KByte
что мы и видим

read(0, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 122880) = 122880

но! это всего наша просьба к ядру. но ядро совершенно не факт что выполнит этот запрос так
как мы его попросили. а именно : мы говрим ядру - прочитай из файла fd/0 (/dev/sda) нам 120 кило
байт. но ядро еще не факт что будет к железке запускать запрос в виде - ей диск выдай мне
120Килобайт начиная с оффсета 0! ядро может к диску сделать зпапрос в виде = эй диск выдай
мне 64 килобайт данных! получит эти 64 килобайта. потом скажет эй диск ! выдай мне еше 64 
килобайта данных начиная с оффсета 64к ! потом соединит эти два куска данных и скопирует 
этот 128КБ кусок в область памяти юзер программы! если еще быть точнее ядро к диску обращается
вообще в другой форме. это мы из юзер программы делаем к ядру запрос в виде

  read (0, ...., 128KB)

  ssize_t read(int fd, void *buf, size_t count);


тоесть мы говоими ядру - прочти нам из файла fd/0 (/dev/sda) 128КБ и отдай их нам . точнее засунь эти байты в нашу область памяти. 

пытливый ум заметит и скажет а где же в запросе указан офффсет? для каждого открытого
файла ядро всвоих кишках еще хранит текущий оффсет. если мы считали 5 байт то офффсет 
сдивгаетя на +5. и следушая операция чтения будет с тоого места где мы закончили в тот раз.
этот оффсет можно руками посмртреть вот тут 

  /proc/PID/fdinfo/N


если у нас в /fd/N это дескиптор файла который мы щас читаем то в 

  /proc/PID/fdinfo/N

монжно посмтеть какой оффсет сейчас для этгого файла в ядре указан
например

pos:  18921986          <============= офффсет текущий
flags:  02100000
mnt_id: 32


поэтому когда мы длеаем read()


  read (0, ...., 128KB)

  ssize_t read(int fd, void *buf, size_t count);


то чтение идет с текущего офсета.
если мы хотим сменить оффсет то мы юзаем 

 off_t lseek(int fd, off_t offset, int whence);


но так все выглядит кода мы читаем из юзерспейса из файла.


ядро же к диску обращается совсем по другому. как я понимаю диск наружу тоесть к ядру 
предоставляет возможность делать запросы в виде того что мы указыаем номер сектора (блока)
с кторого хотим прочитать данные . и сколько блоков мы хотим прочитать тоесть ядро к диску 
обращается в виде

   эй диск! прочти мне 10 блоков начиная с блока 100. 

это назыавется сделать к диску IO реквест. размер блока\сектора на диске обычно имеет размер
512 байт или 4килобайта. (в разной литератутуре одну и туже хуйню все время называют по разному
где то это сектор где то это блок). 


вобщем ядро оно берет наш запрос указанный в килобайтах и коневретррует этот зпрос
к диску в секторах! и как я уже сказал мы можем в read() укзать размер запроса нпример
условно 100МБайт. но ядро такой запрос пихать на диск не будет! потому что ядро както 
то там знает что диск может максимум высрать из себя за один раз скажем 120KByte и не более!
поэтому ядро наш запрос разобьет на кучку запросов к диску. будет их посылать на диск
один за другим и собирать полученные данные. в этом время наш процесс будет помещен 
ядром в sleep состояние. 


так вот очень интересно на это  посмотреть! 
скажем мы заказываем в нашей проге прочитать 10 МБ данных с диска. и 
посмотреть какими кусками ядро будет по факту читать данные с диска!

можно было бы это делать через си программу но не времени писать. 
по факту нам нужно открыть файл open () с определенными флагами там их вагон 
    open()
а потом read() указав размер куска который мы хотим прочитаь из файла
поэтому будем делать тоже самое через dd
при этом размер IO запроса который ядро шлет на диск будем смотреть через iostat

поехали 

$ sudo dd if=/dev/sda  of=/dev/null bs=120k  count=10000  iflag=direct  status=progress

bs=120k  даст то что dd будет запускать read() в котором будет указано что мы хотим
прочитаь из файла кусок размером 120k

iflag=direct  дает то что open() будет запущен с флагом O_DIRECT

      openat(AT_FDCWD, "/dev/sda", O_RDONLY|O_DIRECT) = 3

об этом флаге читай в man 2 open но типа он даст то что как  я понимаю линукс при чтении
из файла не будет этим прочитанным засирать  pagecache то бишь кэш в памяти. это
я так думаю снизить нагрзку на цпу ибо ему ннужно гонять данные в этот кеш . чистить его
если он уже подзаполнен итд. и это еще нам даст то что когда мы будем смотреть  в iostat 
то там будут предсказумые цифры в графе "areq-sz" 
эта графа она показыывает какой в среднем был размер IO был ядром закаазан у диска.
если же мы заустим dd без iflag=direct то там будет хуй знает что. я щас все это покажу
на практике

count=10000  говорит сколько кусков размером bs мы хотим считать из файла. 
если мы считаем только 1 кусок то мы не успеем ничего увидеть в показаниях iostat.
поэтому мы будем читать много кусков одного размера.

делов том что мы запускаем

     $ iostat -cx -s  1  /dev/sda

он считывает статистику из /sys/block/sda  через каждую секунду.
и делает калькуляции.  и поэтому чтобы получать вменяемые цифры в этой стаитисткие нужно 
подать на диск равменомерную нагрузку. на некоторое вретмя

вместо iostat можно статситку расчитывать и самому. делается оно так


$ cat /sys/block/sda/stat
   43184        0 60947512   329385        3        0        0        7        0    60754   329399        0        0        0        0        3        7


значеиие цфиры в каждом столбике
Name            units         description
----            -----         -----------
read I/Os       requests      number of read I/Os processed
read merges     requests      number of read I/Os merged with in-queue I/O
read sectors    sectors       number of sectors read
read ticks      milliseconds  total wait time for read requests
write I/Os      requests      number of write I/Os processed
write merges    requests      number of write I/Os merged with in-queue I/O
write sectors   sectors       number of sectors written
write ticks     milliseconds  total wait time for write requests
in_flight       requests      number of I/Os currently in flight
io_ticks        milliseconds  total time this block device has been active
time_in_queue   milliseconds  total wait time for all requests
discard I/Os    requests      number of discard I/Os processed
discard merges  requests      number of discard I/Os merged with in-queue I/O
discard sectors sectors       number of sectors discarded
discard ticks   milliseconds  total wait time for discard requests

(доп описание https://www.kernel.org/doc/html/v5.3/admin-guide/iostats.html)

значит в 7-ом столбике указано сколько секторов было записано на диск с момента аптайма
системы. (https://www.kernel.org/doc/Documentation/block/stat.txt)
там анписано что в для данных счетчиков важно знать что в данном случае под словом сектор
подразумевается чисто виртульная хрень 512 байт которая к ральному размеру сектора на 
диске не имеет значения. просто они не стали запаривать. тоесть все счетчики все пока
зывают в виртуальных единицах размером 512 байт.

получается если я считаю 7-ой столбик щас и потом через 1 скунду. то я высчитываю разницу.
умножаю ее на 512 байт и получаю сколько байт было записано на диск за эту одну скунду.

5-ый столбик показывает сколько IO запросов ядро направило на диск с момента аптайма.
получается тоже самое если мы возьмем эту цифру щас и через 1 с. посчитаем разницу то 
узнаем сколко io запросов в штуках ядро направило на диск за 1с. 

тееперь если мы поделим число байт запснных на диск за 1 с на число запросов аправленых 
ядром на диск за эту секуду то узнаем СРЕДНИЙ РАЗМЕР запроса на запись который был 
в тчение этой секунды. 

это все касалось кода мы ПИШЕМ на диск. аналогично когда мы с него читаем
то это столбики 3 и 1

что еще забавно что в /proc есть файл который ольединяет статитику по всем диска 
и партиешенам!

$ cat /sys/block/sda/stat
   43212        2 60996928   329516        3        0        0        7        0    60887   329530        0        0        0        0        3        7

$ cat /proc/diskstats 
   8       0 sda 43212 2 60996928 329516 3 0 0 7 0 60887 329530 0 0 0 0 3 7
   8      16 sdb 342 0 15432 220 3 0 0 0 0 170 222 0 0 0 0 3 0
   8      32 sdc 20924 360 2056396 134606 0 0 0 0 0 83524 134606 0 0 0 0 0 0
   8      33 sdc1 314 0 17056 1602 0 0 0 0 0 1370 1602 0 0 0 0 0 0

видно что сточка из /sys в точноти скопрована в /proc

также можно замтить колонки с read merges и write merges. что это за хрень? 
о ней я скажу ниже.


понятно что если нагрузка записи на диск идет равномерная тоесть запросы идут одного размера
то средни размер IO будет ровный красивый скажем 120KB  а если он 120,15KB это значит что 
запросы за секунды были посланы на диск хуй знает какго разного размера. 

вот за нас это делает iostat. 

если у нас диск 100% неисползутся системой. тоесть только мы чрез командуню строку
на нено чтото шлем то count можно поставить равным и 1 . 
и мы тогда увидим статситику

$ sudo dd if=/dev/sda  of=/dev/null bs=120k  count=1  iflag=direct  status=progress

$ iostat -cx -s 1

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               0.00      0.00     0.00    0.00     0.00    0.00   0.00


Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               1.00    120.00     0.00    5.00   120.00    0.01   1.00

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               0.00      0.00     0.00    0.00     0.00    0.00   0.00


нас интерует столбец "areq-sz" он показывает средний размер IO запросы который ядро
заряжало на диск. как яуже сказал это велчичина вычисляемая. 

и вот мы видим что диск был ненагружен. потом проскочил наш запрос . потом опять тишина.
мы видим что мы попрсили у ядра прочитать 120КB с диска. и ядро по факту сделало 
это за 1 запрос. тоест ядро когда формировало запрос к диску то оно ему в запросе
указало чтобы диск в результате этого запроса вернул 120КБ за один раз. тоесть ядро 
сдедало запрос. и диск пока у себя внутри ненаскребет 120КБ он ядру не отвечает.
тоесть диск интерапт не гененририует. я точо незнаю но я подозреваю что диск неможет 
вернуть ответ на запрос больше чем у этого диска внутри есть обьем его RAM кэша.
тоесть он извлекает данные из пластин и складывает их в RAM кэш. если скажем кеш 32МБ 
то это максимумальный размер запроса который можно к диску из ядра запросить. ( я так
понимаю что блоки данных дожны кончено на диске лежать рядом тоест это последовательный запрос).
а если мыхотим больше данных то нужно нам в ядре разбивать наш запрос на куски. условно
говоря если байты это кирпичи. то размер кэша дика это  макс размер коробки в которой
он может ядру отгрузить эти киирпичи. 

а теерь я уберу флаг iflag=direct
это значит что у нас open() будет запущен без флага O_DIRECT

  openat(AT_FDCWD, "/dev/sda", O_RDONLY)  = 3

это говорит ядру (как я понимаю) что когда в даьльнейшем мы начнем запуливать 
read() и write()  и там мы будем укызать рамзеры кусков котоыре мы хотим получить  с файла
то ядро должно искать эти куски прежде всего в своей кеше пейджкеше. также ядро должно 
при чтении того чтомы просим класть эти куски в кеш на случай если это кому то снова
понадоится и это как я понимаю говорит ядру что бы оно самое решало на сколкьо кусков 
разбивать наш исходный запрос если данных в кеше нет. тоесть остусттвие флага O_DIRECT
оно гворит ядру- эй ядро. используй кэш и  направляй запросы к диску как сам считаешь нужным
а не как тебя просит юзер программа. например  мы вгорим ядру bs=20k ядро смотрит у него
такого куска еще нет в кеше. ядро формрует запрос к диску но указывает считать сразу 120KB
на случай если нам в следущем запросе понадобится остальное.  есть даже сисколл толко 
я немогу его щас найти который подсказыавет ядру со стороны нашей программы что мы собаремся
дальше данные чиатть в линейном паттерне чтобы ядро там на основе этого подкручивало запросы

$ sudo dd if=/dev/sda  of=/dev/null bs=20k  count=1 skip=10000  status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               1.00     20.00     0.00    6.00    20.00    0.01   1.00


видно что мы убрали диркет флаг однако ядро считало с диска толко 20КБ и толко то

$ sudo dd if=/dev/sda  of=/dev/null bs=120k  count=1 skip=10000  status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               1.00    120.00     0.00    7.00   120.00    0.01   1.00


а вот еще пример

  $ sudo dd if=/dev/sda  of=/dev/null bs=10M  count=1 skip=10000  iflag=direct status=progress

показания iostat

  Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
  sda               8.00  10240.00     0.00   17.62  1280.00    0.14   2.70

а вот показания /sys до запроса

  $ cat /sys/block/sda/stat
   43256       10 61083968   ...

и после запроса
  $ cat /sys/block/sda/stat
   43264       10 61104448   ...


соосвттвенно 

  tps
 8.00 

в iostat это у нас первая колонка в /sys/... ее разница 43264 - 43256 = 8

 
    kB/s 
10240.00

это третья колонка в /sys/.. ее разница   61104448 - 61083968 = 20480 выражает 
сколкьо килоБайт было считано с диска за это время в фикктивных единицах 512 байт
пеерводим в килоБайты получаем 10240 КБ 


видим что мы заказали  у ядра считать с диска 10MB (полнцоенных мегбайт)
а ядро это сделало за 10 запросов к диску  - поле tps , те самые IOps , которые в 5-ой колонке
cat /sys/block/sda/stat отображаются. 
итак ядро сделало к диску 10 запросов. каждый запрос был заказан размером 1152 KB

итак мы теперь знаем из каких столбиков в /sys/.../stat наш iostat высчитывает 
свои столбики "tps" и  "kB/s"

теперь что мы увидели. мы заказали из программы у ядра прочитать и выдать нам с 
файла (то бишь с диска) 10МБ. а ядро послало на диск 8 запросов. средний размер 
каждого запроса был 1280.00 КБ , да мы незнаем какой размер был у каждого запроса но в средний
размер запросов был 1280.00 КБ 
итак dd через read() сделал запрос у ядра выдать нам в результате запроса 10МБ данных, 
одним куском. кусок этот ядро должно положить в память ядра в укаазанное место. 
как я уже говорил что ядро когда принимает через сисколл read() этот запрос то ядро поместит 
процесс в S стейт до тех пор ядро не выполнить этот запрос полностью. для юзер процесса 
выглдяит все так. что она закаала 10МБ и бах ядро ей его выдало (пока оно спало). а по факту
ядро делало 8 отдельных запросов к диску. собирало в кучку все эти данные. и только потом скопировало эти данные в область памяти процесса. 
в область памяти процесса


теперь возгикает вопрос откуда ядро взяло цифру 1280 КБ. почему оно направляло к диску
запросы именно такого размера. и вот он ответ 
в свойствах диска есть вот такой файл

$ cat /sys/block/sda/queue/max_sectors_kb 
1280

в нем указано какой максимального размера в КБ запрос ядру разрешается направлять к диску.
видим 1280 вот ядро такой запрос и наравило к диску.

есть еще такой файл

$ cat /sys/block/sda/queue/max_hw_sectors_kb 
32767

он показывает какой макс по длинне запрос диск отрапрортовал ядру он диск может выполнить 
если у него попросят. я так считаю что это размер RAM кэша внутри диска. 
второй файл мы менять конечно не можем а первый можем.

# echo "4096" > /sys/block/sda/queue/max_sectors_kb

делаю тот же самый запрос

$ sudo dd if=/dev/sda  of=/dev/null bs=10M  count=1 skip=10000  iflag=direct status=progress

iostat покзыывает
Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               3.00  10240.00     0.00   21.67  3413.33    0.07   2.60

видно что в итоге ядро к диску зарядило 3 запроса. средня длина запроса 3413.33 КБ
общий обьем счиьтанного 3413.33*3=10239,99 KB  втоже время в колонке "kB/s" мы видим 10240 KB
несовпдаение потому что поле tps и kB/s оно считывается из /sys  а поле areq-sz оно
уже после этого вычсисчтывается путем того что

    areq-sz = (kB/s)  /   tps 

ну и понятно что число 10240 оно на 3 никак не делится нацелом. вот и получаем округление 
в сторону меньшего

тоесть из /sys мы знаем что было считано 10240 KB и что иопсов было 3 штуки. но какие это 
были иопсы по размеру нам ядро несообщает. потому мы можем только посчитать среднее. 

    areq-sz = 10240/3 ~ 3413.33  

я считаю что было дело так. ядро зарядило 2 запроса по 4096КБ как мы прописали  и последний
запрос  был 10240-(9192)= 1048KB

изменим наш изначальный запрос так чтобы его полная длинна была кратная 4096

$ sudo dd if=/dev/sda  of=/dev/null bs=12288K  count=1 skip=10000  iflag=direct status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               3.00  12288.00     0.00   24.00  4096.00    0.07   3.40


все подтвердилось! у нас ядро запрос к ядру был считать 12288 КБ с файла. ядро порезало
этот запрос на запросы того макс размера которое ему разреенено делать. 12288/4096 =3 
вот ядро и сделало 3 запроса длинной по 4096 !

итак мы увидели вот что - если юзер программа просит через свой запрос read() некоторый 
кусок данных некотрого размера то ядро смотрит  этот размер куска боольше чем max_sectors_kb
или нет. если нет то ядро считает с диска запрощенный кусок данных за 1 проход. а если
кусок запрошенный больше то ядро режет запрос на эти куски и начинает их запраивать из диска.
показываю.
запросим кусок размером 4095КБ

$ sudo dd if=/dev/sda  of=/dev/null bs=4095K  count=1 skip=10000  iflag=direct status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               1.00   4095.00     0.00   14.00  4095.00    0.01   1.60

запросим кусок 4095+4096

$ sudo dd if=/dev/sda  of=/dev/null bs=8191K  count=1 skip=10000  iflag=direct status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               2.00   8191.00     0.00   21.50  4095.50    0.04   2.40

тоесть ядро сделало первый запрос размером 4096 КБ  и второй запрос 4095 КБ

запросим кусок размером 13МБ , а это 3 полных куска размером 4096 КБ и еще остаточный кусок.
значит ядро должно сделать 4 запроса на диск. проверяем

$ sudo dd if=/dev/sda  of=/dev/null bs=13M  count=1 skip=10000  iflag=direct status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               5.00  13312.00     0.00   27.00  2662.40    0.14   3.60

кхм.. ну окей. ядро сделало 5 запросов а не 4. 

еще раз сделаем запрос который точно кратно делится на 4096КБ

$ sudo dd if=/dev/sda  of=/dev/null bs=16M  count=1 skip=10000  iflag=direct status=progress

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               4.00  16384.00     0.00   29.25  4096.00    0.12   4.00

все как по нотам

"% util" это процент времени от интервала измерения в течение кторого на диск
отправлялись запросы. нет это не количество времени в течение которого диск находился в 
обработке запросов. это количество времени когда ядро "пуляло"  новый запрос к диску. в данном случае это выглядело так. ядро получило сисколл. оно сформировало
очередь запросов в своей памяти. потом оно послало первый запрос к диску. потом ядро ждало 
когда диск отвветит. потом диск ответил ядро послало на диск второй запрос из очереди. и так 
до конца. я не очень понимаю каонечно как это время высчитыается. может это сколько заняло 
времени у ядра чтобы засунуть запросы в диск. суммарно. 

"await" это сколько времени в среднем запрос провел в очереди на исполнение в памяти ядра
плюс сколько времени его диск потом обслуживал. видим что это 29.25 мс
а теперь вот что поймем. нашей программе ненужен было какойто отдельный реквест. ей нужно 
было получить все три реквеста одним куском. поэтому получается что наша программа ждала
свой кусок 16МБ по времени 87,75мс вобще то это конечно дохрена. с другой стороны реальная 
программа вряд ли данные считывает для какйото своей тразакции такими огромными кусками. 
тоолько если какойто отчет. и я еще вот что не понимаю. когда диск отдает ядру этот отдеьный 
iops то он же попадает как я понимаю именно в область памяти ядра. а потом ведь его еще нужно
скопировать из ядра в область памяти  юзео процесса это же тоже занимает время или это 
копейки? в люлом случае я хочу сказать что у наас юзер программа может запросить у ядра 
некоторый кусок данных который она хочет получить одним куском. и она будет это ждать.
ядро может разбть этот кусок на мелкие запросы. и тогда получается что время ожидания 
у программы будет соатавлять сумму от времени обслуживания всех кусков. тоесть если мы видим
что один запрос на интервале таймлапаса обслуживался 1мс. а запрсов была 1000. то мы не 
сможем понять так сколько времени програамма ждала свой кусок. может быть 1мс а может быть
500мс потому что ей нужно было сразу 500 кусков одновременно. вот в чем тоже засада!

от того что среднее время обработки одного иопса 1мс это незначит что конечная программа
получает запорошенный ею кусок за 1мс. она может оджидать кусок из несколких ипосов! поэтому
ее время ожидания будет сумма. причем у нее часть кусков может оказаться вначале очереди
а часть в концце!

а ну ка запуьльнем запрос на 4КБ

Device             tps      kB/s    rqm/s   await  areq-sz  aqu-sz  %util
sda               1.00      4.00     0.00    5.00     4.00    0.01   1.00

видим что он исполнялся 5мс
ну я незнаю.. чото много. почему хотя бы не 1мс

теперь вот о чем поймем. а какой физ смысл менять параметр 

  max_sectors_kb  ??


а я скажу в чем! вот у нас есть три приложения.  у нас много цпу . каждая прога сидит на своем
ядре цпу. значит они все одновременно выполняются и они они все пуляют к ядру запрос чтения
одновременно. получается в ядре появилось одноврменно три запроса на чтение. положим запросы
на чтение направлыены на 1 физ диск. значит далее ядро берет один из запросов и пуляет 
его на этот диск. получается что остальные запросы стоят ждут в очереди. приходит ответ
от диска. ядро отправляет его первой проге она щастлива. потом второй запрос идет на диск.
вторая программа ждет ответа и третья программа ждет ответа. итак диск отвечает и вторая 
прграмма получает данные. и наконец третий зпрос идет на диск, диск отвечает  и ядро передает
его третьей программе и она щаслива. получается вот что - третья программа стояла и ждала 
свои данные пока их получат первая программа и вторая. хреново. а теперь вот что представим.
у нас есть одна программа которая делает запрос блока данных большого размера который больше 
чем размер в   max_sectors_kb, пусть ее размер составляет два блока размером max_sectors_kb, а остальные две программы у них размер запроса меньше чем max_sectors_kb. тогда ядро для первой 
программы ядро создаси в очереди два иопса. а для втрой и третьей програмы ядро создась по одному
иопсу. итого 4 иопса.  дальше конечно непонятно как эти иопсы будут расположены в очереди на
диск. по мне по идее два иопса для первой программы будут стояь друг за другом и тогда прикола
никкого нет. а вот если они будут перепутаны то получается вот что. что программа два и три
они уже получат свои данные до того как первая программа получит свой второй иопс. и это дает 
то что если программа высирает из себя мелкий запрос то он будет выполнен быстрее чем
она высирает из себя крупный запрос.  вот показываю еть три программы A B C
и их запросы в очереди в ядре

   ядро   C1  A2 B1 A1  --> диск


и получается что пограмма B успеет получить свои данные и начать свою работу уже до того
как программа A получит оба своих куска. получается если куски перемешаны то программы с маленкими
запросами данных будут их получать более оперативно чем программа у которой запрос нужно делить
на кучу подзапросов. но это снижает отклик для программ с жирными запросами.

а если у нас  max_sectors_kb большой  то программа делающая жирные запросы будет получать
ответ быстрее но программы с мелкими запросаим будут от этого страдать.

вопорс в том  а как это монтирить? ведь iostat тоесть /sys/.... он показывает статистику
уже после того как ядро порезало изанчальные запросы на подзапросы. тоесть надо что сидеть
с strace и мониторить read() write() у процессов ? 

вот пример. пусть 
  max_sectors_kb     =  4 KB
  изначлаьные запроы read() write() =  4 КБ(A)  4КБ(B)   4КБ(C)   12КБ(D)

в скобках указана программа заказчик
ядро разобьет это на субзпросы к диску

4 КБ(A)  4КБ(B)   4КБ(C)  4КБ(D)  4КБ(D)  4КБ(D)

соовтственно из стаститки iostat нам будет никак не понять про исходную картину 
нагрузки

едиснвтеное что приходит в голову. сидим смотрит сатстику iostat и если видим что 
срений размер блока  areq-sz  сильно рваный тоесть  120,57 КБ то надо подкручивать
max_sector_kb чтобы небыло  чисел после нуля. может так?






ладно. теперт пеерйдем к еще одной хитрой хрени это 

        rqm/s
        0.00

пока что она была все время ноль. но щас я ее сделаю не ноль. и будем о ней 
разговваривать

при операциях чтения   я чтто не могу спровоцировать что этот параметр стал нерваен нулю.
значит помотрим на операции записи

для начала меняю параметр

 # echo "16" > /sys/block/sda/queue/max_sectors_kb


 $ sudo dd if=/dev/zero of=/dev/sda   bs=16k  count=1     status=none

Device    r/s     rkB/s   %rrqm r_await rareq-sz     w/s  wkB/s   wrqm/s  %wrqm w_await wareq-sz
sda     99.00   1068.00    0.00    0.20    10.79    1.00  16.00     3.00  75.00    6.00    16.00

видно что ядро прежде чем записать 16Кб ядро читает 1068 КБ потом видимо 
в памяти меняет этот блок и запмсывет этот , но не пойму почему обратно он записывает 
всего 16кб

# echo "4096" > /sys/block/sda/queue/max_sectors_kb

$ sudo dd if=/dev/zero of=/dev/sda   bs=16k  count=1 oflag=nocache     status=none

Device   r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz   
sda    48.00   1068.00     0.00   0.00    0.21    22.25    1.00     16.00     3.00  75.00    5.00    16.00   

вобшем  с запистью я нихуя не понял. при попытке записаь 16кб ядро вначале читает 1068 КБ
причем размер при чтении иопса какото странный. внезависиомсоти от выставленого max_sectors_kb
потом он нарпвляет на диск запрос размером 16к  понятно что сам диск внутри своих кишок
делает copy-on-write. мы этоого не видим. так зачем ядро читает 1068КБ причем непонянтно оно
это делает до записи или после?  или он читает до записи 534кб потом дает на диск команду
на запись и потом еще раз перечитывает ? таким макаром проверяя что диск реально записал? 
а почему тогда 534кб ?

меняю запись с 16кб на 1М

sudo dd if=/dev/zero of=/dev/sda   bs=1M   count=1 oflag=nocache     status=none


Device   r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm 
sda    48.00   1068.00     0.00   0.00    1.12    22.25    2.00   1024.00   254.00  99.22 


меня на запись 8М

sudo dd if=/dev/zero of=/dev/sda   bs=8M   count=1 oflag=nocache     status=none


Device   r/s     rkB/s   rrqm/s  r_await rareq-sz     w/s     wkB/s   wrqm/s  w_await wareq-sz 
sda    48.00   1068.00     0.00     0.21    22.25   13.00   8192.00  2035.00     9.38   630.15


оьбем прочитанных данных не меняется. непоимаю. зачем вобше читает данные. почему
именно такой обьем. почему такой рзмер иопс. это что касается чтения

теппрь про запись. теперь про поле wrqm/s rrqm/s как я понимаю это из /sys/block/sda/stat
это  колонки 2 и 6, и тут такая ситуация. как я понимаю ядро формирует очередь запросов
в два этапа. проходит первый. потом ядро анализирует а можно ли эти запросы обьединить
слить несколько мелких в один крупный. это все неточно но я как понимаю то 

    w\s
    13.00

это сколко команд на запись ядро дало в коненом итоге на диск
а

     wrqm/s
    2035.00

это сколько было предварительных запрсов на запись на первом этапе которые он слил вместе
в то число запросов которые w\s в итоге стали

вобщем пока туманно. надо это все проверять на шпиндельном диске. там должно быть попроше
статистика при записи. 

запустил на шпиндельном
у него max_sectors_kb=120

$ sudo dd if=/dev/zero of=/dev/sdd   bs=360k   count=1  iflag=fullblock oflag=direct      status=none

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdd             53.00   1068.00     0.00   0.00    1.06    20.15    3.00    360.00     0.00   0.00 1034.00   120.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.16  17.00



ничео не понимаю.
я даже написал программу на си (смотри ее файле O_DIRECT.txt )которая сама делает запись на диск и даже использует хинт для ядра через posix_fadvise о том что я не собараюсь читать секвценциально
но все равно при запись на диск идут фантомные чтения.


Device   r/s     w/s     rkB/s     wkB/s   r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sda    46,00    1,00   1064,00     32,00     10,78    1,00   0,43    23,13    32,00  10,55  49,60
 

опубликова воспрос на стекэксчендж про это дело 
  https://unix.stackexchange.com/questions/778539/linux-phatom-reads
  



а вот еще интерсный про это дело пост
  https://stackoverflow.com/questions/40120967/tuning-sequential-disk-reads-for-performance

вот еще пост интересный
  https://questdb.io/blog/investigating-linux-phantom-disk-reads/#:~:text=This%20means%20that%20whenever%20we,be%20written%20to%20the%20disk.

и еще
  https://tungdam.medium.com/our-lessons-on-linux-writeback-do-dirty-jobs-the-right-way-fa1863a5a3dc




тамже указано про сисколл madvise() но он полезен если мы открыли файл через mmap
а если мы просто открыли файл без mmap то хинтом для ядра явлется 
сисколл  posix_fadvise() который  я не мог вспомнить
( про него написано тут https://www.oreilly.com/library/view/linux-system-programming/9781449341527/ch04.html вроде бы у меня есть эта книга)




далее диск 
у котрого max_hw_sectors_kb=32768
но при установке max_sectors_kb=32768
и bs=32768 у нас iostat покзывает что ядро максимум 
пуляет запрос к диску размером 8192
заодно я посмтретл как меняется скорость чения


# total_read_GiB="1"; for i in $(seq 1 13 ); do  bs="$(( 2*(2**$i) ))"; echo "bs=$bs"k; count=$( echo "$total_read_GiB*1024*1024 /  $bs " | bc) && dd if=/dev/sda  of=/dev/null bs="$bs"k count=$count  iflag=direct  status=progress 2>&1 | grep GiB; done

bs=4k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 19.4914 s, 55.1 MB/s
bs=8k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 10.7953 s, 99.5 MB/s
bs=16k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 6.69757 s, 160 MB/s
bs=32k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.56354 s, 235 MB/s
bs=64k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 3.50849 s, 306 MB/s
bs=128k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 3.3725 s, 318 MB/s
bs=256k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.84377 s, 378 MB/s
bs=512k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.5217 s, 426 MB/s
bs=1024k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.35215 s, 456 MB/s
bs=2048k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.05888 s, 522 MB/s
bs=4096k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2 s, 536 MB/s
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.00342 s, 536 MB/s
bs=8192k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.95462 s, 549 MB/s
bs=16384k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.94004 s, 553 MB/s


# lsblk -S
NAME HCTL       TYPE VENDOR   MODEL                       REV SERIAL           TRAN
sda  0:0:0:0    disk ATA      KINGSTON SV300S37A240G 603ABBF0 50026B725506D960 sata
sdb  1:0:0:0    disk ATA      SanDisk SSD U110 16GB   U21B001 143230407760     sata
sdd  7:0:0:0    disk          USB DISK 2.0               PMAP 071C29CE048E5254 usb



# lsblk
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
loop0    7:0    0 140.2M  1 loop /run/miso/sfs/livefs
loop1    7:1    0 971.5M  1 loop /run/miso/sfs/mhwdfs
loop2    7:2    0   1.3G  1 loop /run/miso/sfs/desktopfs
loop3    7:3    0 791.9M  1 loop /run/miso/sfs/rootfs
sda      8:0    0 223.6G  0 disk 
sdb      8:16   0  14.9G  0 disk 
sdd      8:48   1  14.5G  0 disk 
├─sdd1   8:49   1   3.3G  0 part 
└─sdd2   8:50   1     4M  0 part 



# lsblk -S
NAME HCTL       TYPE VENDOR   MODEL                       REV SERIAL           TRAN
sda  0:0:0:0    disk ATA      KINGSTON SV300S37A240G 603ABBF0 50026B725506D960 sata
sdb  1:0:0:0    disk ATA      SanDisk SSD U110 16GB   U21B001 143230407760     sata
sdd  7:0:0:0    disk          USB DISK 2.0               PMAP 071C29CE048E5254 usb



# disk="sdb"; total_read_GiB="1"; for i in $(seq 1 13 ); do  bs="$(( 2*(2**$i) ))"; echo "bs=$bs"k; count=$( echo "$total_read_GiB*1024*1024 /  $bs " | bc) && dd if=/dev/sdb  of=/dev/null bs="$bs"k count=$count  iflag=direct  status=progress 2>&1 | grep GiB; done
bs=4k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 23.7303 s, 45.2 MB/s
bs=8k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 12.9357 s, 83.0 MB/s
bs=16k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 7.69586 s, 140 MB/s
bs=32k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 5.24944 s, 205 MB/s
bs=64k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.46181 s, 241 MB/s
bs=128k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.25376 s, 252 MB/s
bs=256k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21551 s, 255 MB/s
bs=512k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21529 s, 255 MB/s
bs=1024k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21609 s, 255 MB/s
bs=2048k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21506 s, 255 MB/s
bs=4096k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21641 s, 255 MB/s
bs=8192k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21805 s, 255 MB/s
bs=16384k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.22263 s, 254 MB/s



# cat /sys/block/sdb/queue/max_hw_sectors_kb 
32767


# cat /sys/block/sdb/queue/max_sectors_kb 
1280

# echo "32767"  > /sys/block/sdb/queue/max_sectors_kb 


# disk="sdb"; total_read_GiB="1"; for i in $(seq 1 13 ); do  bs="$(( 2*(2**$i) ))"; echo "bs=$bs"k; count=$( echo "$total_read_GiB*1024*1024 /  $bs " | bc) && dd if=/dev/sdb  of=/dev/null bs="$bs"k count=$count  iflag=direct  status=progress 2>&1 | grep GiB; done
bs=4k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 23.9826 s, 44.8 MB/s
bs=8k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 13.119 s, 81.8 MB/s
bs=16k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 7.74873 s, 139 MB/s
bs=32k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 5.24054 s, 205 MB/s
bs=64k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.39802 s, 244 MB/s
bs=128k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.23244 s, 254 MB/s
bs=256k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21546 s, 255 MB/s
bs=512k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21503 s, 255 MB/s
bs=1024k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21656 s, 255 MB/s
bs=2048k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.216 s, 255 MB/s
bs=4096k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.21715 s, 255 MB/s
bs=8192k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.22019 s, 254 MB/s
bs=16384k
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 4.22215 s, 254 MB/s


