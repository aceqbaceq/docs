| tcp


для самого начал вот rfc в котором показны стейты у tcp конекта
	https://www.ietf.org/rfc/rfc793.txt
	https://datatracker.ietf.org/doc/html/rfc9293

а вот большая диаграмма оттуда


September 1981                                                          
                                           Transmission Control Protocol
                                                Functional Specification



                                    
                              +---------+ ---------\      active OPEN  
                              |  CLOSED |            \    -----------  
                              +---------+<---------\   \   create TCB  
                                |     ^              \   \  snd SYN    
                   passive OPEN |     |   CLOSE        \   \           
                   ------------ |     | ----------       \   \         
                    create TCB  |     | delete TCB         \   \       
                                V     |                      \   \     
                              +---------+            CLOSE    |    \   
                              |  LISTEN |          ---------- |     |  
                              +---------+          delete TCB |     |  
                   rcv SYN      |     |     SEND              |     |  
                  -----------   |     |    -------            |     V  
 +---------+      snd SYN,ACK  /       \   snd SYN          +---------+
 |         |<-----------------           ------------------>|         |
 |   SYN   |                    rcv SYN                     |   SYN   |
 |   RCVD  |<-----------------------------------------------|   SENT  |
 |         |                    snd ACK                     |         |
 |         |------------------           -------------------|         |
 +---------+   rcv ACK of SYN  \       /  rcv SYN,ACK       +---------+
   |           --------------   |     |   -----------                  
   |                  x         |     |     snd ACK                    
   |                            V     V                                
   |  CLOSE                   +---------+                              
   | -------                  |  ESTAB  |                              
   | snd FIN                  +---------+                              
   |                   CLOSE    |     |    rcv FIN                     
   V                  -------   |     |    -------                     
 +---------+          snd FIN  /       \   snd ACK          +---------+
 |  FIN    |<-----------------           ------------------>|  CLOSE  |
 | WAIT-1  |------------------                              |   WAIT  |
 +---------+          rcv FIN  \                            +---------+
   | rcv ACK of FIN   -------   |                            CLOSE  |  
   | --------------   snd ACK   |                           ------- |  
   V        x                   V                           snd FIN V  
 +---------+                  +---------+                   +---------+
 |FINWAIT-2|                  | CLOSING |                   | LAST-ACK|
 +---------+                  +---------+                   +---------+
   |                rcv ACK of FIN |                 rcv ACK of FIN |  
   |  rcv FIN       -------------- |    Timeout=2MSL -------------- |  
   |  -------              x       V    ------------        x       V  
    \ snd ACK                 +---------+delete TCB         +---------+
     ------------------------>|TIME WAIT|------------------>| CLOSED  |
                              +---------+                   +---------+

                      TCP Connection State Diagram


ее пока ненужно пытаться понять. к ней можно потом вернуться позже



итак когда у нас между компами есть tcp конеткт то физически фактически это означает 
что у нас на обоих концах есть по сокету. так вот тцп конект это интернет 
как канал связи и два сокета на обоих концах.

и поэтому тцп конект опредлеяется состоянием этих сокетов на двух концах.
тцп конект это труба и две дырки на обоих концах.
и эти дырки могу находится в разных состояниях.

когда у нас конект "установлен" то оба сокета имеют состояием ESTABLISHED


  (комп-А сокет)                   (комп-Б сокет)
  ESTABLISHED -------------------  ESTABLISHED 


так вот посмотрим как у нас закрывается тцп соединеие. 
положим комп-А иницииирует конец конекта. как процесс-А инициирует конец конекта. а вот 
как в самом простом случае он запускает команду

  close(s);

где s это номер сокета. эта команда сообщает ядру что наш процесс дальше болше не желает
ни читать из этого сокета и его сетевого конекта ни писать в этот сокет и сеть.
так как тцп конект составляет два сокета тоесть у тцп конекта два участника то "разьединение"
тцп конекта это несклоько шаговый процесс.
прикол в том что ядро компа-А возвращает управление процессу мгновенно. тоесть с точки зрения
процеса-А тцп конект заканчивается мгновенно. на на самом деле это совершенно не так.
тоесть ядро-А и ядро-Б и процесс-Б еще долго будут ебать себе мозги прежде чем для них 
конект закончится. но повторю для процесса-А тцп конект заканчиыется мгновенно. процесс-А
дает команду close(s) и ядро моментально возвращается из этой функции. и проесс-А дальше
делает свои дела. для него конект "закрылся" закнчился мгновенно. но для ядра-А , ядра-Б и 
процесса-Б тлооько все начинается. 
так вот ядро компа-А шлет FIN пакет. важно что отправляя FIN пакет ядро на компе-А хочет сообщить той стороне тот момент что ядро-А далее больше не планирует ОТПРАВЛЯТЬ пакеты. именно не планирует
отправлять а не принимать. тоесть еще раз FIN пакет по своей сути означает - хей! я больше 
не буду ничего отправлять.
когда пакет покидает комп-А то сокет его приобретает статус FIN-WAIT-1


  FIN-WAIT-1  --------FIN>----------  ESTABLISHED 


комп-Б получает этот пакет. ядро-Б понимает что комп-А больше не будет ничего отправлять.
в ответ на прилетевший FIN ядро-Б  шлет пакет ACK
при этом статус сокета получается CLOSE-WAIT

  FIN-WAIT-1  --------------<ACK----  CLOSE-WAIT

причем насклоько я понимаю согласно той большой диаграмме CLOSE-WAIT наступает не тогда когда ядро приняло FIN а именно когда ядро приняло FIN ПЛЮС отправило обратно ACK.

когда ядро компе-А получает ACK
то ядро на компе-А изменит статус своего сокета на FIN-WAIT-2

(комп-А)  FIN-WAIT-2  ------------------  CLOSE-WAIT (комп-Б)


тут еще один важый момент. ну ядро-Б  поняло что с той стороны больше ничего не прилетит.
но как об этом узнает приложение на компе-Б? так вот у приложения два варианта об этом узнать. само ядро оно никакое уведомление об этом неделает. но если приложение после этого запустит 
команду read() либо recv() то эти функции вернут значение 0 в коде возврата.

  int n = read()
  int n = recv()

вот это n будет равно 0. таким макаром ядро сообщает что конект с той стороной уже оаказывается
закончен. дело в том что если конект еще ESTABLISHED но в сокете пусто (ничего с той стороны
не прилетело) и мы запустим read() или recv() то ядро такую функцию заблокирует. а когда в сокет
чтото прилетит то ядро вернетя из функции и в n вернет число байт которое было перекачано из 
сокета в память процесса. тоесть если в сокете щас ничего нет то ядро не будеи возвращать фнкцию
и n=0. ядро просто заблокирует фнукци до тех пор пока в сокетчтото не прилетит. и потом будет
возврат и n<>0. так что n=0 вернется только в случае если состояние сокета находится
в состоянии CLOSE-WAIT. а это состояние наступает только в случае когда к нам стой стороны прилетел
FIN и ядро на автомате отправило туда ACK. когда ядро отправило ACK то оно ждет от юзер процесса
что процесс каким то макаром догадается что с той стороны уже больше ничего не прилетит и ничего
больше не делает. так вот первый путь как приложение может даогаься что состояние сокета равно
CLOSE-WAIT это то что при вызовве read() recv() мы получаем n=0 и далее наш процесс должен 
проанализировать этот факт. и ДАЛЕЕ наш процесс должен сам после этого в своем коде запустить
либо 

    close(s);

либо 

   shutdown(s, SHUT_RDWR);
   close(s);

первый путь более короткий. второй более длинный. подчеркну то что ядро не заставляет наш 
процесс это делать. но ядро подразумевает что мы так сделаем. ядро после того как послало ACK
и установило статсус сокета в CLOSE-WAIT начинает ждать что процесс догадается о том что произошло
это событие. событие о том что удаенный комп больше не будет нам ничего присылать. (тоесть 
что комп-а инициировал окончание тцп конекта). как процессс может об этом догадаться. при очередном
вызове read() recv() ядро вернет n=0. наш процессс должен на это обратить внимание и по своей
воле вызвать команды что я указал выше. словесный смысл статуса сокета CLOSE-WAIT вот какой - 
он означает что наше ядро получило с той стороны FIN кторый означает что с той стороны комп-А
нам сообщил что он несобирается дальше нам больше ничего не передавать, наш комп отпавил
подрверждение о том что мы получили эту новость, и наше ядро ожидает от процесса ччто процесс
подаст команду о том что процесс тоже больше не собирается посылать данные на ту сторону,тоесть
ждет от процесса знака о том что процесс "закрывает" конект со своей стороны тоже.
но наш процесс необязан делать нихрена. например наш процесс может со своей стороны продолжать
жить как жил. и может например спокойно слать данные по сети на ту стороны. он может это
делать. это можно делать. наш проецесс может и читать данные из сокета. просто читать будет
нечего. читать не запрешено. просто операция чтения будет "блокировться" , тоесть висеть
бесконечно в суспенде. но никакой ошибки не будет. и может отправлять данные по сети.они
будут доолетать до комп-А и там ядро будет их принимать и собирать в буфер.тоесть из за того
что комп-А нам сообщил чтоон нам больше ничего не будет присылать это не значит что мы неможем
отслыть ему данные. можем и еще как можем. 
так вот наш процесс наконец тоже понял что отсылать данные болше не хочет. 
тогда нащ процесс сообщает ядру о том что больше ничего отсылать данные в есть несобирается.это может сделать вот через эти команды


    close(s);

либо 

   shutdown(s, SHUT_RDWR);
   close(s);

ксатти есть еще второй способ как процессу узнать о том что наступил CLOSE-WAIT.
есл у нас есть дескриптор сокета, и мы за ним наблюдаем через например epoll и отслеживаем
событие EPOLLRDHUP (важно отсележитвать именно это событие. есть похожее событие EPOLLHUP
оно про другое. оно про событие что конект был завершен но не потому что та сторона прислала
FIN а из за какойто аварийно причины).   так вот если мы отслеживаем сокет через дескриптор
и через epoll событие EPOLLRDHUP то он при наступлении CLOSE-WAIT на сокете сразу среагирует и вернет нам ивент. событие EPOLLRDHUP  как раз и происодит если статус сокета стал CLOSE-WAIT.
это второй сопособ как процессу узнать что наступил CLOSE-WAIT. ниже я приведу прогармму
которая ловит событие именно через epoll.
первым или вторым способом наше приложение узнало что статсу сокета стал CLOSE-WAIT
далее наше прилодение сделало все дела и тоже решило больше ничего не слать  в сеть. 
и оно сообщает об этом своему ядру через команды

    close(s);

либо 

   shutdown(s, SHUT_RDWR);
   close(s);


здесь я сразу скажу что если предположим процесс-Б запустил команду close(s) то ядро
мгнвоенно возвращается из этой фукнции в процесс-Б и для него тцп конект уже закончился
разорвался. но для ядра-Б и ядра-А это совсем не так. они еще будут продолжать ебать мозги
себе по этому поводу. поговрим об этом.
ядро-Б получает эти команды от процесса и понимает 
что процесс тоже готов закончить конект. что процесс сигнализирует что он тоже больше ничего
не планирует дальше отсылать в сеть. тогда ядро-Б оно шлет пакет FIN на ту сторону
при этом статус сокета становится LAST-ACK


  FIN-WAIT-2  -----------<FIN-------  LAST-ACK


важно еще раз понять что само ядро компа-Б от себя слать этот пакет не будет. ядро это 
делает только по приказу процеса и никак иначе. тоесть если ACK ядро шлет автоматом не спршаивая
порцесс то FIN пакет ядро шлет только по приказу процесса. без спроса без указания от процеса
ядро этого делать небудет.  
так вот ядро-А оно когда получает FIN то оно автоматом (тоесть без разрешения от процесса) в ответ шлет ACK и в этот момент статус сокета становися TIME-WAIT

  TIME-WAIT  --ACK>----------------  LAST-ACK

когда ACK пакет долетает до комп-Б до его ядра, то статус сокета становится CLOSED

  TIME-WAIT  -----------------------  CLOSED


как только на компе-Б статус сокета станоится CLOSED то ядро уничтожает этот сокет и от 
него не остается и следа. а вот на компе-А сокет так и висит в памяти! да для процесс-А
якобы конект давно уже закрыт. для него его не сущетвует. у процесса-А кактолько он 
запустил close(s) то ядро убирает дескриптор s из доступных для этому процессу. но для
самого ядра сокет жив живехонек. так вот в ядре этот сокет продолжает висеть и иметь статус
TIME-WAIT
так вот он будет там висеть. спрашивается а сколько ?  а вот сколько 

	$ cat /proc/sys/net/ipv4/tcp_fin_timeout
	60

по умаолчанию 60 секунд
можно руками поменять. а можно через sysctl

	net.ipv4.tcp_fin_timeout = 30


спрашивается а нахуя он там висит? а смысл вот такой что мы же с комп-А послали ACK
но нам же в ответ уже ничего не прилетит поэтому ядро-А достверно не знает а долетел ли 
пакет или нет. если он недолетел то через какоето время ядро-Б повторит отправку FIN пакета,
мы его получим и повторим отправке ACK пакета. поэтому ядро-А и не уничтожает сокет на случай
недолета пакета. хочу заметить что вот этот висящий на таумауте сокет в статусе TIME-WAIT
будет только на той стороне КОТОРАЯ ИНЦИИРОВАЛА РАЗРЫВ КОНЕКТА. а на той стороне которая вторая
там ничего висящего не будет (при условии что все пакеты долетели). так что если мы видим
на компе в списке сокетов TIME-WAIT строки то можно смело сказать что для этгого конекта 
инцииатором разрыва был наш комп а не вторая сторона. так вот из за того что у на хосте
будут вот такие висяие в воздуже TIME-WAIT сокеты можно словить проблему. щас о ней
погвоорим. а пока вопрос - а кто обычно является инициатором разрыва тцп конекта. я имею 
ввиду если мы воьмем ноутбук то обычно программы на нотубке которые лазиют в сеть то они 
обычно с точки зрения СОЗДАНИЯ конекта являются tcp клиентами. тоесть именно проги на компе
первыми куда то стучат. а сереры в интернете обычно в с этой точки зрения являются tcp серверами.
а вот кто из них обычно первый рвет конект? что касается веб сервером то обычно первыми они
ициниируют разрыв конекта. потому что им нужно после ответа на реквест как можно быстрее
избавиться от конекта и разрузить ресурсы у себя. поэтому когда мы на компе лазиим через 
браузер в интернет то у нас на компе сокетов с TIME-WAIT по идее быть не должно. а вот 
на удаленном веб сервере по идее должно быть дохера сокетов в состоянием TIME-WAIT.
хотя я вот проверил. я сделал куча запросов к жинкс (127.0.0.1:8888) через curl
в итоге куча висящих сокето TIME-WAIT у меня возникла на клиенте а не насервере. 
тоесть именно curl инциирует разрыв тцп конекта


$ ss -4tnpa | grep -E "Recv|TIME-WAIT"
State     		Local Address:Port    Peer Address:Port Process                            
TIME-WAIT 		127.0.0.1:48010       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:48034       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:48052       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:48024       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:47996       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:47988       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:48042       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:57978       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:47972       127.0.0.1:8888                                    
TIME-WAIT 		127.0.0.1:47984       127.0.0.1:8888   

тоесть тут четко видно на основе колонки   "Peer Address:Port Process" что удаленным
пиром был 127.0.0.1:8888  тоест наш жинкс. тоесть это TIME-WAIT сокеты именно с клиенсткого
компа. именно клиент curl инициировал разрыв тцп конектов


итак к настоящему моменту я рассмотрел важные вопроы:
  1) как происходит разрыв конекта и как менятся при этом статус на сокетах
  2) как процессу об этом узнать
  3) как появляются висящие TIME-WAIT сокеты и с какой стороны


на компе есть таблица в которой есть инфо об сокетах. это 

$ cat /proc/net/tcp | head -n2
sl  local_address rem_address   st tx_queue rx_queue tr tm->when retrnsmt  uid  timeout inode                                                     
0: 3500007F:0035 00000000:0000 0A 00000000:00000000 00:00000000 00000000   996  0 16246 1 00000000746bcba6 100 0 0 10 5                     

каждая строчка она именно берется из каокгто сокета. таблица основвыется на сокетах.
столбик inode он показывает как раз номер иноды сокета. 
в эту таблицу лазить неудобно. ее трудо дешифровать поэтому удобнее тоже самое смотреть
через 

   $ ss ...


но чисто для понимания это надо знать. так как каждая строка это инфо из сокета то это значит
что в этой таблице нет инфомрации о тцп конектах которые транзитные. об этих конетах нужно
смреть инфо в таблице conntrack 

  $ conntrack -L

также замечу что ss он черпает инфо о сокетах не путем чтения из /proc/net/tcp 
а через netlink но об этом щас не будем углублвться

так вот я возвращаюсь теперь к вопросу когда и как могут возникнуть проблемы из за этих 
висящих на тайм ауте сокетах TIME-WAIT
так вот сокет он с точки зрения ядра имеет как минмиум 5 парамтров. первый параметр 
это протоколы которые в нем сидят. для простоты скажу что это либо ipv4+TCP
либо ipv4+UDP. а далее идет 4 числа 

   local_IP     local_port    remote_ip   remote_port

итого 5 параметров

ipv4+TCP/UDP     local_IP     local_port    remote_ip   remote_port


так вот у ядра неможет быть два сокета у которых все эти 5 параметров одинаковые.
но даже еще хуже - при создании сокета когда мы пытаемся через bind() записать в него
вот эти два параметра 

   local_IP     local_port

а именно этим bind() и заниматеся то ядро проверяет а нет ли уже на компе сокетов 
у которых есть точно такие же две колонки. если есть то ядро пошлет нас нахер при запуске bind()
нет ну на самом деле немножко нетак. ядро позволяет иметь несклько сокетов с одинаковыми 
двумя параметрами 

   local_IP     local_port

но толкьо в отдельном случае. это разрешается если у нас все эти сокеты это слушающий сокет
и его клоны. показвыаю

$ ss -4tnpla | grep -E "Recv|9191"
State  Recv-Q Send-Q Local Address:Port  Peer Address:Port Process                                                           
LISTEN 1      1            0.0.0.0:9191       0.0.0.0:*     users:(("nc",pid=2272594,fd=3))                                  
ESTAB  25     0          127.0.0.1:9191     127.0.0.1:55534                                                                  
ESTAB  0      0          127.0.0.1:9191     127.0.0.1:49856 users:(("nc",pid=2272594,fd=4))                                  


как видно у нас три сокета. один слушающий и два сокета уже с конкретными конектами
которые явлются его клоном. и все три сокета имеют одинаковую колонку
		
		Local Address:Port
		127.0.0.1:9191


такое разрешается. все дело в том что эти сокета которые не слушающий а другие мы их создали
не руками а их автоматом создало ядро через наш accept().  поэтому все эти три сокета 
могу сущестовавать.
но такая ситуация невозможна вот в каком случае. вот у меня есть на компе сокет неважно
с каким статусом 

$ ss -4tnpla | grep -E "Recv|9191"
State  Recv-Q Send-Q    Local Address:Port  Peer Address:Port Process                                                           
ESTAB  25     0          127.0.0.1:9191     127.0.0.1:55534                                                                  

вот у  этого сокета эта колонка

	Local Address:Port
	127.0.0.1:9191


так вот ЕСЛИ Я СОЗДАМ еще один  сокет РУКАМИ. и попробую в него через bind() 
заснууть точно такие параметры

	Local Address:Port
	127.0.0.1:9191

то ядро меня пошлет нахер.
чаще всего на практиике такая ситция получается если у нас есть какойто tcp сервер
для него мы создаем сокет руками. потом мы в него записываем параметры local ip+port через bind()
потом мы делаем его слушающим через listen()
например мы туда записвыаем 

	Local Address:Port
	127.0.0.1:9191

если у нас сокет только слушающий то у токого сокета у него небывает разных состояний (таких 
как close-wait, fin-wait-1 итп) у него бывает тлоько состояние LISTEN и все. и если я программу
закрываю то ядро этот сокет УБИВАЕТ МГНОВЕННО. тоесть конкртено с ним проблем нет! он 
в памяти висеть не остается. НО! если мы принмиаем конекты то у нас же но основе этого сокета
будут наклонированы новые другие сокеты у которых будет такой же самый


	Local Address:Port
	127.0.0.1:9191

так вот эти сокеты они уже просто так из памяти неисчезнут при закрытии процесса. 
если у нас конекты будуут разрываться со стороны удаленого пира то они скорее всего да - они
тоже тогда умрут быстренько. мгнвоенно. он прйодут через стадии CLOSE-WAIT и CLOSED и ядро
их мгновенно уничтожит. НО! если у нас разрыв конекта инциирует наш процесс то тогда 
все эти сокеты зависнут в состоянии TIME-WAIT. щас покажу как это выглядит

 $ ss -4tnpa | grep -E "Recv|8080"
State     Recv-Q Send-Q Local Address:Port  Peer Address:Port Process                                                           
LISTEN    0      5          127.0.0.1:8080       0.0.0.0:*     
TIME-WAIT 0      0          127.0.0.1:8080     127.0.0.1:38142                                                                  
TIME-WAIT 0      0          127.0.0.1:8080     127.0.0.1:52656    


далее я закрываю процесс. и у нас LISTEN сокет мновенное ядро его унитожает.
и остаются висеть эти два сокета. будут они висеть как я покзаал выше 60с

 $ ss -4tnpa | grep -E "Recv|8080"
State     Recv-Q Send-Q Local Address:Port  Peer Address:Port Process 
TIME-WAIT 0      0          127.0.0.1:8080     127.0.0.1:38142                                                                  
TIME-WAIT 0      0          127.0.0.1:8080     127.0.0.1:52656    


и когда я запускаю программу которая tcp сервер еще раз то я получу на экране
ошибку

		не удалось сделать bind: Address already in use

потому что моя прогамма создает сокет (с этим нет проблем). но далее я делаю 
bind(127.0.0.1:8080)
тоесть пытаюьс записать в этот сокет вот эти параметры 

		 Local Address:Port
         127.0.0.1:8080

но у нас уже есть два сокета которые имеют такой же параметр. и ядро недает записать 
в сокет такие же параметры. почему? ну типа чтобы не было путаницы с конектами. хотя 
я не вижу никакой прблемы. так как у сокета помима парамтеров

		 Local Address:Port

есть же еще параметры

		Peer Address:Port

поэтому мы ненарушаем требование к уникальности параметров в сокете.
но видимо ядро расуждает так что а вдруг мы после того как записали данные в сокет через bind()
захотим его сделать слушающим через listen() и вдруг на те сокеты которые висят в TIME-WAIT
прилетят из сети пакеты и получается что как бутто те сокеты являются клонами нашего нвоого
слушающего сокета и тогда навеное то что прилетело из сети нужно пихать на процесс. но 
наш процесс неимеет отношения  к тем сокетам. вобшем короче говорят хуй знает но ядро 
недаст выполнить bind()
НО! это можно обойти. можно после создания сокета но преед тем как вызывать bind() 
можно в сокет записать параметр 

		SO_REUSEADDR 

он приводит к тому что то что было нельзя становится можно. а если из сети все таки приетают
пакеты на сокеты которые в состоянии TIME-WAIT то ядро с этими пакетами там както само разбирается
и в процесс который имеет слушающий сокет их не доставляет.
в коде на си задание этого параметра выглядит вот так


    s = socket(AF_INET, SOCK_STREAM, 0);
    int reuseaddr = 1;
    setsockopt(s, SOL_SOCKET, SO_REUSEADDR, &reuseaddr, sizeof(reuseaddr));


таким образом я описал то как сокеты в состоянии TIME-WAIT могут стать проблемой.
и то как это скажем так можно обойти через  SO_REUSEADDR 
также хочу отметить что в целом неважно какой статус у сокет будет ли он TIME-WAIT 
или LISTENING или другой. главное какой у него  		 

		Local Address:Port
        127.0.0.1:8080

так вот если мы создали еще один сокет и  в него через bind() пытается засунуть 
такое же занчение  127.0.0.1:8080 то ядро пошлет нахер. тоесть как яуже сказал
если у нас висят сокеты в состяонии TIME-WAIT

State     Recv-Q Send-Q Local Address:Port  Peer Address:Port Process                                                           
TIME-WAIT 0      0          127.0.0.1:8080     127.0.0.1:38142                                                                  
TIME-WAIT 0      0          127.0.0.1:8080     127.0.0.1:52656    


то мы не сможем создав новый сокет руками в него через bind засунуть 127.0.0.1:8080
но также если у нас на компе есть уже сокет с сотоянием LISTEN

State     Recv-Q Send-Q Local Address:Port  Peer Address:Port Process                                                           
LISTEN    0      5          127.0.0.1:8080       0.0.0.0:*     

то тоже самое. мы не сможем создав еще один сокет руками сунуть в него через bind() 
127.0.0.1:8080

так вот через опцию SO_REUSEADDR  мы можем обойти этот запрет если у нас висят сокеты
в состоянии TIME-WAIT. но нам эта опция не поможет если у нас висит уже сокет в состоянии
LSITEN. но на этот случай  у нас есть другая опция это 

		SO_REUSEPORT

она позволяет создать новый сокет с таким же 

	Local Address:Port
	127.0.0.1:8080

даже если у нас уже есть такой же сокет со статусом LISTEN
обычно опцию SO_REUSEPORT используют если мы хотим создаьт второй сокет который тоже 
будет иметь слушающий статус и сидеть на том же IP и порту. хотя мы необязаны делать
его именно слушающим. формально эта опция нам всего навсего позволяет чрез bind() заснут
в сокет вот эти параметры

	Local Address:Port
	127.0.0.1:8080

не смотря на то что на компе уже есть сокет с такими параметрами и статус того сокета LISTEN
повторюсь мы свой новый сокет не обязаны вводить в состояние LISTEN. 
на практике обычно этот сокет всеже вводят тоже в состояие LISTEN. мы получаем на компе
два сокета слушающих сидящих на одном ип и порту. нахер это надо? если у нас есть два
процесса скажем два воркера жинкс. то можно один процесс посадить наодин слушающий сокет
а вторйопроцесс посадить на второй слушающий сокет. и тогда ядро будет складывать входящие конекты
то  в один сокет то в другой. а каждый процесс будет черпать эти новые конекты из своего 
личного сокета. им неужно будет конфликтовать за доступ к слашующему сокету.
вот нахуя это делается на практкие.

таким макаром я рарасотрел на хуя нужны опции 

		SO_REUSEADDR 
		SO_REUSEPORT



на практике как можно по быстрому смоделировать ситуацию чтобы у нас на компе 
стало много TIME-WAIT сокетов.
в одном терминале мы запускаем

  $ ./358.exe

а потом во второмтерминале запускаем несколько раз 

  $ echo "123"  | nc localhost 8080


тогда мы получим вот такое

 $ ss -4tnpa | grep -E "Recv|8080"
State      Recv-Q Send-Q Local Address:Port  Peer Address:Port Process                                                           
LISTEN     0      5          127.0.0.1:8080       0.0.0.0:*     
TIME-WAIT  0      0          127.0.0.1:8080     127.0.0.1:53080                                                                  
TIME-WAIT  0      0          127.0.0.1:8080     127.0.0.1:53060                                                                  
TIME-WAIT  0      0          127.0.0.1:8080     127.0.0.1:53074                                                                  
TIME-WAIT  0      0          127.0.0.1:8080     127.0.0.1:53090                                                                  
TIME-WAIT  0      0          127.0.0.1:8080     127.0.0.1:53096   

мы поулили кучу сокетов TIME-WAIT у которых 

		Local Address:Port
		127.0.0.1:8080

если я повторно запущу 358.exe и если  бы таам не было параметра SO_REUSEADDR
то меня бы bind() послал бы нахер с ошибкой

   bind error ip address already in use.

но так как я там исползую эту опцию то ошибки при повторно запуске прогаммы не будет.
еще раз подчеркну при каком услвии нам нужна эта опция  - если  унас программа это tcp 
сервер. тоесть  унее есть слушающий сокет. и если она сама инциирует разрыв тцп конектов.
если она инициатоор разрвывов. то если мы закроем программуи и сразу ее запустим обратно
то мы обязательно полуичим ошибку

   bind error ip address already in use.

если только мы в теле прогаммы не будем юзать опцию   SO_REUSEADDR


так об этом поговорили.


походу пьесы скажу о том что утилита nc у нее есть опция -q которая выглядит так что 
она задает некий тааймаут и в течение этого таймайта nc незакрыавет свою работу. я думал
что  в теение этого таймаута у нас незакрывается tcp конект. оакзывает нет. -q на это 
не вляет. -q влияет на то что незакрывается сама прогамма сам процесс. а что там с тцп 
конектом этоотдеьная история.  если мы задаем -q 20 то процесс  задает хендлер для
сигнала SIGALARM. и ядро через 20 секунд присылает процессу сигнал SIGAALARM. проецесс
его принимает и уже тгда закрыается. так что -q неимеет отошения к  тайаумуту сколько вреени
nc держит тцп конект откртым. 
если мы юзаем nc вместо с пайпом

  $ echo "123" | nc .....

то nc азкрывает tcp конект как только разыавется пайп. 
есди же я шлю в nc байты в интеративном ржеиме

  $ nc ....

то тцп конект будет закрыть тода когда мы намжмем Ctrl+C

nc ксати если мы его запускаем в качестве сервера то он исползует обе опции

 $ strace -e setsockopt nc -k -l 9292
setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
setsockopt(3, SOL_SOCKET, SO_REUSEPORT, [1], 4) = 0



ТЕПЕРЬ
поговорим отдельно про    shutdown(s, SHUT_RD/SHUTWR/SHUT_RDWR);
если я пишу 


		shutdown(s, SHUT_RD)

то это всего навсего мой процесс сообщает местному ядру что процесс далее больше не 
собирается читать из сокета. и просит ядро запретить доступ на чтение к сокету. если
процесс попроует проичитать из сокета ядро его пошлет нахер.
при этом в сеть ничего нешлетс. поэтому вторая сторона не подозревает об этом.
если втора сторона будет слать нам байты то они просто будут складаываться в буфер в ядре
в сокете. но прочитат мы их уже не сможем.

если я пишу 


		shutdown(s, SHUT_WR)

то этим я сообщаю ядру что мой процесс далее больше не собрается ничего писать в сокет.
и мы просим заарпрерить ядру писать нам чтото в сокет. при этом ядро пошлет в сеть FIN 
пакет соощая той стороне что с нашей стороны уже процесс ничего писать в сеть не будет (потому
что в сокет процесс писать ничго боьше не будет). соосвтенно дальше будут все те последствия
которые происходят при отправке FIN пакета. фактически это означает что со своей стороны
мы инииицировали разрыв конекта.  при этом наш процесс может читать из сокета при условии
что в сокет чтото прилелтит. 


		shutdown(s, SHUT_RDWR)

этим мы сообщаем ядру что мы не собаремся дальше ни читать ни писать из сокета. поэтмоу 
ядро нам запретит и читать и писатть из сокета. и пошлет в сеть FIN пакет. 
при этом у нас по факту дескриптор сокета по прежнму будет открыт.проптсо мы уже с сокетом
ничего не можем сдеалать как только закрыть. команд которые бы обратно позволяли "разблокировать"
сокет - таких команд нет.

команда 

  close(s)

она нетолько удаляет дескриптор сокета из нашего процесса. она еще в себя автоматом включает
команду 


		shutdown(s, SHUT_RDWR)

тоесть когда мы запускаем 

  close(s)

то у нашего процесса удаляется дескриптор сокета. но сам сокет не удаляется. а вместо этого
ядро как бы автоматом запускает команду

		shutdown(s, SHUT_RDWR)

соотвесвтеннно далее сокет будет удален тоолько тогда когда по мнению ядра конект полностью
завершен. тоесть толкьь тоода кода сокет передйет в состяоние CLOSED.

есди мы хотим заверщить tcp конект, удаить сокет из ядра и удалить дескриптор сокета из 
нашего процесса то можно делать двумя путями либо 

   close(s)

либо 

   shutdown(s, SHUT_RDWR)
   close(s)

по сути считай это одно итоже.
хотя второй вариант наверное более помягче.



следущий момент. 
выще я описывал два способа как процессу узнать о том что сокет пеерешел в состояние 
CLOSE-WAIT который для нашего процесса ознаает то что процесс на стороне нам сообщил
что он нам ничего больше не собиратеся преередваь. что  в свою очредь означает что оттуда 
нам ждать уже нечего. что свою очреедь означает намек на разрвыв содеиения. 
так вот один из способов нашему процессу это зафикирирвать это словаить EPOLLRDHUP 
событие через epoll для десиптора который ведет в сокет.
так вот я написал две прграмы. 377.с и 378.с




 $ cat 377.c
#include <stdlib.h>
#include <string.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <unistd.h>
#include <sys/wait.h>
#include <sys/epoll.h>
#include <stdio.h>
#include <errno.h>
#include <fcntl.h>
#include <semaphore.h>
#include <sys/sem.h>
#include <sys/ipc.h>
#define MAX_EVENTS 3


int epfd = -1;  // Глобальная переменная для epoll-дескриптора
pid_t s  = -1;  // перемннная для сокета



int is_fd_valid(int fd) {
    // Проверяем, является ли дескриптор файла валидным
    if (fcntl(fd, F_GETFD) == -1) {
        if (errno == EBADF) {
            // Дескриптор не существует или недействителен
            return 0;  // Дескриптор не существует
        } else {
            // Произошла другая ошибка
            perror("fcntl");
            return -1;  // Ошибка при проверке
        }
    }
    // Дескриптор существует
    return 1;
}





void handle_sigint(int sig) {

    const char *message = "Получен сигнал SIGINT. Завершаем работу...\n";
    write(STDOUT_FILENO, message, strlen(message));  // Используем write вместо printf


    const char *message1 = "закрываем epfd...\n";
    write(STDOUT_FILENO, message1, strlen(message1));  // Используем write вместо printf


    // Закрываем epoll-дескриптор, если он открыт
    if (epfd != -1) {
        close(epfd);
    }


    // закрыаем сокет
    close(s);
    
    exit(0);  // Завершить процесс с кодом успеха

}












int main() {

    
    // Устанавливаем обработчик для SIGINT
    struct sigaction sa;
    sa.sa_handler = handle_sigint;  // Функция-обработчик
    sigemptyset(&sa.sa_mask);       // Не блокируем другие сигналы
    sa.sa_flags = 0;                // Без дополнительных флагов

    if (sigaction(SIGINT, &sa, NULL) == -1) {
        perror("Не удалось установить обработчик SIGINT");
        exit(EXIT_FAILURE);
    }









    int c;
    int reuseaddr = 1;
    struct sockaddr_in addr;
    socklen_t addr_len;

    s = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
    if ( s < 0 ){
        perror("не удалось создать сокет");
    };
    setsockopt(s, SOL_SOCKET, SO_REUSEADDR, &reuseaddr, sizeof(reuseaddr));

    addr.sin_family = AF_INET;
    addr.sin_port = htons(9090);
    addr.sin_addr.s_addr = inet_addr("127.0.0.1");

    addr_len = sizeof(addr);

    if ( bind(s, (struct sockaddr *)&addr, sizeof(addr)) < 0  ){
         perror("не удалось сделать bind");
    };
    
    listen(s, 5);










    // Создаем epoll инстанс
    epfd = epoll_create1(0);
    if (epfd == -1) {
        perror("epoll_create1");
        exit(EXIT_FAILURE);
    }


    // Создаем две переменные одну для сования дескриптороа в ядро
    // а вторую для получения ответа из ядра
    struct epoll_event event;
    struct epoll_event events[MAX_EVENTS];


    // Добавляем дескриптор сокета  в еполл инстанс  плюс  EPOLLIN 
    event.events = EPOLLIN | EPOLLET;
    event.data.fd = s;  // Файл
    if (epoll_ctl(epfd, EPOLL_CTL_ADD, s, &event) == -1) {
        perror("epoll_ctl: file_fd - EPOLLIN");
        exit(EXIT_FAILURE);
    }





for (int j=0; j<2; j++){
    	    int n = epoll_wait(epfd, events, MAX_EVENTS, -1);  // Блокируемся на события
    	    if (n < 0) {
        	 if (errno == EINTR) {
                // Если вызов был прерван сигналом, просто продолжаем цикл
                //continue;
                };
    	    perror("epoll_wait");
    	    exit(EXIT_FAILURE);
    	    }



    	    for (int k = 0; k < n; k++) {

    	        // обработка слушающего сокета
        	if (  (events[k].data.fd == s) && (events[k].events & EPOLLIN)   ){
            	    c = accept(s, (struct sockaddr *)&addr, &(addr_len) );
            	    if (c == -1) {
                        perror("Ошибка при принятии соединения");
                        return 1; 
                        } 
		printf("входящее содеиениени зафиксировано! дескрипотор = %i \n", c);
                    event.events =  EPOLLRDHUP | EPOLLET;
                    event.data.fd = c;  // Файл
                    if (epoll_ctl(epfd, EPOLL_CTL_ADD, c, &event) == -1) {
                          perror("epoll_ctl: file_fd - EPOLLIN");
                          exit(EXIT_FAILURE);
            	    }


                }; //if END
               
  
                // обработка рабочего сокета
                if (  (events[k].data.fd != s) && (events[k].events &  EPOLLRDHUP)   ) {
                     int result = is_fd_valid(events[k].data.fd);
                     if (result == 1) {
                           printf("получено событие о том что удаленный пир закрыл конект\n");
                           printf("удаяляю Дескриптор %d из epoll\n", events[k].data.fd);
                           if (epoll_ctl(epfd, EPOLL_CTL_DEL, events[k].data.fd, NULL) == -1) {
                                   perror("epoll_ctl: EPOLL_CTL_DEL");
                                   exit(EXIT_FAILURE);
                           };

                           printf("жду 30 с и закрываю сокет\n");
                           sleep(30);
                           if (shutdown(events[k].data.fd, O_RDWR) < 0 ){
                    		    perror("shutdown\n");
                    	   };
                    	   close( (events[k].data.fd) );
                    	   printf("дескрипттор %d закрыт\n", events[k].data.fd);
                     } else if (result == 0) {
                            printf("pid = %i, Дескриптор %d не существует.\n", getpid(), events[k].data.fd);
                            break;
                     } else {
                                printf("Произошла ошибка при проверке дескриптора %d.\n", events[k].data.fd);
                                break;
                     }


                };  //if END



            }; //for END

}; //END FOR




           // зарвыаем еполл инстанс
             if (is_fd_valid(epfd) == 1) {
                close(epfd);
              } else {
                  printf("Дескриптор %d уже закрыт.\n", epfd);
              }
      
            // закрываем сокет
             if (is_fd_valid(s) == 1) {
                close(s);
              } else {
                  printf("Дескриптор %d уже закрыт.\n", s);
              }



};







$ cat 378.c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <unistd.h>
#include <arpa/inet.h>



int main() {
    int sockfd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
    if (sockfd < 0) {
        perror("socket");
        exit(1);
    }

    struct sockaddr_in addr;
    memset(&addr, 0, sizeof(addr));
    addr.sin_family = AF_INET;
    addr.sin_port = htons(9090);
    if ( inet_aton("127.0.0.1", &addr.sin_addr) == 0 ){
           perror("aton error");
           return 1;
    };
   if ( connect(sockfd, (struct sockaddr *)&addr, sizeof(addr))  <0 ){
          perror("connect error \n");
          return 1;
   };
   printf("TCP Client running \n" );
   printf("сплю 5с\n");
   sleep(5);
   printf("запускаю shutdown\n");
   if ( shutdown(sockfd, SHUT_RDWR) < 0 ){
         perror ("shutdwn error");
         return 1 ;
   }
   sleep(5);
   close(sockfd);
   printf("close(socket) выполенне\n");
   sleep(90);
   return 0;
}







в 377.c я создаю слушающий сокет.
в 378.с я инициируую конект. апотом чрез кророткое время я инциирую его закрытие. 

получив конект в 377.с   я создаю новый сокет через accept и вставляю его номер дескриптора
в epoll. и потом я наблюдаю за ивентами на этмо сокете и я ловлю ивент EPOLLRDHUP 
и кода он наступает то  я пишу об этом на экране. жду 30 с чтобы можно было посмтреть
что творится в ss. и потом я уичтожаю сокет через close(s).  в ss я должен увидеть 
что в моем процессе 377.exe мой сокет имеет статус CLOSE-WAIT
напмоню статус CLOSE-WAIT означет что разыр конекта инициация произошла по вине
удаленной стороны. и она нам прилала FIN пакет. наше ядро его приняло , на автоматом 
оптпавло обратно ACK пакет. и теперь наше ядро ждет чтобы нащ процесс сказал ядру что
он бооольше не собирается ничего посылать удаленному пиру , тоест чтобы наш процесс 
со своей стороны согласился с разраыванием конекта. 

запускаю 377.exe и 378.exe

на экране увижу вот акое

./377.exe 
входящее содеиениени зафиксировано! дескрипотор = 5 
получено событие о том что удаленный пир закрыл конект
удаяляю Дескриптор 5 из epoll
жду 30 с и закрываю сокет
дескрипттор 5 закрыт

когда на экоане выелезет строка "жду 30 с и закрываю сокет"
то в ss можно увидеть воттакое

 ꁳ  $ ss -4tnpa | grep -E "Recv|9090"
State      Recv-Q Send-Q Local Address:Port  Peer Address:Port Process                              
LISTEN     0      5          127.0.0.1:9090       0.0.0.0:*     users:(("377.exe",pid=2285883,fd=3))
CLOSE-WAIT 1      0          127.0.0.1:9090     127.0.0.1:41142 users:(("377.exe",pid=2285883,fd=5))
FIN-WAIT-2 0      0          127.0.0.1:41142    127.0.0.1:9090  users:(("378.exe",pid=2285888,fd=3))

тоесть 377.exe сокет имеет состяоние CLOSE-WAIT потому что 378.exe инциировал разрыв
конекта и послал FIN. наше ядро ответило обратно ACK.  в частноти из за этого состояние
сокета у 378.exr равно FIN-WAIT-2
пртмру что состояние FIN-WAIT-2 означае то что 378.exe инциировал разрыв, ядро послало FIN,
обратно прилеетел ACK. это порождает FIN-WAIT-2
тоесть все как по учебинику


когда все программы закончат свою жинь то ss покажет
  $ ss -4tnpa | grep -E "Recv|9090"
State      Recv-Q Send-Q Local Address:Port  Peer Address:PortProcess                             
TIME-WAIT  0      0          127.0.0.1:41142    127.0.0.1:9090                                    

это у нас отсался висеть сокет от 378.exe котоыр до этого висел в статусе FIN-WAIT-2
потому что кода программа 377.exe тоже со совей стороны послала FIN (при этом статус ее 
сокета изменился с CLOSE-WAIt на LAST-ACK) то 378.exe это 
принял отвправл обратно ACK. при этом его статус сокета стал TIME-WAIT и этот сокет
будет висеть еще 60с в ядре прежде чем яжро его удалит. а та сторона коорая имела LAST-ACK
она кооагда получила ACK то ее статус стал CLOSED и в этот момент ядро удалило тот сокет.
поэтмоу в итоге остался висеть один сокет. также при закрытии 377.exe ее слушающий LISTEN 
сокет ядро удалило мгнвоенно. ну точнее не при закрытии прогарммы а из за того что в 
коде есть команда close(s) она ведь просит закрыь именно слушающий сокет.



ниже кнонтент на туже тему только несколкьо более старый
вот у нас между компом-А и компом-Б установлен tcp конект. неважно кто был инициатором.
после создания конекта оба компа становятся абсолютно равнозначными.
так вот далее начинается очень важная хрень. Положим у нас комп-А имеет слушающий сокет.
тоесть он выступает при создании соединения сервером тоесть пассивной хренью. тоесть
конект инцииурется от компа-Б. я уже сказал что якобы неважно кто инициирует конект - да
это в целом неважно но для более легкого понимания я покажу на примере компа который
при создании конкета явлется сервером. и какая проблема там может возникнуть. эта же
проблема может возникнуть и на компе которй инцииатор(тоесть клиент) но это рассмтотрим потом
там это реже возникает хотя тоже вомзожно. просто легче поймать проблему о которой пойдетречь
именно на tcp сервере хотя в целом это неважно! так вот соединение установлено. теперь
у нас одна из сторон начинает гасить конект. очень важно оказывается кто это делает. 
в нашем примерер инцииатором закончить конект будет комп-А (который tcp сервер).
так вот у нас tcp конект характиеруеизется состоянием двух сокетов. сокет на компе-А
и сокет на компе-Б. так вот комп-А шлет FIN пакет на комп-Б
как только комп-А из себя высрал FIN пакет то статус сокета у него становится равным
FIN-WAIT-1


  комп-А (FIN-WAIT-1)--------FIN------>комп-Б

какой статус на компе-Б когда он плучает пакет щас неважно.
в ответ комп-Б шлет пакет ACK+FIN
флагом ack он подтвеждает что получил пакет с флагом fin а флагом fin он показывает что 
тоже со совей стороны закончил соединение. 


  комп-А (FIN-WAIT-1)<--------FIN+ACK-----комп-Б


когда этот пакет влетает на комп-А то у него статус сокета прерващается в FIN-WAIT-2


  комп-А (FIN-WAIT-2)---------------------комп-Б


в ответ комп-А шлет пакет ACK. этим он потдтвеждтае что получил полсанный ему пакет
как только комп-А высрал ack пакет в сеть то статус сокета становится TIME-WAIT

  комп-А (TIME-WAIT)------ACK------------->комп-Б

 

статус сокета на компе-Б становится CLOSE.
на этом обмен пакетами заканчиается.
так как на компе-Б статус сокета становится CLOSE то я не знаю удаляет ли его ядро
из памяти сразу. но по крайней мере сокет удаляется из списка так назыамых активных сокетов.
а это значит что сокета уже как бы нет. тоесть ядро не анализирует его контент при принятии решений при создании нвоых сокетов. дело в том что новый сокет при создании может конфликтовать
с другим старым сокетом и ядро это проверяет. так вот сокет в состоянии CLOSE его уже не
видно в ss. потому его нет в спсике активных сокетов. поэтому его уже как бы и вообще нет.
в таблцие netfuleter вроде бы еще можо видеиь .вот пример


$ sudo conntrack -L | grep -i close
tcp      6 1 CLOSE src=172.16.10.1 dst=172.16.10.11 sport=55434 dport=2080 src=172.16.10.11 dst=172.16.10.1 sport=2080 dport=55434 [ASSURED] mark=0 use=1

тоесть конект в стаблице есть. но такой сокет его уже ненайти в ss выводе.

тоесть в итоге считай что на компе-Б сокет удален из памяти. а вот на компе-А нихуя!
на компе-А сокет еще будет висеть в памяти в списке активных сокетов. хотя достоверно
уже считай что известно что и наш комп тому сообщил что конекта больше нет. и тот комп
нам собщил что у нено тоже корнекта больше нет. зачем же тогда на компе-А висит это сокет?
а висеть он будет порядка 2 минут. есть ключ в /proc котоырй регулирует время таймауте.
так нахуй он висит? а он висит потому что мы отправили на комп-Б ACK пакет но нам соверешенно
незисветно долетел ли он илли нет. потому что ответа неподразумевается в протоколе tcp
поэтому наслучай если этот пакет потерялсяи комп-Б будет слать повторно свой FIN-ACK
то на компе-А сокет и висит в ядре. а если от компа-Б за 2 минуты нчего не прилетит значит
наш комп считает что пакет доолеетли и тогда только ядра удалеяет наконец сокет.
так вот здесь я замечу суперважную вещь. что в итоге сокет зависает в памяти компа только на
компе который инциировал разрыв конекта. тоесть если бы комп-Б ицниировал разрыв то у нас
на компе сокет бы получил статус CLOSE и ядро бы его нахрен удалило. память была бы чиста.
а на комп-Б висел бы сокет в статусе TIME-WAIT 2 минуты. поэтому по окончании работы tcp
конекта у нас сиутация на обоих концах неодинаковая на сокетах. тот кто иницирует разырвыв
конекта тот в итоге и страдает. такого нет что оба сокета висят в состоянии TIME-WAIT.
если мы видим на компе в ss сокеты в состоянии TIME-WAIT то это значит что разрыв tcp конекта
был именно с нашей стороны. мы были инциаторами этого.
вот пример с компа

$ ss -tnla  | grep -E "Recv-Q|TIME-WAIT"
State     Recv-Q Send-Q Local Address:Port  Peer Address:Port
TIME-WAIT 0      0        172.16.10.1:44390 172.16.10.11:22  

тоесть эта строка на симгнализирует что неважно кто инциировал этот конект хотя тут 
это очеивдно . так как в колонке Peer Address:Port мы видим порт 22 то очевидно что инцииролваи
конект мы стучась на ssh сервер. но вцелом неважно кто инциировал конект. хотя опятьже
если мы сидим на нуотбуке то обычно именно наш хост и иницирует 99% всех конектов. 
так вот важно другое. что раз статус этого сокета TIME-WAIT то мы можем точно утвержать что
иниициировали разрыв ИМЕННО МЫ.
так что это вот такое метаасообеие если мы видим сокет в стаусе TIME_WAIT

так вот теперь в чем пизда такого сокета.
мы видим что у каждого ip сокета есть столбики

    Local Address:Port      Peer Address:Port


и у сокета в состянии TIME-WAIT есть тоже такие столбики. 
ксатти если у нас есть слушающий сокет то если мы програму закрываем то ядро такой сокет
сразу вычищает из памяти. с ним пролем нет. он умирает мгновенно. статусы типа ESTABLISHED,
FIN-WAIT-*, TIME-WAIT имеют только tcp сокеты которые именно оабрабатвыали конкретный конект
тоесть чрез этот сокет шла прокачака бвайтов.

так вот если мы на компе запускаем программу в которой мы запусккаем функцию bind()
обычно эут фкунуию запускают если мы хотим создат на компе слушающий сокет. но это необязательно.
вцелом bind() может запускать ии при исходящий конектах но это редкость. так вот в любом
случае если мы запускаена компе bind() то в нем мыуказываем IP+port и эта хрень в итоге
этой фнукцией записывается в новый сокет. НО! это произодет тлоько если у нас на космпе
в списке акивных сокетов нет сокета у которого в колонке 

      Local Address:Port

такого же параметра как мы указываем при запуске bind()
например  вот у нас на компе есть


$ ss -tnla  | grep -E "Recv-Q|TIME-WAIT"
State     Recv-Q Send-Q Local Address:Port  Peer Address:Port
TIME-WAIT 0      0        172.16.10.1:44390 172.16.10.11:22  

значи получается у нас у этого сокета колонка

   Local Address:Port = 172.16.10.1:44390


если я попробую запустить bind(172.16.10.1:44390) то ядро мне выдаст ошибку!
