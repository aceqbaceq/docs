| zfs
| заметки


заметки исследования про зфс

у меня ест датасет с рекордсайз 1М
на нем есть файл размером 10ГБ


вот его структура


        # zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-1024K 3  |  head -n 30

Dataset boot-pool/ds2-1024K [ZPL], ID 381, cr_txg 407, 10.4G, 9 objects, rootbp DVA[0]=<0:1adafd1000:1000> DVA[1]=<0:60028f4000:1000> [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=11235L/11235P fill=9 cksum=1320a58d0b:33cb3413c4ee:4acb2472b10ba9:4c7ddf31b5ad20ce

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
         3    3    32K     1M  10.0G     512    10G  100.00  ZFS plain file
                                               168   bonus  System attributes
	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED 
	dnode maxblkid: 10239
	path	/fio.dat
	uid     0
	gid     0
	atime	Tue Dec  9 15:45:12 2025
	mtime	Wed Dec 10 10:42:59 2025
	ctime	Wed Dec 10 10:42:59 2025
	crtime	Tue Dec  9 15:45:12 2025
	gen	6199
	mode	100644
	size	10737418240
	parent	34
	links	1
	pflags	40800000004
Indirect blocks:
               0 L2   0:2704791000:1000 8000L/1000P F=10240 B=8372/8372 cksum=cb1fb480d7:252a6deb756a8:38a66478db7e6c5:bb4f6fea38bfbe17
               0  L1  0:ac0fb00000:2000 8000L/2000P F=256 B=8371/8371 cksum=27fcead258f:a70dad83f25f1:1baa06e692470493:d34e3737e3ccc6c5
               0   L0 0:6f5a5a000:100000 100000L/100000P F=1 B=8355/8355 cksum=112aec52e2293:2565151540b084c7:b82f6cfe48c5c81c:318c9a8059033ce3
          100000   L0 0:84696d6000:100000 100000L/100000P F=1 B=8365/8365 cksum=112af452e2293:2566153540b084c7:dc5450e48c5c81c:372247e059033ce3
          200000   L0 0:8481635000:100000 100000L/100000P F=1 B=8366/8366 cksum=112afc52e2293:2567155540b084c7:635b1d1e48c5c81c:3cb7f54059033ce3
          300000   L0 0:8481935000:100000 100000L/100000P F=1 B=8366/8366 cksum=112b0452e2293:2568157540b084c7:b8f0f52e48c5c81c:424da2a059033ce3
          400000   L0 0:8481835000:100000 100000L/100000P F=1 B=8366/8366 cksum=112b0c52e2293:2569159540b084c7:e86cd3e48c5c81c:47e3500059033ce3
          500000   L0 0:8481b35000:100000 100000L/100000P F=1 B=8366/8366 cksum=112b1452e2293:256a15b540b084c7:641ca54e48c5c81c:4d78fd6059033ce3
          600000   L0 0:711b5a000:100000 100000L/100000P F=1 B=8356/8356 cksum=112b1c52e2293:256b15d540b084c7:b9b27d5e48c5c81c:530eaac059033ce3


меняя интересует сколько Л0 Л1 Л2 блоков есть в этом ффайле


[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-1024K 3  | grep " L2 "  | wc -l
       1
[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-1024K 3  | grep " L1 "  | wc -l
      40
[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-1024K 3  | grep " L0 "  | wc -l
   10240



L0 блоки это уже дата блоки с нашим иполезными даннными.
блоки Л1 и Л2 это блоки которые хранят адреса где же эти блоки лежат на диске. адреса.


работает это так: в сорока Л1 блоках хранятся адреса Л0 блоков. получается по 256 адресов
на один блок Л1.

а в блоке Л2 содержатся адреса где же эти Л1 блоки лежат на диске. 

и шарманка работает так.  зфс нужно считать блок Л0 с номером 1234. зфс  читает с диска или
из арк Л2 блок.  потом ищет в этом блоке  адрес такогто Л1 блока на диске. номер Л1 зфс
узнает путем формулы которая говорит   какой порядоковый номер Л1  нужно заюзать для 
поиска Л0 блока 1234.   далее зфс узнав адрес на диске нужного Л1 блока леезет на диск
или  в арк. и ищет в нем адрес на диске для Л0 блока 1234. и потом уже лезет на диск и 
считывает Л0 блок нужный нам.


это нас приводит к тому что прикаждом считывании Л0 блока зфс нужно залезть в Л2 блок
потом в нужный Л1 блок. и только потом на диске найти нужный Л0 блок.  тоесть чтобы сделать
один иопс для наших данных зфсну нужно сделать два иопса на метаданные. если все Л2 Л1
блоки лежат в арк то он это делает в памяти и не лезет на диск а на диск леезет только 
уже за Л0 блоком.  также если мы послеователно читаем 256 соседних ( в плане порядоквых номеров)
Л0 блоков то нам в Л2 блок лазить ненужно нам нужно читтать все из одного и тогоже Л1 блока.

и вот  я запускаю рандом рид тест

#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/boot-pool/ds2-1024K/fio.dat  --bs=$(( 1024 *1024)) --iodepth=1  --runtime=45  --readwrite=randread --numjobs=1  --group_reporting


и запускаю arcstat 

]# arcstat 1
    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
13:23:16   114    55     48    55   48     0    0     0    0  169M  275M   2.1G



у меня датасет который кеширует только метаданные. 
так как mmis  mm% равны нулю это значит что все метаданные имелись в арк. значит 
с диска читалось только юезер данные Л0 блоки.

всего в арк было сделано read=114 обращений. при этом miss=55 из этих обращений былы впустую 
промах. значит эти 55 это дата блоки Л0 которых там небыло. что собвстенно 
доказывает dmis=55 

значит 114-55=59 обращений в арк это были обращения за метаданными

зачем нам нужен Л2 блок? он хранит адреса на диске блоков Л1. так как у нас Л1 блоки
уже лежат в арк то их на диске искать ненужно! зфс их хранит в арк и прккрасно знает 
где их там искать. поэтому в л2 блок лазить уже ненужно. номер нужного нам Л1 блока
зфс вычситывает по формуле в завсимосити от того номера Л0 блока который нужно найти на 
диске. таким оразом чтобы скачать нужный Л0 блок с диска , зфсу нужно всего навсего 
заглянуть в нужный Л1 блок. Л1 блоки уже лежат в арк. зфс определяет в какой именно
блок Л1 ему нужно заглянуть прсто тупо по форуле в завсисмости от номер Л0 который 
мы хотим считать. это значит что  для того чтобы считать с диска один Л0 блок зфсу 
нужно сделать одно чтение нужного Л1 блока из арк. что сосвбетнно как мы видим
и ипроисходит! у нас на 55 запросов на диске Л0 блоков пришлось 59 запросов за метаданными
к арк. это буквально 1:1 !!!! тоесть все совпдаает! более того. если запустить аркстаат
на холостом ходу то видно что зфс каждую секунду делает 4-7 обращений к арк.
атаким образом 59-4=55 . тоесть мы получаем точное совпдание 
    55:55
для скачивания 55 Л0 дата блоков зфс сделало 55 обращений к аркс за метааданными!
все совплао 100% точно.




посмтим сколько занимают метаблоки


			L2   0:2704791000:1000 8000L/1000P


P это физичеки размер на диске
L это логичекский размер в памяти.

тоесть 

L = hex 8000 = 32768 = 32K
P = hex 1000 = 4096  = 4K

это значит что блок метаданных на диске хранится в сжатом виде куском 4К

посмотрим сколкьо занимает на диске L1 блок

               0  L1  0:ac0fb00000:2000 8000L/2000P F=256 B=8371/8371 cksum=27fcead258f:a70dad83f25f1:1baa06e692470493:d34e3737e3ccc6c5


Logical = 8000 hex = 32KB
Physial = 2000 hex = 8K



Итак если я храню файл на датасете с рекордсайз 1М 
и файл размером 10Г то его b-trreee структура метаблоков это 1 Л2, 40Л1, и 10240 Л0.
на чтение 1Л0 нужно сделать 1 чтение в Арк в Л1. если метаданные лежат все  в арк
то с диска читаются только Л0 блоки. 
по факту они будут читаться с диска 1М запросами. 
тоесть зфс деает 1 запрос в арк в нужный Л1. и потом делает запрос к диску размером 1М 
уже чтобы считать Л0.
подверждено опытами.

файл у меня специально стильно фрмагментрован чтобы я явно видел какими кусками зфс 
хранит данные на диске. 



вот показания arcstat

    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
13:59:54   142    69     48    69   48     0    0     0    0  170M  275M   2.2G
13:59:55   113    53     46    53   46     0    0     0    0  170M  275M   2.2G
13:59:56   114    55     48    55   48     0    0     0    0  170M  275M   2.2G
13:59:57   114    55     48    55   48     0    0     0    0  170M  275M   2.2G


вот показаия iostat

 KB/t  tps  MB/s 
 1024   55  54.9 
 1024   53  53.5 
 1024   54  54.0 
 1024   54  54.0 
 1024    4   4.0 
  0.0    0   0.0 


итак еще раз - при рекорсайзе 1М ( и файле небольше 10Г) ПРИ ЗАПРОСЕ НА ЧТЕНИЕ зфс делает 1 запрос в арк (в Л1 блок) и 1 запрос рамзером 1М  на диск




512K
тепрперь посмотрим датасет с рекордсетом 512К с файлом 10ГБ

[root@truenas ~]# ls -i /boot-pool/ds2-512K/fio.dat 
2 /boot-pool/ds2-512K/fio.dat


# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-512K 2  | head -n 40
Dataset boot-pool/ds2-512K [ZPL], ID 283, cr_txg 6278, 10.4G, 9 objects, rootbp DVA[0]=<0:4825e62000:1000> DVA[1]=<0:461335a000:1000> [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=11241L/11241P fill=9 cksum=1344a69be3:33ac1dd016fd:499b7f85d873e9:4a11a330f61f174f

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
         2    3    32K   512K  10.0G     512    10G  100.00  ZFS plain file
                                               168   bonus  System attributes
	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED 
	dnode maxblkid: 20479
	path	/fio.dat
	uid     0
	gid     0
	atime	Tue Dec  9 15:52:06 2025
	mtime	Wed Dec 10 10:44:48 2025
	ctime	Wed Dec 10 10:44:48 2025
	crtime	Tue Dec  9 15:52:06 2025
	gen	6284
	mode	100644
	size	10737418240
	parent	34
	links	1
	pflags	40800000004
Indirect blocks:
               0 L2   0:22008ef000:1000 8000L/1000P F=20480 B=8400/8400 cksum=1750608ef4a:30a053343d999:4057fc6176602c9:fafb19631c7cbe06
               0  L1  0:20028a1000:2000 8000L/2000P F=256 B=8399/8399 cksum=2400c32bb47:9bd2130ac8fd1:1a1c3ed2bcddd476:3c5cb26803322a3b
               0   L0 0:273f661000:80000 80000L/80000P F=1 B=8387/8387 cksum=89313f39758c:893fe5a181577459:fa17628c799b6c3f:79598428c28c7066
           80000   L0 0:2281fe2000:80000 80000L/80000P F=1 B=8395/8395 cksum=89315f39758c:894005a981577459:f74c890799b6c3f:8f618f80c28c7066
          100000   L0 0:22999e2000:80000 80000L/80000P F=1 B=8396/8396 cksum=89317f39758c:894025b181577459:24d22e94799b6c3f:a5699ad8c28c7066
          180000   L0 0:2299a62000:80000 80000L/80000P F=1 B=8396/8396 cksum=89319f39758c:894045b981577459:3a2f9498799b6c3f:bb71a630c28c7066
          200000   L0 0:22693e2000:80000 80000L/80000P F=1 B=8394/8394 cksum=8931bf39758c:894065c181577459:4f8cfa9c799b6c3f:d179b188c28c7066



посмтрим сколько каких блоков

[root@truenas ~]# 
[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-512K 2  | grep " L3 "  | wc -l
       0
[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-512K 2  | grep " L2 "  | wc -l
       1
[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-512K 2  | grep " L1 "  | wc -l
      80
[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-512K 2  | grep " L0 "  | wc -l
   20480


здесь срутктура b-tree таже самая(при данном размере файла).
значит при запросе на четние зфс будет делать 1 запрос в АРК к Л1 блоку 
и потом запрос к Л0 блоку на диск
тоесть будет 1:1

запускаю тест

#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/boot-pool/ds2-512K/fio.dat  --bs=$(( 1024 *1024)) --iodepth=1  --runtime=45  --readwrite=read --numjobs=1  --group_reporting

что значит размер --bs   он означает что мы просим ядро (то бишь зфс) прочитать с диска файл
начиная с такого то оффсета внутри файла плюс 1М. тоесть условно мы просим ядро вернуть 
нам с диска содержимое файла между 1М и 2МБ. с точки зрения файла он состоит из байтов
эти байты имеют порядоквые номера оффсеты. эти байты както там хрантся на диске мы незнаем
как нам неважно они могут хранится как угодно. тоесть байт с номером Х  может хранится на диске
по адресу У1 а байт с номером У может хранится по адресу У2. нам все равно. мы просто 
просим ядро в нашу память вернуть содержимое файла начиная с офссета скажем 2МБ (тоесть номер
байты с точки зрения адресации внутри файла 2 миллиона ) и закачниавая байтом с офсетом 3МБ.
а сколько запросов к диску или дискам будет сделано и какого размера будет каждый запрос
это уже нас не колышет. это уже завсиисит от ФС и ее драйвера. 

файл максимально фрагменторован. если зфс хранит Л0 блоки кусками по 512К байт
то зфс нужно будет сделать два ИОПСА по 512Кбайт 


что мы  и видим

[root@truenas ~]# iostat -d  ada1  1
            ada1 
 KB/t  tps  MB/s 
  519    5   2.3 
  512   78  39.2 
  512   80  40.1 
  512   74  36.9 
  512   77  38.6 
  512   72  35.9 
  512   79  39.3 
  512   75  37.4 
  512   77  38.4 
  512   80  40.1 
  512   77  38.7 

чтение с диска идет кусками по 512К байт

но число иопсов которое покзывает фио [R(1)][42.2%][r=36.2MiB/s][r=36 IOPS][eta 00m:26s]
в два раза меньше чем покывзает иостат. потому что чтобы засунут в фио 1МБ данных с диска
(тоест 1 иопс с точки зрения фио) зфс нужно с диска сделать два иопса по 512Кбайт.

далее я запустил рандом рид


#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/boot-pool/ds2-512K/fio.dat  --bs=$(( 512 *1024)) --iodepth=1  --runtime=45  --readwrite=randread --numjobs=1  --group_reporting




[root@truenas ~]# arcstat 1
    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
14:33:39     8     4     50     4   50     0    0     0    0  170M  275M   2.2G
14:33:40   164    77     46    77   46     0    0     0    0  170M  275M   2.2G
14:33:41   160    78     48    78   48     0    0     0    0  169M  275M   2.2G
14:33:42   159    77     48    77   48     0    0     0    0  170M  275M   2.2G
14:33:43   155    75     48    75   48     0    0     0    0  170M  275M   2.2G
14:33:44   154    75     48    75   48     0    0     0    0  170M  275M   2.2G
14:33:45   165    79     47    79   47     0    0     0    0  170M  275M   2.2G


и мы видим что зфс делает в арк 165 запросов. из них 79 мисс.
79 это датаблоки которых в арк нет. это Л0 блоки. 165-79=86 метаданных в арк есть.
и мы получаем подержвение о том что чтобы считать 1 блок с диска зфс нужно счатть 1 блок из арк


далее заустил вот такой тест

#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/boot-pool/ds2-512K/fio.dat  --bs=$(( 4*1024 *1024)) --iodepth=1  --runtime=45  --readwrite=read --numjobs=1  --group_reporting


тоесть фио пуляет в ядро запрос длиной 4МБ.
данные на диске хранятся кусками по 512Кбайт. возникает вопрос - как зфс будет пихать
запросы на диск. сразу сунет 4 запроса по 512К на диск создав очередь равно iodepth=4
или зфс будет по одному пихать эти реквесты надиск?

согласно gstat -зфс пихает на диск запросы с L(q)=1 
хотя на графике он 0.
но тут прикол в том что в процессе выполения гстат показывает 
то 0 то 1 то 2. хер знает я считаю что оно равно 1

dT: 1.010s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    0      0      0      0    0.0      0      0    0.0    0.0| ada0
    0     34     34  17236   16.8      0      0    0.0   39.0| ada1
    0      0      0      0    0.0      0      0    0.0    0.0| ada1p1
    0     34     34  17236   16.8      0      0    0.0   39.0| ada1p2
    0      0      0      0    0.0      0      0    0.0    0.0| gptid/285b499b-d308-11f0-91b2-f0def19938c3


а вот iostat

 KB/t  tps  MB/s 
  512   79  39.6 
  512   79  39.4 
  512   77  38.5 
  512   79  39.5 
  512   76  37.9 
  512   75  37.3 
  512   78  39.2 
  512   79  39.3 


видно что средний размер запроса 512К

получатся зфс не пихает тупо кучу 512К запрос на диск кучей сразу хотя мы мог бы сразу 8 штук
и пусть там диск сам разбирается. он пихает по одному.



чем меньше размер рекордсайза и чем бьлше размер файла тем больше у него Л0 блоков
и тем глубже становится дерево b-tree  и значит чтобы найти адрес на диске Л0 блока
зфс нужно будет делать больше запросов к этому дереву хоть даже оно и все лежит в арк

что интересно кога я запуска рандом рид по файлу то зфс приодися все больше и больше с диска
читать Л1 блоки метаданных (если иих нет в арк). но рано или поздно все Л1 оказываеются 
считанными. и все лежат в арк. и тогда наконец с диска начинаются чиаться только дата блоки.
и тогда средний размер размера блока на чтение ставноится четко равен рекодрдсайзу

вот арк стат

14:52:09   274   123     44   123   44     0    0    21   12  178M  275M   2.2G
14:52:10   270   123     45   123   45     0    0    19   11  178M  275M   2.2G
14:52:11   277   122     44   122   44     0    0    14    8  178M  275M   2.2G
14:52:12   279   125     44   125   44     0    0    18   10  178M  275M   2.2G
14:52:13   318   138     43   138   43     0    0    15    7  179M  275M   2.2G
14:52:14   315   129     40   129   40     0    0     9    4  179M  275M   2.2G
14:52:15   307   126     41   126   41     0    0     9    4  179M  275M   2.2G
14:52:16   317   128     40   127   40     0    0     5    2  179M  275M   2.2G
14:52:17   346   137     39   138   39     0    0     1    0  179M  275M   2.2G
14:52:18   333   133     39   133   39     0    0     8    3  179M  275M   2.2G
14:52:19   305   123     40   123   40     0    0     5    2  179M  275M   2.2G
14:52:20   322   132     40   132   40     0    0     6    3  179M  275M   2.2G
14:52:21   329   131     39   131   39     0    0     2    1  179M  275M   2.2G
14:52:22   328   130     39   130   39     0    0     5    2  180M  275M   2.2G
14:52:23   295   120     40   120   40     0    0     3    1  179M  275M   2.2G
    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
14:52:24   343   131     38   131   38     0    0     2    0  180M  275M   2.2G
14:52:25   335   132     39   132   39     0    0     2    0  180M  275M   2.2G
14:52:26   353   137     38   137   38     0    0     1    0  180M  275M   2.2G
14:52:27   323   125     38   125   38     0    0     0    0  179M  275M   2.2G
14:52:28   356   134     37   134   37     0    0     0    0  180M  275M   2.2G

видно что чтений метаданных с диска все меньше и меньше  я про уменьшающиеся
колонки mmis mm%



а вот iostat

 KB/t  tps  MB/s 
 62.6  120   7.3 
 63.1  131   8.1 
 63.1  130   8.0 
 63.6  134   8.3 
 64.0  123   7.7 
 64.0  134   8.4 
 63.6  128   7.9 
 64.0  125   7.8 
 64.0  135   8.4 
 64.0  130   8.1 
 63.5  123   7.6 
 64.0  117   7.3 
 64.0  122   7.6 
 64.0  126   7.9 
 64.0  129   8.1 
 64.0  123   7.7 
 64.0  127   7.9 
 64.0  128   8.0 
 64.0   21   1.3 

 поэтмоу если делаем запрос на рандом рид а  размер блока при чтении средний с диска 
 меньше это значит что зфс читает с диска недостающие блоки метаданных (по 4К 8К длинной)
 поэтому средний размер блока на чтение иопс и проседает в иостат или gstat

 как только все блоки метаднных окажутся в арк

     time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
14:56:40     7     0      0     0    0     0    0     0    0  186M  275M   2.2G
14:56:41     4     0      0     0    0     0    0     0    0  186M  275M   2.2G
14:56:42     4     0      0     0    0     0    0     0    0  186M  275M   2.2G
14:56:43   428   153     35   153   35     0    0     0    0  190M  275M   2.1G
14:56:44   376   137     36   137   36     0    0     0    0  189M  275M   2.1G
14:56:45   348   125     35   125   35     0    0     0    0  189M  275M   2.1G
14:56:46   372   133     35   133   35     0    0     0    0  189M  275M   2.1G
14:56:47   366   130     35   130   35     0    0     0    0  189M  275M   2.1G
14:56:48   343   125     36   125   36     0    0     0    0  189M  275M   2.1G
14:56:49   370   130     35   130   35     0    0     0    0  189M  275M   2.1G
14:56:50   358   127     35   127   35     0    0     0    0  189M  275M   2.1G
14:56:51   345   125     36   125   36     0    0     0    0  189M  275M   2.1G


срений размер блока на чтение стане четко равен рекордсайзу
32.0  221   6.9 
 32.0  126   3.9 
 32.0  122   3.8 
 32.0  137   4.3 
 32.0  128   4.0 
 32.0  118   3.7 
 32.0  129   4.0 
 32.0  126   4.0 
 32.0  131   4.1 
 32.0  113   3.5 

(по крайней мере с выключенной компрессией на датасете. случай когда компрессия включена
я не рассматривал еще)





далее двигаем.
вот история когда рекордсайз 8К . файл 10ГБ по прежнему

иостат
 KB/t  tps  MB/s 
  8.0  145   1.1 
  8.0  145   1.1 
  8.0  148   1.2 
  8.0  161   1.3 
  8.0  137   1.1 
  8.0  139   1.1 
  8.0  142   1.1 
  8.0  141   1.1 



видно что средний размер запроса 8к. 


    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
15:00:31   447   150     33   150   33     0    0     0    0  216M  275M   2.1G
15:00:32   455   154     33   154   33     0    0     0    0  216M  275M   2.1G
15:00:33   420   141     33   141   33     0    0     0    0  216M  275M   2.1G
15:00:34   462   154     33   154   33     0    0     0    0  216M  275M   2.1G
15:00:35   438   147     33   147   33     0    0     0    0  216M  275M   2.1G
15:00:36   470   159     33   159   33     0    0     0    0  216M  275M   2.1G
15:00:37   399   135     33   135   33     0    0     0    0  216M  275M   2.1G
15:00:38   448   151     33   151   33     0    0     0    0  216M  275M   2.1G


тут видно что что для того чтобы считать 1блок Л0
зфс нужно сделать два запроса к АРК
все потому что из за мелкого рекордсайза и большого файла 
структура b-tree стала глубже потому что число Л0 блоков круто выросло. 



[root@truenas ~]# zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | head -n 40
Dataset boot-pool/ds2-8K [ZPL], ID 1481, cr_txg 9974, 10.5G, 9 objects, rootbp DVA[0]=<0:1adb01d000:1000> DVA[1]=<0:6002933000:1000> [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=11263L/11263P fill=9 cksum=1067400b00:2bd2d0c50754:3e34c78f865c4b:3e70b97aef73c324

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
         2    4    32K     8K  10.1G     512    10G  100.00  ZFS plain file
                                               168   bonus  System attributes
	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED 
	dnode maxblkid: 1310719
	path	/fio.dat
	uid     0
	gid     0
	atime	Wed Dec 10 12:35:35 2025
	mtime	Wed Dec 10 12:44:13 2025
	ctime	Wed Dec 10 12:44:13 2025
	crtime	Wed Dec 10 12:35:35 2025
	gen	9984
	mode	100644
	size	10737418240
	parent	34
	links	1
	pflags	40800000004
Indirect blocks:
               0 L3    0:480a0d2000:1000 8000L/1000P F=1310720 B=10123/10123 cksum=749dcc9802:18ad58d0cac80:2a2b01b002eb322:7c82ef3a4d5f962
               0  L2   0:3df67aa000:3000 8000L/3000P F=65536 B=10123/10123 cksum=4360ea160bf:1a315863442e02:67ffc4e147674852:de8f408dfa511817
               0   L1  0:3d1374a000:2000 8000L/2000P F=256 B=10120/10120 cksum=2694fd98810:a5211730cc430:1b9febea0211817c:404b4a5ee3a22e8e
               0    L0 0:5353207000:2000 2000L/2000P F=1 B=10044/10044 cksum=22701830cb9:89210c503acac:16cfe0c4a63dd1e6:9ad6456a20c15710




zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L3 "  | wc -l
       1


#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L2 "  | wc -l
      20


#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L1 "  | wc -l
    5120

#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L0 "  | wc -l
 1310720


тоесть теперь в дерево добавилось L3
если даже все Л3-Д1 загрузит в АРК. то нам для того чтобы узнать адрес Л0 на диске
ненужно читать только Л3. а Л2 и Л1 надо. поэтому у нас будет на поиск одного Л0
нужно сделать два запроса к арк



а вот датасет с рекордсайз 4К и тем же файлом 10ГБ
что мы видим 



   time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  size     c  avail
15:10:51   252    84     33    84   33     0    0     0    0  240M  275M   2.1G
15:10:52   390   129     33   129   33     0    0     0    0  240M  275M   2.1G
15:10:53   433   143     33   143   33     0    0     0    0  240M  275M   2.1G
15:10:54   366   122     33   122   33     0    0     0    0  240M  275M   2.1G
15:10:55   381   126     33   126   33     0    0     0    0  240M  275M   2.1G
15:10:56   355   119     33   119   33     0    0     0    0  240M  275M   2.1G

видно что на каждый запрос к Л0 на диск зфс делает 2 запроса в АРК.
значит структура Л2 дерева должна быть такаяже самая как и в случае рекордсайза 8К
тоест верщина это Л3. просто число Л2 и прочих стало больше. а глубина получается
осталась таже у дерева.


[root@truenas ~]# ls -i /boot-pool/ds2-4K/fio.dat 
2 /boot-pool/ds2-4K/fio.dat
[root@truenas ~]# 
[root@truenas ~]# 
[root@truenas ~]#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-4K 2  | head -n 40
Dataset boot-pool/ds2-4K [ZPL], ID 1566, cr_txg 10149, 10.6G, 9 objects, rootbp DVA[0]=<0:c2172cc000:1000> DVA[1]=<0:57120b0000:1000> [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=11266L/11266P fill=9 cksum=127ab2a9ca:325c470fa212:4943dbf68e60c7:4b8731b474a52c64

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
         2    4    32K     4K  10.2G     512    10G  100.00  ZFS plain file
                                               168   bonus  System attributes
	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED 
	dnode maxblkid: 2621439
	path	/fio.dat
	uid     0
	gid     0
	atime	Wed Dec 10 12:46:30 2025
	mtime	Wed Dec 10 13:04:51 2025
	ctime	Wed Dec 10 13:04:51 2025
	crtime	Wed Dec 10 12:46:30 2025
	gen	10156
	mode	100644
	size	10737418240
	parent	34
	links	1
	pflags	40800000004
Indirect blocks:
               0 L3    0:480afa5000:1000 8000L/1000P F=2621440 B=10411/10411 cksum=c7d4a47e67:25411c2fce2f5:39a35038f3de195:d74747e04a8004d9
               0  L2   0:8687b31000:3000 8000L/3000P F=65536 B=10411/10411 cksum=43e04240e1b:1b0478d9450f89:6d668220db57b64d:24328667abeba8dd
               0   L1  0:1ab3c68000:2000 8000L/2000P F=256 B=10405/10405 cksum=27fce4bce65:a4ad9de665f5f:1b5ad28e19648594:afc8117a2f58736b
               0    L0 0:3cf958d000:1000 1000L/1000P F=1 B=10319/10319 cksum=10f4454c506:222b4e40ad360:2d81d004203623d:d84cb0967b59c22d
            1000    L0 0:3da7804000:1000 1000L/1000P F=1 B=10218/10218 cksum=10f44554506:222b4e52ad360:2d81d01da93623d:d84cb24813b9c22d
            2000    L0 0:3da7805000:1000 1000L/1000P F=1 B=10218/10218 cksum=10f4455c506:222b4e64ad360:2d81d037323623d:d84cb3f9ac19c22d


так и есть глубина дерева такаяже самая

[root@truenas ~]#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-4K 2  | grep " L3 "  | wc -l
       1
[root@truenas ~]# 
[root@truenas ~]#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-4K 2  | grep " L2 "  | wc -l
      40
[root@truenas ~]# 
[root@truenas ~]#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-4K 2  | grep " L1 "  | wc -l
   10240
[root@truenas ~]# 
[root@truenas ~]#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-4K 2  | grep " L0 "  | wc -l
 2621440


сраним с деревом на 8К


zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L3 "  | wc -l
       1


#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L2 "  | wc -l
      20


#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L1 "  | wc -l
    5120

#  zdb -U /data/zfs/zpool.cache  -ddddd boot-pool/ds2-8K 2  | grep " L0 "  | wc -l
 1310720


тоесть на 4К глубина дерева тажесамая но увеличислось число литьев по горизонтали
на каждом левеле


тоесть трудозатрваты по поиску адреса на диске для Л0 блока не 4К и 8К одинаковые - два
обращения в АРК. просто на 4К размер блоков метаданных теперь в два раза больше будет
занимать в АРК.

