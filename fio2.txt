| freenas
| fio
| iops



| offset


# ./fio-3-28-115 --rw=read --bs=1m --direct=1 --ioengine=libaio --size=10G\
  --group_reporting --filename=/dev/tank/bucket --name=job1 --offset=0G\
  --name=job2 --offset=10G --name=job3 --offset=20G --name=job4 --offset=30G\
  --name=job5 --offset=40G --name=job6 --offset=50G --name=job7 --offset=60G\
  --name=job8 --offset=70G
  (...)
    read: IOPS=4174, BW=4175MiB/s (4378MB/s)(80.0GiB/19622msec)



| общая часть


root@2hst1 .../temp/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=32000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.25
Starting 1 process
test: Laying out IO file (1 file / 32000MiB)
Jobs: 1 (f=1): [r(1)][100.0%][r=108MiB/s][r=27.7k IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2385796: Tue Sep 16 18:09:13 2025
  read: IOPS=20.3k, BW=79.4MiB/s (83.3MB/s)(31.2GiB/402962msec)
   bw (  KiB/s): min= 1320, max=110880, per=99.99%, avg=81307.40, stdev=33287.90, samples=805
   iops        : min=  330, max=27720, avg=20326.84, stdev=8321.97, samples=805
  cpu          : usr=2.96%, sys=14.59%, ctx=2791262, majf=0, minf=5723
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=8192000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=79.4MiB/s (83.3MB/s), 79.4MiB/s-79.4MiB/s (83.3MB/s-83.3MB/s), io=31.2GiB (33.6GB), run=402962-402962msec
root@2hst1 .../temp/fio# 
root@2hst1 .../temp/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=32000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.25
Starting 1 process
^Cbs: 1 (f=1): [w(1)][7.6%][w=3260KiB/s][w=815 IOPS][eta 02h:08m:56s] 
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=2430390: Tue Sep 16 18:35:16 2025
  write: IOPS=978, BW=3912KiB/s (4006kB/s)(2437MiB/637716msec); 0 zone resets
   bw (  KiB/s): min=  192, max= 9160, per=100.00%, avg=3913.46, stdev=1616.22, samples=1275
   iops        : min=   48, max= 2290, avg=978.36, stdev=404.06, samples=1275
  cpu          : usr=0.19%, sys=0.76%, ctx=349138, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,623763,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=3912KiB/s (4006kB/s), 3912KiB/s-3912KiB/s (4006kB/s-4006kB/s), io=2437MiB (2555MB), run=637716-637716msec
root@2hst1 .../temp/fio# 


| nfs-ext (на 2hst1)
| fio
| iops


root@2hst1 .../nfs-ext/temp# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=32000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.25
Starting 1 process
test: Laying out IO file (1 file / 32000MiB)
^Cbs: 1 (f=1): [r(1)][9.4%][r=5789KiB/s][r=1447 IOPS][eta 01h:24m:20s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=2468782: Tue Sep 16 18:52:28 2025
  read: IOPS=1465, BW=5864KiB/s (6005kB/s)(3019MiB/527267msec)
   bw (  KiB/s): min= 4448, max= 6552, per=100.00%, avg=5865.15, stdev=229.89, samples=1054
   iops        : min= 1112, max= 1638, avg=1466.24, stdev=57.50, samples=1054
  cpu          : usr=0.34%, sys=1.30%, ctx=648177, majf=0, minf=99
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=772966,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=5864KiB/s (6005kB/s), 5864KiB/s-5864KiB/s (6005kB/s-6005kB/s), io=3019MiB (3166MB), run=527267-527267msec
root@2hst1 .../nfs-ext/temp# 
root@2hst1 .../nfs-ext/temp# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=32000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.25
Starting 1 process
^Cbs: 1 (f=1): [w(1)][0.1%][w=1492KiB/s][w=373 IOPS][eta 06h:01m:18s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=2486918: Tue Sep 16 18:53:00 2025
  write: IOPS=380, BW=1523KiB/s (1559kB/s)(33.7MiB/22664msec); 0 zone resets
   bw (  KiB/s): min=  848, max= 1760, per=99.94%, avg=1522.84, stdev=177.84, samples=45
   iops        : min=  212, max=  440, avg=380.71, stdev=44.46, samples=45
  cpu          : usr=0.00%, sys=0.36%, ctx=3998, majf=0, minf=5
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.2%, 32=0.4%, >=64=99.3%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,8629,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=1523KiB/s (1559kB/s), 1523KiB/s-1523KiB/s (1559kB/s-1559kB/s), io=33.7MiB (35.3MB), run=22664-22664msec

  

| sas3008
| lsi
| raid 10 x 4 SSD 960GB
| iops
| fio

[root@acs-master fio]# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.19
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
^Cbs: 1 (f=1): [r(1)][0.5%][r=1594KiB/s][r=398 IOPS][eta 02h:01m:41s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=295504: Tue Sep 16 19:45:58 2025
  read: IOPS=348, BW=1392KiB/s (1426kB/s)(50.0MiB/37487msec)
   bw (  KiB/s): min=  976, max= 1744, per=100.00%, avg=1396.49, stdev=170.57, samples=74
   iops        : min=  244, max=  436, avg=349.11, stdev=42.63, samples=74
  cpu          : usr=0.33%, sys=1.41%, ctx=13022, majf=0, minf=73
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.2%, >=64=99.5%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=13048,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=1392KiB/s (1426kB/s), 1392KiB/s-1392KiB/s (1426kB/s-1426kB/s), io=50.0MiB (53.4MB), run=37487-37487msec

Disk stats (read/write):
    dm-0: ios=13325/483, merge=0/0, ticks=2499055/105897, in_queue=2604952, util=99.38%, aggrios=13325/462, aggrmerge=0/20, aggrticks=2500173/53376, aggrin_queue=2553550, aggrutil=99.41%
  sda: ios=13325/462, merge=0/20, ticks=2500173/53376, in_queue=2553550, util=99.41%
[root@acs-master fio]# 
[root@acs-master fio]# 
[root@acs-master fio]# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.19
Starting 1 process
^Cbs: 1 (f=1): [w(1)][0.2%][w=918KiB/s][w=229 IOPS][eta 02h:15m:28s] 
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=296029: Tue Sep 16 19:46:22 2025
  write: IOPS=314, BW=1259KiB/s (1289kB/s)(21.0MiB/17871msec); 0 zone resets
   bw (  KiB/s): min=  496, max= 2056, per=99.80%, avg=1256.51, stdev=420.98, samples=35
   iops        : min=  124, max=  514, avg=314.06, stdev=105.32, samples=35
  cpu          : usr=0.41%, sys=1.12%, ctx=4653, majf=0, minf=6
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.3%, 32=0.6%, >=64=98.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,5625,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=1259KiB/s (1289kB/s), 1259KiB/s-1259KiB/s (1289kB/s-1289kB/s), io=21.0MiB (23.0MB), run=17871-17871msec

Disk stats (read/write):
    dm-0: ios=14/5783, merge=0/0, ticks=1324/1178475, in_queue=1179799, util=89.85%, aggrios=14/5782, aggrmerge=0/3, aggrticks=1323/1179962, aggrin_queue=1181285, aggrutil=90.00%
  sda: ios=14/5782, merge=0/3, ticks=1323/1179962, in_queue=1181285, util=90.00%




===========================================================


ГОЛЫЕ AHCI SATA порты


| micron 5300 PRO 480GB 6Gbit/s


порт SATA AHCI 3Gbit/s


root@test:/mnt/sdc/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
^Cbs: 1 (f=1): [r(1)][50.9%][r=193MiB/s][r=49.4k IOPS][eta 00m:26s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=684: Wed Sep 17 07:50:57 2025
  read: IOPS=49.3k, BW=193MiB/s (202MB/s)(5087MiB/26424msec)
   bw (  KiB/s): min=196656, max=197528, per=100.00%, avg=197208.62, stdev=236.24, samples=52
   iops        : min=49164, max=49382, avg=49302.15, stdev=59.06, samples=52
  cpu          : usr=13.12%, sys=29.00%, ctx=1293942, majf=0, minf=72
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=1302184,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=193MiB/s (202MB/s), 193MiB/s-193MiB/s (202MB/s-202MB/s), io=5087MiB (5334MB), run=26424-26424msec

Disk stats (read/write):
  sdc: ios=1289849/4, merge=48/4, ticks=1666221/8, in_queue=1666232, util=68.75%

root@test:/mnt/sdc/1/fio# 
root@test:/mnt/sdc/1/fio# 
root@test:/mnt/sdc/1/fio# dd if=/dev/sdc of=/dev/null bs=4MiB count=100
100+0 records in
100+0 records out
419430400 bytes (419 MB, 400 MiB) copied, 1.46834 s, 286 MB/s
root@test:/mnt/sdc/1/fio# dd if=/dev/sdc of=/dev/null bs=4MiB count=1000
1000+0 records in
1000+0 records out
4194304000 bytes (4.2 GB, 3.9 GiB) copied, 13.2697 s, 316 MB/s




PORT 6Gb\s

RAND READ

root@test:/mnt/sda/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=335MiB/s][r=85.8k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=589: Wed Sep 17 07:57:32 2025
  read: IOPS=85.8k, BW=335MiB/s (352MB/s)(9.77GiB/29822msec)
   bw (  KiB/s): min=340096, max=347320, per=100.00%, avg=343650.44, stdev=1400.44, samples=59
   iops        : min=85024, max=86830, avg=85912.75, stdev=350.10, samples=59
  cpu          : usr=17.89%, sys=48.35%, ctx=2101910, majf=0, minf=72
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=335MiB/s (352MB/s), 335MiB/s-335MiB/s (352MB/s-352MB/s), io=9.77GiB (10.5GB), run=29822-29822msec

Disk stats (read/write):
  sda: ios=2543725/0, merge=1611/0, ticks=1865656/0, in_queue=1865656, util=76.70%
root@test:/mnt/sda/1/fio# 




RAND WRITE

root@test:/mnt/sda/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=342MiB/s][w=87.6k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=682: Wed Sep 17 08:03:11 2025
  write: IOPS=86.2k, BW=337MiB/s (353MB/s)(9.77GiB/29692msec); 0 zone resets
   bw (  KiB/s): min=325640, max=354560, per=100.00%, avg=344899.12, stdev=6809.60, samples=59
   iops        : min=81410, max=88640, avg=86224.85, stdev=1702.35, samples=59
  cpu          : usr=19.37%, sys=51.88%, ctx=1784190, majf=0, minf=6
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=337MiB/s (353MB/s), 337MiB/s-337MiB/s (353MB/s-353MB/s), io=9.77GiB (10.5GB), run=29692-29692msec

Disk stats (read/write):
  sda: ios=0/2552727, merge=0/2713, ticks=0/1862992, in_queue=1862993, util=84.35%





LINEAR READ

root@test:/mnt/sda/1/fio# dd if=/dev/sda of=/dev/null bs=4MiB count=1000
1000+0 records in
1000+0 records out
4194304000 bytes (4.2 GB, 3.9 GiB) copied, 7.83174 s, 536 MB/s








INTEL DCS3110 256GB


PORT SATA AHCI 6Gb/s


RAND READ

root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=298MiB/s][r=76.4k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=661: Wed Sep 17 08:00:24 2025
  read: IOPS=76.1k, BW=297MiB/s (312MB/s)(9.77GiB/33627msec)
   bw (  KiB/s): min=302424, max=310440, per=99.98%, avg=304454.09, stdev=1185.86, samples=67
   iops        : min=75606, max=77610, avg=76113.46, stdev=296.43, samples=67
  cpu          : usr=12.70%, sys=42.85%, ctx=2249194, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=297MiB/s (312MB/s), 297MiB/s-297MiB/s (312MB/s-312MB/s), io=9.77GiB (10.5GB), run=33627-33627msec

Disk stats (read/write):
  sdb: ios=2539156/0, merge=1363/0, ticks=2098909/0, in_queue=2098909, util=85.64%





RAND WRITE


root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=261MiB/s][w=66.9k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=668: Wed Sep 17 08:02:09 2025
  write: IOPS=72.7k, BW=284MiB/s (298MB/s)(9.77GiB/35231msec); 0 zone resets
   bw (  KiB/s): min=245328, max=356624, per=100.00%, avg=290814.51, stdev=30699.63, samples=70
   iops        : min=61332, max=89156, avg=72703.66, stdev=7674.90, samples=70
  cpu          : usr=16.06%, sys=42.82%, ctx=1732438, majf=0, minf=7
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=284MiB/s (298MB/s), 284MiB/s-284MiB/s (298MB/s-298MB/s), io=9.77GiB (10.5GB), run=35231-35231msec

Disk stats (read/write):
  sdb: ios=0/2551609, merge=0/2402, ticks=0/2197811, in_queue=2197865, util=85.32%





====================


контроллер 3ware 700-3260-10D
JBOD режим скорость порта на 3ware контролерер 3 Gbit/s
диск micron 5300 PRO 480GB (макс скорость порта диска самого 6Gbit/s )


root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [r(1)][31.1%][r=55.8MiB/s][r=14.3k IOPS][eta 02m:04s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=608: Wed Sep 17 08:19:02 2025
  read: IOPS=14.2k, BW=55.6MiB/s (58.3MB/s)(3146MiB/56561msec)
   bw (  KiB/s): min=55096, max=57736, per=100.00%, avg=57003.04, stdev=367.11, samples=113
   iops        : min=13774, max=14434, avg=14250.76, stdev=91.79, samples=113
  cpu          : usr=10.96%, sys=29.62%, ctx=679751, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=805280,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=55.6MiB/s (58.3MB/s), 55.6MiB/s-55.6MiB/s (58.3MB/s-58.3MB/s), io=3146MiB (3298MB), run=56561-56561msec

Disk stats (read/write):
  sdb: ios=802835/1, merge=0/0, ticks=3592781/5, in_queue=3592785, util=77.33%
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [w(1)][6.7%][w=27.9MiB/s][w=7142 IOPS][eta 05m:34s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=615: Wed Sep 17 08:19:32 2025
  write: IOPS=7166, BW=28.0MiB/s (29.4MB/s)(683MiB/24412msec); 0 zone resets
   bw (  KiB/s): min=28480, max=28936, per=100.00%, avg=28696.00, stdev=121.89, samples=48
   iops        : min= 7120, max= 7234, avg=7174.00, stdev=30.47, samples=48
  cpu          : usr=6.36%, sys=18.62%, ctx=174896, majf=0, minf=10
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,174938,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=28.0MiB/s (29.4MB/s), 28.0MiB/s-28.0MiB/s (29.4MB/s-29.4MB/s), io=683MiB (717MB), run=24412-24412msec

Disk stats (read/write):
  sdb: ios=0/174833, merge=0/0, ticks=0/1555190, in_queue=1555190, util=77.20%
root@test:/mnt/sdb/1/fio# 







контроллер 3ware 700-3260-10D
JBOD режим скорость порта на 3ware контролерер 3 Gbit/s
диск INTEL DCS3110 256GB
 (макс скорость порта диска самого 6Gbit/s )



root@test:/mnt/sdc/1/fio# 
root@test:/mnt/sdc/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [r(1)][22.8%][r=54.6MiB/s][r=14.0k IOPS][eta 02m:22s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=633: Wed Sep 17 08:23:05 2025
  read: IOPS=13.9k, BW=54.5MiB/s (57.1MB/s)(2283MiB/41915msec)
   bw (  KiB/s): min=54872, max=56416, per=100.00%, avg=55829.01, stdev=414.91, samples=83
   iops        : min=13718, max=14104, avg=13957.28, stdev=103.72, samples=83
  cpu          : usr=10.93%, sys=29.90%, ctx=518319, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=584417,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=54.5MiB/s (57.1MB/s), 54.5MiB/s-54.5MiB/s (57.1MB/s-57.1MB/s), io=2283MiB (2394MB), run=41915-41915msec

Disk stats (read/write):
  sdc: ios=584136/1, merge=0/0, ticks=2669961/6, in_queue=2669968, util=78.10%
root@test:/mnt/sdc/1/fio# 
root@test:/mnt/sdc/1/fio# 
root@test:/mnt/sdc/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [w(1)][2.4%][w=12.7MiB/s][w=3252 IOPS][eta 13m:42s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=640: Wed Sep 17 08:23:32 2025
  write: IOPS=3058, BW=11.9MiB/s (12.5MB/s)(245MiB/20508msec); 0 zone resets
   bw (  KiB/s): min= 4432, max=13096, per=99.94%, avg=12228.80, stdev=2003.69, samples=40
   iops        : min= 1108, max= 3274, avg=3057.20, stdev=500.92, samples=40
  cpu          : usr=3.57%, sys=10.53%, ctx=62739, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,62730,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=11.9MiB/s (12.5MB/s), 11.9MiB/s-11.9MiB/s (12.5MB/s-12.5MB/s), io=245MiB (257MB), run=20508-20508msec

Disk stats (read/write):
  sdc: ios=0/62370, merge=0/0, ticks=0/1301541, in_queue=1301542, util=85.15%
root@test:/mnt/sdc/1/fio# 







контроллер 3ware 700-3260-10D
RAID0 режим  скорость порта на 3ware контролерер 3 Gbit/s
диски intel+micron


root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
^Cbs: 1 (f=1): [r(1)][11.8%][r=62.3MiB/s][r=15.9k IOPS][eta 02m:22s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=651: Wed Sep 17 08:31:37 2025
  read: IOPS=16.0k, BW=62.3MiB/s (65.4MB/s)(1178MiB/18897msec)
   bw (  KiB/s): min=63488, max=64464, per=100.00%, avg=63904.22, stdev=271.00, samples=37
   iops        : min=15872, max=16116, avg=15976.05, stdev=67.75, samples=37
  cpu          : usr=11.19%, sys=31.20%, ctx=202968, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=301565,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=62.3MiB/s (65.4MB/s), 62.3MiB/s-62.3MiB/s (65.4MB/s-65.4MB/s), io=1178MiB (1235MB), run=18897-18897msec

Disk stats (read/write):
  sdb: ios=300534/0, merge=0/0, ticks=1197801/0, in_queue=1197802, util=78.15%

root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [w(1)][3.3%][w=25.7MiB/s][w=6577 IOPS][eta 07m:54s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=658: Wed Sep 17 08:31:59 2025
  write: IOPS=5321, BW=20.8MiB/s (21.8MB/s)(325MiB/15636msec); 0 zone resets
   bw (  KiB/s): min= 8480, max=26560, per=99.96%, avg=21278.97, stdev=7309.18, samples=31
   iops        : min= 2120, max= 6640, avg=5319.81, stdev=1827.32, samples=31
  cpu          : usr=5.08%, sys=15.03%, ctx=70638, majf=0, minf=7
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,83208,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=20.8MiB/s (21.8MB/s), 20.8MiB/s-20.8MiB/s (21.8MB/s-21.8MB/s), io=325MiB (341MB), run=15636-15636msec

Disk stats (read/write):
  sdb: ios=0/81737, merge=0/0, ticks=0/981357, in_queue=981357, util=83.70%
root@test:/mnt/sdb/1/fio# 

==


LSI MegaRAID SAS 2108

Product Name = ServeRAID M5015 SAS/SATA Controller
Serial Number = SV21106943
SAS Address =  500605b004872b90
PCI Address = 00:01:00:00
System Time = 09/18/2025 08:07:13
Mfg. Date = 03/12/12
Controller Time = 09/18/2025 12:07:12
FW Package Build = 12.12.0-0047
FW Version = 2.120.53-1235
BIOS Version = 3.22.00_4.11.05.00_0x05020000
Driver Name = megaraid_sas
Driver Version = 07.719.03.00-rc1
Vendor Id = 0x1000
Device Id = 0x79
SubVendor Id = 0x1014
SubDevice Id = 0x3B2
Host Interface = PCI-E
Device Interface = SAS-6G


--------------------------------------------------------------
DG/VD TYPE   State Access Consist Cache Cac sCC     Size Name 
--------------------------------------------------------------
0/0   RAID10 Optl  RW     No      NRWTD -   ON  1.744 TB      
--------------------------------------------------------------


root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=305MiB/s][r=78.0k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=920: Thu Sep 18 08:03:48 2025
  read: IOPS=76.5k, BW=299MiB/s (313MB/s)(9.77GiB/33473msec)
   bw (  KiB/s): min=297160, max=313856, per=100.00%, avg=306095.27, stdev=4463.30, samples=66
   iops        : min=74290, max=78464, avg=76523.79, stdev=1115.79, samples=66
  cpu          : usr=22.88%, sys=52.82%, ctx=100108, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=299MiB/s (313MB/s), 299MiB/s-299MiB/s (313MB/s-313MB/s), io=9.77GiB (10.5GB), run=33473-33473msec

Disk stats (read/write):
  sdb: ios=2554208/1, merge=0/0, ticks=1843885/1, in_queue=1843886, util=52.75%
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=200MiB/s][w=51.1k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=927: Thu Sep 18 08:04:57 2025
  write: IOPS=50.3k, BW=197MiB/s (206MB/s)(9.77GiB/50847msec); 0 zone resets
   bw (  KiB/s): min=197048, max=205232, per=100.00%, avg=201547.01, stdev=2289.84, samples=101
   iops        : min=49262, max=51308, avg=50386.81, stdev=572.47, samples=101
  cpu          : usr=16.85%, sys=38.16%, ctx=87377, majf=0, minf=7
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=197MiB/s (206MB/s), 197MiB/s-197MiB/s (206MB/s-206MB/s), io=9.77GiB (10.5GB), run=50847-50847msec

Disk stats (read/write):
  sdb: ios=0/2549855, merge=0/0, ticks=0/2677049, in_queue=2677050, util=54.22%





=====


LSI MegaRAID SAS 2108

Product Name = ServeRAID M5015 SAS/SATA Controller
Serial Number = SV21106943
SAS Address =  500605b004872b90
PCI Address = 00:01:00:00
System Time = 09/18/2025 08:07:13
Mfg. Date = 03/12/12
Controller Time = 09/18/2025 12:07:12
FW Package Build = 12.12.0-0047
FW Version = 2.120.53-1235
BIOS Version = 3.22.00_4.11.05.00_0x05020000
Driver Name = megaraid_sas
Driver Version = 07.719.03.00-rc1
Vendor Id = 0x1000
Device Id = 0x79
SubVendor Id = 0x1014
SubDevice Id = 0x3B2
Host Interface = PCI-E
Device Interface = SAS-6G




RAID0 
INTEL SSDSC2KB960G7



root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
Jobs: 1 (f=1): [r(1)][14.3%][r=246MiB/s][r=63.1k IOPS][eta 00m:36s]
^Cbs: 1 (f=1): [r(1)][53.7%][r=247MiB/s][r=63.2k IOPS][eta 00m:19s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=995: Thu Sep 18 08:12:14 2025
  read: IOPS=63.0k, BW=246MiB/s (258MB/s)(5199MiB/21134msec)
   bw (  KiB/s): min=248280, max=253344, per=100.00%, avg=252162.10, stdev=814.25, samples=42
   iops        : min=62070, max=63336, avg=63040.52, stdev=203.54, samples=42
  cpu          : usr=21.17%, sys=48.66%, ctx=154537, majf=0, minf=73
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=1330975,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=246MiB/s (258MB/s), 246MiB/s-246MiB/s (258MB/s-258MB/s), io=5199MiB (5452MB), run=21134-21134msec

Disk stats (read/write):
  sdb: ios=1318618/0, merge=0/0, ticks=1277415/0, in_queue=1277415, util=47.29%
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# 
root@test:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [w(1)][37.2%][w=230MiB/s][w=58.9k IOPS][eta 00m:27s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=1002: Thu Sep 18 08:12:34 2025
  write: IOPS=58.9k, BW=230MiB/s (241MB/s)(3693MiB/16062msec); 0 zone resets
   bw (  KiB/s): min=232040, max=236984, per=100.00%, avg=235610.00, stdev=965.50, samples=32
   iops        : min=58010, max=59246, avg=58902.50, stdev=241.38, samples=32
  cpu          : usr=17.38%, sys=37.41%, ctx=152214, majf=0, minf=7
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,945452,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=3693MiB (3873MB), run=16062-16062msec

Disk stats (read/write):
  sdb: ios=0/935784, merge=0/4, ticks=0/975429, in_queue=975429, util=48.09%
root@test:/mnt/sdb/1/fio# 


=====

LSI MegaRAID SAS 2108

Product Name = ServeRAID M5015 SAS/SATA Controller
Serial Number = SV21106943
SAS Address =  500605b004872b90
PCI Address = 00:01:00:00
System Time = 09/18/2025 08:07:13
Mfg. Date = 03/12/12
Controller Time = 09/18/2025 12:07:12
FW Package Build = 12.12.0-0047
FW Version = 2.120.53-1235
BIOS Version = 3.22.00_4.11.05.00_0x05020000
Driver Name = megaraid_sas
Driver Version = 07.719.03.00-rc1
Vendor Id = 0x1000
Device Id = 0x79
SubVendor Id = 0x1014
SubDevice Id = 0x3B2
Host Interface = PCI-E
Device Interface = SAS-6G







1 диск
RAID0 
INTEL SSDSC2KB960G7



root@debian:/mnt/sda/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=201MiB/s][r=51.5k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1814: Thu Sep 18 11:15:02 2025
  read: IOPS=50.1k, BW=196MiB/s (205MB/s)(9.77GiB/51140msec)
   bw (  KiB/s): min=182824, max=206456, per=100.00%, avg=200362.20, stdev=8635.42, samples=102
   iops        : min=45706, max=51614, avg=50090.53, stdev=2158.85, samples=102
  cpu          : usr=14.52%, sys=57.54%, ctx=388748, majf=0, minf=187
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=196MiB/s (205MB/s), 196MiB/s-196MiB/s (205MB/s-205MB/s), io=9.77GiB (10.5GB), run=51140-51140msec

Disk stats (read/write):
  sda: ios=2559902/2, merge=0/0, ticks=3158090/2, in_queue=3158091, util=51.00%
root@debian:/mnt/sda/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=159MiB/s][w=40.7k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1953: Thu Sep 18 11:16:23 2025
  write: IOPS=40.7k, BW=159MiB/s (167MB/s)(9.77GiB/62916msec); 0 zone resets
   bw (  KiB/s): min=156632, max=163160, per=100.00%, avg=162856.45, stdev=691.91, samples=125
   iops        : min=39158, max=40790, avg=40714.14, stdev=172.99, samples=125
  cpu          : usr=14.18%, sys=52.51%, ctx=889890, majf=0, minf=124
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=159MiB/s (167MB/s), 159MiB/s-159MiB/s (167MB/s-167MB/s), io=9.77GiB (10.5GB), run=62916-62916msec

Disk stats (read/write):
  sda: ios=0/2558699, merge=0/0, ticks=0/3938846, in_queue=3938846, util=55.70%





===



LSI MegaRAID SAS 2108

Product Name = ServeRAID M5015 SAS/SATA Controller
Serial Number = SV21106943
SAS Address =  500605b004872b90
PCI Address = 00:01:00:00
System Time = 09/18/2025 08:07:13
Mfg. Date = 03/12/12
Controller Time = 09/18/2025 12:07:12
FW Package Build = 12.12.0-0047
FW Version = 2.120.53-1235
BIOS Version = 3.22.00_4.11.05.00_0x05020000
Driver Name = megaraid_sas
Driver Version = 07.719.03.00-rc1
Vendor Id = 0x1000
Device Id = 0x79
SubVendor Id = 0x1014
SubDevice Id = 0x3B2
Host Interface = PCI-E
Device Interface = SAS-6G







4 диска
RAID10 

---------------------------------------------------------------------------------------
EID:Slt DID State DG       Size Intf Med SED PI SeSz Model                     Sp Type 
---------------------------------------------------------------------------------------
34:0     21 Onln   0 893.137 GB SATA SSD N   N  512B Micron_5300_MTFDDAK960TDS U  -    
34:1     22 Onln   0 893.137 GB SATA SSD N   N  512B INTEL SSDSC2KB960G7       U  -    
34:2     39 Onln   0 893.137 GB SATA SSD N   N  512B INTEL SSDSC2KB960G7       U  -    
34:3     40 Onln   0 893.137 GB SATA SSD N   N  512B INTEL SSDSC2KB960G7       U  -    
---------------------------------------------------------------------------------------




root@debian:/mnt/sdc/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=275MiB/s][r=70.5k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2119: Thu Sep 18 11:23:10 2025
  read: IOPS=71.3k, BW=278MiB/s (292MB/s)(9.77GiB/35929msec)
   bw (  KiB/s): min=277536, max=309976, per=100.00%, avg=285214.42, stdev=7099.08, samples=71
   iops        : min=69384, max=77494, avg=71303.61, stdev=1774.77, samples=71
  cpu          : usr=17.00%, sys=79.84%, ctx=20282, majf=0, minf=191
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=278MiB/s (292MB/s), 278MiB/s-278MiB/s (292MB/s-292MB/s), io=9.77GiB (10.5GB), run=35929-35929msec

Disk stats (read/write):
  sdc: ios=2557039/1, merge=0/0, ticks=1632510/1, in_queue=1632510, util=33.25%





root@debian:/mnt/sdc/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randrwrite --rwmixread=75
        valid values: read       Sequential read
                    : write      Sequential write
                    : trim       Sequential trim
                    : randread   Random read
                    : randwrite  Random write
                    : randtrim   Random trim
                    : rw         Sequential read and write mix
                    : readwrite  Sequential read and write mix
                    : randrw     Random read and write mix
                    : trimwrite  Trim and write mix, trims preceding writes
                    : randtrimwrite Randomly trim and write mix, trims preceding writes

fio: failed parsing readwrite=randrwrite
root@debian:/mnt/sdc/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=193MiB/s][w=49.5k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2192: Thu Sep 18 11:24:16 2025
  write: IOPS=49.4k, BW=193MiB/s (202MB/s)(9.77GiB/51820msec); 0 zone resets
   bw (  KiB/s): min=189696, max=200832, per=100.00%, avg=197733.13, stdev=1181.14, samples=103
   iops        : min=47424, max=50208, avg=49433.28, stdev=295.28, samples=103
  cpu          : usr=15.25%, sys=52.89%, ctx=148382, majf=0, minf=124
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=193MiB/s (202MB/s), 193MiB/s-193MiB/s (202MB/s-202MB/s), io=9.77GiB (10.5GB), run=51820-51820msec

Disk stats (read/write):
  sdc: ios=0/2550976, merge=0/0, ticks=0/2606447, in_queue=2606448, util=40.51%




# history | grep -i megaraid
    5  /opt/MegaRAID/storcli/storcli64 /c0 /v0 delete force
    6  /opt/MegaRAID/storcli/storcli64 /c0 add vd type=raid10 drives=252:0,252:1,252:2,252:3 pdperarray=2
    7  /opt/MegaRAID/storcli/storcli64 /c0 show
    8  /opt/MegaRAID/storcli/storcli64 /c0 add vd type=raid10 drives=34:0,34:1,34:2,34:3 pdperarray=2
    9  /opt/MegaRAID/storcli/storcli64 /c0/v0 set iopolicy=direct
   10  /opt/MegaRAID/storcli/storcli64 /c0 /v0 set iopolicy=direct
   11  /opt/MegaRAID/storcli/storcli64 /c0 /v0 set wrcache=wt
   12  /opt/MegaRAID/storcli/storcli64 /c0 show
   43  history | grep -i megaraid



==========================



root@ocsamsh1:/mnt/01/fio# storcli64 /c0 show
Generating detailed summary of the adapter, it may take a while to complete.

CLI Version = 007.3405.0000.0000 May 15, 2025
Operating system = Linux 6.14.8-2-pve
Controller = 0
Status = Success
Description = None

Product Name = ServeRAID M5015 SAS/SATA Controller
Serial Number = SV21106943
SAS Address =  500605b004872b90
PCI Address = 00:04:00:00
System Time = 09/19/2025 18:48:02
Mfg. Date = 03/12/12
Controller Time = 09/19/2025 15:48:02
FW Package Build = 12.12.0-0047
FW Version = 2.120.53-1235
BIOS Version = 3.22.00_4.11.05.00_0x05020000
Driver Name = megaraid_sas
Driver Version = 07.727.03.00-rc1
Vendor Id = 0x1000
Device Id = 0x79
SubVendor Id = 0x1014
SubDevice Id = 0x3B2
Host Interface = PCI-E
Device Interface = SAS-6G
Bus Number = 4
Device Number = 0
Function Number = 0
Domain ID = 0
Security Protocol = None
Drive Groups = 2

TOPOLOGY :
========

------------------------------------------------------------------------------
DG Arr Row EID:Slot DID Type   State BT       Size PDC  PI SED DS3  FSpace TR 
------------------------------------------------------------------------------
 0 -   -   -        -   RAID10 Optl  N    1.744 TB dsbl N  N   dflt N      N  
 0 0   -   -        -   RAID1  Optl  N  893.137 GB dsbl N  N   dflt N      N  
 0 0   0   34:0     21  DRIVE  Onln  N  893.137 GB dsbl N  N   dflt -      N  
 0 0   1   34:1     22  DRIVE  Onln  N  893.137 GB dsbl N  N   dflt -      N  
 0 1   -   -        -   RAID1  Optl  N  893.137 GB dsbl N  N   dflt N      N  
 0 1   0   34:2     39  DRIVE  Onln  N  893.137 GB dsbl N  N   dflt -      N  
 0 1   1   34:3     40  DRIVE  Onln  N  893.137 GB dsbl N  N   dflt -      N  
 1 -   -   -        -   RAID1  Optl  N  278.464 GB dflt N  N   dflt N      N  
 1 0   -   -        -   RAID1  Optl  N  278.464 GB dflt N  N   dflt N      N  
 1 0   0   34:12    32  DRIVE  Onln  N  278.464 GB dflt N  N   dflt -      N  
 1 0   1   34:13    33  DRIVE  Onln  N  278.464 GB dflt N  N   dflt -      N  
------------------------------------------------------------------------------

DG=Disk Group Index|Arr=Array Index|Row=Row Index|EID=Enclosure Device ID
DID=Device ID|Type=Drive or RAID Type|Onln=Online|Rbld=Rebuild|Optl=Optimal
Dgrd=Degraded|Pdgd=Partially degraded|Offln=Offline|BT=Background Task Active
PDC=PD Cache|PI=Protection Info|SED=Self Encrypting Drive|Frgn=Foreign
DS3=Dimmer Switch 3|dflt=Default|Msng=Missing|FSpace=Free Space Present
TR=Transport Ready

Virtual Drives = 2

VD LIST :
=======

----------------------------------------------------------------
DG/VD TYPE   State Access Consist Cache Cac sCC       Size Name 
----------------------------------------------------------------
0/0   RAID10 Optl  RW     No      NRWTD -   ON    1.744 TB      
1/1   RAID1  Optl  RW     No      NRWBD -   ON  278.464 GB      
----------------------------------------------------------------

VD=Virtual Drive| DG=Drive Group|Rec=Recovery
Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded
Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady
B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack
AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled
Check Consistency

Physical Drives = 6

PD LIST :
=======

---------------------------------------------------------------------------------------
EID:Slt DID State DG       Size Intf Med SED PI SeSz Model                     Sp Type 
---------------------------------------------------------------------------------------
34:0     21 Onln   0 893.137 GB SATA SSD N   N  512B Micron_5300_MTFDDAK960TDS U  -    
34:1     22 Onln   0 893.137 GB SATA SSD N   N  512B INTEL SSDSC2KB960G7       U  -    
34:2     39 Onln   0 893.137 GB SATA SSD N   N  512B INTEL SSDSC2KB960G7       U  -    
34:3     40 Onln   0 893.137 GB SATA SSD N   N  512B INTEL SSDSC2KB960G7       U  -    
34:12    32 Onln   1 278.464 GB SAS  HDD N   N  512B ST300MM0006               U  -    
34:13    33 Onln   1 278.464 GB SAS  HDD N   N  512B ST300MM0006               U  -    
---------------------------------------------------------------------------------------

EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup
DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare
UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface
Med=Media Type|SED=Self Encryptive Drive|PI=PI Eligible
SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign
UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded
CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded
UBUnsp=UBad Unsupported|Rbld=Rebuild

Enclosures = 2

Enclosure LIST :
==============

--------------------------------------------------------------------------------------------
EID State Slots PD PS Fans TSs Alms SIM Port#                      ProdID    VendorSpecific 
--------------------------------------------------------------------------------------------
 34 OK       15  6  0    0   0    0   0 Port 0 - 3 & Port 4 - 7 x8  Expander                
252 OK        8  0  0    0   0    0   1 -                          SGPIO                    
--------------------------------------------------------------------------------------------

EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count
TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID


BBU_Info :
========

-----------------------------------------------------------------------
Model  State   RetentionTime Temp Mode MfgDate    Next Learn           
-----------------------------------------------------------------------
iBBU08 Optimal 48 hours +    28C  5    2012/01/16 2025/10/17  17:14:30 
-----------------------------------------------------------------------



RAND READ

/mnt/01/fio
root@ocsamsh1:/mnt/01/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
^Cbs: 1 (f=1): [r(1)][47.1%][r=293MiB/s][r=75.1k IOPS][eta 00m:18s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=9032: Fri Sep 19 18:46:02 2025
  read: IOPS=75.7k, BW=296MiB/s (310MB/s)(4930MiB/16671msec)
   bw (  KiB/s): min=288288, max=320424, per=100.00%, avg=303056.00, stdev=5805.60, samples=33
   iops        : min=72072, max=80106, avg=75764.00, stdev=1451.40, samples=33
  cpu          : usr=15.67%, sys=53.11%, ctx=54722, majf=0, minf=82
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=1262203,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=296MiB/s (310MB/s), 296MiB/s-296MiB/s (310MB/s-310MB/s), io=4930MiB (5170MB), run=16671-16671msec

Disk stats (read/write):
    dm-5: ios=1256861/0, sectors=10054888/0, merge=0/0, ticks=991957/0, in_queue=991957, util=99.44%, aggrios=1262211/0, aggsectors=10099672/0, aggrmerge=0/0, aggrticks=941186/0, aggrin_queue=941186, aggrutil=55.43%
  sda: ios=1262211/0, sectors=10099672/0, merge=0/0, ticks=941186/0, in_queue=941186, util=55.43%
root@ocsamsh1:/mnt/01/fio# 
root@ocsamsh1:/mnt/01/fio# 







RAND WRITE

root@ocsamsh1:/mnt/01/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite --rwmixread=75
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
^Cbs: 1 (f=1): [w(1)][48.1%][w=191MiB/s][w=48.8k IOPS][eta 00m:27s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=9140: Fri Sep 19 18:46:32 2025
  write: IOPS=49.3k, BW=193MiB/s (202MB/s)(4831MiB/25079msec); 0 zone resets
   bw (  KiB/s): min=179104, max=202344, per=100.00%, avg=197379.04, stdev=3554.30, samples=50
   iops        : min=44776, max=50586, avg=49344.76, stdev=888.58, samples=50
  cpu          : usr=11.27%, sys=40.07%, ctx=41887, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,1236806,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=193MiB/s (202MB/s), 193MiB/s-193MiB/s (202MB/s-202MB/s), io=4831MiB (5066MB), run=25079-25079msec

Disk stats (read/write):
    dm-5: ios=0/1227279, sectors=0/9818220, merge=0/0, ticks=0/1272004, in_queue=1272004, util=99.47%, aggrios=12/1236810, aggsectors=3072/9894468, aggrmerge=0/0, aggrticks=29/1310411, aggrin_queue=1310440, aggrutil=50.68%
  sda: ios=12/1236810, sectors=3072/9894468, merge=0/0, ticks=29/1310411, in_queue=1310440, util=50.68%





SEQUENTIAL WRITE


root@ocsamsh1:/mnt/01/fio# dd if=/dev/zero of=./test bs=4MiB count=2000
2000+0 records in
2000+0 records out
8388608000 bytes (8.4 GB, 7.8 GiB) copied, 13.2475 s, 633 MB/s



============

585g7

Model name:            AMD Opteron(TM) Processor 6276
BIOS Model name:     AMD Opteron(TM) Processor 6276                    CPU @ 2.3GHz




Smart Array P410i in Slot 0 (Embedded)
   Bus Interface: PCI
   Slot: 0
   Serial Number: 5001438023EC2230
   Cache Serial Number: PACCQID12480X2X
   Controller Status: OK
   Hardware Revision: C
   Firmware Version: 5.76
   Rebuild Priority: Low
   Expand Priority: Medium
   Surface Scan Delay: 15 secs
   Surface Scan Mode: Idle
   Parallel Surface Scan Supported: No
   Queue Depth: Automatic
   Monitor and Performance Delay: 60  min
   Elevator Sort: Enabled
   Degraded Performance Optimization: Disabled
   Inconsistency Repair Policy: Disabled
   Wait for Cache Room: Disabled
   Surface Analysis Inconsistency Notification: Disabled
   Post Prompt Timeout: 0 secs
   Cache Board Present: True
   Cache Status: OK
   Cache Ratio: 100% Read / 0% Write
   Drive Write Cache: Enabled
   Total Cache Size: 256 MB
   Total Cache Memory Available: 144 MB
   No-Battery Write Cache: Disabled
   Battery/Capacitor Count: 0
   SATA NCQ Supported: True
   Number of Ports: 2 Internal only
   Driver Name: hpsa
   Driver Version: 3.4.20
   Driver Supports HPE SSD Smart Path: True
   PCI Address (Domain:Bus:Device.Function): 0000:03:00.0
   Host Serial Number: CZ33109AJ9
   Sanitize Erase Supported: False
   Primary Boot Volume: logicaldrive 1 (600508B1001CFBAB2623FB21A81DBB09)
   Secondary Boot Volume: None


raid0

      physicaldrive 1I:1:2
         Port: 1I
         Box: 1
         Bay: 2
         Status: Predictive Failure
         Drive Type: Data Drive
         Interface Type: Solid State SATA
         Size: 179.2 GB
         Drive exposed to OS: False
         Native Block Size: 512
         Firmware Revision: LHF002D
         Serial Number: BTLA9466085G256CGN
         Model: ATA     INTEL SSDSC2KI25
         SATA NCQ Capable: True
         SATA NCQ Enabled: True
         Current Temperature (C): 39
         Maximum Temperature (C): 40
         SSD Smart Trip Wearout: Not Supported
         PHY Count: 1
         PHY Transfer Rate: 3.0Gbps
         Sanitize Erase Supported: False




root@p565h1:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=130MiB/s][r=33.4k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=272279: Wed Sep 24 21:59:20 2025
  read: IOPS=33.9k, BW=132MiB/s (139MB/s)(9.77GiB/75541msec)
   bw (  KiB/s): min=129592, max=136976, per=100.00%, avg=135631.30, stdev=1283.92, samples=150
   iops        : min=32398, max=34244, avg=33907.43, stdev=320.93, samples=150
  cpu          : usr=13.33%, sys=67.32%, ctx=301929, majf=0, minf=91
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=132MiB/s (139MB/s), 132MiB/s-132MiB/s (139MB/s-139MB/s), io=9.77GiB (10.5GB), run=75541-75541msec

Disk stats (read/write):
  sdb: ios=2554900/0, merge=0/0, ticks=4407348/0, in_queue=4407348, util=100.00%
root@p565h1:/mnt/sdb/1/fio# 









root@p565h1:/mnt/sdb/1/fio# 
root@p565h1:/mnt/sdb/1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=105MiB/s][w=26.9k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=272741: Wed Sep 24 22:02:17 2025
  write: IOPS=26.7k, BW=104MiB/s (109MB/s)(9.77GiB/95808msec); 0 zone resets
   bw (  KiB/s): min=37716, max=113384, per=100.00%, avg=106944.64, stdev=9403.84, samples=191
   iops        : min= 9429, max=28346, avg=26735.85, stdev=2350.94, samples=191
  cpu          : usr=12.54%, sys=60.37%, ctx=341206, majf=0, minf=28
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=104MiB/s (109MB/s), 104MiB/s-104MiB/s (109MB/s-109MB/s), io=9.77GiB (10.5GB), run=95808-95808msec

Disk stats (read/write):
  sdb: ios=0/2555454, merge=0/0, ticks=0/5800922, in_queue=5800922, util=100.00%
root@p565h1:/mnt/sdb/1/fio# 






=================

585G7  сервер
smart array p410i  контрроллер

ceph из 3-х дисков  (три OSD)           INTEL SSDSC2KI25


выдает 17 000-18 000 IOPS на тесте
 fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/mnt/fio/1/test --bs=4k --iodepth=64 --size=10000M --readwrite=randread


если убавить osd до двух то чтение падает до 12 000 IOPS тоесть  один диск (oSD ) дает 5000-6000 IOPS


сам контроллер с этого диска на RAID0 может снять на этом тесте 26 000 IOPS.
а если этот диск сунуть в AHCI порт то на этом тесте с этого диска можно снять 86 000 IOPS

однозначно я упираюсь в скорость ядра!
Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdb           4356.00  17424.00     0.00   0.00    0.19     4.00    2.00     12.00     1.00  33.33    0.50     6.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.84  54.50

потому что видно что диск занят по busytime на 54%
(остальные иопсы читаются по сети от других нод)



на текущем этапе как я понимаю упираемся в скорость 1-го ядра цпу.
также если запустить две виртуалки или два контейнера то скорость делится между 
ними. тоесть 18000 IOPS делится между контейнерами

сеть 10Gb при этом загружена на 22 МБ/с в обе стороны


====================

ceph 
performance

8 хостов
на каждом хосте стоит по 3 OSD. (итого 24 OSD)
каждый OSD состоит из 
       1 HDD 2.5'' 900GB SAS 10000 RPM шпиндель диска 
          плюс 
        SSD диск INTEL SSDSC2KG48 под журнал

сеть 10Gb 


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread --rwmixread=75
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
^Cbs: 1 (f=1): [r(1)][9.8%][r=9024KiB/s][r=2256 IOPS][eta 15m:08s] 
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=17845: Fri Sep 26 18:48:32 2025
  read: IOPS=2542, BW=9.93MiB/s (10.4MB/s)(985MiB/99189msec)
   bw (  KiB/s): min= 5984, max=12984, per=100.00%, avg=10192.67, stdev=1340.03, samples=198
   iops        : min= 1496, max= 3246, avg=2547.99, stdev=335.03, samples=198
  cpu          : usr=1.09%, sys=8.09%, ctx=212931, majf=0, minf=89
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=252154,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=9.93MiB/s (10.4MB/s), 9.93MiB/s-9.93MiB/s (10.4MB/s-10.4MB/s), io=985MiB (1033MB), run=99189-99189msec

Disk stats (read/write):
  rbd1: ios=252129/22, merge=0/0, ticks=6314288/497, in_queue=6314785, util=100.00%


ИТОГО random read 2256 IOPS (пиздец как медленно)



# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
 bs: 1 (f=1): [w(1)][9.2%][w=7235KiB/s][w=1808 IOPS][eta 17m:46s]
fio: terminating on signal 2
Jobs: 1 (f=0): [f(1)][100.0%][w=4316KiB/s][w=1079 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=17852: Fri Sep 26 18:50:36 2025
  write: IOPS=2174, BW=8698KiB/s (8907kB/s)(916MiB/107817msec); 0 zone resets
   bw (  KiB/s): min= 3711, max=15014, per=100.00%, avg=8720.94, stdev=2247.92, samples=215
   iops        : min=  927, max= 3753, avg=2180.09, stdev=562.01, samples=215
  cpu          : usr=0.99%, sys=7.10%, ctx=180594, majf=0, minf=14
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,234453,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=8698KiB/s (8907kB/s), 8698KiB/s-8698KiB/s (8907kB/s-8907kB/s), io=916MiB (960MB), run=107817-107817msec

Disk stats (read/write):
  rbd1: ios=0/234330, merge=0/0, ticks=0/6850661, in_queue=6850660, util=99.98%




ИТОГО random write 1808 IOPS (пиздец как медленно)


======================


ZFS 
pool вот такой

  pool: strip-2mirror
 state: ONLINE
  scan: scrub repaired 0B in 08:46:21 with 0 errors on Sun Sep 14 08:46:24 2025
config:

  NAME                                            STATE     READ WRITE CKSUM
  strip-2mirror                                   ONLINE       0     0     0
    mirror-0                                      ONLINE       0     0     0
      gptid/04b6cc93-dca8-11ea-af22-782bcb09ac2d  ONLINE       0     0     0
      gptid/04becd1b-dca8-11ea-af22-782bcb09ac2d  ONLINE       0     0     0
    mirror-1                                      ONLINE       0     0     0
      gptid/04c381f9-dca8-11ea-af22-782bcb09ac2d  ONLINE       0     0     0
      gptid/04df1d47-dca8-11ea-af22-782bcb09ac2d  ONLINE       0     0     0
    mirror-2                                      ONLINE       0     0     0
      gptid/04fa174a-dca8-11ea-af22-782bcb09ac2d  ONLINE       0     0     0
      gptid/b4ed60c8-7b3c-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
    mirror-3                                      ONLINE       0     0     0
      gptid/aa2e44c8-7b3b-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
      gptid/aa3c7586-7b3b-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
    mirror-4                                      ONLINE       0     0     0
      gptid/0714e2bd-7b3c-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
      gptid/0725ae55-7b3c-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
    mirror-5                                      ONLINE       0     0     0
      gptid/69564a87-a431-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
      gptid/6991aa26-a431-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
    mirror-6                                      ONLINE       0     0     0
      gptid/4d34f1b1-9c8b-11ed-9ab1-782bcb09ac2d  ONLINE       0     0     0
      gptid/4d5598d8-9c8b-11ed-9ab1-782bcb09ac2d  ONLINE       0     0     0
    mirror-7                                      ONLINE       0     0     0
      gptid/8b53c1e9-b6dc-11ee-9b3c-782bcb09ac2d  ONLINE       0     0     0
      gptid/8b80fc53-b6dc-11ee-9b3c-782bcb09ac2d  ONLINE       0     0     0
    mirror-8                                      ONLINE       0     0     0
      gptid/67481144-2df8-11ef-9b3c-782bcb09ac2d  ONLINE       0     0     0
      gptid/6764fb75-2df8-11ef-9b3c-782bcb09ac2d  ONLINE       0     0     0
    mirror-9                                      ONLINE       0     0     0
      gptid/95bedf1a-2df8-11ef-9b3c-782bcb09ac2d  ONLINE       0     0     0
      gptid/95aa8d1a-2df8-11ef-9b3c-782bcb09ac2d  ONLINE       0     0     0
    mirror-10                                     ONLINE       0     0     0
      gptid/a9b90f50-2df8-11ef-9b3c-782bcb09ac2d  ONLINE       0     0     0
      gptid/a9c2aecc-2df8-11ef-9b3c-782bcb09ac2d  ONLINE       0     0     0
    mirror-11                                     ONLINE       0     0     0
      gptid/60fdf86b-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
      gptid/6109c0d5-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
    mirror-12                                     ONLINE       0     0     0
      gptid/7258d993-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
      gptid/726281c0-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
    mirror-13                                     ONLINE       0     0     0
      gptid/85fc24bc-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
      gptid/85f1da7e-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
    mirror-14                                     ONLINE       0     0     0
      gptid/b4f96ce9-7b3c-11eb-8ac6-782bcb09ac2d  ONLINE       0     0     0
      gptid/90d7907b-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
    mirror-15                                     ONLINE       0     0     0
      gptid/9c46ff52-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
      gptid/9c948a9a-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
    mirror-16                                     ONLINE       0     0     0
      gptid/ab56f791-5d07-11f0-9585-782bcb09ac2d  ONLINE       0     0     0
      gptid/a89a9348-0963-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
    mirror-17                                     ONLINE       0     0     0
      gptid/e7a009d4-5bed-11f0-ae79-782bcb09ac2d  ONLINE       0     0     0
      mfid24                                      ONLINE       0     0     0
  spares
    gptid/ab4cba0d-5d07-11f0-9585-782bcb09ac2d    AVAIL   
    gptid/bcdadc57-713d-11f0-9585-782bcb09ac2d    AVAIL   


по факту это все 2ТБ шпидель диски

и он расшарен по NFS


тестирую скорость


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=./test --filename=test --bs=4k --iodepth=64 --size=32000M --readwrite=randread 
./test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.25
Starting 1 process
^Cbs: 1 (f=1): [r(1)][0.6%][r=2506KiB/s][r=626 IOPS][eta 02h:00m:18s] 
fio: terminating on signal 2
Jobs: 1 (f=1): [r(1)][0.7%][r=1469KiB/s][r=367 IOPS][eta 02h:02m:00s]
./test: (groupid=0, jobs=1): err= 0: pid=3033789: Fri Sep 26 22:42:53 2025
  read: IOPS=1112, BW=4451KiB/s (4558kB/s)(209MiB/48010msec)
   bw (  KiB/s): min=  848, max=65640, per=100.00%, avg=4493.00, stdev=7958.88, samples=95
   iops        : min=  212, max=16410, avg=1123.24, stdev=1989.71, samples=95
  cpu          : usr=0.68%, sys=2.76%, ctx=37579, majf=0, minf=388
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=53428,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=4451KiB/s (4558kB/s), 4451KiB/s-4451KiB/s (4558kB/s-4558kB/s), io=209MiB (219MB), run=48010-48010msec



ИТОГО: какой то пиздец  RAND READ 626 IOPS


======================

nvme диск

$ nvme list
Model                                    Namespace  Usage                      Format             
--------------------- --------------------- -------------------- -----------------------------
WDC PC SN520 SDAPMUW-512G-1101           0x1        512.11  GB / 512.11  GB      4 KiB +  0 B   


 N 㸒  $ fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=431MiB/s][r=110k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=153276: Sat Sep 27 19:16:59 2025
  read: IOPS=113k, BW=440MiB/s (461MB/s)(9.77GiB/22727msec)
   bw (  KiB/s): min=419896, max=470736, per=100.00%, avg=450731.38, stdev=8717.04, samples=45
   iops        : min=104974, max=117684, avg=112682.84, stdev=2179.26, samples=45
  cpu          : usr=18.01%, sys=59.32%, ctx=2322, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=440MiB/s (461MB/s), 440MiB/s-440MiB/s (461MB/s-461MB/s), io=9.77GiB (10.5GB), run=22727-22727msec

Disk stats (read/write):
    dm-0: ios=2548971/43, sectors=20391768/1544, merge=0/0, ticks=781053/14, in_queue=781067, util=99.62%, aggrios=2560000/43, aggsectors=20480000/1544, aggrmerge=0/0, aggrticks=739692/14, aggrin_queue=739714, aggrutil=86.34%
  nvme0n1: ios=2560000/43, sectors=20480000/1544, merge=0/0, ticks=739692/14, in_queue=739714, 
  util=86.34%


итого рандом рид 110 000 IOPS


а вот еще
 N 舔  $ sudo dd if=/dev/nvme0n1   of=/dev/null bs=8MiB status=progress
3531603968 bytes (3,5 GB, 3,3 GiB) copied, 5 s, 703 MB/s^C
439+0 records in
438+0 records out
3674210304 bytes (3,7 GB, 3,4 GiB) copied, 5,60163 s, 656 MB/s

ну както маловато. согласно спеку
    https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/product/internal-drives/pc-sn520-ssd/data-sheet-pc-sn520-compute.pdf

он должен на секвеншиал рид выжимать 1700 MB/s



=========================

AHCI порт 6Gb/s
cpu Intel(R) Core(TM) i3-4130 CPU @ 3.40GHz
диск  INTEL SSDSC2KI256G8

linear read
 dd if=/dev/sda2 of=/dev/null bs=8MiB status=progress
24754782208 bytes (25 GB, 23 GiB) copied, 44 s, 563 MB/s


random read  76 000 IOPS

o# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=301MiB/s][r=76.9k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1113: Tue Sep 30 07:54:43 2025
  read: IOPS=76.6k, BW=299MiB/s (314MB/s)(9.77GiB/33406msec)
   bw (  KiB/s): min=304360, max=308032, per=99.96%, avg=306399.88, stdev=737.70, samples=66
   iops        : min=76090, max=77008, avg=76599.97, stdev=184.41, samples=66
  cpu          : usr=18.48%, sys=29.29%, ctx=2401403, majf=0, minf=71
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=299MiB/s (314MB/s), 299MiB/s-299MiB/s (314MB/s-314MB/s), io=9.77GiB (10.5GB), run=33406-33406msec

Disk stats (read/write):
  sda: ios=2555449/0, merge=3482/0, ticks=2121214/0, in_queue=2121214, util=79.07%


random write 68 800 IOPS

# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=269MiB/s][w=68.8k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1120: Tue Sep 30 07:56:09 2025
  write: IOPS=72.9k, BW=285MiB/s (298MB/s)(9.77GiB/35136msec); 0 zone resets
   bw (  KiB/s): min=256696, max=358720, per=100.00%, avg=291689.83, stdev=31415.96, samples=70
   iops        : min=64174, max=89680, avg=72922.49, stdev=7853.97, samples=70
  cpu          : usr=19.87%, sys=40.21%, ctx=1419012, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=285MiB/s (298MB/s), 285MiB/s-285MiB/s (298MB/s-298MB/s), io=9.77GiB (10.5GB), run=35136-35136msec

Disk stats (read/write):
  sda: ios=0/2542649, merge=0/1107, ticks=0/2146857, in_queue=2146895, util=83.84%




диск  Micron_5300_MTFDDAK480TDS


LINEAR READ
# dd if=/dev/sdc2 of=/dev/null bs=8MiB status=progress
23882366976 bytes (24 GB, 22 GiB) copied, 45 s, 531 MB/s




RANDOM READ  83.6k IOPS


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=327MiB/s][r=83.6k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1184: Tue Sep 30 08:04:44 2025
  read: IOPS=84.0k, BW=328MiB/s (344MB/s)(9.77GiB/30463msec)
   bw (  KiB/s): min=330336, max=341280, per=100.00%, avg=336455.60, stdev=2010.31, samples=60
   iops        : min=82584, max=85320, avg=84113.87, stdev=502.54, samples=60
  cpu          : usr=22.57%, sys=43.84%, ctx=2204234, majf=0, minf=72
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=328MiB/s (344MB/s), 328MiB/s-328MiB/s (344MB/s-344MB/s), io=9.77GiB (10.5GB), run=30463-30463msec

Disk stats (read/write):
  sdc: ios=2552055/0, merge=2546/0, ticks=1925696/0, in_queue=1925696, util=72.27%




RANDOM WRITE   87.2k IOPS


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=340MiB/s][w=87.2k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1191: Tue Sep 30 08:05:44 2025
  write: IOPS=86.4k, BW=338MiB/s (354MB/s)(9.77GiB/29622msec); 0 zone resets
   bw (  KiB/s): min=329488, max=350888, per=100.00%, avg=345828.75, stdev=6021.10, samples=59
   iops        : min=82372, max=87722, avg=86457.15, stdev=1505.16, samples=59
  cpu          : usr=20.06%, sys=39.11%, ctx=2116614, majf=0, minf=7
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=338MiB/s (354MB/s), 338MiB/s-338MiB/s (354MB/s-354MB/s), io=9.77GiB (10.5GB), run=29622-29622msec

Disk stats (read/write):
  sdc: ios=0/2539291, merge=0/1424, ticks=0/1853982, in_queue=1853983, util=83.09%

==================================================================


HBA контроллер LSI 9217-8i  , 6Gb/s  (чипсет 2308)
  ( не путать с другим чипсетом 2308_2 и картой SAS9207-8i !! ) 




диск  INTEL SSDSC2KI256G8


LINEAR READ 483 MB/s




RANDOM READ  49.8k IOPS


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=195MiB/s][r=49.8k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1243: Tue Sep 30 08:16:06 2025
  read: IOPS=49.4k, BW=193MiB/s (202MB/s)(9.77GiB/51801msec)
   bw (  KiB/s): min=192672, max=203568, per=100.00%, avg=197688.85, stdev=981.26, samples=103
   iops        : min=48168, max=50892, avg=49422.25, stdev=245.31, samples=103
  cpu          : usr=14.05%, sys=42.12%, ctx=1574887, majf=0, minf=79
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=193MiB/s (202MB/s), 193MiB/s-193MiB/s (202MB/s-202MB/s), io=9.77GiB (10.5GB), run=51801-51801msec

Disk stats (read/write):
  sda: ios=2548663/0, merge=2044/0, ticks=3274666/0, in_queue=3274666, util=78.01%





RANDOM WRITE 40.8k IOPS

# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=159MiB/s][w=40.8k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1536: Tue Sep 30 08:25:57 2025
  write: IOPS=40.8k, BW=159MiB/s (167MB/s)(9.77GiB/62740msec); 0 zone resets
   bw (  KiB/s): min=150656, max=169928, per=100.00%, avg=163220.74, stdev=3468.80, samples=125
   iops        : min=37664, max=42482, avg=40805.17, stdev=867.20, samples=125
  cpu          : usr=14.31%, sys=41.35%, ctx=1880876, majf=0, minf=19
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=159MiB/s (167MB/s), 159MiB/s-159MiB/s (167MB/s-167MB/s), io=9.77GiB (10.5GB), run=62740-62740msec

Disk stats (read/write):
  sda: ios=0/2552193, merge=0/2547, ticks=0/3947846, in_queue=3947893, util=84.92%




диск  Micron_5300_MTFDDAK480TDS


LINEAR READ 460 MB/s



RANDOM READ  41.6k IOPS


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=162MiB/s][r=41.6k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1305: Tue Sep 30 08:19:47 2025
  read: IOPS=37.8k, BW=148MiB/s (155MB/s)(9.77GiB/67770msec)
   bw (  KiB/s): min=135832, max=177560, per=100.00%, avg=151160.65, stdev=11261.55, samples=135
   iops        : min=33958, max=44390, avg=37790.19, stdev=2815.37, samples=135
  cpu          : usr=12.62%, sys=35.15%, ctx=1710786, majf=0, minf=79
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=148MiB/s (155MB/s), 148MiB/s-148MiB/s (155MB/s-155MB/s), io=9.77GiB (10.5GB), run=67770-67770msec

Disk stats (read/write):
  sdd: ios=2552457/2, merge=2639/0, ticks=4277346/4, in_queue=4277352, util=77.48%



RANDOM WRITE 40.1k IOPS


# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=157MiB/s][w=40.1k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=1588: Tue Sep 30 08:26:11 2025
  write: IOPS=40.2k, BW=157MiB/s (165MB/s)(9.77GiB/63721msec); 0 zone resets
   bw (  KiB/s): min=159072, max=164736, per=100.00%, avg=160714.08, stdev=750.45, samples=127
   iops        : min=39768, max=41184, avg=40178.54, stdev=187.62, samples=127
  cpu          : usr=16.15%, sys=45.16%, ctx=1997986, majf=0, minf=17
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560000,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=157MiB/s (165MB/s), 157MiB/s-157MiB/s (165MB/s-165MB/s), io=9.77GiB (10.5GB), run=63721-63721msec

Disk stats (read/write):
  sdd: ios=0/2553514, merge=0/2881, ticks=0/3982685, in_queue=3982689, util=84.14%



ОБЩИЙ ВЫВОД : видно что карта  LSI 9217-8i (на чипе 2308) в режиме HBA 
дает в 2 раза скорость меньше чем обычный AHCI порт на простом компе !!


итак мы выяснили что данный хба контррллер режет теряет скорость в 1.5-2 раза
по сравннеию какая скорость у диска вобщето может быть.

причем я проверил что нет разницы какая прошивка у контроллера IR или IT
тоесть если у вас контррлллер  на прошивке IR (рейдовая) то особого смысла
перешивать ее в IT смысла нет.

еще раз скажу что этот контрроллер работате на чипе 2308

====================================

AHCI
диск Samsung SSD 860 EVO 1TB



o# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=384MiB/s][r=98.4k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=803: Thu Oct  2 10:25:05 2025
  read: IOPS=98.1k, BW=383MiB/s (402MB/s)(9.77GiB/26101msec)
   bw (  KiB/s): min=387288, max=396072, per=100.00%, avg=392423.69, stdev=892.87, samples=52
   iops        : min=96822, max=99018, avg=98106.00, stdev=223.22, samples=52
  cpu          : usr=25.85%, sys=39.00%, ctx=2415048, majf=0, minf=74
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=383MiB/s (402MB/s), 383MiB/s-383MiB/s (402MB/s-402MB/s), io=9.77GiB (10.5GB), run=26101-26101msec

Disk stats (read/write):
  sdb: ios=2538005/1, merge=1336/0, ticks=1640310/1, in_queue=1640311, util=71.87%

===========================

| LSI
| SAS3008


КАРТА:
root@debian:/opt/SAS3IRCU# ./sas3ircu list
Avago Technologies SAS3 IR Configuration Utility.
Version 17.00.00.00 (2018.04.02) 
Copyright (c) 2009-2018 Avago Technologies. All rights reserved. 


         Adapter      Vendor  Device                       SubSys  SubSys 
 Index    Type          ID      ID    Pci Address          Ven ID  Dev ID 
 -----  ------------  ------  ------  -----------------    ------  ------ 
   0     SAS3008       1000h   97h    00h:01h:00h:00h      1000h   3090h 
SAS3IRCU: Utility Completed Successfully.
root@debian:/opt/SAS3IRCU# 
root@debian:/opt/SAS3FLASH# chmod +x sas3flash 
root@debian:/opt/SAS3FLASH# ./sas3flash -list
Avago Technologies SAS3 Flash Utility
Version 16.00.00.00 (2017.05.02) 
Copyright 2008-2017 Avago Technologies. All rights reserved.

  Adapter Selected is a Avago SAS: SAS3008(C0)

  Controller Number              : 0
  Controller                     : SAS3008(C0)
  PCI Address                    : 00:01:00:00
  SAS Address                    : 5d494e8-0-f1b2-8000
  NVDATA Version (Default)       : 02.06.00.02
  NVDATA Version (Persistent)    : 02.06.00.02
  Firmware Product ID            : 0x2721 (IR)
  Firmware Version               : 02.00.00.00
  NVDATA Vendor                  : LSI
  NVDATA Product ID              : SAS3008
  BIOS Version                   : 08.05.00.00
  UEFI BSD Version               : 02.00.00.00
  FCODE Version                  : N/A
  Board Name                     : SAS3008
  Board Assembly                 : N/A
  Board Tracer Number            : N/A

  Finished Processing Commands Successfully.
  Exiting SAS3Flash.




диск: Samsung SSD 860 EVO 1TB

CPU :  Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz


RANDOM READ 57.6k IOPS 
 
root@debian:/mnt/sdb1/fio# dd if=/dev/zero of=/dev/sdd bs=1M count=100^C
root@debian:/mnt/sdb1/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
test: Laying out IO file (1 file / 10000MiB)
Jobs: 1 (f=1): [r(1)][100.0%][r=225MiB/s][r=57.6k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=16218: Thu Oct  2 12:38:55 2025
  read: IOPS=57.5k, BW=225MiB/s (236MB/s)(9.77GiB/44519msec)
   bw (  KiB/s): min=181664, max=240440, per=100.00%, avg=230088.54, stdev=7168.94, samples=89
   iops        : min=45416, max=60110, avg=57522.16, stdev=1792.25, samples=89
  cpu          : usr=12.20%, sys=43.54%, ctx=1359624, majf=0, minf=85
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=225MiB/s (236MB/s), 225MiB/s-225MiB/s (236MB/s-236MB/s), io=9.77GiB (10.5GB), run=44519-44519msec

Disk stats (read/write):
  sdb: ios=2547957/4, merge=1982/4, ticks=2817383/731, in_queue=2818296, util=88.13%


вывод: результат отстой. из за медленного контролллера



запустил тест сразу на двух дисках.
резулттат один и тот же. 
тоесть диски друг другу немешают.  с каждого читается RANDOM READ плюс минус 
со скоростью 57.6k IOPS


а вот тест RANDOM WRITE  r=59.3k IOPS

o# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=232MiB/s][r=59.3k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=16491: Thu Oct  2 12:45:39 2025
  read: IOPS=59.3k, BW=232MiB/s (243MB/s)(9.77GiB/43139msec)
   bw (  KiB/s): min=235976, max=239776, per=100.00%, avg=237420.47, stdev=377.66, samples=86
   iops        : min=58994, max=59944, avg=59355.12, stdev=94.42, samples=86
  cpu          : usr=13.23%, sys=42.01%, ctx=1703688, majf=0, minf=84
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=232MiB/s (243MB/s), 232MiB/s-232MiB/s (243MB/s-243MB/s), io=9.77GiB (10.5GB), run=43139-43139msec

Disk stats (read/write):
  sdc: ios=2555925/0, merge=3818/0, ticks=2734927/0, in_queue=2734928, util=83.69%


вывод: контрроллер медленный . диски свою скорость не раскрывают.


ДАЛЕЕ:
я перешил контролллер на прошивку IT результатт тот же самый! RAND READ 59.9k IOPS !!
тоесть пршивка невлияет IR или IT!


===========

ОБЩИЙ ВЫВОД НА ДАННЫЙ МОМЕНТ на счет контроллеров:

 лучше всего выжимает голый AHCI порт. скажем 98 kIOPS (samsung 860) тоесть 100% от 
   возможностей диска
 потом с потерей в 2 раза 50-60 KIOPS выжимает LSI 2308\3008 
 потом еще хуже smart array P410. он выжимает всего 26 kIOPS. (тоесть еще в 2 раза хуже)


===========================

lsi 9300 (sas 3008)
диск Model Family:     Hitachi/HGST Ultrastar 7K4000
Device Model:     HGST HUS724020ALA640


лин чтение 150-170 МБ/с
лин запись 150-170 МБ/с

ранд рид 33 ИОПС
ранд врайт 33 ИОПС



пул из 8 таких дисков на zfs
лин запись 460 МБ/с
лин чтение 650 МБ/с
ранд рид всего 150 ИОПС (почему то кадый диск выдает всего 18-30 иопс)
ранд врайт  150 ИОПС (при этом видно что идут паразитные чтения перед записью
нейронка говорит это норм. это он читает метаданные. типа без этого никак)



==============================

| lsi
| sas2008
| SAS9207-8i
| Samsung SSD 860 EVO 1TB
| INTEL SSDSC2KB480GZ


вот такой чип

root@pve:/opt/LSI# ./sas2ircu  list
LSI Corporation SAS2 IR Configuration Utility.
Version 20.00.00.00 (2014.09.18) 
Copyright (c) 2008-2014 LSI Corporation. All rights reserved. 


         Adapter      Vendor  Device                       SubSys  SubSys 
 Index    Type          ID      ID    Pci Address          Ven ID  Dev ID 
 -----  ------------  ------  ------  -----------------    ------  ------ 
   0     SAS2308_2     1000h    87h   00h:02h:00h:00h      1028h   1f38h 
SAS2IRCU: Utility Completed Successfully.



вот такая карта

root@pve:/opt/LSI# ./sas2flash -list
LSI Corporation SAS2 Flash Utility
Version 20.00.00.00 (2014.09.18) 
Copyright (c) 2008-2014 LSI Corporation. All rights reserved 

  Adapter Selected is a LSI SAS: SAS2308_2(D1) 

  Controller Number              : 0
  Controller                     : SAS2308_2(D1) 
  PCI Address                    : 00:02:00:00
  SAS Address                    : 500605b-1-2345-6777
  NVDATA Version (Default)       : 14.01.00.06
  NVDATA Version (Persistent)    : 14.01.00.06
  Firmware Product ID            : 0x2214 (IT)
  Firmware Version               : 20.00.07.00
  NVDATA Vendor                  : LSI
  NVDATA Product ID              : SAS9207-8i
  BIOS Version                   : 07.39.02.00
  UEFI BSD Version               : 07.27.01.01
  FCODE Version                  : N/A
  Board Name                     : SAS9207-8i
  Board Assembly                 : N/A
  Board Tracer Number            : N/A

  Finished Processing Commands Successfully.
  Exiting SAS2Flash.



диск INTEL SSDSC2KB480GZ



RAND READ [70.0k] IOPS

root@pve:/opt/LSI# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdc --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=276MiB/s][r=70.6k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=5018: Tue Oct 14 18:51:18 2025
  read: IOPS=54.5k, BW=213MiB/s (223MB/s)(9.77GiB/46962msec)
   bw (  KiB/s): min=159360, max=283448, per=99.79%, avg=217582.19, stdev=45479.13, samples=93
   iops        : min=39840, max=70862, avg=54395.59, stdev=11369.74, samples=93
  cpu          : usr=20.75%, sys=66.76%, ctx=311765, majf=0, minf=83
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=213MiB/s (223MB/s), 213MiB/s-213MiB/s (223MB/s-223MB/s), io=9.77GiB (10.5GB), run=46962-46962msec

Disk stats (read/write):
  sdc: ios=2550859/0, sectors=20427744/0, merge=2619/0, ticks=1441791/0, in_queue=1441791, util=99.84%


для сраврния этот же диск на  такомже тесте на AHCI 6Gb порту выдает [76.9k] IOPS


этот же контррллер этот же тест
но диск 

   Samsung SSD 860 EVO 1TB


root@pve:/opt/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=./test  --bs=4k --iodepth=64 --size=10000M --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=211MiB/s][r=54.1k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=5367: Tue Oct 14 18:53:18 2025
  read: IOPS=58.3k, BW=228MiB/s (239MB/s)(9.77GiB/43942msec)
   bw (  KiB/s): min=121048, max=271056, per=99.87%, avg=232725.15, stdev=55396.98, samples=87
   iops        : min=30262, max=67764, avg=58181.31, stdev=13849.20, samples=87
  cpu          : usr=17.25%, sys=67.31%, ctx=140265, majf=0, minf=73
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=2560000,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=228MiB/s (239MB/s), 228MiB/s-228MiB/s (239MB/s-239MB/s), io=9.77GiB (10.5GB), run=43942-43942msec

Disk stats (read/write):
    dm-1: ios=2556265/17, sectors=20450120/562, merge=0/0, ticks=2001246/35, in_queue=2001281, util=99.82%, aggrios=2556019/16, aggsectors=20484096/562, aggrmerge=3997/1, aggrticks=2005842/29, aggrin_queue=2005872, aggrutil=72.56%
  sda: ios=2556019/16, sectors=20484096/562, merge=3997/1, ticks=2005842/29, in_queue=2005872, util=72.56%



для сраранеия этот же диск на AHCI порту выдает [98.4k] IOPS
а на 3008 чипе выдает [57.6k] IOPS


этотже контролер проверим лин чтение.
диск INTEL SSDSC2KB480GZ  

o# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdc  --bs=1M --iodepth=1 --size=10000M --readwrite=read
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=libaio, iodepth=1
fio-3.39
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=398MiB/s][r=398 IOPS][eta 00m:00s]
( на AHCI этот диск выдает 361 MB/s)



проверим лин чтение для     Samsung SSD 860 EVO 1TB

# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=./test  --bs=1M --iodepth=1 --size=10000M --readwrite=read
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=libaio, iodepth=1
fio-3.39
Starting 1 process
Jobs: 1 (f=0): [f(1)][100.0%][r=450MiB/s][r=450 IOPS][eta 00m:00s]
( на AHCI этот диск выдает 482 MB/s)




итого на вот этом контролере
 sas2008
 SAS9207-8i

из вот этого диска 
диск INTEL SSDSC2KB480GZ  

он выжимает 
RAND READ [70.0k] IOPS   из максимально возможных  [76.9k] IOPS
LIN READ r=398MiB/s  из такой же максимально возможной


из вот этого диска 
диск    Samsung SSD 860 EVO 1TB

он выжимает 
RAND READ [54.1k] IOPS   из максимально возможных  [98.4k] IOPS
LIN READ r=450MiB/s      из максимально возможной  482 MB/s



еще раз тестирую


диск INTEL SSDSC2KB480GZ  
  RAND READ 
    fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=./test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
        AHCI(linux):                    [84.9k] IOPS
        sas2008(SAS9207-8i)(linux)      [69.9k] IOPS
        sas2008(SAS9207-8i)(solaris)    [15.7K] IOPS   ***солярис полный провал!!***


  LIN READ
    fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdc  --bs=1M --iodepth=1 --size=10000M --readwrite=read
      AHCI(linux):                      403MiB/s
      sas2008(SAS9207-8i)(linux)        401MiB/s
      sas2008(SAS9207-8i)(solaris)      322.0MiB/s   ***солярис провал***




диск Samsung SSD 860 EVO 1TB
  RAND READ 
    fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=./test --bs=4k --iodepth=64 --size=10000M --readwrite=randread
      AHCI(linux)                     [98.9k] IOPS
      sas2008(SAS9207-8i)(linux)      [57.2k] IOPS
      sas2008(SAS9207-8i)(solaris)    [16.8K] IOPS   ***солярис полный провал!!***



  LIN READ
    fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdc  --bs=1M --iodepth=1 --size=10000M --readwrite=read
      AHCI(linux):                    502MB/s
      sas2008(SAS9207-8i)(linux)      484MB/s
      sas2008(SAS9207-8i)(solaris)    449MiB/s   





==============================================================================


связка двух компов

host1
    dell r720 
      Intel(R) Xeon(R) CPU E5-2643 v2 @ 3.50GHz   x 24 ядра
      RAM 192 GB
    proxmox 9
    NIC 10GiB Intel Corporation 82599ES  1xport  9000 mtu


host2
    dell r720 
      Intel(R) Xeon(R) CPU E5-2643 v2 @ 3.50GHz x 24 ядра
      RAM 256 GB
    truenas 25.10.0.1 - Goldeye 
    NIC 10GiB  Intel Corporation 82599ES  1xport  9000 mtu

    диски :
      6 Х HDD HGST_HUS724020ALA640    2TB   (содеиненны в страйп из мирроров)
      1 X SSD INTEL_SSDSC2KB480GZ  480 GB    (исползуется как SLOG)



на хост с прокскскс маунтится NFS шара от трунаса.
в папке зпускается fio  3.39-1 

файл для тестирования размером 3.8 ТБ



LIN READ   1049MiB/s
 
root@pve:/mnt/pve/NFS-2/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test   --filename=/mnt/pve/NFS-2/fio/fio.dat        --bs=4M --iodepth=64 --size=47000M  --readwrite=read
test: (g=0): rw=read, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=1049MiB/s][r=262 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=52913: Mon Nov 24 23:42:01 2025
  read: IOPS=286, BW=1147MiB/s (1203MB/s)(45.9GiB/40972msec)
   bw (  MiB/s): min=  768, max= 1192, per=99.93%, avg=1146.29, stdev=51.45, samples=82
   iops        : min=  192, max=  298, avg=286.56, stdev=12.88, samples=82
  cpu          : usr=0.47%, sys=22.51%, ctx=11391, majf=0, minf=196650
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.3%, >=64=99.5%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=11750,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=1147MiB/s (1203MB/s), 1147MiB/s-1147MiB/s (1203MB/s-1203MB/s), io=45.9GiB (49.3GB), run=40972-40972msec





LIN WRITE   440MiB/s
 

root@pve:/mnt/pve/NFS-2/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test   --filename=/mnt/pve/NFS-2/fio/fio.dat        --bs=4M --iodepth=64 --size=47000M  --readwrite=write
test: (g=0): rw=write, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
^Cbs: 1 (f=1): [W(1)][20.0%][w=440MiB/s][w=110 IOPS][eta 01m:32s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=53497: Mon Nov 24 23:44:48 2025
  write: IOPS=104, BW=417MiB/s (437MB/s)(9700MiB/23270msec); 0 zone resets
   bw (  KiB/s): min=163840, max=614400, per=98.54%, avg=420625.39, stdev=87241.13, samples=46
   iops        : min=   40, max=  150, avg=102.67, stdev=21.31, samples=46
  cpu          : usr=2.88%, sys=12.05%, ctx=1603, majf=0, minf=213253
  IO depths    : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.7%, 32=1.3%, >=64=97.4%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2425,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=417MiB/s (437MB/s), 417MiB/s-417MiB/s (437MB/s-437MB/s), io=9700MiB (10.2GB), run=23270-23270msec





RAND READ    24.7k IOPS

root@pve:/mnt/pve/NFS-2/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test   --filename=/mnt/pve/NFS-2/fio/fio.dat        --bs=4k --iodepth=64 --size=47000M  --readwrite=randread
test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
^Cbs: 1 (f=1): [r(1)][2.3%][r=96.4MiB/s][r=24.7k IOPS][eta 07m:54s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=53711: Mon Nov 24 23:45:43 2025
  read: IOPS=25.0k, BW=97.8MiB/s (103MB/s)(1141MiB/11664msec)
   bw (  KiB/s): min=96808, max=104096, per=100.00%, avg=100322.43, stdev=2150.38, samples=23
   iops        : min=24202, max=26024, avg=25080.61, stdev=537.60, samples=23
  cpu          : usr=12.83%, sys=35.93%, ctx=99131, majf=0, minf=176
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=292141,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=97.8MiB/s (103MB/s), 97.8MiB/s-97.8MiB/s (103MB/s-103MB/s), io=1141MiB (1197MB), run=11664-11664msec
root@pve:/mnt/pve/NFS-2/fio# 




RAND WRITE   2804 IOPS

root@pve:/mnt/pve/NFS-2/fio# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test   --filename=/mnt/pve/NFS-2/fio/fio.dat        --bs=4k --iodepth=64 --size=47000M  --readwrite=randwrite
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.39
Starting 1 process
^Cbs: 1 (f=1): [w(1)][0.9%][w=11.0MiB/s][w=2804 IOPS][eta 36m:56s]
fio: terminating on signal 2

test: (groupid=0, jobs=1): err= 0: pid=53867: Mon Nov 24 23:46:35 2025
  write: IOPS=5354, BW=20.9MiB/s (21.9MB/s)(450MiB/21523msec); 0 zone resets
   bw (  KiB/s): min= 7432, max=84128, per=100.00%, avg=21429.95, stdev=16149.43, samples=43
   iops        : min= 1858, max=21032, avg=5357.49, stdev=4037.36, samples=43
  cpu          : usr=5.45%, sys=14.22%, ctx=82054, majf=0, minf=15
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,115249,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=20.9MiB/s (21.9MB/s), 20.9MiB/s-20.9MiB/s (21.9MB/s-21.9MB/s), io=450MiB (472MB), run=21523-21523msec
root@pve:/mnt/pve/NFS-2/fio# 


=====================================================================

сервер huawei v2 RAM 256GB
HBA карта 
  Controller                     : SAS2308_2(D1) 
  Firmware Product ID            : 0x2214 (IT)
  NVDATA Vendor                  : LSI
  NVDATA Product ID              : SAS9207-8i
  Board Name                     : LSISAS2308



ПУЛ:
record set 128KB
8xHDD(2TB) mirror (по два диска)



ТЕСТ:
LINEAR WRITE (1 поток)
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda  --bs=4M --iodepth=1  --size=2G  --readwrite=write

  TRUENAS 25 LINUX   [46.7MiB/s]
  FREENAS 13 FREEBSD [35.0MiB/s]

тут победил линукс


LINEAR WRITE (64 поток)
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda  --bs=4M --iodepth=64  --size=2G  --readwrite=write

  TRUENAS 25 LINUX   [83.6MiB/s]
  FREENAS 13 FREEBSD [22.4MiB/s]

тут победил линукс


LINEAR READ (1 поток)
по сути это тест чтения из ARC
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda  --bs=4M --iodepth=1  --size=20G  --readwrite=read

  TRUENAS 25 LINUX   [547MiB/s]
  FREENAS 13 FREEBSD [354MiB/s]

тут победил линукс



LINEAR READ (64 поток)
по сути это тест чтения из ARC
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda  --bs=4M --iodepth=64  --size=20G  --readwrite=read

  TRUENAS 25 LINUX   [1161MiB/s]
  FREENAS 13 FREEBSD [1176MiB/s]

тут победил фрибсд



RANDOM READ (1 поток)
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda --bs=4k --size=1G --iodepth=1 --readwrite=randread

  TRUENAS 25 LINUX   [IOPS=3856]
  FREENAS 13 FREEBSD [IOPS=6204]

тут победил фрибсд



RANDOM READ (64 поток)
-gtod_reduce=1 --name=test --filename=/dev/sda --bs=4k --size=10G --iodepth=64 --readwrite=randread

  TRUENAS 25 LINUX   [IOPS=25.1k]
  FREENAS 13 FREEBSD [IOPS=81.2k]

тут победил фрибсд




RANDOM WRITE (1 поток)
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda --bs=4k --size=20M --iodepth=1 --readwrite=randwrite

  TRUENAS 25 LINUX   [IOPS=93]
  FREENAS 13 FREEBSD [IOPS=48]

тут победил линукс



RANDOM WRITE (64 поток)
 fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda --bs=4k --size=400M --iodepth=64 --readwrite=randwrite

  TRUENAS 25 LINUX   [IOPS=1192]
  FREENAS 13 FREEBSD [IOPS=654]

тут победил линукс



RANDOM RW (64 потока)
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda --bs=4k --size=600M --iodepth=64  --readwrite=randrw --rwmixread=75

  TRUENAS 25 LINUX   [read IOPS=2503, write=833]
  FREENAS 13 FREEBSD [read IOPS=1575, write=524]

тут победил линукс



СУММАРНО: я все отдаю победу фрибсд













Добавляю SLOG
ПУЛ:
record set 128KB
8xHDD(2TB) mirror (по два диска)
+SLOG 480GB INTEL SSDSC2KB480GZ  (mirror из двух штук)



LINEAR WRITE (64 поток)
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda  --bs=4M --iodepth=64  --size=20G  --readwrite=write

  TRUENAS 25 LINUX   [359MiB/s]  (для сравнения без SLOG было [83.6MiB/s])
  FREENAS 13 FREEBSD [273MiB/s]  (для сравнения без SLOG было [22.4MiB/s])

видим что добавлка slog дает прибавку в скорости в x5-x10раз

тут победил линукс



RANDOM WRITE (64 поток)
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sda --bs=4k --size=5G --iodepth=64 --readwrite=randwrite

  TRUENAS 25 LINUX   [IOPS=3551] (для сравнения без SLOG было [IOPS=1192])
  FREENAS 13 FREEBSD [IOPS=5537] (для сравнения без SLOG было [IOPS=654])

видим что добавление slog дает прибавку в х3-х10раз

тут победил фрибсд






ТЕСТ:
LINEAR WRITE [383MiB\s]  
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdb  --bs=4M --iodepth=64  --size=10G  --readwrite=write
test: (g=0): rw=write, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=0): [f(1)][100.0%][w=384MiB/s][w=96 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=884: Sun Nov 30 13:24:33 2025
  write: IOPS=95, BW=383MiB/s (402MB/s)(10.0GiB/26706msec); 0 zone resets
   bw (  KiB/s): min=40960, max=434176, per=98.30%, avg=385951.40, stdev=50259.63, samples=53
   iops        : min=   10, max=  106, avg=94.23, stdev=12.27, samples=53
  cpu          : usr=0.89%, sys=0.37%, ctx=2712, majf=0, minf=7
  IO depths    : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.6%, 32=1.2%, >=64=97.5%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=383MiB/s (402MB/s), 383MiB/s-383MiB/s (402MB/s-402MB/s), io=10.0GiB (10.7GB), run=26706-26706msec

Disk stats (read/write):
  sdb: ios=51/10150, merge=0/0, ticks=15/6483057, in_queue=6483073, util=99.10%



тоесть добавление SLOG уведиичисло лин скрость записи в x3раза



тотже тест но на трунас трукор который на фрибсд
ТЕСТ:
LINEAR WRITE [BW=320MiB/s]  
root@stor5:/home/er# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdb  --bs=4M --iodepth=64  --size=10G  --readwrite=write
test: (g=0): rw=write, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=228MiB/s][w=57 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=574: Sun Nov 30 15:57:30 2025
  write: IOPS=80, BW=320MiB/s (336MB/s)(10.0GiB/31995msec); 0 zone resets
   bw (  KiB/s): min=40960, max=548864, per=99.07%, avg=324689.27, stdev=85934.82, samples=63
   iops        : min=   10, max=  134, avg=79.27, stdev=20.98, samples=63
  cpu          : usr=0.86%, sys=0.22%, ctx=665, majf=0, minf=8
  IO depths    : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.6%, 32=1.2%, >=64=97.5%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=0,2560,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=320MiB/s (336MB/s), 320MiB/s-320MiB/s (336MB/s-336MB/s), io=10.0GiB (10.7GB), run=31995-31995msec

Disk stats (read/write):
  sdb: ios=51/10227, merge=0/0, ticks=7/7833665, in_queue=7833672, util=99.24%


далее я запустил вот такой тест
  # dd if=/dev/urandom  of=/dev/sdb bs=4M status=progress

на двух одинаковых абсолюнтно системах.(одинаковые сервера и одинаковые пулы)
но одна из них это трунас25 набазе линукс и сторадж примаунчен по NFS4.2
  12691963904 bytes (13 GB, 12 GiB) copied, 60 s, 211 MB/s
при это скорость скачет. то 211 то 214 то 220
а вторая трунас core13 на базе фрибсд и сторадж примаунчен по NFS3
и скорсть держится просто железная
  12691963904 bytes (13 GB, 12 GiB) copied, 60 s, 261 MB/s
и только через 10 минут он плавно упал до 259МБ/с , но поток держит очень стабильно
а потом  я подключился к этому фрибсд трунас кору через NFS V4 и он уже стал показвыат скорость
  12691963904 bytes (13 GB, 12 GiB) copied, 60 s, 301 MB/s   (!!)
причем стабильно без каких либо рывков
причем взомжно можно лить еще быстрее просто такак urandom то я уперся в скорость цпу одного ядра.



ТЕСТ:
RANDOM RW  [r=2529,w=847 IOPS]
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdb  --bs=4k --iodepth=64   --readwrite=randrw --rwmixread=75 --runtime=120
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [m(1)][100.0%][r=5236KiB/s,w=1904KiB/s][r=1309,w=476 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=893: Sun Nov 30 13:29:00 2025
  read: IOPS=2529, BW=9.88MiB/s (10.4MB/s)(1187MiB/120126msec)
   bw (  KiB/s): min= 3984, max=33888, per=100.00%, avg=10126.03, stdev=8809.10, samples=240
   iops        : min=  996, max= 8472, avg=2531.50, stdev=2202.27, samples=240
  write: IOPS=847, BW=3390KiB/s (3471kB/s)(398MiB/120126msec); 0 zone resets
   bw (  KiB/s): min= 1248, max=11616, per=100.00%, avg=3392.60, stdev=2925.09, samples=240
   iops        : min=  312, max= 2904, avg=848.15, stdev=731.27, samples=240
  cpu          : usr=2.28%, sys=4.66%, ctx=271286, majf=0, minf=9
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwts: total=303830,101800,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=9.88MiB/s (10.4MB/s), 9.88MiB/s-9.88MiB/s (10.4MB/s-10.4MB/s), io=1187MiB (1244MB), run=120126-120126msec
  WRITE: bw=3390KiB/s (3471kB/s), 3390KiB/s-3390KiB/s (3471kB/s-3471kB/s), io=398MiB (417MB), run=120126-120126msec

Disk stats (read/write):
  sdb: ios=303696/101715, merge=0/0, ticks=4506113/3155963, in_queue=7662076, util=99.97%


почемуто увеличмлся read iops, а write iops увеличился в х2раза


тоже самое на трукор фрибсд
ТЕСТ:
RANDOM RW  [r=10.4k,w=3444 IOPS] (!!!!)
root@stor5:/home/er# 
root@stor5:/home/er# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdb  --bs=4k --iodepth=64   --readwrite=randrw --rwmixread=75 --runtime=120
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [m(1)][21.7%][r=40.7MiB/s,w=13.5MiB/s][r=10.4k,w=3444 IOPS][eta 01m:34s]
fio: terminating on signal 2

просто рвет лиункс



добавляю L2ARC
ПУЛ:
8xHDD(2TB) mirror (по два диска)
+SLOG 480GB INTEL SSDSC2KB480GZ  (mirror из двух штук)
+L2ARC 480GB INTEL SSDSC2KB480GZ








ПУЛ:
8xHDD(2TB) mirror (по два диска)
+L2ARC 480GB INTEL SSDSC2KB480GZ 


ТЕСТ:
RANDOM RW  [r=2841,w=965 IOPS]
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdb  --bs=4k --iodepth=64   --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [m(1)][4.2%][r=11.1MiB/s,w=3860KiB/s][r=2841,w=965 IOPS][eta 06h:27m:09s] 


ПУЛ:
8xHDD(2TB) mirror (по два диска)
+PHUSION VDEV 480GB INTEL SSDSC2KB480GZ 


ТЕСТ:
RANDOM RW  [r=1573,w=510 IOPS]
# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdb  --bs=4k --iodepth=64   --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [m(1)][2.8%][r=6292KiB/s,w=2040KiB/s][r=1573,w=510 IOPS][eta 11h:53m:35s]  


из чего я делаю вывод если выбор стоит добавить L2ARC или PHUSION 
лучше добаить L2ARC и будет прибавка в скорости чтения в х5раз и в записи как ни странно в х5раз

===============================


AHCI
диск ST380811AS
тестируею просто как блочное устрйотвао ada2 (никкой фс пока еще нет)

LIN READ 1 pototok  68.0MiB/s
# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada2  --bs=4M --iodepth=1  --size=20G  --readwrite=read



LIN READ 6 pototok  159.0MiB/s
# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada2  --bs=4M --iodepth=1  --size=20G  --readwrite=read --numjobs=6



LIN WRITE 1 potok  44.0MiB/s
# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada2  --bs=4M --iodepth=1  --size=20G  --readwrite=write

при записи болше потоков нет смысла давать




RAND READ 1 potok   130 IOPS

# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada2 --bs=4k --size=1G --iodepth=1 --readwrite=randread


RAND READ 6 potok   177 IOPS




RAND WRITE 1 potok   244 IOPS

RAND WRITE 6 potok   244 IOPS






пеереходим к ФС  и датасетам
датасет POOL-01/ds1-128K  
        с рекордсайз 128К

# dd if=/dev/urandom of=/POOL-01/ds1-128K/fio.dat bs=10M count=1000 status=progress
  10475274240 bytes (10 GB, 9990 MiB) transferred 216.169s, 48 MB/s 


датасет POOL-01/ds1-16K  
        с рекордсайз 16К


 # dd if=/dev/urandom of=/POOL-01/ds1-16K/fio.dat  bs=10M count=1000 status=progress
  10454302720 bytes (10 GB, 9970 MiB) transferred 206.049s, 51 MB/s 
1000+0 records in
1000+0 records out
10485760000 bytes transferred in 206.795741 secs (50705880 bytes/sec)


видно счто скорости не отлиаются. что ожидаемо. потому что при записи все блоки ложатся
рядом другс друогрм и на диск летят уже макрозапросы на запись. 


такеже видно что скорсть запси совпдает когда мы деали тоже самое но в сырое устройствао /dev/ada2


















root@fr:~ # dd if=/dev/urandom bs=129K count=1 of=/POOL-01/ds1-128K/t1.dat
1+0 records in
1+0 records out
132096 bytes transferred in 0.223978 secs (589772 bytes/sec)
root@fr:~ # 
root@fr:~ # zdb POOL-01/ds1-128K -O t1.dat
^C
root@fr:~ # ls -i /POOL-01/ds1-128K/t1.dat 
128 /POOL-01/ds1-128K/t1.dat
root@fr:~ # zdb -ddddd POOL-01/ds1-128K  128
^C
root@fr:~ # 
root@fr:~ # zdb -ddddd POOL-01/ds1-128K  128
Dataset POOL-01/ds1-128K [ZPL], ID 82, cr_txg 18, 9.77G, 9 objects, rootbp DVA[0]=<0:20102000:1000> DVA[1]=<0:400c0000:1000> [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=792L/792P fill=9 cksum=000000127f1a41e7:0000310d4041d42b:00452baaeaf56abf:4505ac86c91e5b13

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
       128    2   128K   128K   264K     512   256K  100.00  ZFS plain file
                                               168   bonus  System attributes
  dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED 
  dnode maxblkid: 1
  path  /t1.dat
  uid     0
  gid     0
  atime Wed Dec  3 21:44:51 2025
  mtime Wed Dec  3 21:44:51 2025
  ctime Wed Dec  3 21:44:51 2025
  crtime  Wed Dec  3 21:44:51 2025
  gen 792
  mode  100644
  size  132096
  parent  34
  links 1
  pflags  40800000004
Indirect blocks:
               0 L1  0:7e0d0b000:1000 20000L/1000P F=2 B=792/792 cksum=0000008a1055c177:0001f8e9783a42f0:039e04881bc455af:6f998cb7a9e6e034
               0  L0 0:200b8000:20000 20000L/20000P F=1 B=792/792 cksum=00003fe244f8f8d5:0fff1f0d050b8d04:7393811299ba483b:002826f1577fa6d3
           20000  L0 0:200d8000:20000 20000L/20000P F=1 B=792/792 cksum=00000083017f37d0:00413ecee08a7013:3f6dc51befcd721f:97564c0d7d9a0ebb

    segment [0000000000000000, 0000000000040000) size  256K

root@fr:~ # 
root@fr:~ # dd if=/dev/urandom bs=17K count=1 of=/POOL-01/ds1-16K/t2.dat
1+0 records in
1+0 records out
17408 bytes transferred in 0.000139 secs (125097912 bytes/sec)
root@fr:~ # 
root@fr:~ # ls -i /POOL-01/ds1-16K/t2.dat 
3 /POOL-01/ds1-16K/t2.dat
root@fr:~ # 
root@fr:~ # zdb -ddddd POOL-01/ds1-16K  3
Dataset POOL-01/ds1-16K [ZPL], ID 90, cr_txg 23, 9.82G, 9 objects, rootbp DVA[0]=<0:7e0d34000:1000> DVA[1]=<0:8008f000:1000> [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=830L/830P fill=9 cksum=0000001010bbfbcf:00002b96b8401d5c:003f06069ee60b2f:406c88b76c56e9a6

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
         3    2   128K    16K    40K     512    32K  100.00  ZFS plain file
                                               168   bonus  System attributes
  dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED 
  dnode maxblkid: 1
  path  /t2.dat
  uid     0
  gid     0
  atime Wed Dec  3 21:49:32 2025
  mtime Wed Dec  3 21:49:32 2025
  ctime Wed Dec  3 21:49:32 2025
  crtime  Wed Dec  3 21:49:32 2025
  gen 830
  mode  100644
  size  17408
  parent  34
  links 1
  pflags  40800000004
Indirect blocks:
               0 L1  0:e2134b000:1000 20000L/1000P F=2 B=830/830 cksum=00000088011c83ca:0001f0ccf3b0b99c:038e02430fb013c5:5a7e4794462e127e
               0  L0 0:7e0cea000:4000 4000L/4000P F=1 B=830/830 cksum=000007f34c07b8f9:004052918a339357:58705b8f51f31591:45bc9fa6fff2aa6a
            4000  L0 0:7e0ce6000:4000 4000L/4000P F=1 B=830/830 cksum=000000803bafb030:0007c3d05cc320bc:3c383ca009ad19cc:88f03314d87278b8

    segment [0000000000000000, 0000000000008000) size   32K

root@fr:~ # 



полное подврждение ттогго как работает рекордсайз



как приавлно создать даатсет для тестов fio

 #  zfs create  -o compression=off -o  primarycache=metadata  -o secondarycache=metadata -o recordsize=4KiB  POOL-02/ds2-4K



(!!!) на даный момент я оанражил поразительную вещь. если у нас есть датасет с рекордсайз=А
и я читаю линейно с такиже размером блока запроса с него то число иопс в фио совпадает с числом иопс в 
гстат. а если я читаю рандомно то число иопс в фио падает в 2 раза чем число иопс которое читается с диксска.
при том что среднее число блока запроса если поедлить срупут на число иопсов в гстат осатется все равно 
раным рекодрсайз! тоесть зфс при чтении каждого полезного блока данных читает еще и чтото паразиитное этого же 
размера! я поумал можетэто какоето вырываниение несовпдает. но я создал пул с ашифт=9. и создал его 
на базе просто диска без всяких партиций. и ситуация такаяже саамая. приче неважно какой рекодрсет хотьт 128к
хотьь 16к хоть 4к. ...пц ккойто непонятный. нейронка утведрлает что арк некещиуерует все мтаданные а только 
метаднынеуровня1. а их там может быть 6 уровней. и они считаютс с диска. и то что при рандомном чтении
зфс читает адрес где искать блок. непонятно пока где правада...





