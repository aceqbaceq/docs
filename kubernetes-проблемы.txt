---

вылезла такая тема.

как обычно начал выводить ноду из кластера через 
# kubectl uncordon
# kubectl drain --ignore-daemonsets

и оно мне счегото написало что он там немжоет удалить 
под metrics-server и чтобы его убрать с ноды то надо дополнительно
заюзать ключ  --delete-local-data

это конечно отстой какойото.

язаюзал этот ключ. нода убралась из кластера.

далее мне надо было погасить еще одну ноду в кластере.
 я решил ее небирать из кластера через cordon и drain  чтобы неюзать
 этот стремный ключ --delete-local-data  а просто ноду выключить. 
 
 а потом в контрол панели после этого уже все таки ее исключить из кластера.
 
 и такой трюк вроде как бы прошел. по крайней мере уже drain нетребовал 
  --delete-local-data
  
  далее. там когда я запускад
  
  # kubectlk drain --ignore-daemonsets  --delete-local-data
  
  то у меня команда застывала впадая в цикл и на экране шли повторяющиеся строчки
  
 с
  
  что говорило о том что как я понял кубстл посылает реквест в апи навыполеннеие 
  а он его неотвечает . невыпленяет его.
  
  в итоге я нажал ctrl+C чтобы заврешить этот незаканчивающийся дрейн процесс.
  
  
  какоето время спустя  все ноды 
  были вернуты в кластер я проверил статус нод 
  и получил что  ноды имеют статус not ready.
  причем и те ноды что я исключал и те что нетрогал. и дата ноды и мастера.
  
  вот ради этой проблемы я   и пишу.
  
  
  начал я с мастеров.
  
  перезапуск kubectl ничеонедает.
  перезауск докера тоже ничего недает.
  
  что только работало - это перезапуск всей ноды целиком.
  
  статус пеерходил в ready.
  
  однако перезапускать дата ноды нехотелось. это уже нетакая ерунда как перезапуск
  мастер нод.
  
  и тут я решил проверил статус нетлоько нод но и подов.
  и увидел что оно показывает что куча подов находится в terminated состоянии.
  хотя по факту поды пахали отлично.
  
  
  витоге я предоположил что может быть надо перезапустуть ту контрол панель
  из которой я пулял команды.
  
  я ее перезагрузил.
  
  и тут же и все поды и все ноды в статусе показали = ready.
  
  вот такая история.
  
  
  куб это мудота та еще. очень высокая стоимость облсуживания по мне.
  
  мне кажется ошибки тут такие:
  1) если нода была выключена. то ненадо ее дрейном выводить из кластера задним числом 
  пока она недоступна
  2) если запульнул команду kubectl drain 
  а она все никак неможет отработать засирая экран сообщением 
 
   kubectl Throttling request took
  
  то походу пора остановиься и недвигаться дальше.
  перезагрузить комп с которого ты пуляешь эту команду. и перезагрузит активную 
  мастер ноду.
  
  убедиться что статус нод и подов ready. и тока потом двигаться дальше.
  
  проблема куба в том что он мудак . он сообщает что все ок. а на самом деле
  ок это только то что он принял от тебя желаемое состояние которого мы от него
  хотим достичь. и он может потом часами пытаться его достичь.
  
  вотбщем от этого вот notready состояния нод terminating состояния подов
  спасло перезагрузка мастер ноды скоторой я пулял команды.
  
  полезная вещь:
  чтобы оченить здоороьве куба надо посмотреть статус нод , подов
  
  # kubectl get nodes
  # kubctl get pods -A
  
  и еще полезно вот такое
  
  # kubectl get all -A
  
  
  =============
  
  в докере козлы они недают качать бесплтано образы более чем дцать штук в день.
  поэтому в манифестах везде где мы указываем скачать образ укаыаем опциюб 
  чтобы куб качал образ только если его нет локально
  потому что по дфеолту каждый раз когда он стартует под он старается 
  залещть в инет и скачать образо оттуда
  
  поэтому везде в мангифесте где у нас стоит
  
  image:
  
  рядышком припиываем
   image:  
   imagePullPolicy: IfNotPresent
  
нпример

  image: docker.elastic.co/elasticsearch/elasticsearch:7.7.1
  imagePullPolicy: IfNotPresent

иначе пара перезаусков пода и он больше не старатент

  
==========================================
  
 еще раз подчеркну что если в statefullset
 мы меняем настройки дисков
 то куб такое непримет.
 едиснвтенный выход это удалять текущий стейтфуулсет 
 полностью.
 менять в нем настройки дисков и только потом его накатывать 
 чтобы он его развернул с нуля.
 
 и дальнейшийи момент или подьебка также в том что 
 когда мы удалили стейтфул сет  
 а он в свлою очередь  удалил поды то при это pvc (запрос на аренду
 pv) он остается и система pvc  его нестирает.
 а значит когда мы поменяли настройки дисков то стейтфулл 
 сет будет создавать\запрашивать pvc ровно с такими же именами
 что и были раньше а поскольку они уже есть в системе
 то в конечном итоге к нашим новым подам будуь пдключены наши ровнро старые
 диски.
 
 так что с одной стороны удаление stetfeullset неудаляет данные.
 и это хорошо.
 но с другой стороны после удаления statefullset надо еще и самому руками
 удалять pvc 
 чтобы SS создал новые pvc которые будут конектится к другим PV
 
 ====
 