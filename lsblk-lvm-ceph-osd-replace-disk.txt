| lsblk
| lvm
| ceph
| /dev/sda
| osd
| replace
| disk
| ssd


да уж.... щас трактат будет



беерем lsblk 
и он зачем то рисует иерархию блочных файлов

NAME                                          MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
sda                                             8:0    0 232,9G  0 disk  
├─sda1                                          8:1    0 224,1G  0 part  


что это за хрень


sda                                             
├─sda1                                         


во первых что дает блочный файл для юзер программы.
программа открыаывет этот файл  и далее программма указывает смещение и пихиает 
байты а драйвер этого спец файла уже  используя реальную лоу левел физику диска
пихает на него данные. от юзер программы эта лоу левел физика лоу левел адресация
скрыта. условного говоря мы в программе пишем запиши 4 байта  с позиции 45.
а драйвер уже в реальности использует некую диковинную адресацию (сектор 46, головка 4,
трек 15, блин11).


в чем разница если мы из проги откроем sda1 и туда сунем тот же самый запрос.
запиши 4 байта  с позиции 45.
разница в том что драйвер который сидит за sda1 он сдвинет наш запрос на то расстояние на котором
находится начал партишена 1 на диске. и пптом пеердаст модифицированный запрос уже на
драйвер который обслуживает sda !! вот как хитро! тоесть при налчии на диске партишена 
мы можем в юзер программе и сами вычислить смещение котоорое советсвтует позиции уже на партишене
тоесть мы хотим записать в позции 4 от начала партитишена. мы знаем что партишен начинатся
с с позиции 100. тогда мы можем сказать запиши в позиции 104 и передать запрос на sda.
но если мы не хотим парится думать и выяснять и помнить  скакой позиции у нас 
начинается пртишен то нам проще переадать запрос на sda1 а он уже сам посчитает насклко 
нужно сдвинуть наш оффсет при передаче запроса к драйвер sda

таким образом вот эта иерархия

sda                                             
├─sda1                                         


она означает то что если мы откроем sda1 и передадим ему запрос то он поподает в драйвер
обработки этого спец файла тот дарйвер запрос  модфиицирует  и передст на обработку
следующего драйвера  в иерархии тоесть драйвера sda

в данном случае смысл драйвера sda1 очень прстой - это добавлять смещение к исходному
запросы так чтобы в итоге при предаче модфиицрованного запроса на sda в итоге
данные на диске запыавались относительно первого партишена.

но может быть более сложная иерархия


sda                                             
├─sda1                                         
   ├─dm-0
      ├─luks1




она раотае вот как. если мы передадим данные на блочное устройство /dev/luks1
то их обработает шифрование LUKS и передаст в LVM драйвер. тот обрботает и передаст
на sda1 драйвер, тот тоже модфиициирует и передаст на драйвер sda и толкьо потом
данные поппудут на физ диск.
тоесть это как сетевой стек. если мы открывает блоч устройство как мжоно ниже 
в иерархии то наши исхоные данные обрбатвыаются кучей драйверов прежде чем 
они попадут на физ диск. 

что делает LVM драйвер. и его блочные устройства

# lsblk ...

sda
   ├─dm-0


sdb
   ├─dm-0



если я пишу из юзер прграмы в блоч устройство dm-0 
то данные попадают в lvm драйвер. он модфициирует запрос
и посылает данные на sdb и sda диски.
таким образом если юзер приожение пищет в LVM блоч файл то для него
абстрагируется забота на какие физ диски и по какому закону суются байты. эта заботу
беерет дарййвер lvm

тоест теперь сьало понятно как работает lvm и что он конкретно делает 
и какой смысл его блоч файлов.


у lvm его прикольная приколюха.
вот у нас ест исхдное блочное устройство на котором построен lvm 
тоесть диск или раздел. или что угодно но пусть будет просто диск.

$ pvs
 /dev/sdg

котрый входит в состав VG=vg01
на котором нарезан LVS=lv1

так вот мы можем все данные из sdg мигрировать на другой физ диск и это 
будет для юзер приложения прозрачно.

добавляем новый диск в VG

 # pvcreate /dev/sdh
 # vgextend vg01 /dev/sdh
 # pvmove /dev/sdg /dev/sdh

если у нас на sdg неостанется ни одного lvs то 
lsblk покажет пусто

вот для примера




# vgextend vg01 /dev/sde



# lsblk
NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sdc            8:32   0    3G  0 disk 
`-vg01-lvol1 254:1    0    2G  0 lvm  

sdd            8:48   0    2G  0 disk 
|-vg01-lvol0 254:0    0    1G  0 lvm  
`-vg01-lvol1 254:1    0    2G  0 lvm  

sde            8:64   0   10G  0 disk 


# pvmove -n lvol1 /dev/sdc /dev/sde
  /dev/sdc: Moved: 9.98%
  /dev/sdc: Moved: 100.00%


# lsblk
NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sdc            8:32   0    3G  0 disk 

sdd            8:48   0    2G  0 disk 
|-vg01-lvol0 254:0    0    1G  0 lvm  
`-vg01-lvol1 254:1    0    2G  0 lvm  

sde            8:64   0   10G  0 disk 
`-vg01-lvol1 254:1    0    2G  0 lvm  



как видно из sdc все ушло а на sde все пришло.
двигать можно как все lvs так и отдельные lvs



таким макаром можно перемещать данные с диска на диск
под упроавнеим LVM прозрачно для приоложений ОС.
и таким образом менять почти сломанные диски.




я не знаю работает ли такой трюк для диска который загрузочный для ОС
что совсем было бы приколльно.





СЛЕДУЮЩИЙ момент это
то что такой трюк помагает заменить диск в CEPH OSD без необходимости
удалять этот OSD. 

обычно как все делается. удаляется OSD при этом надо дохера ждать пока
цеф пеерместить ПГ с этого диска и перерпапределить их их по кластеру.
(отдельная тема смотри ceph2.txt) 
а потом обратно когда диск заменили то обратно создавать OSD и еще раз
ждать пока данные зальются на диск. это ад какойто.
так вот так как цеф  каждый диск под него она создает VG и LVS да еще
индиивидульно то можно встсваить +1 новый SSD в сервер, 
и используя  pvmove  прозрачно для цеф неудаляя OSD перемесить данные на новый
свежи SSD . а старый потом изьять. и все это без удаления OSD 
и всей этой мегадвижухи на кластере по перемещению ПГ.!!!!
там толко мудота как найти этот LVS/VG на сервере с цеф. 

ниже набросок такого кейса
правда хреного расписан в диалоге с ИИ:






слушай прикольная щштука pvmove
он позвялет передвинуть как все блоки lvm из диска1 на диск2 так и только 
блоки кооторые связаны с конкретным lvs

вот смотри я подвинул lvs lvol1
с одногодиска на другой 

root@HOSTNAME1:~# lsblk
NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda            8:0    0   15G  0 disk 
|-sda1         8:1    0    1M  0 part 
|-sda3         8:3    0    1G  0 part 
`-sda4         8:4    0   14G  0 part 
sdb            8:16   0   15G  0 disk 
|-sdb1         8:17   0    1M  0 part 
|-sdb3         8:19   0    1G  0 part 
`-sdb4         8:20   0   14G  0 part 
sdc            8:32   0    3G  0 disk 
`-vg01-lvol1 254:1    0    2G  0 lvm  
sdd            8:48   0    2G  0 disk 
|-vg01-lvol0 254:0    0    1G  0 lvm  
`-vg01-lvol1 254:1    0    2G  0 lvm  
sde            8:64   0   10G  0 disk 
sr0           11:0    1 1024M  0 rom  
root@HOSTNAME1:~# pvmove -n lvol1 /dev/sdc /dev/sde
  /dev/sdc: Moved: 9.98%
  /dev/sdc: Moved: 100.00%
root@HOSTNAME1:~# 
root@HOSTNAME1:~# lsblk
NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda            8:0    0   15G  0 disk 
|-sda1         8:1    0    1M  0 part 
|-sda3         8:3    0    1G  0 part 
`-sda4         8:4    0   14G  0 part 
sdb            8:16   0   15G  0 disk 
|-sdb1         8:17   0    1M  0 part 
|-sdb3         8:19   0    1G  0 part 
`-sdb4         8:20   0   14G  0 part 
sdc            8:32   0    3G  0 disk 
sdd            8:48   0    2G  0 disk 
|-vg01-lvol0 254:0    0    1G  0 lvm  
`-vg01-lvol1 254:1    0    2G  0 lvm  
sde            8:64   0   10G  0 disk 
`-vg01-lvol1 254:1    0    2G  0 lvm  
sr0           11:0    1 1024M  0 rom  


и явот о чем подумал. 

возьмем ОСД у цеф. который состоит из двух дисков . один шпиндельный под дата блоки и второй ssd под rocks.db который на ссд.
этот ссд под нагрузкой он протирается. и его нужно менять.
как это делается обычно. обычно удаляется osd . меняется диск. потом заново создается осд.
это очень долго потому что происходит движние ПГ на кластере.

аможно это сделать незавметно для цеф.

вот берем ОСД

====== osd.3 =======

  [db]          /dev/ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a/osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712

      block device              /dev/ceph-4ea7bc65-5f81-4fe8-b764-254e10ed98a9/osd-block-11979a88-4b56-491b-adb2-6c9db70f653f
      block uuid                ZSbXJ2-voGU-hjrI-eMAv-6Gc6-1vQd-peGhLz
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        hdd
      db device                 /dev/ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a/osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712
      db uuid                   Mhl176-LGXv-PVhf-Bt5z-aOP9-jbqV-f2TGfy
      encrypted                 0
      osd fsid                  11979a88-4b56-491b-adb2-6c9db70f653f
      osd id                    3
      osdspec affinity          
      type                      db
      vdo                       0
      devices                   /dev/sdg

  [block]       /dev/ceph-4ea7bc65-5f81-4fe8-b764-254e10ed98a9/osd-block-11979a88-4b56-491b-adb2-6c9db70f653f

      block device              /dev/ceph-4ea7bc65-5f81-4fe8-b764-254e10ed98a9/osd-block-11979a88-4b56-491b-adb2-6c9db70f653f
      block uuid                ZSbXJ2-voGU-hjrI-eMAv-6Gc6-1vQd-peGhLz
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        hdd
      db device                 /dev/ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a/osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712
      db uuid                   Mhl176-LGXv-PVhf-Bt5z-aOP9-jbqV-f2TGfy
      encrypted                 0
      osd fsid                  11979a88-4b56-491b-adb2-6c9db70f653f
      osd id                    3
      osdspec affinity          
      type                      block
      vdo                       0
      devices                   /dev/sdb



во первых я щас перегруппирую вывод
уберу мусор

  [db]          /dev/ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a/osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712
      type                      db
      devices                   /dev/sdg





  [block]       /dev/ceph-4ea7bc65-5f81-4fe8-b764-254e10ed98a9/osd-block-11979a88-4b56-491b-adb2-6c9db70f653f
      type                      block
      devices                   /dev/sdb


значит у нас osd использует два диска. причем он их использует
через lvm 
 

значит под базу юзает блочное устройство 
 /dev/ceph-4ea7bc65-5f81-4fe8-b764-254e10ed98a9/osd-block-11979a88-4b56-491b-adb2-6c9db70f653f

далее я  делаю lvdisplay 
и узнаю что это блочное устройство это lvs

m# lvdisplay  | grep   osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712      -A14 -B2
File descriptor 9 (pipe:[3946960155]) leaked on lvdisplay invocation. Parent PID 799954: bash
File descriptor 11 (pipe:[3946960156]) leaked on lvdisplay invocation. Parent PID 799954: bash
  --- Logical volume ---
  LV Path                /dev/ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a/osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712
  LV Name                osd-db-86858d7d-99eb-4aad-ba93-a17cd8fe3712
  VG Name                ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a
  LV UUID                Mhl176-LGXv-PVhf-Bt5z-aOP9-jbqV-f2TGfy
  LV Write Access        read/write
  LV Creation host, time hst1, 2025-08-08 17:06:15 +0300
  LV Status              available
  # open                 12
  LV Size                <83.84 GiB
  Current LE             21462
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0


также я узнаю VG а зная его узнаю диск на котором лежит VG а значит и lvs 
с базой
# pvs | grep -E "VG|ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a"
File descriptor 9 (pipe:[3946960155]) leaked on pvs invocation. Parent PID 799954: bash
File descriptor 11 (pipe:[3946960156]) leaked on pvs invocation. Parent PID 799954: bash
  PV         VG                                        Fmt  Attr PSize    PFree   
  /dev/sdg   ceph-058ec30e-367a-45a0-b0ba-7c5fd0724a3a lvm2 a--  <447.10g  363.26g


это /dev/sdg

так вот!
мы можем втсвить в сервер +1 новый ссд диск.
и использовать команду

pvmove /dev/sdg /dev/sdq

и тогда мы переменстим базу OSD.4 невидимо и прозрачно для цефа. и ненужно 
для этого удалять ОСД а значит двигать ПГ на кластере!
а после того как мы сдвинем данные из sdg 
мы можем искючить его из VG и убрать из сервереа.

толко коенчно муторно все это искать.


