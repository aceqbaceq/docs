--
следущая тема 
local pv volumes - local persistent volumes
что это такое и 
в чем разница с hostpath volumes

hostpath монтирует файл и каталог в под.
local volme тоже позволет монтировать файл каталог в под.
local volume нельзя создавать динамически. то есть PV которые опираются
на local volume приедтся создавать только руками.

когда мы создаем PV который опирается на Local нужно в PV указать
nodeAffinity такми макаром мы знаем и куб знает что этот PV будет создан 
с привязкой к папке на такой то конкретной ноде. 

пример PV которй опирается на local

apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node


PV базируется на папке /mnt/disks/ssd1 которая лежит на хосте example-node

режим монтирования volumeMode: Filesystem можно указать нетолько filesystem но и блок тогдда вольюм будет как блочный примаунчен (так для справки)

хотя PV которые опираются на local немогут быть созданы динамически 
а только руками но при этом обращаться к таким PV можно через storage class (просто фишка в том что изначально сторадж классы придуманы для случаев динамического создлания PV. типа под создаем pvc который обращается к storageclass а сторадж класс по запросу создает pv и связывает pvc и pv.) это нам дает то что мы можем в развораичвать поды
опирающиеся на PV которые опираются на local в полуавтомтическом режиме.
мы руками создаем кучу PV (local) . потом создаем storageclass. и потом
в например statfullset укащываем этот класс и скажем выставляем в стейтфуллсет реплика = 10. и у нас стетйфулл сет каждый раз обращается 
к сторадж классу и он автоматом находит pv для очередного пода в реплике
таким образом мы разворачиваем 10 подов в автоматическом режиме указав
один раз страджкласс для всех этих подов. единственное что сами PV мы создаем неавтоматическом режиме а руками.

сторадж класс который умеет работать с pv(local) 

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

он опирается на провижионер kubernetes.io/no-provisioner

(уходя в сторону что эти мудаки из куба они самую насущную и самую 
частую ситуацию когда ты хочешь при развороте подов использовать локальные
диски сервера сделали самой сложной в реализации. дебилы. в 100% раз прощше завести поды на glusterfs ceph nfs итп но только не на локальных
дисках)

они что то еще про эту штуку отмечают 
	volumeBindingMode: WaitForFirstConsumer
но я пока немогу понять что они говорят. походу она означает то что в классическом варианте когда у нас создается pvc то он мгнровенно ищет подходящий для себя PV им приклеивается к нему. непонимаю в чем тут проблема. а с этой настройкой pvc который создан ник чему нихера неприклеивается ни к какой PV. только когда шедулер собирается создать pod на какойто ноде и этот pod будет юзать конкретный pvc только тогда 
pvc начнет искать подходящий PV на этой ноде. типа того.
возможно логика тут вот в чем. возможно - вот мы создали первым pvc. пода еще нет. и этот pvc по признаку размер диска + имя сторадж класса видит что есть несколько pv которые лежат на разных нодах и этот pvc приклелаятеся как я понимаю раз уж под выбор попадает несколько pv к одному из них рандомно. и поскольку эти pv несетвые а жестко доступны 
только в рамках такой то ноды то получается что наш pvc жетско привязан к такойто ноде. далее мы хотим создать под и в свойствах пода мы прописали наш конкртеный тот самый pvc. тогда получается что теперь мы обязаны запустить этот pod только на этой ноде ну и типа наверно это непраивильно ибо по каким то причинам нам может быть невыгодно запускать под на этой ноде по сравнению с другими ( типа наноде щас мало памяти или все процы заняты) и нам было бы выгоднее запускать этот под на другой ноде но уже поздно так как этот pvc приклеился не к тому pv.
и типа общеархитектурно более правильно чтобы шедулер вначале решил на какой ноде исходя из цпу\память\прочее выгоднее запустить новый под, окей мы определись с нодой а только после этого давать команду для pvc кооторый прописан в поде чтобы он искал именно на ЭТОЙ НОДЕ которую выбрал шедулер на ней искать подходящий по размер диска\имя сторадж класса подходящий PV. таким макаром pvc подсраивается под запросы пода(или шедулера) а не под подстрваиется под pvc. 
вомзожно вот такая здесь логика. опять же спасибо дебьильному описанию куба. итак еще раз эта насройка volumeBindingMode: WaitForFirstConsumer говорит pvc когда она создана чтобы она неломилась искать подходящий pv. а ждала когда шедулер выберет на какую ноду он хочет публиковать под.и типа вот именно на этой ноде тогда чтобы pvc искала подходящий pv. я так понял.
так что эту опцию надо обязателно вставлять в сторадж класс. а лучше сразу юзать тот сторадж класс который сами кубисты предлагают для pv(local)


в другом месте  я нащел что типа hostpath предназначен тока для тестинга
и девелопинга и для куба состоящего из одной ноды. это я все пытаюсь
погять в чем разница между hostpath и local. из этой дурацкого описания
можно типа делать вывод что про hostpath можно забыть нахер. 
будем считать что hostpath для одногонодового кластера а local это тоже самое но уже для многонодового кластера. вот типа и разница.

далее они пишут(в другом месте) что hostpath может быть указан напрямую в поде без участия pvc а local неможет быть прописан напрямую в поде к local можно обратиться только через pvc.

дальше они пишут очень путанную и темную вещь - что pod который связан с 
hostpath его шедулер может начать двигать на другую ноду что жопа. возникает вопрос а какого хера шедулер так делает? а дескать pod который связан с local его шедулер на другую ноду двигать никогда небудет. возникает вопрос почему в чем разница. а потому они говорят что шедулер знает что local к которой  под подвязан лежит на такой то ноде. охты! а почему для hostpath шедулер этого незнает? дебилизм. а знает шедулер это скорей всего потому что (тут тоже темнота) в pv(local) вроде как обязательно надо прописывать nodeaffinity. так вопрос а что в PV(hostpath) что мешает nodeafinity прописать.

общий вывод такой - от hostpath в продакшн отказатся от греха подальше.
он типа для девелопмент тестовых однонодовых кубов. а вот для продакшена 
вместо него local. хотя все это из пальца высосано. запутано на пустом месте. 

возможно на практике мы имеем вот что. мы описваыем pod template в statefull set и там мы нифига неописываем никакие Nodeaffinity а просто указываем сторадж класс. в коотором тоже никаких нод аффинити нет. это нам дает возможность аобсолютно свободно прибавлять реплкии подов в стейтфуллсет. для нее это все прозрачная херня под капотом. зато мы указвыаем nodeaffinity в свойствах pv(local) и типа шедулер понимает что  этот pv на этой ноде а тот pv на той ноде. ну и типа шедулер небудет двигать пытаться поды с ноды на ноду . либо под работает на той же ноде либо вобще неработает. а стейтфулл сет эттго ничего незнает для него это все прозрачно.

описано все дебильно. запутанои недоговрено на пустом месте 


еще раз замечу что если в pvc прописан сторадж класс и в pv прописан 
тот же сторадж класс то этот pvc приклеивается к pv. при этом сам сторадж класс вообще может отсутстовать в системе и он нихера неделеат ничего. сторадж класс просто как признак соотвествия.


далее идет целая портянка на тему того что из того что я выше описал
получается что pv(local) надо создавать руками. это конечно же гавно.
и кубисты придумали способы как это неделать руками. вот здесь старт 
описания как они это предлагаю автоматизировать
https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner

общая канва логики такая. создать некотоыре хрени в кубе. после этого на каждой ноде будет достаточно создавать папки по определенным путям а куб будет автоматом создавать для них PV.
чтож это лучше чем все это делать руками.

далее в другом месте у них я нашел что они пишут явно что pv(local) содержит инфо о nodeaffinity поэтому система(что за система?) использует
эту инфо чтобы распределять нужные поды куда надо. 


про local provisioner. он нифига небудет работать (то есть он не будет создавать PV) пока 
несоздать storage class. !!!!!

как установить local provisioner:
	собственнно об этом путанно написано здесь:
		https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner

мой упрощенный вариант:
	$ git clone --depth=1 https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner.git

	$ helm template ./helm/provisioner > deployment/kubernetes/provisioner_generated.yaml

	открываем файл provisioner_generated.yaml и редактируем его.
	там надо будет заменить 'RELEASE-NAME' на чтото другое типа local например в итоге у нас будет вот такое :
	
	
		---
		# Source: provisioner/templates/serviceaccount.yaml
		apiVersion: v1
		kind: ServiceAccount
		metadata:
		  name: local-provisioner
		  namespace: default
		  labels:
			helm.sh/chart: provisioner-3.0.0
			app.kubernetes.io/name: provisioner
			app.kubernetes.io/managed-by: Helm
			app.kubernetes.io/instance: local
		---
		# Source: provisioner/templates/configmap.yaml
		apiVersion: v1
		kind: ConfigMap
		metadata:
		  name: local-provisioner-config
		  namespace: default
		  labels:
			helm.sh/chart: provisioner-3.0.0
			app.kubernetes.io/name: provisioner
			app.kubernetes.io/managed-by: Helm
			app.kubernetes.io/instance: local
		data:
		  storageClassMap: |
			fast-disks:
			  minResyncPeriod: "1m0s"
			  hostDir: /mnt/fast-disks
			  mountDir: /mnt/fast-disks
			  blockCleanerCommand:
				- "/scripts/shred.sh"
				- "2"
			  volumeMode: Filesystem
			  fsType: ext4
			  namePattern: "*"
		---
		# Source: provisioner/templates/rbac.yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRole
		metadata:
		  name: local-provisioner-node-clusterrole
		  labels:
			helm.sh/chart: provisioner-3.0.0
			app.kubernetes.io/name: provisioner
			app.kubernetes.io/managed-by: Helm
			app.kubernetes.io/instance: local
		rules:
		- apiGroups: [""]
		  resources: ["nodes"]
		  verbs: ["get"]
		---
		# Source: provisioner/templates/rbac.yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  name: local-provisioner-pv-binding
		  labels:
			helm.sh/chart: provisioner-3.0.0
			app.kubernetes.io/name: provisioner
			app.kubernetes.io/managed-by: Helm
			app.kubernetes.io/instance: local
		subjects:
		- kind: ServiceAccount
		  name: local-provisioner
		  namespace: default
		roleRef:
		  kind: ClusterRole
		  name: system:persistent-volume-provisioner
		  apiGroup: rbac.authorization.k8s.io
		---
		# Source: provisioner/templates/rbac.yaml
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRoleBinding
		metadata:
		  name: local-provisioner-node-binding
		  labels:
			helm.sh/chart: provisioner-3.0.0
			app.kubernetes.io/name: provisioner
			app.kubernetes.io/managed-by: Helm
			app.kubernetes.io/instance: local
		subjects:
		- kind: ServiceAccount
		  name: local-provisioner
		  namespace: default
		roleRef:
		  kind: ClusterRole
		  name: local-provisioner-node-clusterrole
		  apiGroup: rbac.authorization.k8s.io
		---
		# Source: provisioner/templates/daemonset.yaml
		apiVersion: apps/v1
		kind: DaemonSet
		metadata:
		  name: local-provisioner
		  namespace: default
		  labels:
			helm.sh/chart: provisioner-3.0.0
			app.kubernetes.io/name: provisioner
			app.kubernetes.io/managed-by: Helm
			app.kubernetes.io/instance: local
		spec:
		  selector:
			matchLabels:
			  app.kubernetes.io/name: provisioner
			  app.kubernetes.io/instance: local
		  template:
			metadata:
			  labels:
				app.kubernetes.io/name: provisioner
				app.kubernetes.io/instance: local
			  annotations:
				checksum/config: 10c516b3c21dfad25dbaca23150f1ae7f20a86ccf6b17d468381138c5666f6ed
			spec:
			  serviceAccountName: local-provisioner
			  containers:
				- name: provisioner
				  image: quay.io/external_storage/local-volume-provisioner:v2.3.4
				  securityContext:
					privileged: true
				  env:
				  - name: MY_NODE_NAME
					valueFrom:
					  fieldRef:
						fieldPath: spec.nodeName
				  - name: MY_NAMESPACE
					valueFrom:
					  fieldRef:
						fieldPath: metadata.namespace
				  - name: JOB_CONTAINER_IMAGE
					value: quay.io/external_storage/local-volume-provisioner:v2.3.4
				  ports:
				  - name: metrics
					containerPort: 8080
				  volumeMounts:
					- name: provisioner-config
					  mountPath: /etc/provisioner/config
					  readOnly: true
					- name: provisioner-dev
					  mountPath: /dev
					- name: fast-disks
					  mountPath: /mnt/fast-disks
					  mountPropagation: HostToContainer
			  volumes:
				- name: provisioner-config
				  configMap:
					name: local-provisioner-config
				- name: provisioner-dev
				  hostPath:
					path: /dev
				- name: fast-disks
				  hostPath:
					path: /mnt/fast-disks

	публикуем эту штуку в кубе 
	$ kubectl apply -f ...
	
	но этой портянки недостаточно. нужно еще создать storage class.
	иначе поды провижионера небудут создавать pv а будут писать в логах ошибку 
	
	 Failed to discover local volumes: failed to get ReclaimPolicy from storage class "fast-disks": storageclass.storage.k8s.io "fast-disks" not found

	прямой необходимости иметь storage class при создании PV скажем руками
	нет. но провижионер просто хочет из сторадж класса получить инфо какую же reclaimPolicy установить для PV. именно поэтому также нужен и стораджкласс чтобы провижионер мог создать PV.
	
	как выглядит сторадж класс fast-disks
	
	# Only create this for K8s 1.9+
	apiVersion: storage.k8s.io/v1
	kind: StorageClass
	metadata:
	  name: fast-disks
	provisioner: kubernetes.io/no-provisioner
	volumeBindingMode: WaitForFirstConsumer
	# Supported policies: Delete, Retain
	reclaimPolicy: Delete
	
	после того как мы опубликуем сторадж класс то должны начать появлясться созданные этим провижионером PV. причем они мгновенно 
	появляются. походу эта опция minResyncPeriod: "1m0s" несвязана как часто провижионер проверяет появление новых бекендов.
	
	теперь я скажу о важных моментах от которых зависит появление новых PV. на каждой дата ноде провижионер автоматом создамт папку 
		/mnt/fast-disks
	и внутри этой папки он ожидает что появятся подпапки и (важно!) нужно чтобы подпапка была точкой монтирования тоесть просто подпапки
	недосттаточно и если подпапка это точка монтирования то провижионер создаст из нее PV. новые PV будут имена вида local-pv-aff30297
	также легко обнаружить новые PV если в kubectl get pv смотреть на время создания этого PV. самые молодые и будут новосозданные PV.
	итак на то чтобы провижионер начал создавать автоматом PV влияет два
	фактора 
		1) созданный сторадж класс
		2) наличие /mnt/fast-disks/папка№X точек монтирования
		
	настройки провижионера хранятся в верхней портянке в ConfigMap
	что в этом конфигмапе за что отвечает написано здесь https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/provisioner.md
	
	еще раз скажу про важнуб строку в сторадж классе
		volumeBindingMode: WaitForFirstConsumer
	она делает вот что. когда у нас создается в кубе pvc то подефолту
	куб старается сразу найти для этого pv и прилепить одно к другому.
	если после этого при создании пода указать этот pvc его имя то получается видимо проблема о том что pv лежит на одной ноде а шедулер
	хочет опубликовать под на другой. тогда эта настройка делает вот что
	она говорит кубу чтобы он для новой pvc неискал pv. а искал pv только 
	тогда когда под который этот pvc будет юзать уже должрне быть опубликован на таком то хосте (так рещил шедулер исходя из того как много памяти цпу и тп на той ноде) и тогда как я понял куб ищет для этого pvc pv непросто из всех доступных а только из pv которые лежат на той ноде на которой шедулер хочет под опубликовать . таким образом 
	не под подстраивается под pvc в плане ноды а pvc подстраивается под под в плане ноды.
	
с local provisoner-ом и pv(local) разобрались. теперь можно на дата 
нодах насоздават точек монтирования /mnt/fast-disks/X и куб автоматом
насоздает PV(local) и после этого можно через stateful сет можно публикровать поды. и все это в автоматическом режиме ненужно для каждого пода вручную указывать PV.

эти штуки нам дают возможность имея на дата нодах локальные диски публиковать на них поды считай в автоматическом режиме как бутто это сетевые диски а нелокальные. а это очень дешевое решение в плане сколько
денег надо потратить на локальное хранилище vs на сетевое. и это дает возможность удобно пуликовать приложения у которых редунданси данных их репликация защищена на уровне самого приложения типа как в эластике	

что за что отвечает в верхней портянке (rbac итп) я пока нетрогаю неразбираю. пока что.


следущая мысль. вот мы развернули local provisioner.  его конкретную модификацию в которой в configmap прописано какой storage class
использовать для создаваемых pv так вот как я понимаю в configmap
можно прописать несколько сторадж классов каждый из которых будет применяться к своим бекенд папкам таким образом у нас один провижинер
будет создавать несколько типов PV. типа PV для эластика , PV для еще чегото. например если папка начинается с такойого слова то будут создаваться PV с одним стораж класом, если папка начинается с другого слова
то будет создаваться PV с другим в нем прописанным сторадж классом.


=== (конец про local provisioner и pv(local)

теперь пользуясь storage class: fast-disks
можно на локаьных дисках опубликовать эластик через helm

