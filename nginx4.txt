| nginx


[pid  1665] socket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 24
[pid  1665] setsockopt(24, SOL_SOCKET, SO_INCOMING_CPU, [1], 4) = 0
[pid  1665] setsockopt(24, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
[pid  1665] setsockopt(24, SOL_SOCKET, SO_REUSEPORT, [1], 4) = 0
[pid  1665] socket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 47
[pid  1665] setsockopt(47, SOL_SOCKET, SO_INCOMING_CPU, [1], 4 <unfinished ...>
[pid  1666] setsockopt(24, SOL_TCP, TCP_MAXSEG, [1220], 4) = 0
[pid  1665] setsockopt(47, SOL_SOCKET, SO_REUSEADDR, [1], 4 <unfinished ...>
[pid  1666] setsockopt(24, SOL_SOCKET, SO_REUSEADDR, [1], 4 <unfinished ...>
[pid  1665] setsockopt(47, SOL_SOCKET, SO_REUSEPORT, [1], 4 <unfinished ...>
[pid  1666] bind(24, {sa_family=AF_INET, sin_port=htons(953), sin_addr=inet_addr("127.0.0.1")}, 16 <unfinished ...>
[pid  1665] setsockopt(47, SOL_TCP, TCP_MAXSEG, [1220], 4) = 0
[pid  1666] listen(24, 5)               = 0
[pid  1665] setsockopt(47, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
[pid  1665] bind(47, {sa_family=AF_INET, sin_port=htons(953), sin_addr=inet_addr("127.0.0.1")}, 16) = 0
[pid  1665] listen(47, 5)               = 0

видны спец флаги SO_REUSEADDR SO_REUSEPORT значт реально открыатся два слущающх
сокета на один биндинг. странно то что это делатся не на  биндинг который будет клиентов 
обслжуиваь а на биндиг который управляющий . вот что странно!


в жинксе по дейолту один солушающий сокет. и все воркеры имеют на него декриптор. и конкурируют
выгребая из него данные. но можно исправить.  в конфиге всено навсего надо сделать вотт так

listen 127.0.0.1:8888 reuseport;


ттгда

 
# ss -nlpte | grep 8888
LISTEN 127.0.0.1:8888       0.0.0.0:*    ino:27483423 sk:10b3          
LISTEN 127.0.0.1:8888       0.0.0.0:*    ino:27483422 sk:10b4          
LISTEN 127.0.0.1:8888       0.0.0.0:*    ino:27483421 sk:10b5          

вот эти разные INO как раз докыдват что это три разных слушаюих сокета

ии ли вот так


# lsof -Pn -p 2406063 2>/dev/null | grep -E "PID|TCP"
COMMAND     PID USER  FD      TYPE             DEVICE SIZE/OFF      NODE NAME
nginx   2406063 http   5u     IPv4           27483421      0t0       TCP 127.0.0.1:8888 (LISTEN)
nginx   2406063 http   6u     IPv4           27483422      0t0       TCP 127.0.0.1:8888 (LISTEN)
nginx   2406063 http   7u     IPv4           27483423      0t0       TCP 127.0.0.1:8888 (LISTEN)

вот видно что у каждого сокета свой DEVICE

насколько я понмаю можно заусть два процесса совреещно не связанных.
ели на первом открыть сокет с параметрами SO_REUSEADDR + SO_REUSEPORT
и если потом зпусить соверенно друой процесс ругой бинарник и этот процесс будет иметь
такиеже права как и перйый процесс то он сможет создать второй расашаернный слушающий сокет!
это ответает на вопрос - могут ли два процесса сидеть на одном биндинге. ответ - при особых
уловиях могут!!



---
начинаю описание с конца:
как сделать жинкс чтобы он стартанул пользуясь своим конфигом но сделать 
это через cli а не через systemd



		$ nginx -g 'daemon off;'


теперь как в жинкс так его запустить чтобы он создал не один слушающих TCP сокет
а несколько. для этого надо заюзать reuseport слово. и все так просто

пример

	server {
    	...
    	listen 192.168.10.12:443 ssl http2 default_server reuseport;
    	...
	} 



теперь разьяснение. как  обычно действует жинкс. вот соддатсется первый процесс. он создает
слушающий сокет tcp и получает дескрпитор. потом этот процесс себя клонирует мы получаем второй
процесс. при этом в дочернем процессе будет склонирован и дескприптор на слушающий сокет. 
тут важно прояснить что у дочернего процесса ядро будет создан свой дескприторп но!- вести 
он будет на тот же сокет в ядре что и у первого процесса! тоесть сокет в памяти ядра один!
один! просто у нас появилось две ссылки на него. одна в десприторре у одного процесса а
второй дескриптору второго процесса. а сокет по факту еще раз скажу один ! щас покажу
как это выглядит  - я щас рассматриваю пока вариант дефолтовой загрузки жинкса без опции resuseport
смотрим:
	головной процесс
	# lsof -Pn -p 484 2>/dev/null | grep -E "PID|TCP"
COMMAND PID USER   FD   TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   484 root    5u  IPv4              15717      0t0    TCP 127.0.0.1:443 (LISTEN)

	воркер
	# lsof -Pn -p 736 2>/dev/null | grep -E "PID|TCP"
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   736 www-data    5u     IPv4              15717      0t0    TCP 127.0.0.1:443 (LISTEN)


тоесть есть два процесса 484 и 736. у кажого есть свой дескриптор 5 fd/5  который
ведет на один и тот же сокет (кусок памяти) в памяти ядра


		/proc/484/fd/5 ---->\_____ TCP 127.0.0.1:443
   		/proc/736/fd/5 ---->/     


как точно понять что сокет один а не два их. это можно понять по тому что у них один и тотже
DEVICE=15717.  девай это номер иноды этого сокета на какйто то там внутренней внутри ядра файловой
системе и типа того хрени. в общем сокет имеет внутри ядра некий порядковый номер называется инодой

посморим на этот сокет через ss

# ss -4enlpt  2>/dev/null | grep -E "Recv|nginx"
State  Local Address:Port  Peer Address:Port  Process                                                                                                                                
LISTEN 127.0.0.1:443       0.0.0.0:*          users:(("nginx",pid=736,fd=5),("nginx",pid=484,fd=5)) 
                                              ino:15717 
                                              sk:b 
                                              cgroup:/system.slice/nginx.service

так вот ss это команда которая прежде всего покаызает информацию не по сетевым соединениям
как это нам парят а по деталям об СЕТЕВЫХ СОКЕТАХ. так вот каждоая строка в выводе ss
она создется только для отдельного сокета! поэтому если нет сокета то и не строки . поэтому
вот мы видим биндинг 

	LISTEN 127.0.0.1:443  

он является частью параметров котоые сохранены в сокете. биндинг у нас определяет параетры 
сетевого потока тоесть все пакеты котоыре удовлетворяют этому свойству приичичляются ядром к 
одному потоку. и все пакеты этого пакета суются на тот сокет (кусок памяти) который имеет этот
биндинг в своих своствах. так вот у нас есть сокет одна штука потому что одна строка и 
этот сокет имеет в своих своствах параметры потока через биндинг указанный. и на этот сокет 
ведут ДВА файл дескриптора от двух процессов 

		users:(("nginx",pid=736,fd=5),("nginx",pid=484,fd=5))

тоесть один дескриптор это 5 от процесса 736 и второй дескрипттор это 5 от процесса 484 
ТОЕСТЬ я хочу тут подеркнуть что у нас в строке ss для биндинга указано два процесса а точнее
указано два процесса и файл дескрипттор для каждого. и это значит что в обоих процессах 
указанные файл дескприторы ведут в качестве бекенда на один и тот же сокет в памяти ядра!

в выводе указан и некий внутрненний пордоковый номер этого сокета ino:15717

поэтому сокет у нас один! просто несть несколько файл дсексрипторов котоыре на него ведут.
это могут быть декпритторы принаделделеажащие одному процессу а могут быть деспиторы принаделле
жащие несколькоим процессам.

поэтому мы полчили однозначный ответ на вопрос что вот мы видим список жммнкс процессов

# lsof -Pn -p 484 -p 736 2>/dev/null | grep -E "PID|TCP"
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   484     root    5u     IPv4              15717      0t0    TCP 127.0.0.1:443 (LISTEN)
nginx   736 www-data    5u     IPv4              15717      0t0    TCP 127.0.0.1:443 (LISTEN)

и вопрос был в том что у каждого из процессов свой слушающий сокет или это один и тотже? 
ответ - это один и тот же! файл дсекрисптор в каждом процессе в итоге ведет на один и тот же
сокет! а сокет это кусок памяти в ядре. так делает жинкс по дефолту!

тогда вопрос - вот прилетает поток пакетов из сети и ядро их сует в сокет в памяти ядра.
но в какой из процессо то в итоге попадут данные из осокета? ведь сокет это кусок памяти в ядре!
а нам нужно данные засунуть в память процесса! кстати у процесса может быть куча тредов и там
ее надо смотреть в каком треде есть дескпритор которые ведет на сокет. но у нас просто - у нас
процесс каждый состоит из одного треда. поэтому про треды можно забыть. точнее в данном случае
процесс это и есть тред. нуладно буду назвыать процесс. тут еще важно напомнить что сокеты
быавют двух типов. один тип это слущающий сокет как у нас на картинке. в этот сокет (кусок памяти
в ядре) ядро записывает параметры кажого нового соединения. тоесть каждый src_IP+src_port 
от которого в наше ядро со стороны того ядра прилетел трешаговый хендшейк. через данный сокет
процесс не может получать\отпралять данные на удаленный хост. если данный тип сокета есть
у процесса то это дает процессу только гипотетическую взможность работать с сетью. тоесть 
процесс чтобы начать реально работать с каикто удаленыным компом имея данный сокет обязателно 
должен запустить accept() в нем указать как раз дескприттор N который ведет на слушающий сокет
(слушающий сокет это как пул ). тоесть accept() невзомжно запустить если у нас нет дескпритора
со слушающим сокетом. что дает accept() -  управление передается ядру и оно 
смотрит если ли в слушающем сокете который указан как N новые конекты. если есть то ядро 
создает новый сокет в ядре. потом создает новые дескриптор для процесса. привзяеывает дескиптор
к этому новому сокету. и сообщает номер этого деспритора процессу. только теперь процесс
используя этот новый сокет может читаьт и писать из того конкнретного конекта который был 
вытащен из стопки конектоы которые хранилрись в слушабщем сокете например.
вот у нас  в слубаще сокете хранится два конекта

    1) 127.0.0.1.443 <----> 12.12.12.12:2345
    2) 127.0.0.1.443 <----> 13.13.13.13:5667

эта инфо хранится в слушащем сокете который к нашему процессу привзяан через дескпритор 10,
мы запускаем accept(10,...) . ядро лезет в дескрпитор 10 лезет в слушабщий сокет берет конект
сверху (1) и создает для него новый сокет  


    127.0.0.1.443 <----> 12.12.12.12:2345


создае новый дескпритор 11 который смотрит в этот новый сокет. и возрвщает упрвление процессу
и сообает ему что новый дескпиотоор имеет номер 11

и тепер наш процсс может читать писать данные в сторону удаленного хоста 12.12.12.12:2345
но опять же процесс может этого и неделать вовсе. то есть процесс вытащил конект из стопки
но может ничего и не писать читать в этот конект! процесс не обязан! тоест удалнный процесс 
уже может накидыать в этот сокет в ядро данные нам слать. и они уже будут приняты и лежать
в нашем ядре. но наш процесс необязан эти данные забрать из ядра. необязан. 
теперь возвращаюсь. вот у нас два процесса каждый имеет дескрпитор который ведет в слущающий
сокет который лежит в ядреи и в цеом никаому процессу не принадлежит. вместо этого может 
быть один или более дескпиторов от одного или некольктх процессов точнее их тредов которые
ведут в этот сокет. так вот новый конект который приетает в ядро и фиксируется им в слушающем
сокете он этот конект пока что не приандележит ни одному процессу. а далее  тот процесс который
у себя запустит  accept() он этот конект на себя и получит. так вот посмотрит какой из двух
процессов жинкса запускает accept()  . может оба процесса?
подкбючаемся к обоим процессам через strace

головной процесс мастер процесс жинкса сидит и ждет возврата вот из такого сисколла

	rt_sigsuspend([], .... )


а второй процесс которй воркер сидит и жддет возврата из сисколлла

	epoll_wait(12, ... )


пока не очень понятно. rt_sigsuspend это ссколл котоырй связан с ожилаением прилета сигнала. 
тоесть головной процесс он хоть и иимеет дескриптор связнанный с сетевым сокетом но он ничего
с ним делать не собирается! он всго навсего сидит и ждет не прилетел ли сигнал от ядра. и все.

насчет epoll пока непонятно что он делает. но уже понятно что два процесса они по крайней
мере не коннкуиируют за сетевые конекты котоыре сидят в слушающем сокете.
приглядимся к процессу 736

# lsof -Pn -p 736 2>/dev/null 
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   736 www-data    0u      CHR                1,3      0t0      4 /dev/null
nginx   736 www-data    1u      CHR                1,3      0t0      4 /dev/null
nginx   736 www-data    2w      REG              254,1        0 393385 /var/log/nginx/error.log
nginx   736 www-data    4u     unix 0x0000000026894a9b      0t0  20230 type=STREAM (CONNECTED)
nginx   736 www-data    5u     IPv4              15717      0t0    TCP 127.0.0.1:443 (LISTEN)
nginx   736 www-data   10w      REG              254,1        0 393385 /var/log/nginx/error.log
nginx   736 www-data   11w      REG              254,1       86 393382 /var/log/nginx/access.log
nginx   736 www-data   12u  a_inode               0,14        0     51 [eventpoll:4,5,13]
nginx   736 www-data   13u  a_inode               0,14        0     51 [eventfd:15]

видим что на дескрпиторе 12 котоый слущает еполл у нас сидит прослушивание дескипторов 4,5,13
из укаазанных сокетов нас интерсует щас только номер 5


nginx   736 www-data    5u     IPv4              15717      0t0    TCP 127.0.0.1:443 (LISTEN)

поэтому воркер процесс он сидит и слушает ждет когда на  слушающем сокете появится новый конект.

поэтому хотя у жинкса два процесса. и каждый из них имеет дескрпитор который вдеет на слуша
ющий сокет но вреальности только воркер процесс следит за измнеенеием сутации на этом сокете
а мастер процесс на эту тему не парится не следит ничего недалет.

щас запущу HTTP реккест на жинкс и посмтрим через стрейс как оба процесса отреагируеют
значит делаю запрос

	$ curl http://localhost:443 1>/dev/null

трейс на мастер процессе

strace: Process 484 attached
rt_sigsuspend([], 8

ничего не происходит

трейс на воркер процессе
strace: Process 736 attached
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, -1) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(35328), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 3
epoll_ctl(12, EPOLL_CTL_ADD, 3, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556981744, u64=140555361743344}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981744, u64=140555361743344}}], 512, 60000) = 1
recvfrom(3, "G", 1, MSG_PEEK, NULL, NULL) = 1
recvfrom(3, "GET / HTTP/1.1\r\nHost: localhost:"..., 1024, 0, NULL, NULL) = 77
writev(3, [{iov_base="HTTP/1.1 400 Bad Request\r\nServer"..., iov_len=152}, {iov_base="<html>\r\n<head><title>400 The pla"..., iov_len=202}, {iov_base="<hr><center>nginx/1.22.1</center"..., iov_len=53}], 3) = 407
write(11, "127.0.0.1 - - [25/Nov/2024:12:14"..., 86) = 86
close(3)                                = 0
epoll_wait(12, 


и вот видно что когда прилетел новый конект на слушающий сокет то на воркере был запущен 
accept() . создается новый сокет. получем от ядра новый дескпритор 3. и в из него читается
поток от удаленного сервиса . потом в обратку пишется текст. и потом этот дескприторп закрывается.

итак между мастер процессом и воркер процессом нет нкиакйо конкуренции за сетевые потоки
поэтому вопрос - а вот прищетаеют новые конекты из сети и как там эти два прцоесса жинкса
разабираются с  этим? ответ - мастер процесс нихерна неделает. всем занимается воркер процесс
он принимает все сетевые конекты. он принимает конект внутрь себя. он оттуда читает. он туда
пишет. он закрывает конект. все это делает воркер процесс. у него один тред между прочим.
я мало знаю как работает еполл() котоый у нас следит за ново поступающими конектами на 
слушающий сокет. вот мы приняли конект. вот мы начинаем из него читать писать через 

   recvfrom()
   writev()

вот надо смотреть детали в том плане что являются ли блокирующими эти вызовы тоесть заторама
живают ли они прием новых конектов. ну попобуем понять. запущу ка я 10 конекто одновременно
на этот воркер. 


	 curl  --parallel  --parallel-immediate   http://localhost:443    http://localhost:443 http://localhost:443   http://localhost:443  http://localhost:443  http://localhost:443  http://localhost:443  http://localhost:443  http://localhost:443  http://localhost:443    http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443     http://localhost:443  

кстати с курл я сделал ошибку я написал  

		--parallel-immediate 10

а нужно

		--parallel-immediate


и это приводит к тому что курл воспринимает эту 10  как url вот такой : http://0.0.0.10 !! 
ну ладно это я отвлекся
и вот как выглядит трейс на воркере жинкса


accept4(5, {sa_family=AF_INET, sin_port=htons(41386), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 3
epoll_ctl(12, EPOLL_CTL_ADD, 3, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556987745, u64=140555361749345}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 60000) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41394), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 6
epoll_ctl(12, EPOLL_CTL_ADD, 6, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556987505, u64=140555361749105}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 60000) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41396), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 7
epoll_ctl(12, EPOLL_CTL_ADD, 7, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556987265, u64=140555361748865}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 60000) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41412), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 8
epoll_ctl(12, EPOLL_CTL_ADD, 8, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556987025, u64=140555361748625}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 59999) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41424), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 9
epoll_ctl(12, EPOLL_CTL_ADD, 9, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556986785, u64=140555361748385}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 59999) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41432), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 14
epoll_ctl(12, EPOLL_CTL_ADD, 14, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556986545, u64=140555361748145}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 59999) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41438), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 15
epoll_ctl(12, EPOLL_CTL_ADD, 15, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556986305, u64=140555361747905}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 59999) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41452), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 16
epoll_ctl(12, EPOLL_CTL_ADD, 16, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556986065, u64=140555361747665}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}], 512, 59998) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(41456), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 17
epoll_ctl(12, EPOLL_CTL_ADD, 17, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556985825, u64=140555361747425}}) = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}, {events=EPOLLIN, data={u32=2556987745, u64=140555361749345}}], 512, 59998) = 2
accept4(5, {sa_family=AF_INET, sin_port=htons(41468), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 18
epoll_ctl(12, EPOLL_CTL_ADD, 18, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556985585, u64=140555361747185}}) = 0
recvfrom(3, "G", 1, MSG_PEEK, NULL, NULL) = 1
recvfrom(3, "GET / HTTP/1.1\r\nHost: localhost:"..., 1024, 0, NULL, NULL) = 77
writev(3, [{iov_base="HTTP/1.1 400 Bad Request\r\nServer"..., iov_len=152}, {iov_base="<html>\r\n<head><title>400 The pla"..., iov_len=202}, {iov_base="<hr><center>nginx/1.22.1</center"..., iov_len=53}], 3) = 407
write(11, "127.0.0.1 - - [25/Nov/2024:14:03"..., 86) = 86
close(3)                                = 0
epoll_wait(12, [{events=EPOLLIN, data={u32=2556981264, u64=140555361742864}}, {events=EPOLLIN, data={u32=2556987505, u64=140555361749105}}, {events=EPOLLIN, data={u32=2556987265, u64=140555361748865}}, {events=EPOLLIN, data={u32=2556987025, u64=140555361748625}}, {events=EPOLLIN, data={u32=2556986785, u64=140555361748385}}, {events=EPOLLIN, data={u32=2556986545, u64=140555361748145}}, {events=EPOLLIN, data={u32=2556986305, u64=140555361747905}}, {events=EPOLLIN, data={u32=2556986065, u64=140555361747665}}, {events=EPOLLIN, data={u32=2556985825, u64=140555361747425}}, {events=EPOLLIN, data={u32=2556985585, u64=140555361747185}}], 512, 59998) = 10
accept4(5, {sa_family=AF_INET, sin_port=htons(41474), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 3
epoll_ctl(12, EPOLL_CTL_ADD, 3, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=2556987744, u64=140555361749344}}) = 0
recvfrom(6, "G", 1, MSG_PEEK, NULL, NULL) = 1
recvfrom(6, "GET / HTTP/1.1\r\nHost: localhost:"..., 1024, 0, NULL, NULL) = 77
writev(6, [{iov_base="HTTP/1.1 400 Bad Request\r\nServer"..., iov_len=152}, {iov_base="<html>\r\n<head><title>400 The pla"..., iov_len=202}, {iov_base="<hr><center>nginx/1.22.1</center"..., iov_len=53}], 3) = 407
write(11, "127.0.0.1 - - [25/Nov/2024:14:03"..., 86) = 86
close(6)                                = 0
recvfrom(7, "G", 1, MSG_PEEK, NULL, NULL) = 1
recvfrom(7, "GET / HTTP/1.1\r\nHost: localhost:"..., 1024, 0, NULL, NULL) = 77
writev(7, [{iov_base="HTTP/1.1 400 Bad Request\r\nServer"..., iov_len=152}, {iov_base="<html>\r\n<head><title>400 The pla"..., iov_len=202}, {iov_base="<hr><center>nginx/1.22.1</center"..., iov_len=53}], 3) = 407


видно что идет перемешка accept4() с recvfrom() writev()
причем видно что accept4() идет с флагом SOCK_NONBLOCK это значит что дескпритор который 
создается им с ним потом работа будет идти в неблокирующем режиме тоесть  recvfrom() writev()
будет выполняться в неблокирующем режиме

 MSG_PEEK <<<>>>>  ЗАКОНЧИЛ ТУТ




(!!)кстати tcp пакеты подтверждения они нам собщают что инфо доставлена ДО ЯДРА УДАЛЕННОГО КОМПА,
что пакет до удаленного ядра приняло пакет от отправителя. но это совершенно ничего не означает
о том был ли  пеердан пакет от удаленного ядра к процессу на удаленной сситеме!!!  


(!!)ксатти жинкс еще создает от мастер процесса к каждому воркеру двунаправленный локальный сокет
через socketpair() но об этом отдльно я буду говрить



--

https://www.f5.com/company/blog/nginx/socket-sharding-nginx-release-1-9-1





https://stackoverflow.com/questions/30559164/nginxs-reuseport-for-same-ipport-pair-on-different-virtual-hosts

https://serverfault.com/questions/1000365/correct-way-to-use-reuseport-in-nginx-virtual-host


https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/


https://stackoverflow.com/questions/4526242/what-is-the-difference-between-fastcgi-and-fpm

https://en.wikipedia.org/wiki/Common_Gateway_Interface

https://datatracker.ietf.org/doc/html/rfc3875



https://medium.com/@aryanvania03/what-is-common-gateway-interface-cgi-ea8cc3ca80bf

https://stackoverflow.com/questions/18903002/apache-scriptalias-cgi-bin-directory



https://alvinalexander.com/perl/edu/articles/pl020001.shtml

https://stackoverflow.com/questions/824730/when-should-i-use-perl-cgi-instead-of-php-or-vice-versa


https://serverfault.com/questions/652382/php-servers-without-apache-nginx-cgi-stack

https://stackoverflow.com/questions/2712825/what-is-mod-php

https://www.php.net/manual/en/tutorial.firstpage.php



https://stackoverflow.com/questions/18861300/how-to-run-nginx-within-a-docker-container-without-halting

