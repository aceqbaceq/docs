terraform

установка для убунту 16

$ sudo snap install terraform


установка (для убунту >16)

$ sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl

$ curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -

$ sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"

$ sudo apt-get update && sudo apt-get install terraform



установили

проверка что он встал

$ terraform -help

установить автодополнение по TAB

$ touch ~/.bashrc

$ terraform -install-autocomplete



пробный проект

$ mkdir learn-terraform-docker-container

$ cd learn-terraform-docker-container

$  touch main.tf

terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 2.13.0"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx" {
  name         = "nginx:latest"
  keep_locally = false
}

resource "docker_container" "nginx" {
  image = docker_image.nginx.latest
  name  = "tutorial"
  ports {
    internal = 80
    external = 8000
  }
}

$ terraform init

$ sudo terraform init

надо научится прописывать sg. и стопить машину.

=

как проверить коректность и запустить план 


$ terraform fmt
$ terraform validate
$ terraform plan  -out plan.txt

как запустить уже выполеннеие
$ terraform apply plan.txt

=

создаем файл в котором прописан реурс  типа вольюм

$ cat volumes.tf 
// volumes.tf
resource "aws_ebs_volume" "elk" {
  availability_zone = "eu-central-1a"
  size = 12
  type = "gp2"
  tags = {
          "Name" = "elk-root_vol" 
          "elk"  = "rootvol"
        }


}


вопрос как его тераформ будет искать а амазоне.
нууууу. ответ такой что id вольюма почемуто нельзя указать.
но можно импортровать этот вольюм

$ terraform import aws_ebs_volume.elk vol-номер_вольюма

он импортрутся куда то  в кишки тераформа в файл terraform.tfstate
ну и как то после этого терафоом понимает что вольюм указанный в volumes.tf 
и тот который он импортровал это одно и тоже.
а как быть если мы  с нуля слздаем вольюм на амазоне. я так понимаю что 
почемуто в конфиге непроисать id. а он будет автоматом прописан тераформом  а файле terraform.tfstate
если вольюм создается с нуля. не очень както прикольно.

если мы хотим посмотреть что будет поменяно на амазоне не для всех обьектов а для конкретного то 
надо юзать ключ -target

$ terraform plan -target=aws_ebs_volume.elk -out plan.txt

эта команда покажет план что траформ будет менять на амазоне только 
для вольюма.

==
надо научится расширять фс через тераформ

==
важная штука

если в модуле aws_instance
мы указываем
security_groups 
то теарформ создает типа ec2 classic instance. в чем его жопа это то что 
у него нельзя поеменять security group без его уничтожения.
а если указать vpc_security_group_ids
то уже все окей. натаком инстансе можно менять секуююрити группу без переуничтожения инстанса

-====

еще раз как накатывать плейбук

$ terraform fmt
$ terraform validate
$ terraform plan -out plan.txt
$ terraform apply plan.txt

===
начала заново учить тераформ.
на примере яндекс облака

вначале надо поставит на комп yandex cli
как это сделать и прочая полезная штука про яблоко смотри в файле yandex_cloud.txt

вот мы поставили якли. в якли есть профили. профилей может быть несколько.
профиль это  набор настроек с которым якли
конектится к яблоку. эти настройки вклчают в себя: креды,номер папки яблока, номер облака яблока.
в один момент времени может быть активен только один профиль якли. с его настройками
якли и стучится на яблоко.

так вот нам для тераформа надо чтобы текущий профиль якли был с кредами сервис акаунта( сервис акаунт
это такой один из типов юзеров с которым можно подключаться к яблоку) от имени которого мы 
и будем подключаться из тераформа к яблоку. 

возникает вопрос какая связь между якли и тераформом. связь такая что тераформу чтобы подключаься 
к яблоку надо получить от IAM (сервис аутентификации яблока) специальный токен = IAM токен.
этот токен это как пропуск. который тераформ будет предьявлять сервисам яблока для работс ними.
так вот этот токен это проще всего получить через якли постучавшись на яблоко от имени сервис акаунта.


когда поставили яндекс кли. и там прописали в профиле папку на яблоке, креды сервис акаунта
то теперь надо добавить этому сервис акаунту права нужные тераформа.
а именно (как я понял из этого говенного описания https://cloud.yandex.com/en/docs/tutorials/infrastructure-management/terraform-quickstart) надо выдать этому акаунту права "editor"
сделать это тоесть добавить прав сервис акаунта можно через веб морду. окей защли в веб морду
выдали права "editor"

возвращаемся к нашему якли.
проверяем в нашем якли профиле какой сервис акаунт указан, какая яблоко  папка указана,
какое яблоко облако указано
$ yc config list
service-account-key:
  id: aje9ru
  service_account_id: ajed54
  created_at: "2023-03-09T19:15:13.886916958Z"
  key_algorithm: RSA_2048
  public_key: |
    -----BEGIN PUBLIC KEY-----
    MIIBIjAQAB
    -----END PUBLIC KEY-----
  private_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQ+cVgQvUE5AowIHtW/D0=
    -----END PRIVATE KEY-----
cloud-id: b1g8sk79560kbvb6nr2j
folder-id: b1gobq5rv8qm8qi76hig

тоесть проверяем 
service_account_id
cloud-id
folder-id

если это то что мы ожидали видеть то двигаем дальше.
а именно получаем IAM токен через якли от IAM сервиса 

$ yc iam create-token
t1.9euelZqM...


тоесть якли стучится от имени сервис акаунта на IAM сервис и говорит я сервис акаунт такой то
дай мне IAM токен.
если все ок то на экране мы получим наш IAM токен. он действиует 12 часов.
его можно обновить если приспичило в течение этих 12 часов но янедкс пишет что это делать 
рекеомнудется не чаще чем 1 раз в час.


этот iam токен это наш паспорт пропуск теперь  к сервисам яблока. мы его подставим в тераформ 
и он сможет успешно подключаться к сервисам яблока.
вот именно ради возможности получит IAM токен мы и ставили на комп якли.


длаее созаем папку

$ mkdir ~/cloud-terrafom
$ cd ~/cloud-terrafom
$ touch main.tf

дальше дока яндекса рекоменует переимновать файл .terraformrc в котором находится
конфиг настроек тераформа для текущего юзера.
$ mv ~/.terraformrc ~/.terraformrc.old

этого файла может и небыть.

создаем заново этот файл
$ touch  ~/.terraformrc

в этот файл мы вставляем кусок

$ cat ~/.terraformrc
provider_installation {
  network_mirror {
    url = "https://terraform-mirror.yandexcloud.net/"
    include = ["registry.terraform.io/*/*"]
  }
  direct {
    exclude = ["registry.terraform.io/*/*"]
  }
}



далее в наш main.tf
мы вставляем кусок:

terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"
}

provider "yandex" {
  token     = "<OAuth>"
  cloud_id  = "<cloud ID>"
  folder_id = "<folder ID>"
  zone      = "<default availability zone>"
  
}


соответвенно сюда надо вставить наш текущий iam-токен
котоыйр мы ранее полуили через якли от яблока,
а cloud-id, folder-id, zone availability  можно посмотреть в свойствах якли 
$ yc config list

далее как я понял из этого мудацкого описания https://cloud.yandex.com/en/docs/tutorials/infrastructure-management/terraform-quickstart
переходим в папку с main.tf 
и запускаем команды

$  terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 yandex-cloud/yandex
$  terraform init


далее в main.tf 
добавляем кусок


resource "yandex_compute_instance" "vm-1" {
name = "terraform1"

resources {
    cores  = 2
    memory = 2
}

boot_disk {
    initialize_params {
    image_id = "fd87va5cc00gaq2f5qfb"
    }
}

network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
}

metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
}
}

resource "yandex_compute_instance" "vm-2" {
name = "terraform2"

resources {
    cores  = 4
    memory = 4
}

boot_disk {
    initialize_params {
    image_id = "fd87va5cc00gaq2f5qfb"
    }
}

network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
}

metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
}
}

resource "yandex_vpc_network" "network-1" {
name = "network1"
}

resource "yandex_vpc_subnet" "subnet-1" {
name           = "subnet1"
zone           = "ru-central1-a"
network_id     = yandex_vpc_network.network-1.id
v4_cidr_blocks = ["192.168.10.0/24"]
}

output "internal_ip_address_vm_1" {
value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}

output "internal_ip_address_vm_2" {
value = yandex_compute_instance.vm-2.network_interface.0.ip_address
}


output "external_ip_address_vm_1" {
value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}

output "external_ip_address_vm_2" {
value = yandex_compute_instance.vm-2.network_interface.0.nat_ip_address
}


далее проверяем валидность этого конфига
$ terraform validate

далее просим тераформ переписать наш конфиг так чтобы он стал "красивым"
$ terraform fmt


$ terraform plan  -out plan.txt

далее формируем файл изменений которые тераформ собирается делать. так называемый
план. при этом на яблооке ничего небудет изменено мы просто увидим подробный план
того что тераформ собирается делать на экране а в файле будет запсана некая служебная хрень
$ terraform plan  -out plan.txt

ПОЛЕЗНАЯ ВЕЩЬ - в веб морде яблока нажимаем на имя клауда и справа будет закладка Quotas
в ней указаны лимиты на разные ресурсы в яблоке. там же можно увеличить эти лимиты

ПОЛЕЗНАЯ ВЕЩЬ - как посмотреть имаджи операционок доступные на яблоке
$ yc compute image list --folder-id standard-images | grep -E "ID|ubuntu-22"

далее применяем наш план уже в жизнь
$ terraform apply  "plan.txt"


готово. виртуалки и другие ресурсы из плана созданы.

как останвоить виртулки через тераформ пока непонятно.
приходится через якли это делать

$ yc compute instance list
$ yc compute instance stop  --name  terraform1
$ yc compute instance stop  --name  terraform2


далее. хотел сказать про значение файлов

.terraform.lock.hcl = в этом файле как понял прописвыаются  список провайдеров и их версии
здесь нет стейтов инфраструктуры


terraform.tfstate = в этом файле записано состояние инфраструктуры


далее как оказалось нам нафик ненадо завязываться в тераформе
на аутентификацию через iam токен. ведь его же над обновлять каждые 12 часов.
это не наш вариант. можно вместо этого указать в тераформе путь к 
RSA ключам от service акаунт а тераформ уже будет на основе этого
сам автоматом запрашиваь у iam сервиса iam токен. делается это вот так:
в main.tf
надо прописать


provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/key.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gobq5rv8qm8qi76hig"
  zone                     = "ru-central1-a"
}


где key.json это файл от яндекс кли в котором прописаны RSA ключи 
от service account в формате JSON. как получить этот файл смотри в файле yandex_cloud.txt


ДАЛЕЕ.
вопрос вот у нас есть папка тераформа, какие файлы из нее нужно взять 
чтобы на основе их создать такую же папку. тоесть как пеереносить папку тераформа.
значит ответ такой, заходим в папку с тераформом и забираем оттуда только ТРИ файла.
этого достаточно:

main.tf  (здесь наш конфиг)
.terraform.lock.hcl  (здесь описаны провайдеры)
terraform.tfstate   (здесь стейт инфраструктуры на момент последнего apply)

копируем эти три файла в новую папку. далее
в этой папке запускаем команды

$  terraform  init
$  terraform validate 
$  terraform fmt
$  terraform plan -out "plan.txt"
$  terraform apply "plan.txt"

и вуаля мы восстановили папку тераформа в новом месте.


ДАЛЕЕ.
новая задача перенести файл "terraform.tfstate" из хранения в локальной папке на компе в яблоко .
зачем нам это надо.  это надо для того чтобы пользоваться тераформом можно было нескольким юзерам.
поскольку файл стейта очень важен то чтобы исключить человеческую ошибку будет класно если 
на лкальном компе ни у кого этого файла небудет а юзер будет обращаться за этим файлам куда то в сеть.
переносим файл стейта  в яблоко в его s3 хранилище в бакет. хранилище называется у яблока как "object storage"
для этого надо зайти на яблоко сервис object storage и создать там бакет. я покажу как это делать
позже. а пока  считай что мы создали бакет со всеми нужными штуками
далее в main.tf в раздел terraform в его конец вставляем кусок


terraform {
...

  backend "s3" {
    endpoint   = "storage.yandexcloud.net"
    bucket     = "имя_бакета"
    region     = "ru-central1-a"
    key        = "folder1/terraform.tfstate"
    access_key = "YCAJ..."
    secret_key = "YCM0..."

    skip_region_validation      = true
    skip_credentials_validation = true
  }

}


обьясняю поля
key        = "folder1/terraform.tfstate"
дело вот в чем. бакет это типа как папка. в этой папке будет создана подпапка с именем "folder1"
и в эту папку уже будет скопирован файл terraform.tfstate
возникает вопрс так что же в этом ключе прописывать. какой путь. 
самое смешеное что этот параметр может быть абсолютно любым. вот какой путь мы пропишем по такому 
пути тераформ и засунет наш стетй файл с локальной системы в этот бакет. вопрос а как засунуть то на практике ? ответ будет дальше.

а пока продолжаю про поля
access_key = "YCAJ..."
secret_key = "YCM0..."

это параметры так называемого static access key. откуда его взять? когда мы создали бакет (покажу как это делать ниже) то мы в ACL бакета указыаем кому в него можно лазить. в моем случае это serivce account
так вот идем в веб морде в яблоко folder. в ее свойствах заходим в наш сервис акаунт. и справа вверху 
есть кнопка "create new key" , там можно выбрать какой тип нового ключа мы хотим создать. 
есть "create authorized key" это наш старый знакомый RSA пара ключей который мы прописыаем в якли
либо в тераформ и они используя этот authorized key получают от iam сервиса iam токен. вобщем  это не то что 
нам надо на данынй момент потому что (хз знает почему) сервис object storage в котором сидит бакет 
он не аутентифицирует юезра как я понял через iam токен. он аутентифицирует юзера через "static aceesss key"
поэтому мы выбираем в том меню "create static access key" и он нам выдаст нам пару ключей. 
это и есть access_key и secret_key ! вот их то и надо указать в конфиге main.tf
так с этим разобрались. итак  с полями разобрадись в main.tf
итак еще раз поля access_key(это как бы id ключа) и secret_key(это сам ключ) = так вот эти поля это 
ключ доступа (метод аутнетификации) со стороны тераформа к яндекс бакету. тоесть эта хрень нужна 
в этой сеции чтобы тераформ смог получить доступ до бакета. 
и я тутже скажу что чтобы не харнить эти ключи в main.tf их можно сохраниьт в отдельном месте 
а именно например в профиле aws cli (смотри об этом где то дальше) и тогда в main.tf можно
указать вместо ключей название aws cli профилс тоесть

один вариант (ключи к бакету храним в самом main.tf )

  backend "s3" {
...
    access_key = "YCAJ..."
    secret_key = "YCM0..."
...
  }


и второй вариант ( ключи к бакету храним в отдельном месте - профиле aws cli)

 backend "s3" {
  ...
    profile  = "service-account-2"
  ...
  
  }



далее надо запустить команду

$ terraform init
она скопирует файл terraform.state в бакет на яблоко. 
бояться этой команды ненало. она не сотреть все инициализацией. она именно толтко
перенесет terraform.state в бакет на яблоко
именно этой командой мы и переносим стейт файл в облако.
также важно заметить что исходный стейт файл останется по прежнему
локальной папаке и надо тут же его стереть в папке руками !


далее  провеяряем что наш тераформ успешно делаем свои дела
подключаясь к удаленному стейт файлу

$ terraform validate
$ terraform fmt
$ terrafrom plan -out "plan.txt"
$ terraform aplly "plan.txt"

классно!

а сейчас я ввозращаюсь к вопросу как создать бакет:
идем в веб морду в object storage и создаем новый бакет. 
в его свойствах прописываем в ACL что нащ service account имеет права на чтение\запись 
в этот бакет.


все - теперь стейт файл хранится в яблоке в бакете. это просто отлично!

если мы потом хотим перенести наш стейт файл в новый бакет
или в этом же бакете но по новому пути то для этого надо
1) обновить сеекцию 
backend "s3" {
   ...
  }

2) скачать рукамт стейт файл из старого бакета и разместиь его 
в локальной папке под именем terraform.state

3) запустить 
$ terraform init -migrate-state

и все. наш стейт файл скопирван в новый бакет. также надо теперь 
удалить руками стейт файл из локальной папки.

также важно отметмть что если мы перенесли стейт файл в бакет то 
для переноса тераформ папки нам теперь надо не три файла а всего два:

main.tf  (здесь наш конфиг)
.terraform.lock.hcl  (здесь описаны провайдеры)

но это еще не конец мудежа. наща конечная задача это чтбы если один юзер запустил
команду на исполннеие тераформа то второй юзер должен немочь ее запукать чтобы 
неиспортить инфраструктуру. для этого длполенительно надо заюзать dynamo
для этого юзаю эту статью https://habr.com/ru/post/711982/








#$#!!!
описать:
3.s надо чтобы все аспекты с нуля до конца были сделаны через командную строку.
например создание бакета и все такое. сейчас многое все делается руками а надо 
чтобы все было только через скрипт!!!!

тераформ работает от имени сервис акаунта в яблоке а сервис акаунт 
привязывается к конкретной папке яблока поэтому создавать папку яблока и 
сервис акаунт для этой папки через тераформ нельзя а можно только сторонниим методами
наприер через якли

переключаемся на якли профиль который раотает от имени веб юзера\яндекс юзера
$ yc config profile list
default
service_account_profile-krivosheev-test ACTIVE

актвируем профиль в котором прописан веб юзер
$ yc config profile activate default
Profile 'default' activated

проверяем что это рельно профиль от веб юзера
$ yc config list
token: y0_AgA...
cloud-id: b1g8s
folder-id: b1gobq
compute-default-zone: ru-central1-a


создаем новую яблокоо папку
$ yc resource-manager folder create test-folder-2
id: b1gop5honld
cloud_id: b1g8s2j
created_at: "2023-03-13T09:48:45.867259773Z"
name: test-folder-2
status: ACTIVE


создаем сервис акаунт для этой папки

$ yc iam service-account create --name service-account-2 \
  --description "сервис акаунт для папки test-folder-2 для тераформа" \
  --folder-name "test-folder-2"

id: ajec4h
folder_id: b1go
created_at: "2023-03-13T09:51:46.935971935Z"
name: service-account2
description: сервис акаунт для папки test-folder-2 для тераформа


создаем пару authorize keys для этого сервис акаунта. на основе этого атораайз key 
наш якли будет аутентифицироваться на иам сервисе и получать от него иам токен  который 
в свою очередь я кли будет предьявлять сервисам яблока для аутентиицикации.
таким макаром аторайз ки используется для аутентификации перед иам сервисом  но для доступа 
к другим сервисам яблока например vpc надо предьявлять иам токен который выдает иам сервис


$ yc iam key create --service-account-name service-account-2 --folder-name "test-folder-2" -o $HOME/.config/yandex-cloud/service-account-2-auth-keys.json

id: aje6rqernq
service_account_id: aje5n
created_at: "2023-03-13T10:05:51.187614866Z"
key_algorithm: RSA_2048


обращаю вниманеи что надо указывать папку в которой сидит сервис акаунт иначе  система небудет 
его находить.
файл с ключами мы сразу сохраняем в папку настроек якли.

даем этому сервис акаунту роль (права) на создание\редактирование виртуалок в этой папке
$ yc resource-manager folder add-access-binding test-folder-2 \
  --role compute.admin \
  --service-account-name service-account-2 \
  --folder-name test-folder-2


подставляем выше
$ yc resource-manager folder add-access-binding test-folder-2 \
     --role compute.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2

также добавляем роль чтобы акаунт мог сети в vpc создавать
$ yc resource-manager folder add-access-binding test-folder-2 \
     --role vpc.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2



также добавляем ему права на редактирование ydb баз "ydb.admin"
$ yc resource-manager folder add-access-binding test-folder-2  \
     --role ydb.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2


также добавляем роль чтобы этот акаунт мог создавать бакеты

$ yc resource-manager folder add-access-binding test-folder-2 \
     --role storage.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2



генерируем для этого сервис акаунта его static access key ( аля aws static key)
$ yc iam access-key create \
   --service-account-name service-account-2 \
   --folder-name "test-folder-2" \
   --description "статик ключ для доступа на aws подобные ресурсы"

access_key:
  id: aje7nled85chli
  service_account_id: aje5na
  created_at: "2023-03-13T19:57:42.009502688Z"
  description: статик ключ для доступа на aws подобные ресурсы
  key_id: YCAJE...
secret: YCNuNIf...


устанавливаем aws cli чтобы туда записать полученный   access static key
потому что якли этот ключ использовать неможет а aws cli может (дебилизм конечно но я нашел что есть 
отдельный cli родной от ydb возможно он умеет делать то что надо. вот докумен8тация от родного cli https://ydb.tech/en/docs/getting_started/cli  но в данной инструкции мы будем использовать aws cli)
нам этот aws cli понадобится чуть ниже когда мы будем работать с YDB базой данных.

# apt-get install awscli
прописываем наш статический ключ который мы получили выше в aws cli конфиг используя aws cli профили  
для того чтобы мы имели возможность если нужно  записать
несколько стат ключей
$ aws configure --profile service-account-2
AWS Access Key ID [None]: YCAJE...
AWS Secret Access Key [None]: YCNuNIf...
Default region name [None]: ru-central1

настройки будут сохранены в папку $HOME/.aws/

возвращаеися обратно к якли.
создаем профиль в якли с этим сервим акаунтом и его папкой в яблоке
$ yc config profile create service-account-2

далее подключаем  к текущему активному профилю наш ранее созданный файл с ключами RSA:
напомню что файл service-account-2-auth-keys.json должен лежать в папке $HOME/.config/yandex-cloud
$ yc config set service-account-key   service-account-2-auth-keys.json

далее полезно прписать в профиле cloud-id
сам id придется посмотреть через веб морду потому что как я понял с текушими ролями
наш сервис акаунт неможет его определить
$ yc config set  cloud-id b1g8sk79560kbvb6nr2j 

дале полезно добавит в профиль folder-id.
его мы уже можетм определить через cli
$ yc resource-manager folder list
+----------------------+---------------+--------+--------+
|          ID          |     NAME      | LABELS | STATUS |
+----------------------+---------------+--------+--------+
| b1gop5honldmvbpmg516 | test-folder-2 |        | ACTIVE |
+----------------------+---------------+--------+--------+


$ yc config set  folder-id  b1gop5honldmvbpmg516

в целом создание папки и сервис акаунта в яблоке закончена. также как бонус
мы создали профиль в якли с этим сервиc акаунтом.

теперь надо создать яблоко бакет (размером 50МБ) где мы будем хранить тераформ стейт для данной яблоко
папки

$ yc storage bucket create \
     --name bucket-2aw  \
     --default-storage-class standard \
     --max-size 52428800 \
     --grants grant-type=grant-type-account,grantee-id=aje5na0qfqnci747bgs5,permission=permission-write \
     --grants grant-type=grant-type-account,grantee-id=aje5na0qfqnci747bgs5,permission=permission-read

name: bucket-2aw
folder_id: b1gop5honldmvbpmg516
anonymous_access_flags:
  read: false
  list: false
default_storage_class: STANDARD
versioning: VERSIONING_DISABLED
max_size: "1"
acl: {}
created_at: "2023-03-13T20:53:02.424678Z"


теперь надо создать базу YDB  для того чтобы тераформ там ставил метку 
если тераформ начал чтот делать с инфраструктурой чтобы только один юзер в один момент времени
мог запускать тераформ. 

$ yc ydb database create terraform-state-lock-2 --serverless

проставляем пермишнсы для нашего сервис акаунта на эту базу
замечу что если сервис акаунт не имеет роли "ydb.admin" то команда успешно не пройдет.
я подчеркиваю я имею ввиде не про роль  "--role  ydb.editor" указанную в этой команде
а именно роль которую мы назначали акаунту выше.
$ yc ydb database add-access-binding  terraform-state-lock-2 \
     --role  ydb.editor \
     --service-account-id "aje5na0qfqnci747bgs5"


но базу создать мало надо создать таблицу в ней. и тут прикол в том что мы неможем этого сделтаь 
через якли. мы можем это сделать только через aws cli который мы поставили ранее. и прописали туда 
статик ключ от сервис акаунта.
для начала нам нужно опрееделить endpoint у нашей базы
$ yc ydb database get  --name   terraform-state-lock-2 
name: terraform-state-lock-2
document_api_endpoint: https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7

итак создаем таблицу в базе
$ aws dynamodb create-table \
   --profile service-account-2 \
   --table-name terraform-state-lock-2 \
   --attribute-definitions   AttributeName=LockID,AttributeType=S \
   --key-schema   AttributeName=LockID,KeyType=HASH \
   --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5  \
   --endpoint https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7

замечу что в этой команде я указал опцию --profile service-account-2 
это указан профиль самой утилиты aws cli чтобы aws cli стучался на яблоко под нужным ключом - под ключом именно сервис акаунта "service-account-2"



итак что сделано: 
создан сервис акаунт
ему выданы роли
создан бакет
создана база

это все нам дает теперь возможность прописать этот сервис акаунт в тераформ и он будет от его имени
стучаться на яблоко и создавать наконец виртуалки, также стейт тераформа будет хранися в бакете
и тераформ будет лочить свое состояние через YDB базу так что другой юзер не сможет запустить тераформ
если другой юзер уже запустил тераформ.

создаем папку
$ mkdir ~/terraform/01

создаем в папке два файла
$ cat .terraform.lock.hcl 
provider "registry.terraform.io/yandex-cloud/yandex" {
  version = "0.86.0"
  hashes = [
    "h1:hW7eHexpzEQXpun1kChUdfB+fFNepFxybhoxxbqOIjc=",
  ]
}




$ cat main.tf 


terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"







  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }

}





provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/service-account-2-auth-keys.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gop5honldmvbpmg516"
  zone                     = "ru-central1-a"

}







resource "yandex_compute_instance" "vm-1" {
  name                      = "terraform1"
  platform_id               = "standard-v3"
  allow_stopping_for_update = true

  scheduling_policy {
    preemptible = true
  }


  resources {
    cores         = 2
    memory        = 1
    core_fraction = 20
  }


  boot_disk {
    initialize_params {
      image_id = "fd8haecqq3rn9ch89eua"
    }
  }

  network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = false
  }

  metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/foxy.pub")}"
  }
}




resource "yandex_vpc_network" "network-2" {
  name = "network1"
}

resource "yandex_vpc_subnet" "subnet-1" {
  name           = "subnet1"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.network-2.id
  v4_cidr_blocks = ["192.168.10.0/24"]
}


output "internal_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}


output "external_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}



в этмом конфиг я хочу выделить блок

  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }


в нем мы прописваем

    bucket   = "bucket-2aw"                  # имя бакета созданного
    key      = "folder1/terraform.tfstate"   # куда в бакете класть стейт файл
    profile  = "service-account-2"           # имя AWS профиля на компе где искать static ключи для 
                                             # доступа к бакету
    dynamodb_endpoint=                       # точка доступа к базе 
    dynamodb_table =                         # имя таблицы в базе где хранить будем lock запуска тераформ



также хотел этот кусок выделить

provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/service-account-2-auth-keys.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gop5honldmvbpmg516"
  zone                     = "ru-central1-a"

}

ключ service_account_key_file =  описывает где иcкать authorize key через который мы стучимся на iam сервис
который в свою очередь выдает вответ тераформу iam токен который он уже как паспорт предьявляет сервисам
яблока


в целом main.tf тераформ создаст +1 vpc  внем создаст подсеть. и в ней создаст одну виртулку

запускаем наконец тераформ

 $  terraform init
 $  terraform validate 
 $  terraform fmt
 $  terraform plan -out "plan.txt"
 $  terraform apply  "plan.txt"




далее.
как в main.tf  в свойствах виртуалки
прописывается ssh ключ
ответ

  metadata = {
    ssh-keys           = "ubuntu:${file("~/.ssh/foxy.pub")}"
    serial-port-enable = 0
  }

где ubuntu = это типа имя линук юзера которому будет этот ключ присунут. 
в данном случае это ubuntu. прикол в том что если мы вместо ubuntu напишем vasya то 
тераформ все равно добавить ключ к юзеру убунту. почему? потому что :

Regardless of the username specified, the key is assigned to the user given in the cloud-init configuration by default. In different images, these users differ. (об этом написано здесь https://cloud.yandex.com/en/docs/compute/concepts/vm-metadata)

далее.
как через один хост ssh зайти на другой хост по ssh
$ ssh -J ubuntu@51.250.14.188:22 ubuntu@192.168.10.16
где 51.250.14.188 = первый хост
и 192.168.10.16 = второй хост (конечный)



|далее.
как создать пустой доп диск и прикрепить его к ВМ.
вначале создаем пустой диск

resource "yandex_compute_disk" "vm-1---disk2" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---disk2"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}


а потом внутри настроек виртуалки добалвяем этот диск
resource "yandex_compute_instance" "vm-1" {
...
  secondary_disk {
    disk_id     = yandex_compute_disk.vm-1---disk2.id
    auto_delete = true
    mode        = "READ_WRITE"
  }
...
}


возникает вопрос  а как добавит третий черветрй диск на виртуалку?
ответ создаем еще один ресурс с диском
а потом в свойствах виртуалки просто добавляем еще одну секцию  secondary_disk
тоесть



resource "yandex_compute_disk" "vm-1---disk2" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---disk2"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}



resource "yandex_compute_disk" "vm-1---disk3" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---disk3"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}




resource "yandex_compute_instance" "vm-1" {
...
  secondary_disk {
    disk_id     = yandex_compute_disk.vm-1---disk2.id
    auto_delete = true
    mode        = "READ_WRITE"
  }

 secondary_disk {
    disk_id     = yandex_compute_disk.vm-1---disk3.id
    auto_delete = true
    mode        = "READ_WRITE"
  }


...
}


ура !
 



| далее
в яблоке есть такая хрень. 
диски можно менять размер через веб моорду только если ты оставнил виртулаку.
а через тераформ доблавялется доп хуйня состоящая в том что если менять доп диск
то он не уничтожается а именно расширяется. а если менять размер системного диска
то провайдер терформа от яндекса он уничтожает диск уничтожает инстанс и переоздает 
его заново. это просто отлично

|далее.
если у нас виртувлка выелючена то изменения накатаныне через тераформ оставят 
виртуалку выключенной. также нет таких опций у тераформа чтобы выключить или
включить виртуакдку. это тоже очень прекрасно


|далее. как делать импорт ресурсов в тераформ.
пример как импортировать vpc
скажем у нас есть vpc с названием "default" и id="enptqgmqsa4aa8sofj81"
в main.tf создаем кусок

resource "yandex_vpc_network" "default" {
  name = "default"
}

и в командной строке запускаем
$ terraform import yandex_vpc_network.default enptqgmqsa4aa8sofj81

готово.

также - импорт vpc он импортирует только свойства vpc и он не импортирует субнеты.точнее
он импортирует имена субнетов потому что это свойство vpc но свойства субнетов
он не импортирует.  импортировать субнеты надо отдельно руками. 

вот как импортировать субнет:
вот имеем описанный ресурс в main.tf

resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
}


импортируем этот субнет
$ terraform import "yandex_vpc_subnet"."private-1-a-subnet"  e9bc4ns5s3dhbbt8ksnb

смотрим результат (команда STATE SHOW)
$ terraform state show  "yandex_vpc_subnet"."private-1-a-subnet"
# yandex_vpc_subnet.private-1-a-subnet:
resource "yandex_vpc_subnet" "private-1-a-subnet" {
    created_at     = "2022-12-18T22:03:25Z"
    folder_id      = "b1gobq5rv8qm8qi"
    id             = "e9bc4ns5s3dhbbt"
    labels         = {}
    name           = "private-1-a-subnet"
    network_id     = "enptqgmqsa4aa8s"
    route_table_id = "enp1dtv89pussel"
    v4_cidr_blocks = [
        "172.16.1.0/24",
    ]
    v6_cidr_blocks = []
    zone           = "ru-central1-a"

    timeouts {}
}


кстати про разницу в командах
$ terraform state show
$ terraform show

первая команда покзыает инфо о конкетном ресурсе.
а вторая команда показывает  инфо обо всех ресурсах 

но в обоих случаях инфо черпается не из облака а из стейт файла.
поэтому перед этим полезно ввести
$ terraform resfresh
чтобы обновить инфо в стейт файле на основе реального состояния из облака



|далее. команды STATE и REFRESH
команда
$ terraform refresh
делает то что тераформ берет обрашается к облаку и скачиает состояние из облака для всех ресурсов
прописанных в main.tf и если на облоаке ресурс изменился то и в стейт файле тоже состояние будет 
обновлено. это очень хорошо. скажем через веб морду изменил размер диска и через рефреш это изменение 
подтягивается  в стейт файл. очень хорошо.

так теперь про команду state. она позволяет посмртеть свйоства записанные в стейт файл про некотрй ресурс
указанные в main.tf

например имеем в main.tf запись

resource "yandex_vpc_network" "network-1" {
  name = "network1"
}


тогда свойства этого ресурса в стейт файле можно посмотреть как

$ terraform state show yandex_vpc_network.network-1
# yandex_vpc_network.network-1:
resource "yandex_vpc_network" "network-1" {
    created_at = "2023-03-10T09:32:49Z"
    folder_id  = "b1gobq5rv8qm8qi76hig"
    id         = "enp4eaogkheinfabqcdu"
    labels     = {}
    name       = "network1"
    subnet_ids = [
        "e9bbko7j4em15tla0fbd",
    ]
}



чем полезна эта команда у нас в main.tf в свойствах ресурса может быть указана только
часть его пропертис. а вот стейт показывает как я понимаю все поля настроек которые у этого
ресурса по факту есть вот как это видно выше. в main.tf задано только name
а по факту в стейт гораздо больше полей определено.

перед тем как запускать STATE полезна запустить REFRESH чтобы мы увидели не просто записи в стейт
файле а именно то как оно есть на яблоке в реальности прям щас. тоесть

$ terraform refresh
$ terraform state show yandex_vpc_network.network-1



и тут приходит гениальная идея - а можно ли через команду terraform refresh воосстановить
стейт файл если мы его потеряли?
рассмаотрим этот вопрос:
значит я взял из рабочей терафор папки и скопировал только два файла.
main.tf
.terraform.lock.hcl

далее  я взял и вырезал из main.tf полностью весь блок про backend
 а именно вырезал блок

  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "ter1"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "test"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etnfjlci9"
    dynamodb_table    = "Terraform-backend-lock"

  }

теперь полуается у меня файл со стейтом будет хранится локально.
далее я инициализировал папаку

$ terraform init

далее я пытаюсь засосать состояние всех ресурсов из яблока в стейт файл. ибо стейт файла у меня
нет

$ terrafom refresh

однако облом, получаю воттакое

╷
│ Warning: Empty or non-existent state
│ 
│ There are currently no remote objects tracked in the state, so there is nothing to refresh.
╵

значит как я понял то  тераформ может делать refresh только если он знает ID ресурсов 
прописанных в main.tf а если стейт файла нет то и iD ресурсов неизвестен.
поэтому если унас нет стейт файла (мы его потеряли) то восстановить стейт файл через 
команду terraform refresh не получится! 
единсвтенный вариант восстановить стейт файл это мы имеем main.tf  и мы сидим и для каждого
ресурса делаем импорт 
$ terraform import имя_ресурса ID_ресурса.
имя_ресурса мы смотрим в main.tf а id_ресурса мы смотрим через веб морду в яблоке.

так прикол еще и в том что импорт тоже невсегда простое занятие как я почитал. 
поэтому лучше всего никогда нетерять стейт файл иначе будут проблемы большие.


|далее. очень ВАЖНЫЙ момент про КАВЫЧКИ.
по поводу использования кавычек в main.tf
пример

resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
  route_table_id = yandex_vpc_route_table.route-table-2.id
}

если у нас справа константа то заключаем ее в кавычки.
name           = "private-1-a-subnet"

а если у нас справа типа переменная то кавычки категорически нельзя
иначе переменная будет восприниматься тераформом как константа в буквальном смысле
поэтому правильно только вот так
  route_table_id = yandex_vpc_route_table.route-table-2.id

кстати вот еще вариант как это можно коректно прописать через кавычки
  route_table_id = "${yandex_vpc_route_table.route-table-2.id}"




| далее. надо научиться бэкапить стейт файл в бакете на облаке
потеря стейт файла это звездец.
для этого надо включить версионность


$ yc storage bucket  list
+------+----------------------+------------+-----------------------+---------------------+
| NAME |      FOLDER ID       |  MAX SIZE  | DEFAULT STORAGE CLASS |     CREATED AT      |
+------+----------------------+------------+-----------------------+---------------------+
| ter1 | b1gobq5rv8qm8qi76hig | 1073741824 | STANDARD              | 2023-03-12 14:09:35 |
+------+----------------------+------------+-----------------------+---------------------+

[vasya@lenovo Desktop]$ yc storage bucket  get --name ter1
name: ter1
folder_id: b1gobq5rv8qm8qi76hig
anonymous_access_flags:
  read: false
  list: false
  config_read: false
default_storage_class: STANDARD
versioning: VERSIONING_DISABLED
max_size: "1073741824"
created_at: "2023-03-12T14:09:35.619470Z"

$ yc storage bucket  update --name ter1 --versioning versioning-enabled
name: ter1
folder_id: b1gobq5rv8qm8qi76hig
default_storage_class: STANDARD
versioning: VERSIONING_ENABLED
max_size: "1073741824"
acl: {}
created_at: "2023-03-12T14:09:35.619470Z"

что дает активированная версионность - при каждой записи в бакет сохраняется предыдущая копия.
потом через веб морду можно просмотреть и восстановить одну и предыдущих версий. и последняя версия
и все преддыдущие хранятся в этом же бакете. 






1.1 про vpc
про его стркутуру. что в него входит из чего состоит:
* network
* subnet
* route-table
* gateway




* network

на самом верхнам уровне vpc находится network. их может быть несколько.

когда мы создаем network то из свойств  которые можно ей указать это 
только имя и все. 
$ yc vpc network create --help


однако по факту к нетворку крепятся другие элементы. тоесть нетворк внутри себя содержит 
такие элементы как:
subnet
route-table
security-group

просто их нельзя создать на стадии создания нетворка. они создаются отдельными командами
уже после того как нетворк создан.
еще этот нетворк его можно переместить в другую папку(так как все службы яблока они привязаны к папке)

как создать network через cli
$ yc vpc network create --name "network-1"

как создать network через terraform
resource "yandex_vpc_network" "network-1" {
  folder_id = "<folder ID>"
  name = "network1"
}


как импортировать network в тераформ
$ terraform import "yandex_vpc_network"."network-1"   ID_нетворка




* subnet 

субнет по сути это и есть та структура в которой прописаны IP адреса локальной сети 
которые и выдаются потом виртуалкам. тоеть в свойтсвах виртуалки прописывается субнет 


как создать через тераформ
resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
  route_table_id = yandex_vpc_route_table.route-table-2.id
}

значит видно что в свойствах субнета уазано: имя, зона, нетворк к которому субнет прикреплен,
блок ip адресов который закреплен за этим субнетом, и роут таблица.

если мы укажем id этого субнета в свойствах виртуалки то ей будет назначен IP адрес из блока 
этого субнета. вот пример как это пропсиывается в виртуалке



resource "yandex_compute_instance" "vm-3" {
...
network_interface {
    subnet_id = yandex_vpc_subnet.private-1-a-subnet.id
    nat       = false
  }
...
}

таким макаром мы создаем субнеты для описания в них блока Ip адресов чтобы потом через 
эти субнеты раздавать виртуалкам IP адерса.
и роут тейбл в свойтсвах субнета дает то что пакеты  проходя через рут тейбл будут раскидываться по 
разным выходным точкам. например часть пакетов будет уходить в интернет.




* роут тейбл

роут тейбл крепится к нетворку. тоесть при создании роут тейбла
ему нужно указать  к какому нетворку он относится. тоесть нетворк это как
коробка и внутри нее помещается игрушка под названием роут тейбл.

по сути у него такие свойства: имя, нетворк  к которому он прикреплен, набор статических маршрутов.
в статическом маршруте указывается имя гейтвея на который пуляют пакеты которые 
подходят под маршрут


создать через CLI
$  yc vpc route-table create    --help

создать через тераформ
resource "yandex_vpc_route_table" "route-table-2" {
  name       = "route-table-2"
  network_id = "yandex_vpc_network.default.id"

  static_route {
    destination_prefix = "0.0.0.0/0"
    gateway_id         = yandex_vpc_gateway.nat-2.id
  }
}


импортировать через тераформ
$ terraform import "yandex_vpc_route_table"."route-table-2"  enp1dtv89pusseldi3sb





* гетвеи

создать через CLI
$ yc vpc gateway create --help
$ yc vpc gateway create --name gate2
по сути это и все свойтства что есть у гейтвея - только имя
больше у этого ресурса и свойств то нет


создать в тераформ
описание ресурса в  тераформе  https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/vpc_gateway ) 
там написано что пофакту ресурс имеет только свойство "name" и свойство "shared_egress_gateway" который на данный момент неимеет никаких параметров. и еще свойство "descripton"


resource "yandex_vpc_gateway" "nat-2" {
  name = "nat-2"
  description = "шлюз в инет" -> null  
  shared_egress_gateway {}
}



импорт в тераформ
$  terraform import yandex_vpc_gateway.nat-2 enpkq1dil4o7b50hjhal

к ресурсу гейтвей  прикрепляется ресурc route-table. тоесть пакеты влетающие в роут табл 
пуляются на гейтвей.


на счет shared_egress_gateway {}  на что он влияет.
если мы его удалим из описания ресурса то при запуске тераформ он его удалить и на облаке 
но прикол в том что при следущем запуске тераформа он опять нарисует что он будет его удалять тоестьь 
как бутто как только мы удаляем это свойство на облаке оно его автоматом обратно доблавляет поэтому
это свойство получается должно быть must have.


хочу подытожить. гейтвей привязывается к роут-тейбл. роут-тейбл привязывается к субенету. субнет
привязывается  к нетворк.




| далее. outputs
если у нас в main.tf есть какие то outputs то как бы нам посмотеть
сейчас чему они равны. ответ

$ terraform output


| далее. про subnet 
значит вопрос вот у нас есть subnet.
у этого сабнета нет прикрепленного route-table который бы вел на gateway.
далее мы создаем в этом сабнете виртуалку без внешнего ip адреса.
вопрос можем ли мы пдключиться по ssh к этой виртуалке. ответ нет.
потому чтоу этой вирталки нет внешнего ip адреса замапленного.
ну окей. вопрос  если мы зайдем через веб морду через serial console то сможем 
ли мы хотя бы пинговать наружу интернет хосты например 8.8.8.8 
ответ = нет! неможем. почему? 
пускай у нас сабнет имеет диапазон 172.16.3.0\24
так вот на вирталке будет прописан марщурут по умолчанию на 172.16.3.1 
но проблема в том что очевидно далее на том яблочном гейтвее маршрута больше никуда никакого нет!
поэтому в так случае ни из интенета с такой вирталкой не свзяаться ни изнутри в инет связи нет!


привожу чеез тераформ пример такого сабнета и такой виртуалки

resource "yandex_vpc_subnet" "subnet3" {
  name           = "subnet3"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.3.0/24"]
}


resource "yandex_compute_instance" "vm-3" {
...
  network_interface {
    subnet_id = yandex_vpc_subnet.subnet3.id
    nat       = false
  }
...
}



далее. что будет если мы  к сабнету прикрепим route-table котоырй в свою очередь ведет на 
гейтвей.

resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
  route_table_id = yandex_vpc_route_table.route-table-2.id
}



resource "yandex_vpc_route_table" "route-table-2" {
  name       = "route-table-2"
  network_id = yandex_vpc_network.default.id

  static_route {
    destination_prefix = "0.0.0.0/0"
    gateway_id         = yandex_vpc_gateway.nat-2.id
  }
}



resource "yandex_vpc_gateway" "nat-2" {
  name        = "nat-2"
  description = "шлюз в инет"
  shared_egress_gateway {}
}



если мы прикрепим виртуалку к такому сабнету то он сможет изнутри пинговать 8.8.8.8
потому что пакеты уходят на  гейтвей нашего сабнета и натом гейтвее идет переброска уже на гейтвей
указанный у нас в тераформе. но по прежнему у нас не связи с виртуалкой из внешнего мира по ссш.
а как тогда мы можем проверить что из нее всеже можно пинговать 8.8.8.8 - ответ такой что 
можно зайти на вирталку через веб морду через serial интерфейс. еще на нее можно зайти 
через ssh прокси джамп. об этом ниже.  итак если у нас к сабнету прикреплен route-table который в свою
очередь ведет на гейтвей это дает то что  с виртуалки прикрелпенной к этому сабнету мы можем изунтри 
ходить наружу в интернет. инциииовать конектц из виртуалки в интернет хосты.


следущий случай. у нас есть субнет. у него нет прикрепленного route-table
но зато унас у виртуалки есть внешний IP. вот пример такого сабнета и такой виртуалки




resource "yandex_vpc_subnet" "dmz-1-a" {
  name           = "dmz-1-a"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.2.0/24"]
}



resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id = yandex_vpc_subnet.dmz-1-a.id
    nat       = true
  }
...
}


значит net=true нам дает то что наша виртуалка будет иметь примапленный снаружи интернет белый
адрес. это дает то что мы можем поключиться по ssh к нашей виртуалке из интернета. ура!
но также это дает то что изнутри виртуалки мы можем также успешно пинговать внешние серверы напрмиер 
8.8.8.8 хотя сабнет неимеет route-table



таким макаром я расмсмотрел все случай:
когда субнет неимеет рут тейбл
когда субнет имеет рут тейбл
когда виртуалка не имеет внешнего IP адрес или когда она его имеет.

таким макаром мы теерь знаем какие возможности у голого сабнете, что дает прикрпленнй рут-тейбл
к сабнету и что дает внешний IP адрес прикрепленный к виртуалке

тогд сумаарно логично в рамках одно network имет два subnet.
первый сабнет subnet-1 он неимеет роут-тейбл зато машины котоыре мы туда пихаем имеют прикрепленный
внешний IP адрес. а второй сабнет subnet2 имеет прикепрленный роут-тейбл который ведет на гейтвей но сами 
машины неимеют белых прикрелпнных IP адресов. тогда машины из subnet2 к ним нет прямого доступа из
интернета но они сами могут ходитьв интренрнет. а машины из subnet1 к ним есть прямой доступ из 
интернета и они могут ходить в интернет. и еше через них как через прокси можно обеспечить связь с машинами
из subnet2 потому что связь между subnet2 и subnet1 есть по локалке.
воттаакая архитектура!

| далее. статический IP адрес . как его прикрепить к ВМ.

как мы задаем сетевую карту на ВМ


resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = false
  }
}


resource "yandex_vpc_subnet" "dmz-1-a" {
  name           = "dmz-1-a"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.2.0/24"]
}

тоесть унас есть субнет. в нем есть блок адресов. и мы в свойствах вм 
указываем номер субнета. а уже яблоко само и карту создает и адрес из субнета назначает из его блока IP
адресов. тоесть subnet_id это не IP адрес. это номер субнета.



а как нам замапить внешний IP интернет адрес на этот локальный адрес

resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = true
  }
}


тоесть просто добавляем строку nat=true. 
есть только одна проблема то что при каждой перезагрузке виртуалки у нее будет меняться номер 
этого внешнего IP.  а как нам сделать статичекий внешний IP?

для начала надо создать этот статический внешний IP



resource "yandex_vpc_address" "ext-ip-1" {
  name = "ext-ip-1"

  external_ipv4_address {
    zone_id = "ru-central1-a"
  }
}


теперь надо этот внешний IP как прикрепить к ВМ
делается это вот так


resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = true
    nat_ip_address = ??
  }
}


вот осталось понять что же вставлять в поле "??"


чтобы это понять посмотрим как выглядит ресурс "yandex_vpc_address"."ext-ip-1"
для этого вставим в main.tf output

output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1
}

запускаем 
$ terraform refresh
получаем


external_debug_yandex_vpc_address_ext-ip-1 = {
  "created_at" = "2023-03-17T08:27:43Z"
  "deletion_protection" = false
  "description" = ""
  "external_ipv4_address" = tolist([
    {
      "address" = "51.250.3.188"
      "ddos_protection_provider" = ""
      "outgoing_smtp_capability" = ""
      "zone_id" = "ru-central1-a"
    },
  ])
  "folder_id" = "b1gobq5rv8qm8qi76hig"
  "id" = "e9bv1c385ddippqblidv"
  "labels" = tomap({})
  "name" = "ext-ip-1"
  "reserved" = true
  "timeouts" = null /* object */
  "used" = true
}

очевидно что нам надо добавить в наш путь external_ipv4_address чтобы мы стали ближе к получению ip адреса
в запросе
тоесть


output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1.external_ipv4_address
}



заупускаем
$ terraform validate && terraform fmt && terraform plan -out "plan.txt" && terraform apply
получаем

external_debug_yandex_vpc_address_ext-ip-1 = tolist([
  {
    "address" = "51.250.3.188"
    "ddos_protection_provider" = ""
    "outgoing_smtp_capability" = ""
    "zone_id" = "ru-central1-a"
  },
])


значит здесь мы видим что у нас стоит toist. это значит что у нас с точки зрения yaml мы 
имеем список . сосотоящий из одного элемента. как получить доступ к этому списку. доступ к элементам
списка идет через цифры. первый элемент это 0 .
значит добавляем ноль в путь
тоесть


output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1.external_ipv4_address.0
}

проверяем

external_debug_yandex_vpc_address_ext-ip-1 = {
  "address" = "51.250.3.188"
  "ddos_protection_provider" = ""
  "outgoing_smtp_capability" = ""
  "zone_id" = "ru-central1-a"
}


а теперт осталось доабвить address к пути
тоесть


output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1.external_ipv4_address.0.address
}


получаем

external_debug_yandex_vpc_address_ext-ip-1 = "51.250.3.188"

наконецто! мы смогли вычленить из ресурса yandex_vpc_address  параметры IP адреса.
вот эту штуку и подставляем в ВМ тоесть


resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = true
    nat_ip_address = yandex_vpc_address.ext-ip-1.external_ipv4_address.0.address
  }
}

вот это  и есть ответ на вопрос как нам примапить к ВМ статический ip адрес.
также приколв том что через веб интерфейс этого сделать совершенно нельзя.

таким макаром рассмотрено три случая - как виртуалке прописать локальный IP,как ей замапить динмический
IP внешний, как ей замапит стаический IP внешний


| далее. по поводу boot_disk

значит по дефолту загрузочный диск к ВМ прикрепляется вот так

resource "yandex_compute_instance" "vm-3" {
...
  boot_disk {
    initialize_params {
      image_id = "fd8haecqq3rn9ch89eua"
    }
  }
...
}

проблема тут в том что мы незаем какой у нас у диска id
и то что мы неможем ему задать имя.
это значит что если мы установим параметр чтобы яблоко неудаляло диск при удалении виртуалки то после
удаления вируалки мы в списке диско небудем знать какой же диск был у нас загрузочным для виртуалки

поэтому друой более правильный вариант как добавлять загрузочный диск к ВМ.



resource "yandex_compute_disk" "vm-1---bootdisk---disk1" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---bootdisk---disk1"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "10"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
  image_id   = "fd8haecqq3rn9ch89eua"
}



resource "yandex_compute_instance" "vm-1" {
...
  boot_disk {
    disk_id     = yandex_compute_disk.vm-1---bootdisk---disk1.id
    auto_delete = false
    mode        = "READ_WRITE"

  }
...
}


output "vm_1_bootdisk" {
  value = yandex_compute_disk.vm-1---bootdisk---disk1.id
}


теперь если  удалим виртуалку то диск у нас останется. и его можно будет найти в списке 
дисков во первых по его id через вывод output а  во вторых даже глазами в веб морде по имени.


также важны момент. в main.tf надо размещать описание дисков перед описанием вм. иначе 
при создании вм тераформ будет писать ошибку что диск ненайден!


|далее. апдейт про BACKEND
выше я описал как настроить бекенд для тераформа.
так вот теперь апдейт этого вопроса.

значит возникла идея и задача о том тобы разделить код тераформа от данных тераформа.
чтобы кода осталвся тем же самым а данные были ли бы закоодированы через переменные 
а переменные  бы лежали в отдельном файле.

напомню что у меня в main.tf была секция

 backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }



я попробовал сюда начать всталвять перменные.

backend "s3" {
bucket = var.backend_bucket_name
...


и мне выдало ошибку


$ terraform init -reconfigure

Initializing the backend...
╷
│ Error: Variables not allowed
│
│ on main.tf line 19, in terraform:
│ 19: bucket = var.backend_bucket_name
│
│ Variables may not be used here.


таким образом в настройках бекенда нельзя использовать переменные. (об этом есть как раз 
подтверждение тут https://github.com/hashicorp/terraform/issues/13022 ) 


тогда я применил другой подход.
создаем отдельный файл backend.tfbackend в котором я буду хранит настройки бекенда
( о том как надо правильно называть написано тут https://developer.hashicorp.com/terraform/language/settings/backends/configuration#using-a-backend-block )


далее в основном файле main.tf мы удаляем почти все
и оставляем
$ cat main.tf 
...
  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    skip_region_validation      = true
    skip_credentials_validation = true
  }
...

а остальные настройки мы переносим в backend.tfbackend

$ cat  backend.tfbackend

    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"
 
    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"


далее так как мы изменили настройки бекенда теперь надо бекенд переинцилизиаоровать.
и сделать это интересным способом

$  terraform init  -backend-config=./backend.tfbackend  -reconfigure

важно еще отметить что нельзя вот взять и полностью всю секцию backend убрать из main.tf
тогда тераформ нас нахер пошлет.  в минимальном виде обязательно в main.tf должен быть 
вот хотя бы такой блок

$ cat main.tf
...
  backend "s3" {}

....


вот это очень важно знать . потому что я вначале полнотью убрал блок backend из main.tf
потом попробвал инциализировать и там одни только ошибки. поэтому backend "s3" {} обязательно
должен присутовать в main.tf


при таких настройках далее уже можно с терпформом работаь как обычно не указывыая  -backend-config каджый раз.
напрмиер 

$ terraform refresh


это очень важная инфо.

чего мы в итоге достигли.  -  так как переенные нельзя использовать в бекенд секции то 
мы эту секцию убрали из main.tf таким образом мы сделали main.tf более независимым от 
конкретных хардкод настроек бекенда. main.tf стало более универсальным более незавиимым от 
конкретики.





0.a план: описать настройки cloud-init, модуль засунуть в гит и оттуда скачивать. сделать variables.tfsf чтобы
смена кода модуля ненаебнула настройки прода(даже незнаю стоит ли). импорт существующих виртуалок в контуре прод. раскатать туже инфраструрутуру что в проде на тестовой папке но с маленькими размерами. чтобы всегда была для теста стркутуру аналогичная тому что на проде => после этого мы успешно запустили проект тераформ на проде. 
0.1 cloud-init
в main.tf прописано 
  metadata = {
    ssh-key = ...
    }
так вот на хосте служба cloud-init считывает из сети 
этот ключ через ссылку
http://169.254.169.254/2018-09-24/meta-data/public-keys/0/openssh-key (здесь лежит ssh-key)

а вот этот кусок в main.tf 
  metadata = {
    user-data           = data.template_file.cloud_init.rendered
}

его cloud-init счывтает по сети по ссылке
http://169.254.169.254/2018-09-24/user-data

если мы в main.tf поменяли user-data то 
накатываем новые настройки это херни через тераформ 
$ terraform validate
$ terrafrom plan
$ terraform aplly 
заходим на хост и мы сразу увидим эти настройки по ссылке 
http://169.254.169.254/2018-09-24/user-data

далее вопрос как накатить их на хост не перезагружая хост.
вначале нужно стереть файлы
# rm -rf /var/lib/cloud/*

а потом запутсить накат 
# cloud-init init

прикол в том что без стиарния файлов эта сука ненакатывает изменения. 
но и возникает проблема в том что из за стиаиния файлов на хосте меняется ssh_host_key
значит при заходе по ssh система напишет что ключ хоста сменился!


0.000 реальные приколы и проблемы: 
- если назначить стат IP внешний  а потом его убрать из настроек тераформа
(перменаная ext-ip) то по факту этот стат IP все равно осатется прикрепленный  к этой вм. а чтобы
его открепить от этой вм надо отключить нат и обратно включить ( настройка isnat=true).
- еще одноврееннно создаем сетеовй ресурс и виртуалку описарбющуся на этот сет ресурс то 
тераформ эпплай напишет что данная виртуалка неможет быть создана так как остутстует сетевой ресурс. поэтому
приходиься вначале созадвать сетовой ресурс а потмо уже только виртуалку с этим сет ресурсом.
- далее клауд-инит вот мы задали настройки в cloud-init.cfg и раскатали виртуалку. а потом мы взяи 
и измнили файл cloud-iit.cfg и прикол в том что клауд инит больше настроки с эттго файла сука ненакатывает.
хоть перегружай хост хоть не прегржужай. возможно с клауд инит у меня изначально и неполучалось потому что
новые настройки работают только и только на абсолютно чистой системе при первой загрузке и все а ято 
пытался внести настройки на машине на которой уже было 100 перезагрузок и на которой клауд инит уже
отработал поэтому там ничего и не срабатывало
- удаление ресурсов лучше делать аккуратно и по частям
- массированное удаление с массированным созаданием ресурсов лучше разделить на части


0.0 переделать main.tf в test-folder и test-folder2 
чтобы вверху шли первостепенные ресурсы  а внизу  второстепенные
0.1  надо по взоможности те ресурсы которые надо создавать первыми размещать в main.tf вверху
а ресурсы котоыре создавать посоедними размещать внизу потому что если в файле намешано то тераформ
плохо понимает какое ресурс надо создавать первым  а потом пишет что немогу найти ресур.
например вверху main.tf надо рарзмешатть диски а виртулаки уже ниже чтобы неылло проблем.
0.2 надо локальный IP адрес создавать отдельно от виртуалки а потом уже к ней его клеить . почему.
потому что при пересоании виртуалки ей будет назначен другой локлаьный адрес и это может быть проблемой.
0.0 как правильно удалять ресурсы из тераформа?
убрал часть ресурсов из main.tf 
думал что он удалит после terfaorm aplly 
но нет. он выдал ошибку
тггда приходьистя руками удалять ресурсы их стет файла и потому уже руками из веба
удалять. 

$ terraform state list
yandex_compute_instance.vm-1
yandex_compute_instance.vm-2
yandex_compute_instance.vm-3
yandex_vpc_gateway.nat-2
yandex_vpc_network.default
yandex_vpc_route_table.route-table-2
yandex_vpc_subnet.dmz-1-a
yandex_vpc_subnet.private-1-a-subnet


и потом пользуя это можно убарть ресурс из стейт файла 
если тераформ немоет его удалитть сам

$ terraform state rm yandex_vpc_network.network-1

а так общий вывод что нужно ресурсы внаачалае закоменчивать. и только если уон удачно 
удален то уже можно удалять из конфига!!

конмет блока выгдяит вот так

/* 
....
....
*/



|далее. hostname в описании ВМ


разберем некотоыре поля в ВМ

resource "yandex_compute_instance" "vm-3" {
  name                      = "vm-3---terraform"
  hostname                  = "vm-3---terraform"
  description               = "тест машина. создана через тераформ"


name это название виртуалки которые видно через веб морду.
description это описание виртуалки в ввеб морде если на нее нажать.
тоесть эти два поля они вобще то для веб морды. и для якли

hostname это линукс хостнейм который нам будет виден когда мы войдем по ssh на виртуалку.

если мы хотим поменять name или description то тераформ это делает налету.
а вот если мы хотим поменяеть hostname то тераформ (внимание ) убивает виртуаку и пересоздает ее
заново!!!

какой огромный бонус есть от hostname - можно пинговать все виртуалки котоыре входят в состав network
по их hostname (он резволится через локальный яндекс dns сервер. для нас это все автоматически настроек делать руками ненадо)

|далее. STATE LIST

очень полезная комнанда

$ terraform  state list
yandex_compute_disk.vm-1---bootdisk---disk1
yandex_compute_disk.vm-1---disk2
yandex_compute_disk.vm-1---disk3
yandex_compute_instance.vm-1
yandex_compute_instance.vm-2
yandex_compute_instance.vm-3
yandex_vpc_address.ext-ip-1
yandex_vpc_gateway.nat-2
yandex_vpc_network.default
yandex_vpc_route_table.route-table-2
yandex_vpc_subnet.dmz-1-a
yandex_vpc_subnet.private-1-a-subnet
yandex_vpc_subnet.subnet3

показвыает все ресурсы

$ terraform show 
покажет другое


| далее. CONSOLE
в консоли можно посмтреть переменные. пример.
$ head variables.tf 
variable "cloud_id" {
  description = "cloud id"
  type        = string
  default     = "b1g8sk79560kbvb6nr2j"
}


запускаем консоль и смотрим чему равна переменная
$ terraform console
> var.cloud_id
"b1g8sk79560kbvb6nr2j"
> exit



| далее. в названии resource нельзя использовать переменные.
тоесть 

resource "yandex_vpc_address" "ext-ip-1" {
}

так вот в этой строке resource нельзя применять переменные а только константы . все.






| далее. DEBUG 
как активировать выполнение тераформа в режиме дебага

$ export  TF_LOG=TRACE
$ terrafrom ...

лог выполнения будет на экране прям. в лог файл лазить никакой ненадо.

а чтобы выключить дебаг то надо 
$ unset TF_LOG





| далее. ошибка failed to read provider configuration schema for ... provider
это значит что был установлен старый провайдер. надо его убрать из конфига и удалить через
$ terraform init
а потом хорошенько поискать как правильно поставить нужную версию провайдера






|далее. VARIABLES , MAP, LIST, CONSOLE
описание перменных. работа с ними. дебаг в консоли тераформа.

в тераформе переменные лучше всего обьявлять в спец файле
для этого variables.tf тогда тераформ автоматом засасывает этот файл,
обьявляется переменная  вот по такому правилу


variable "имя_переменной" {
  type = тип_переменной
  default = {
   
   значение переменной

  }
}


какие бывают типы перменных:
string
number
bool
list
map


самые интересные типы это list и map. они  в точности соотвестуют ямловским типа список и словарь тоесть
list и dictionary

как выглядит примеры с list и dictionary


variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [
    "10.0.101.0/24",
    "10.0.116.0/24"
  ]
}


соотсветсвенно можно тоже самое записать чуть по другому

variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [ "10.0.101.0/24", "10.0.116.0/24" ]
}


важно заметить что мы не просто пишем list а еще и указываем тип данныех внутри этого листа,
 в доках тераформа эти суки об этом ничего не пишут.
 значит по идее тогда таких типов list() может быть несколько

list(string)
list(number)
list(bool)

и еще может быть один спец тип списка
list(any)

заметим что значение описывается через квадратгые скобки [] ровно так как это и есть у ансибля в его ямль
в ансибле аналогичная переменная бы имела вид

private_subnet_cidr_blocks: [ "10.0.101.0/24", "10.0.116.0/24" ]

либо

private_subnet_cidr_blocks: 
    - "10.0.101.0/24"
    - "10.0.116.0/24" 






а вот пример dictionary

variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}



или вот в таком виде

variable "ami_image_id" {
  type = map(string)
  default = {  "ru-central1-a" = "fd8haecqq3rn9ch89eua",  "ru-central2-a" = "fd8haecqq3rn9chaaaaa"  }
}


а вот как эта бы перменная выгяедла бы в ансибле

ami_image_id: {  "ru-central1-a" = "fd8haecqq3rn9ch89eua",  "ru-central2-a" = "fd8haecqq3rn9chaaaaa"  }

либо

ami_image_id: 
   "ru-central1-a": "fd8haecqq3rn9ch89eua"
   "ru-central2-a": "fd8haecqq3rn9chaaaaa"  



тип map тоже не может быть просто типа map. он также должен быть прояснен какие виды значений
лежат в этом словаре тоесть
  type = map(string)

либо

  type = map(bool)

ну и так далее

а еще можно вотак

  type = map(any)



а теперь возникает очень важный вопрос - а как нам получать доступ к элементам внутри list
или dictionary. ответ такой - если это список то поиск идет по номеру элемента в списке ( по номеру строки
в списке) 

а если это dictionary то поиск идет по ключу ( так как дикшонари это ключ=значение то сама логика
просит нас делать поиск по ключу. у списка нет клюей поэтому там был вариант искать либо по номеру строки либо по значениею в итоге в списке ищут по номеру строки)

показываю на практике
имеем в variables.tf

$ cat variables.tf

variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}


variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [ "10.0.101.0/24", "10.0.116.0/24" ]
}



заходим  в консоль
$ terraform console


> var.ami_image_id
tomap({
  "ru-central1-a" = "fd8haecqq3rn9ch89eua"
  "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
})



> var.ami_image_id["ru-central1-a"]
"fd8haecqq3rn9ch89eua"



> var.private_subnet_cidr_blocks
tolist([
  "10.0.101.0/24",
  "10.0.116.0/24",
])


> var.private_subnet_cidr_blocks.0
"10.0.101.0/24"
> var.private_subnet_cidr_blocks.1
"10.0.116.0/24"



а теперь покажу что можно при вызове переменной исползовать другую переменную.


вот у нас есть две переменные

variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}


variable "zone" {
  description = "availability zone"
  type        = string
  default     = "ru-central1-a"
}



и тогда можно вот так

$ terraform console

> var.ami_image_id[var.zone]
"fd8haecqq3rn9ch89eua"


единственное надо постоянно помнить что мы когда вызываем переменную то надо приплюсовать var.
тоесть var.zone а не просто zone


итак еще раз если у нас есть переменная vasya у которой тип list это значит что внутри 
переменной есть списочек элементов
и можно получать доступ к элементам списка через вот так

var.vasya.0 или var.vasya.1

а если у нас переенная vasya имеет тип map то переменная состоит из элементов каждый из которых это 
ключ=значение. и доступ к элементу идет через значение ключа. тоесть 

var.vasya[ключ]


хочу еще раз рассмотреть момент определения сколько элементов имеет переменная list или map
в себе. потому что навсикду всегда непонятно сколько же элеметов имеет переменная типа list или map
еще раз рассмотрим переменные которые мы выше смотрели


variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}


variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [ "10.0.101.0/24", "10.0.116.0/24" ]
}



еще раз посмотрим как выглядели бы эти переменные в ансибле


ami_image_id:
    ru-central1-a: "fd8haecqq3rn9ch89eua"
    ru-central2-a: "fd8haecqq3rn9chaaaaa"


private_subnet_cidr_blocks: 
    -  "10.0.101.0/24" 
    -  "10.0.116.0/24" 


тогда у первой переменной есть два элемента  'ru-central1-a: "fd8haecqq3rn9ch89eua"' и
'ru-central2-a: "fd8haecqq3rn9chaaaaa"''

у второй переменной есть два элемента  '"10.0.101.0/24"' и '"10.0.116.0/24"' 

надеюсь вопрос сколько элементов имеет list или map стал более понятным.


теперь рассмотрим еще один пример переменной

variable "project" {
  description = "Map of project names to configuration."
  type        = map(any)

  default = {
    client-webapp = {
      public_subnets_per_vpc  = 2,
      private_subnets_per_vpc = 2,
      instances_per_subnet    = 2,
      instance_type           = "t2.micro",
      environment             = "dev"
    },
    internal-webapp = {
      public_subnets_per_vpc  = 1,
      private_subnets_per_vpc = 1,
      instances_per_subnet    = 2,
      instance_type           = "t2.nano",
      environment             = "test"
    }

  }

}



посмотрим как выглядела бы эта переменная в ансибле


project: 
   client-webapp: 
      public_subnets_per_vpc: 2
      private_subnets_per_vpc: 2
      instances_per_subnet: 2
      instance_type: "t2.micro"
      environment: "dev"
   internal-webapp: 
      public_subnets_per_vpc: 1
      private_subnets_per_vpc: 1
      instances_per_subnet: 2
      instance_type: "t2.nano"
      environment: "test"


а теперь посмотрим как мы внутри этой переменной мы будем вычленять элементы и субэлементы


$ terraform console
> var.project
tomap({
  "client-webapp" = {
    "environment" = "dev"
    "instance_type" = "t2.micro"
    "instances_per_subnet" = 2
    "private_subnets_per_vpc" = 2
    "public_subnets_per_vpc" = 2
  }
  "internal-webapp" = {
    "environment" = "test"
    "instance_type" = "t2.nano"
    "instances_per_subnet" = 2
    "private_subnets_per_vpc" = 1
    "public_subnets_per_vpc" = 1
  }
})



> var.project.client-webapp
{
  "environment" = "dev"
  "instance_type" = "t2.micro"
  "instances_per_subnet" = 2
  "private_subnets_per_vpc" = 2
  "public_subnets_per_vpc" = 2
}


> var.project.internal-webapp
{
  "environment" = "test"
  "instance_type" = "t2.nano"
  "instances_per_subnet" = 2
  "private_subnets_per_vpc" = 1
  "public_subnets_per_vpc" = 1
}

> var.project.internal-webapp.environment
"test"

> var.project.client-webapp.environment
"dev"

> var.project.client-webapp.instance_type
"t2.micro"










1. почемуто output из модуля нихуя неработает сук бляь
1.3 conditinals

если нам надо if then else

locals {
  test = "${ condition ? value : (elif-condition ? elif-value : else-value)}"
}

(нашел здесь https://stackoverflow.com/questions/55555963/how-to-write-an-if-else-elsif-conditional-statement-in-terraform)


мой пример. в конфиге задается две переенные
vm_name
vm_hostname

моя задача : 
если незадан vm_name но при этом задан vm_hostnme то vm_name=vm_hostname инааче vm_name = random
если незадан vm_hostname но при этом задан vm_name то vm_hostname=vm_name иначе vm_hostname = random

решение

locals {

   check_name     = var.vm_name != "" ? var.vm_name : (var.vm_hostname != "" ? var.vm_hostname : "name-error")
   check_hostname = var.vm_hostname != "" ? var.vm_hostname : (var.vm_name != "" ? var.vm_name : "hostname-error")

   vm_name       = "${local.check_name}"
   vm_hostname   = "${local.check_hostname}---terraform"
   vm_desc       = "${local.check_hostname}  ${var.vm_desc}"

   disk1_desc    = "${local.check_name}  ${var.disk1_desc}"
   disk1_name    = "${local.check_name}---${var.disk1_name}"

}




0.0 как передать внутрт модуля наример параметр vpc network 
ответ надо передать через string имя vpc а внутри модуля создать через data этот vpc
прмиер
задаем переменную

variable "subnet" {
  description = "id сети "
  type        = string
  default     = "subnet3"
}


вот внутри модуля пишем

data "yandex_vpc_subnet" "this" {
  name = var.subnet

}

при том что сам vpc network созан не в модуле. мы лишь суда через data передали
его параметр

теперь внутри модуля можно обращаться е этому network vpc
  network_interface {
    subnet_id = data.yandex_vpc_subnet.this.id
    nat       = var.isnat
  }

  это круто!




0.0 запилить сеть в отдельный модуль!
0.a variables.tf vs terraform.tfvars
https://stackoverflow.com/questions/56086286/terraform-tfvars-vs-variables-tf-difference

1.0 alien-terrafrom вопросы 
[ как подключить второй и третий диск через модуль?]
for_each (как юзать):
variable "vpc_name" {
  description    = "список vpc нетворков"
  type           = list(string)
  default        = [ 
                     "network-1",
                     "network-2"
                                 ]
}


resource "yandex_vpc_network" "this" {
  for_each = toset(var.vpc_name)
  name = each.value
}


результат

$ terraform show

# yandex_vpc_network.this["network-1"]:
resource "yandex_vpc_network" "this" {
    created_at = "2023-03-18T22:24:22Z"
    folder_id  = "b1gobq5rv8qm8qi76hig"
    id         = "enpf3vgubl2fpm6nmfg0"
    labels     = {}
    name       = "network-1"
    subnet_ids = []
}

# yandex_vpc_network.this["network-2"]:
resource "yandex_vpc_network" "this" {
    created_at = "2023-03-18T22:24:22Z"
    folder_id  = "b1gobq5rv8qm8qi76hig"
    id         = "enp19jihnisr82lad784"
    labels     = {}
    name       = "network-2"
    subnet_ids = []
}

а $ terraform state show yandex_vpc_network.this["network-1"]
неработает!

походу решение в другой области = https://github.com/yandex-cloud/terraform-provider-yandex/issues/43


витоге решение

variable "vasya" {
  type = list(string)
  default = [ "disk4", "disk5" ]
}





resource "yandex_compute_disk" "this" {
  for_each = toset(var.vasya)
  name       = each.value
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}


resource "yandex_compute_instance" "vm-3" {


 dynamic "secondary_disk" {
    for_each     = var.vasya
    content {
     disk_id     =  yandex_compute_disk.this[secondary_disk.value].id
     auto_delete = true
     mode        = "READ_WRITE"
    }
 }


а если я не хочу ничего создавать тогда перменная должны выглядеть вот так

variable "vasya" {
  type = list(string)
  default = [ ]
}


далее в модуле пишем

data "yandex_vpc_subnet" "this" {
  name = var.subnet
}


(что за хрень  data ??)

тогда в настройках вм пишем

resource compute_instance ...   {
...
  network_interface {
    subnet_id = data.yandex_vpc_subnet.this.id
    nat       = false
  }
...
}



далее прикольно то что задание общей перенной в файле varibles уже никак не влияет 
на перменные уазнные в модуле!
а чтобы переопределти занчение перменнйо в модулу то нужно делать вот так


module "vm3" {
  source        = "./modules/create-vm"
  vasya  =  [ "disk2", "disk3", "disk4" ]

}


тоесть нужно в вызоыве модуля писать новое занчнеиеи переменной в данном случае это 
vasya = 










[ загрузка модуля из гитлаба ]
[ как заставить первым отрабатывать root модуь и только потом осталные модули????  ]
[ yandex security group? что за хрень? ]

1.3 null_provider и null_resurce
как поставить и юзать. для игры с переменными

1.4 list vs set ?
1.2 что за станный тип переменной?
variable "user_information" {
  type = object({
    name    = string
    address = string
  })
  sensitive = true
}
(https://developer.hashicorp.com/terraform/language/values/variables#set)


1.4 depends_on = ?
https://www.scalr.com/blog/terraform-locals#:~:text=How%20do%20locals%20differ%20from,more%20meaningful%20or%20readable%20result.

1.r There is also a concept of variables on Terraform which can be used to assign dynamic values, 
1.3 терраформ console ?
1.5 как дестроить ресурсы = https://spacelift.io/blog/how-to-destroy-terraform-resources

1.6 что такое локальные перменные 
( https://spacelift.io/blog/terraform-locals,
https://www.scalr.com/blog/terraform-locals#:~:text=How%20do%20locals%20differ%20from,more%20meaningful%20or%20readable%20result.)

1.1 импортировать полностью всю структуру в тераформ из папки test-folder
1.4 импортироровать структуру из папки prod 
3.4 проработать на практие опреацию рашинеия диска системного и загрузочного.
так чтбы четко видно что ничего нестирается при этом.
1.1 как создать стат internet IP и прикрепить его к виртулке в тераформ
1.3 стопим виртлку. через веб мрду расширяем размер бут диска. стартуем вирталку.
импортируем изменнеие в тераформ. убеждаемся что ничего не сломано.
1.4 создаем в тераформ бутовый диск отдельно через ресурс. потом уже его добавляем как 
бутовый в виртуалуй с опцией что нельяз удалять. если мы удалим эту вирталку то у нас останется id 
этого диска и мы сможем его перепокдлючить либо удалить черзе тераформ.
далее два варианта: первый это снимаем снэпшт или имадж с диска , и подключаем к другой виртулке ( это все
чере тераформ. ) правим этои диск , отключаем и создаем новую виртуалку. второй вариант удаляем 
виртуалку. берем этот диск пдключаем к дургой виртулке , правим его, отключаем, созаем новую виртуалку.
1.5 научится импортировать впц+субнеты+вм
2. два сервис акаунта для управления одной папкой
3. dynamo + запрет на одновременный модификация двух юзеров + проверить + описать
3.a [ создать кастом диск с lvm      ]
    [ импорт инфрастрктуры в main.tf ] 
3.1 записать сюда мой main.tf 
a.2 модули в тераформ ?
a.3 output что за хрень ?
0.0 как с переменными работать
variable "region" { default = "us-east-1" }
variable "availability_zone" { default = "us-east-1a" }
variable "instance_type" { default = "t2.micro" }
variable "instance_count" { default = "2" }
variable "key_name" {}
variable "public_key_path" {}
variable "connection_user" { default = "ec2-user" }
variable "name" { default = "web" }
variable "environment" { default = "production" }
provider "aws" {
  region = "${var.region}"
}
module "web" {
  source = "../modules/web"
  availability_zone = "${var.availability_zone}"
  connection_user = "${var.connection_user}"
  instance_count = "${var.instance_count}"
  instance_type = "${var.instance_type}"
  key_name = "${var.key_name}"
  public_key_path = "${var.public_key_path}"
  region = "${var.region}"
  web_security_groups = "${module.sg.security_group_id}"
  name = "${var.name}"
  environment = "${var.environment}"
}
module "sg" {
  source = "../modules/sg"
  name = "${var.name}"
  environment = "${var.environment}"
}
module "elb" {
  source = "../modules/elb"
  availability_zones = "${var.availability_zone}"
  name = "${var.name}"
  environment = "${var.environment}"
  web_instance_ids = "${module.web.web_instance_ids}"
}
output "elb_dns_name" {
  value = "${module.elb.dns_name}"
}
0.1 как же делать бэкап бакета? 




















