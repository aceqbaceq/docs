terraform

установка для убунту 16

$ sudo snap install terraform


установка (для убунту >16)

$ sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl

$ curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -

$ sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"

$ sudo apt-get update && sudo apt-get install terraform



установили

проверка что он встал

$ terraform -help

установить автодополнение по TAB

$ touch ~/.bashrc

$ terraform -install-autocomplete



пробный проект

$ mkdir learn-terraform-docker-container

$ cd learn-terraform-docker-container

$  touch main.tf

terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 2.13.0"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx" {
  name         = "nginx:latest"
  keep_locally = false
}

resource "docker_container" "nginx" {
  image = docker_image.nginx.latest
  name  = "tutorial"
  ports {
    internal = 80
    external = 8000
  }
}

$ terraform init

$ sudo terraform init

надо научится прописывать sg. и стопить машину.

=

как проверить коректность и запустить план 


$ terraform fmt
$ terraform validate
$ terraform plan  -out plan.txt

как запустить уже выполеннеие
$ terraform apply plan.txt

=

создаем файл в котором прописан реурс  типа вольюм

$ cat volumes.tf 
// volumes.tf
resource "aws_ebs_volume" "elk" {
  availability_zone = "eu-central-1a"
  size = 12
  type = "gp2"
  tags = {
          "Name" = "elk-root_vol" 
          "elk"  = "rootvol"
        }


}


вопрос как его тераформ будет искать а амазоне.
нууууу. ответ такой что id вольюма почемуто нельзя указать.
но можно импортровать этот вольюм

$ terraform import aws_ebs_volume.elk vol-номер_вольюма

он импортрутся куда то  в кишки тераформа в файл terraform.tfstate
ну и как то после этого терафоом понимает что вольюм указанный в volumes.tf 
и тот который он импортровал это одно и тоже.
а как быть если мы  с нуля слздаем вольюм на амазоне. я так понимаю что 
почемуто в конфиге непроисать id. а он будет автоматом прописан тераформом  а файле terraform.tfstate
если вольюм создается с нуля. не очень както прикольно.

если мы хотим посмотреть что будет поменяно на амазоне не для всех обьектов а для конкретного то 
надо юзать ключ -target

$ terraform plan -target=aws_ebs_volume.elk -out plan.txt

эта команда покажет план что траформ будет менять на амазоне только 
для вольюма.

==
надо научится расширять фс через тераформ

==
важная штука

если в модуле aws_instance
мы указываем
security_groups 
то теарформ создает типа ec2 classic instance. в чем его жопа это то что 
у него нельзя поеменять security group без его уничтожения.
а если указать vpc_security_group_ids
то уже все окей. натаком инстансе можно менять секуююрити группу без переуничтожения инстанса

-====

еще раз как накатывать плейбук

$ terraform fmt
$ terraform validate
$ terraform plan -out plan.txt
$ terraform apply plan.txt

===
начала заново учить тераформ.
на примере яндекс облака

вначале надо поставит на комп yandex cli
как это сделать и прочая полезная штука про яблоко смотри в файле yandex_cloud.txt

вот мы поставили якли. в якли есть профили. профилей может быть несколько.
профиль это  набор настроек с которым якли
конектится к яблоку. эти настройки вклчают в себя: креды,номер папки яблока, номер облака яблока.
в один момент времени может быть активен только один профиль якли. с его настройками
якли и стучится на яблоко.

так вот нам для тераформа надо чтобы текущий профиль якли был с кредами сервис акаунта( сервис акаунт
это такой один из типов юзеров с которым можно подключаться к яблоку) от имени которого мы 
и будем подключаться из тераформа к яблоку. 

возникает вопрос какая связь между якли и тераформом. связь такая что тераформу чтобы подключаься 
к яблоку надо получить от IAM (сервис аутентификации яблока) специальный токен = IAM токен.
этот токен это как пропуск. который тераформ будет предьявлять сервисам яблока для работс ними.
так вот этот токен это проще всего получить через якли постучавшись на яблоко от имени сервис акаунта.


когда поставили яндекс кли. и там прописали в профиле папку на яблоке, креды сервис акаунта
то теперь надо добавить этому сервис акаунту права нужные тераформа.
а именно (как я понял из этого говенного описания https://cloud.yandex.com/en/docs/tutorials/infrastructure-management/terraform-quickstart) надо выдать этому акаунту права "editor"
сделать это тоесть добавить прав сервис акаунта можно через веб морду. окей защли в веб морду
выдали права "editor"

возвращаемся к нашему якли.
проверяем в нашем якли профиле какой сервис акаунт указан, какая яблоко  папка указана,
какое яблоко облако указано
$ yc config list
service-account-key:
  id: aje9ru
  service_account_id: ajed54
  created_at: "2023-03-09T19:15:13.886916958Z"
  key_algorithm: RSA_2048
  public_key: |
    -----BEGIN PUBLIC KEY-----
    MIIBIjAQAB
    -----END PUBLIC KEY-----
  private_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQ+cVgQvUE5AowIHtW/D0=
    -----END PRIVATE KEY-----
cloud-id: b1g8sk79560kbvb6nr2j
folder-id: b1gobq5rv8qm8qi76hig

тоесть проверяем 
service_account_id
cloud-id
folder-id

если это то что мы ожидали видеть то двигаем дальше.
а именно получаем IAM токен через якли от IAM сервиса 

$ yc iam create-token
t1.9euelZqM...


тоесть якли стучится от имени сервис акаунта на IAM сервис и говорит я сервис акаунт такой то
дай мне IAM токен.
если все ок то на экране мы получим наш IAM токен. он действиует 12 часов.
его можно обновить если приспичило в течение этих 12 часов но янедкс пишет что это делать 
рекеомнудется не чаще чем 1 раз в час.


этот iam токен это наш паспорт пропуск теперь  к сервисам яблока. мы его подставим в тераформ 
и он сможет успешно подключаться к сервисам яблока.
вот именно ради возможности получит IAM токен мы и ставили на комп якли.


длаее созаем папку

$ mkdir ~/cloud-terrafom
$ cd ~/cloud-terrafom
$ touch main.tf

дальше дока яндекса рекоменует переимновать файл .terraformrc в котором находится
конфиг настроек тераформа для текущего юзера.
$ mv ~/.terraformrc ~/.terraformrc.old

этого файла может и небыть.

создаем заново этот файл
$ touch  ~/.terraformrc

в этот файл мы вставляем кусок

$ cat ~/.terraformrc
provider_installation {
  network_mirror {
    url = "https://terraform-mirror.yandexcloud.net/"
    include = ["registry.terraform.io/*/*"]
  }
  direct {
    exclude = ["registry.terraform.io/*/*"]
  }
}



далее в наш main.tf
мы вставляем кусок:

terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"
}

provider "yandex" {
  token     = "<OAuth>"
  cloud_id  = "<cloud ID>"
  folder_id = "<folder ID>"
  zone      = "<default availability zone>"
  
}


соответвенно сюда надо вставить наш текущий iam-токен
котоыйр мы ранее полуили через якли от яблока,
а cloud-id, folder-id, zone availability  можно посмотреть в свойствах якли 
$ yc config list

далее как я понял из этого мудацкого описания https://cloud.yandex.com/en/docs/tutorials/infrastructure-management/terraform-quickstart
переходим в папку с main.tf 
и запускаем команды

$  terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 yandex-cloud/yandex
$  terraform init


далее в main.tf 
добавляем кусок


resource "yandex_compute_instance" "vm-1" {
name = "terraform1"

resources {
    cores  = 2
    memory = 2
}

boot_disk {
    initialize_params {
    image_id = "fd87va5cc00gaq2f5qfb"
    }
}

network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
}

metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
}
}

resource "yandex_compute_instance" "vm-2" {
name = "terraform2"

resources {
    cores  = 4
    memory = 4
}

boot_disk {
    initialize_params {
    image_id = "fd87va5cc00gaq2f5qfb"
    }
}

network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
}

metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
}
}

resource "yandex_vpc_network" "network-1" {
name = "network1"
}

resource "yandex_vpc_subnet" "subnet-1" {
name           = "subnet1"
zone           = "ru-central1-a"
network_id     = yandex_vpc_network.network-1.id
v4_cidr_blocks = ["192.168.10.0/24"]
}

output "internal_ip_address_vm_1" {
value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}

output "internal_ip_address_vm_2" {
value = yandex_compute_instance.vm-2.network_interface.0.ip_address
}


output "external_ip_address_vm_1" {
value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}

output "external_ip_address_vm_2" {
value = yandex_compute_instance.vm-2.network_interface.0.nat_ip_address
}


далее проверяем валидность этого конфига
$ terraform validate

далее просим тераформ переписать наш конфиг так чтобы он стал "красивым"
$ terraform fmt


$ terraform plan  -out plan.txt

далее формируем файл изменений которые тераформ собирается делать. так называемый
план. при этом на яблооке ничего небудет изменено мы просто увидим подробный план
того что тераформ собирается делать на экране а в файле будет запсана некая служебная хрень
$ terraform plan  -out plan.txt

ПОЛЕЗНАЯ ВЕЩЬ - в веб морде яблока нажимаем на имя клауда и справа будет закладка Quotas
в ней указаны лимиты на разные ресурсы в яблоке. там же можно увеличить эти лимиты

ПОЛЕЗНАЯ ВЕЩЬ - как посмотреть имаджи операционок доступные на яблоке
$ yc compute image list --folder-id standard-images | grep -E "ID|ubuntu-22"

далее применяем наш план уже в жизнь
$ terraform apply  "plan.txt"


готово. виртуалки и другие ресурсы из плана созданы.

как останвоить виртулки через тераформ пока непонятно.
приходится через якли это делать

$ yc compute instance list
$ yc compute instance stop  --name  terraform1
$ yc compute instance stop  --name  terraform2


далее. хотел сказать про значение файлов

.terraform.lock.hcl = в этом файле как понял прописвыаются  список провайдеров и их версии
здесь нет стейтов инфраструктуры


terraform.tfstate = в этом файле записано состояние инфраструктуры


далее как оказалось нам нафик ненадо завязываться в тераформе
на аутентификацию через iam токен. ведь его же над обновлять каждые 12 часов.
это не наш вариант. можно вместо этого указать в тераформе путь к 
RSA ключам от service акаунт а тераформ уже будет на основе этого
сам автоматом запрашиваь у iam сервиса iam токен. делается это вот так:
в main.tf
надо прописать


provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/key.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gobq5rv8qm8qi76hig"
  zone                     = "ru-central1-a"
}


где key.json это файл от яндекс кли в котором прописаны RSA ключи 
от service account в формате JSON. как получить этот файл смотри в файле yandex_cloud.txt


ДАЛЕЕ.
вопрос вот у нас есть папка тераформа, какие файлы из нее нужно взять 
чтобы на основе их создать такую же папку. тоесть как пеереносить папку тераформа.
значит ответ такой, заходим в папку с тераформом и забираем оттуда только ТРИ файла.
этого достаточно:

main.tf  (здесь наш конфиг)
.terraform.lock.hcl  (здесь описаны провайдеры)
terraform.tfstate   (здесь стейт инфраструктуры на момент последнего apply)

копируем эти три файла в новую папку. далее
в этой папке запускаем команды

$  terraform  init
$  terraform validate 
$  terraform fmt
$  terraform plan -out "plan.txt"
$  terraform apply "plan.txt"

и вуаля мы восстановили папку тераформа в новом месте.


ДАЛЕЕ.
новая задача перенести файл "terraform.tfstate" из хранения в локальной папке на компе в яблоко .
зачем нам это надо.  это надо для того чтобы пользоваться тераформом можно было нескольким юзерам.
поскольку файл стейта очень важен то чтобы исключить человеческую ошибку будет класно если 
на лкальном компе ни у кого этого файла небудет а юзер будет обращаться за этим файлам куда то в сеть.
переносим файл стейта  в яблоко в его s3 хранилище в бакет. хранилище называется у яблока как "object storage"
для этого надо зайти на яблоко сервис object storage и создать там бакет. я покажу как это делать
позже. а пока  считай что мы создали бакет со всеми нужными штуками
далее в main.tf в раздел terraform в его конец вставляем кусок


terraform {
...

  backend "s3" {
    endpoint   = "storage.yandexcloud.net"
    bucket     = "имя_бакета"
    region     = "ru-central1-a"
    key        = "folder1/terraform.tfstate"
    access_key = "YCAJ..."
    secret_key = "YCM0..."

    skip_region_validation      = true
    skip_credentials_validation = true
  }

}


обьясняю поля
key        = "folder1/terraform.tfstate"
дело вот в чем. бакет это типа как папка. в этой папке будет создана подпапка с именем "folder1"
и в эту папку уже будет скопирован файл terraform.tfstate
возникает вопрс так что же в этом ключе прописывать. какой путь. 
самое смешеное что этот параметр может быть абсолютно любым. вот какой путь мы пропишем по такому 
пути тераформ и засунет наш стетй файл с локальной системы в этот бакет. вопрос а как засунуть то на практике ? ответ будет дальше.

а пока продолжаю про поля
access_key = "YCAJ..."
secret_key = "YCM0..."

это параметры так называемого static access key. откуда его взять? когда мы создали бакет (покажу как это делать ниже) то мы в ACL бакета указыаем кому в него можно лазить. в моем случае это serivce account
так вот идем в веб морде в яблоко folder. в ее свойствах заходим в наш сервис акаунт. и справа вверху 
есть кнопка "create new key" , там можно выбрать какой тип нового ключа мы хотим создать. 
есть "create authorized key" это наш старый знакомый RSA пара ключей который мы прописыаем в якли
либо в тераформ и они используя этот authorized key получают от iam сервиса iam токен. вобщем  это не то что 
нам надо на данынй момент потому что (хз знает почему) сервис object storage в котором сидит бакет 
он не аутентифицирует юезра как я понял через iam токен. он аутентифицирует юзера через "static aceesss key"
поэтому мы выбираем в том меню "create static access key" и он нам выдаст нам пару ключей. 
это и есть access_key и secret_key ! вот их то и надо указать в конфиге main.tf
так с этим разобрались. итак  с полями разобрадись в main.tf



далее надо запустить команду

$ terraform init
она скопирует файл terraform.state в бакет на яблоко. 
бояться этой команды ненало. она не сотреть все инициализацией. она именно толтко
перенесет terraform.state в бакет на яблоко
именно этой командой мы и переносим стейт файл в облако.
также важно заметить что исходный стейт файл останется по прежнему
локальной папаке и надо тут же его стереть в папке руками !


далее  провеяряем что наш тераформ успешно делаем свои дела
подключаясь к удаленному стейт файлу

$ terraform validate
$ terraform fmt
$ terrafrom plan -out "plan.txt"
$ terraform aplly "plan.txt"

классно!

а сейчас я ввозращаюсь к вопросу как создать бакет:
идем в веб морду в object storage и создаем новый бакет. 
в его свойствах прописываем в ACL что нащ service account имеет права на чтение\запись 
в этот бакет.


все - теперь стейт файл хранится в яблоке в бакете. это просто отлично!

если мы потом хотим перенести наш стейт файл в новый бакет
или в этом же бакете но по новому пути то для этого надо
1) обновить сеекцию 
backend "s3" {
   ...
  }

2) скачать рукамт стейт файл из старого бакета и разместиь его 
в локальной папке под именем terraform.state

3) запустить 
$ terraform init -migrate-state

и все. наш стейт файл скопирван в новый бакет. также надо теперь 
удалить руками стейт файл из локальной папки.

также важно отметмть что если мы перенесли стейт файл в бакет то 
для переноса тераформ папки нам теперь надо не три файла а всего два:

main.tf  (здесь наш конфиг)
.terraform.lock.hcl  (здесь описаны провайдеры)

но это еще не конец мудежа. наща конечная задача это чтбы если один юзер запустил
команду на исполннеие тераформа то второй юзер должен немочь ее запукать чтобы 
неиспортить инфраструктуру. для этого длполенительно надо заюзать dynamo
для этого юзаю эту статью https://habr.com/ru/post/711982/








#$#!!!
описать:
3.s надо чтобы все аспекты с нуля до конца были сделаны через командную строку.
например создание бакета и все такое. сейчас многое все делается руками а надо 
чтобы все было только через скрипт!!!!

тераформ работает от имени сервис акаунта в яблоке а сервис акаунт 
привязывается к конкретной папке яблока поэтому создавать папку яблока и 
сервис акаунт для этой папки через тераформ нельзя а можно только сторонниим методами
наприер через якли

переключаемся на якли профиль который раотает от имени веб юзера\яндекс юзера
$ yc config profile list
default
service_account_profile-krivosheev-test ACTIVE

актвируем профиль в котором прописан веб юзер
$ yc config profile activate default
Profile 'default' activated

проверяем что это рельно профиль от веб юзера
$ yc config list
token: y0_AgA...
cloud-id: b1g8s
folder-id: b1gobq
compute-default-zone: ru-central1-a


создаем новую яблокоо папку
$ yc resource-manager folder create test-folder-2
id: b1gop5honld
cloud_id: b1g8s2j
created_at: "2023-03-13T09:48:45.867259773Z"
name: test-folder-2
status: ACTIVE


создаем сервис акаунт для этой папки

$ yc iam service-account create --name service-account-2 \
  --description "сервис акаунт для папки test-folder-2 для тераформа" \
  --folder-name "test-folder-2"

id: ajec4h
folder_id: b1go
created_at: "2023-03-13T09:51:46.935971935Z"
name: service-account2
description: сервис акаунт для папки test-folder-2 для тераформа


создаем пару authorize keys для этого сервис акаунта. на основе этого атораайз key 
наш якли будет аутентифицироваться на иам сервисе и получать от него иам токен  который 
в свою очередь я кли будет предьявлять сервисам яблока для аутентиицикации.
таким макаром аторайз ки используется для аутентификации перед иам сервисом  но для доступа 
к другим сервисам яблока например vpc надо предьявлять иам токен который выдает иам сервис


$ yc iam key create --service-account-name service-account-2 --folder-name "test-folder-2" -o $HOME/.config/yandex-cloud/service-account-2-auth-keys.json

id: aje6rqernq
service_account_id: aje5n
created_at: "2023-03-13T10:05:51.187614866Z"
key_algorithm: RSA_2048


обращаю вниманеи что надо указывать папку в которой сидит сервис акаунт иначе  система небудет 
его находить.
файл с ключами мы сразу сохраняем в папку настроек якли.

даем этому сервис акаунту роль (права) на создание\редактирование виртуалок в этой папке
$ yc resource-manager folder add-access-binding test-folder-2 \
  --role compute.admin \
  --service-account-name service-account-2 \
  --folder-name test-folder-2


подставляем выше
$ yc resource-manager folder add-access-binding test-folder-2 \
     --role compute.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2

также добавляем роль чтобы акаунт мог сети в vpc создавать
$ yc resource-manager folder add-access-binding test-folder-2 \
     --role vpc.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2



также добавляем ему права на редактирование ydb баз "ydb.admin"
$ yc resource-manager folder add-access-binding test-folder-2  \
     --role ydb.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2


также добавляем роль чтобы этот акаунт мог создавать бакеты

$ yc resource-manager folder add-access-binding test-folder-2 \
     --role storage.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2



генерируем для этого сервис акаунта его static access key ( аля aws static key)
$ yc iam access-key create \
   --service-account-name service-account-2 \
   --folder-name "test-folder-2" \
   --description "статик ключ для доступа на aws подобные ресурсы"

access_key:
  id: aje7nled85chli
  service_account_id: aje5na
  created_at: "2023-03-13T19:57:42.009502688Z"
  description: статик ключ для доступа на aws подобные ресурсы
  key_id: YCAJE...
secret: YCNuNIf...


устанавливаем aws cli чтобы туда записать полученный   access static key
потому что якли этот ключ использовать неможет а aws cli может (дебилизм конечно)
нам этот aws cli понадобится чуть ниже когда мы будем работать с YDB базой данных.

# apt-get install awscli
прописываем наш статический ключ который мы получили выше в aws cli конфиг используя aws cli профили  
для того чтобы мы имели возможность если нужно  записать
несколько стат ключей
$ aws configure --profile service-account-2
AWS Access Key ID [None]: YCAJE...
AWS Secret Access Key [None]: YCNuNIf...
Default region name [None]: ru-central1

настройки будут сохранены в папку $HOME/.aws/

возвращаеися обратно к якли.
создаем профиль в якли с этим сервим акаунтом и его папкой в яблоке
$ yc config profile create service-account-2

далее подключаем  к текущему активному профилю наш ранее созданный файл с ключами RSA:
напомню что файл service-account-2-auth-keys.json должен лежать в папке $HOME/.config/yandex-cloud
$ yc config set service-account-key   service-account-2-auth-keys.json

далее полезно прписать в профиле cloud-id
сам id придется посмотреть через веб морду потому что как я понял с текушими ролями
наш сервис акаунт неможет его определить
$ yc config set  cloud-id b1g8sk79560kbvb6nr2j 

дале полезно добавит в профиль folder-id.
его мы уже можетм определить через cli
$ yc resource-manager folder list
+----------------------+---------------+--------+--------+
|          ID          |     NAME      | LABELS | STATUS |
+----------------------+---------------+--------+--------+
| b1gop5honldmvbpmg516 | test-folder-2 |        | ACTIVE |
+----------------------+---------------+--------+--------+


$ yc config set  folder-id  b1gop5honldmvbpmg516

в целом создание папки и сервис акаунта в яблоке закончена. также как бонус
мы создали профиль в якли с этим сервиc акаунтом.

теперь надо создать яблоко бакет (размером 50МБ) где мы будем хранить тераформ стейт для данной яблоко
папки

$ yc storage bucket create \
     --name bucket-2aw  \
     --default-storage-class standard \
     --max-size 52428800 \
     --grants grant-type=grant-type-account,grantee-id=aje5na0qfqnci747bgs5,permission=permission-write \
     --grants grant-type=grant-type-account,grantee-id=aje5na0qfqnci747bgs5,permission=permission-read

name: bucket-2aw
folder_id: b1gop5honldmvbpmg516
anonymous_access_flags:
  read: false
  list: false
default_storage_class: STANDARD
versioning: VERSIONING_DISABLED
max_size: "1"
acl: {}
created_at: "2023-03-13T20:53:02.424678Z"


теперь надо создать базу YDB  для того чтобы тераформ там ставил метку 
если тераформ начал чтот делать с инфраструктурой чтобы только один юзер в один момент времени
мог запускать тераформ. 

$ yc ydb database create terraform-state-lock-2 --serverless

проставляем пермишнсы для нашего сервис акаунта на эту базу
замечу что если сервис акаунт не имеет роли "ydb.admin" то команда успешно не пройдет.
я подчеркиваю я имею ввиде не про роль  "--role  ydb.editor" указанную в этой команде
а именно роль которую мы назначали акаунту выше.
$ yc ydb database add-access-binding  terraform-state-lock-2 \
     --role  ydb.editor \
     --service-account-id "aje5na0qfqnci747bgs5"


но базу создать мало надо создать таблицу в ней. и тут прикол в том что мы неможем этого сделтаь 
через якли. мы можем это сделать только через aws cli который мы поставили ранее. и прописали туда 
статик ключ от сервис акаунта.
для начала нам нужно опрееделить endpoint у нашей базы
$ yc ydb database get  --name   terraform-state-lock-2 
name: terraform-state-lock-2
document_api_endpoint: https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7

итак создаем таблицу в базе
$ aws dynamodb create-table \
   --profile service-account-2 \
   --table-name terraform-state-lock-2 \
   --attribute-definitions   AttributeName=LockID,AttributeType=S \
   --key-schema   AttributeName=LockID,KeyType=HASH \
   --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5  \
   --endpoint https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7

замечу что в этой команде я указал опцию --profile service-account-2 
это указан профиль самой утилиты aws cli чтобы aws cli стучался на яблоко под нужным ключом - под ключом именно сервис акаунта "service-account-2"



итак что сделано: 
создан сервис акаунт
ему выданы роли
создан бакет
создана база

это все нам дает теперь возможность прописать этот сервис акаунт в тераформ и он будет от его имени
стучаться на яблоко и создавать наконец виртуалки, также стейт тераформа будет хранися в бакете
и тераформ будет лочить свое состояние через YDB базу так что другой юзер не сможет запустить тераформ
если другой юзер уже запустил тераформ.

создаем папку
$ mkdir ~/terraform/01

создаем в папке два файла
$ cat .terraform.lock.hcl 
provider "registry.terraform.io/yandex-cloud/yandex" {
  version = "0.86.0"
  hashes = [
    "h1:hW7eHexpzEQXpun1kChUdfB+fFNepFxybhoxxbqOIjc=",
  ]
}




$ cat main.tf 


terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"







  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }

}





provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/service-account-2-auth-keys.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gop5honldmvbpmg516"
  zone                     = "ru-central1-a"

}







resource "yandex_compute_instance" "vm-1" {
  name                      = "terraform1"
  platform_id               = "standard-v3"
  allow_stopping_for_update = true

  scheduling_policy {
    preemptible = true
  }


  resources {
    cores         = 2
    memory        = 1
    core_fraction = 20
  }


  boot_disk {
    initialize_params {
      image_id = "fd8haecqq3rn9ch89eua"
    }
  }

  network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = false
  }

  metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/foxy.pub")}"
  }
}




resource "yandex_vpc_network" "network-2" {
  name = "network1"
}

resource "yandex_vpc_subnet" "subnet-1" {
  name           = "subnet1"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.network-2.id
  v4_cidr_blocks = ["192.168.10.0/24"]
}


output "internal_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}


output "external_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}



в этмом конфиг я хочу выделить блок

  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }


в нем мы прописваем

    bucket   = "bucket-2aw"                  # имя бакета созданного
    key      = "folder1/terraform.tfstate"   # куда в бакете класть стейт файл
    profile  = "service-account-2"           # имя AWS профиля на компе где искать static ключи для 
                                             # доступа к бакету
    dynamodb_endpoint=                       # точка доступа к базе 
    dynamodb_table =                         # имя таблицы в базе где хранить будем lock запуска тераформ



также хотел этот кусок выделить

provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/service-account-2-auth-keys.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gop5honldmvbpmg516"
  zone                     = "ru-central1-a"

}

ключ service_account_key_file =  описывает где иcкать authorize key через который мы стучимся на iam сервис
который в свою очередь выдает вответ тераформу iam токен который он уже как паспорт предьявляет сервисам
яблока


в целом main.tf тераформ создаст +1 vpc  внем создаст подсеть. и в ней создаст одну виртулку

запускаем наконец тераформ

 $  terraform init
 $  terraform validate 
 $  terraform fmt
 $  terraform plan -out "plan.txt"
 $  terraform apply  "plan.txt"




далее.
как в main.tf  в свойствах виртуалки
прописывается ssh ключ
ответ

  metadata = {
    ssh-keys           = "ubuntu:${file("~/.ssh/foxy.pub")}"
    serial-port-enable = 0
  }

где ubuntu = это типа имя линук юзера которому будет этот ключ присунут. 
в данном случае это ubuntu. прикол в том что если мы вместо ubuntu напишем vasya то 
тераформ все равно добавить ключ к юзеру убунту. почему? потому что :

Regardless of the username specified, the key is assigned to the user given in the cloud-init configuration by default. In different images, these users differ. (об этом написано здесь https://cloud.yandex.com/en/docs/compute/concepts/vm-metadata)




3. dynamo + запрет на одновременный модификация двух юзеров + проверить + описать
3.a [ создать кастом диск с lvm      ]
    [ добавиьт второй диск           ]
    [ импорт инфрастрктуры в main.tf ] 
3.1 записать сюда мой main.tf 
3.5 создать через код новую папку+новые сервис акаунт+новый бакет
вобщем полостью цикл через код без веб морды!
4. Just accidently destroyed VM after changing boot_disk size. (used EC2 for some time and forgot about this feature in yandex-cloud terraform provider)
5. прикольно то что если виртуалки выключены то teraform aply этого не видит. как бы плучается
ему пофиг выключены они или включены. ..кхм...




















