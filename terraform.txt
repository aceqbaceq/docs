terraform

установка для убунту 16

$ sudo snap install terraform


установка (для убунту >16)

$ sudo apt-get update && sudo apt-get install -y gnupg software-properties-common curl

$ curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -

$ sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"

$ sudo apt-get update && sudo apt-get install terraform



установили

проверка что он встал

$ terraform -help

установить автодополнение по TAB

$ touch ~/.bashrc

$ terraform -install-autocomplete



пробный проект

$ mkdir learn-terraform-docker-container

$ cd learn-terraform-docker-container

$  touch main.tf

terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 2.13.0"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx" {
  name         = "nginx:latest"
  keep_locally = false
}

resource "docker_container" "nginx" {
  image = docker_image.nginx.latest
  name  = "tutorial"
  ports {
    internal = 80
    external = 8000
  }
}

$ terraform init

$ sudo terraform init

надо научится прописывать sg. и стопить машину.

=

как проверить коректность и запустить план 


$ terraform fmt
$ terraform validate
$ terraform plan  -out plan.txt

как запустить уже выполеннеие
$ terraform apply plan.txt

=

создаем файл в котором прописан реурс  типа вольюм

$ cat volumes.tf 
// volumes.tf
resource "aws_ebs_volume" "elk" {
  availability_zone = "eu-central-1a"
  size = 12
  type = "gp2"
  tags = {
          "Name" = "elk-root_vol" 
          "elk"  = "rootvol"
        }


}


вопрос как его тераформ будет искать а амазоне.
нууууу. ответ такой что id вольюма почемуто нельзя указать.
но можно импортровать этот вольюм

$ terraform import aws_ebs_volume.elk vol-номер_вольюма

он импортрутся куда то  в кишки тераформа в файл terraform.tfstate
ну и как то после этого терафоом понимает что вольюм указанный в volumes.tf 
и тот который он импортровал это одно и тоже.
а как быть если мы  с нуля слздаем вольюм на амазоне. я так понимаю что 
почемуто в конфиге непроисать id. а он будет автоматом прописан тераформом  а файле terraform.tfstate
если вольюм создается с нуля. не очень както прикольно.

если мы хотим посмотреть что будет поменяно на амазоне не для всех обьектов а для конкретного то 
надо юзать ключ -target

$ terraform plan -target=aws_ebs_volume.elk -out plan.txt

эта команда покажет план что траформ будет менять на амазоне только 
для вольюма.

==
надо научится расширять фс через тераформ

==
важная штука

если в модуле aws_instance
мы указываем
security_groups 
то теарформ создает типа ec2 classic instance. в чем его жопа это то что 
у него нельзя поеменять security group без его уничтожения.
а если указать vpc_security_group_ids
то уже все окей. натаком инстансе можно менять секуююрити группу без переуничтожения инстанса

-====

еще раз как накатывать плейбук

$ terraform fmt
$ terraform validate
$ terraform plan -out plan.txt
$ terraform apply plan.txt

===
начала заново учить тераформ.
на примере яндекс облака

вначале надо поставит на комп yandex cli
как это сделать и прочая полезная штука про яблоко смотри в файле yandex_cloud.txt

вот мы поставили якли. в якли есть профили. профилей может быть несколько.
профиль это  набор настроек с которым якли
конектится к яблоку. эти настройки вклчают в себя: креды,номер папки яблока, номер облака яблока.
в один момент времени может быть активен только один профиль якли. с его настройками
якли и стучится на яблоко.

так вот нам для тераформа надо чтобы текущий профиль якли был с кредами сервис акаунта( сервис акаунт
это такой один из типов юзеров с которым можно подключаться к яблоку) от имени которого мы 
и будем подключаться из тераформа к яблоку. 

возникает вопрос какая связь между якли и тераформом. связь такая что тераформу чтобы подключаься 
к яблоку надо получить от IAM (сервис аутентификации яблока) специальный токен = IAM токен.
этот токен это как пропуск. который тераформ будет предьявлять сервисам яблока для работс ними.
так вот этот токен это проще всего получить через якли постучавшись на яблоко от имени сервис акаунта.


когда поставили яндекс кли. и там прописали в профиле папку на яблоке, креды сервис акаунта
то теперь надо добавить этому сервис акаунту права нужные тераформа.
а именно (как я понял из этого говенного описания https://cloud.yandex.com/en/docs/tutorials/infrastructure-management/terraform-quickstart) надо выдать этому акаунту права "editor"
сделать это тоесть добавить прав сервис акаунта можно через веб морду. окей защли в веб морду
выдали права "editor"

возвращаемся к нашему якли.
проверяем в нашем якли профиле какой сервис акаунт указан, какая яблоко  папка указана,
какое яблоко облако указано
$ yc config list
service-account-key:
  id: aje9ru
  service_account_id: ajed54
  created_at: "2023-03-09T19:15:13.886916958Z"
  key_algorithm: RSA_2048
  public_key: |
    -----BEGIN PUBLIC KEY-----
    MIIBIjAQAB
    -----END PUBLIC KEY-----
  private_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQ+cVgQvUE5AowIHtW/D0=
    -----END PRIVATE KEY-----
cloud-id: b1g8sk79560kbvb6nr2j
folder-id: b1gobq5rv8qm8qi76hig

тоесть проверяем 
service_account_id
cloud-id
folder-id

если это то что мы ожидали видеть то двигаем дальше.
а именно получаем IAM токен через якли от IAM сервиса 

$ yc iam create-token
t1.9euelZqM...


тоесть якли стучится от имени сервис акаунта на IAM сервис и говорит я сервис акаунт такой то
дай мне IAM токен.
если все ок то на экране мы получим наш IAM токен. он действиует 12 часов.
его можно обновить если приспичило в течение этих 12 часов но янедкс пишет что это делать 
рекеомнудется не чаще чем 1 раз в час.


этот iam токен это наш паспорт пропуск теперь  к сервисам яблока. мы его подставим в тераформ 
и он сможет успешно подключаться к сервисам яблока.
вот именно ради возможности получит IAM токен мы и ставили на комп якли.


длаее созаем папку

$ mkdir ~/cloud-terrafom
$ cd ~/cloud-terrafom
$ touch main.tf

дальше дока яндекса рекоменует переимновать файл .terraformrc в котором находится
конфиг настроек тераформа для текущего юзера.
$ mv ~/.terraformrc ~/.terraformrc.old

этого файла может и небыть.

создаем заново этот файл
$ touch  ~/.terraformrc

в этот файл мы вставляем кусок

$ cat ~/.terraformrc
provider_installation {
  network_mirror {
    url = "https://terraform-mirror.yandexcloud.net/"
    include = ["registry.terraform.io/*/*"]
  }
  direct {
    exclude = ["registry.terraform.io/*/*"]
  }
}



далее в наш main.tf
мы вставляем кусок:

terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"
}

provider "yandex" {
  token     = "<OAuth>"
  cloud_id  = "<cloud ID>"
  folder_id = "<folder ID>"
  zone      = "<default availability zone>"
  
}


соответвенно сюда надо вставить наш текущий iam-токен
котоыйр мы ранее полуили через якли от яблока,
а cloud-id, folder-id, zone availability  можно посмотреть в свойствах якли 
$ yc config list

далее как я понял из этого мудацкого описания https://cloud.yandex.com/en/docs/tutorials/infrastructure-management/terraform-quickstart
переходим в папку с main.tf 
и запускаем команды

$  terraform providers lock -net-mirror=https://terraform-mirror.yandexcloud.net -platform=linux_amd64 yandex-cloud/yandex
$  terraform init


далее в main.tf 
добавляем кусок


resource "yandex_compute_instance" "vm-1" {
name = "terraform1"

resources {
    cores  = 2
    memory = 2
}

boot_disk {
    initialize_params {
    image_id = "fd87va5cc00gaq2f5qfb"
    }
}

network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
}

metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
}
}

resource "yandex_compute_instance" "vm-2" {
name = "terraform2"

resources {
    cores  = 4
    memory = 4
}

boot_disk {
    initialize_params {
    image_id = "fd87va5cc00gaq2f5qfb"
    }
}

network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = true
}

metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/id_rsa.pub")}"
}
}

resource "yandex_vpc_network" "network-1" {
name = "network1"
}

resource "yandex_vpc_subnet" "subnet-1" {
name           = "subnet1"
zone           = "ru-central1-a"
network_id     = yandex_vpc_network.network-1.id
v4_cidr_blocks = ["192.168.10.0/24"]
}

output "internal_ip_address_vm_1" {
value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}

output "internal_ip_address_vm_2" {
value = yandex_compute_instance.vm-2.network_interface.0.ip_address
}


output "external_ip_address_vm_1" {
value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}

output "external_ip_address_vm_2" {
value = yandex_compute_instance.vm-2.network_interface.0.nat_ip_address
}


далее проверяем валидность этого конфига
$ terraform validate

далее просим тераформ переписать наш конфиг так чтобы он стал "красивым"
$ terraform fmt


$ terraform plan  -out plan.txt

далее формируем файл изменений которые тераформ собирается делать. так называемый
план. при этом на яблооке ничего небудет изменено мы просто увидим подробный план
того что тераформ собирается делать на экране а в файле будет запсана некая служебная хрень
$ terraform plan  -out plan.txt

ПОЛЕЗНАЯ ВЕЩЬ - в веб морде яблока нажимаем на имя клауда и справа будет закладка Quotas
в ней указаны лимиты на разные ресурсы в яблоке. там же можно увеличить эти лимиты

ПОЛЕЗНАЯ ВЕЩЬ - как посмотреть имаджи операционок доступные на яблоке
$ yc compute image list --folder-id standard-images | grep -E "ID|ubuntu-22"

далее применяем наш план уже в жизнь
$ terraform apply  "plan.txt"


готово. виртуалки и другие ресурсы из плана созданы.

как останвоить виртулки через тераформ пока непонятно.
приходится через якли это делать

$ yc compute instance list
$ yc compute instance stop  --name  terraform1
$ yc compute instance stop  --name  terraform2


далее. хотел сказать про значение файлов

.terraform.lock.hcl = в этом файле как понял прописвыаются  список провайдеров и их версии
здесь нет стейтов инфраструктуры


terraform.tfstate = в этом файле записано состояние инфраструктуры


далее как оказалось нам нафик ненадо завязываться в тераформе
на аутентификацию через iam токен. ведь его же над обновлять каждые 12 часов.
это не наш вариант. можно вместо этого указать в тераформе путь к 
RSA ключам от service акаунт а тераформ уже будет на основе этого
сам автоматом запрашиваь у iam сервиса iam токен. делается это вот так:
в main.tf
надо прописать


provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/key.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gobq5rv8qm8qi76hig"
  zone                     = "ru-central1-a"
}


где key.json это файл от яндекс кли в котором прописаны RSA ключи 
от service account в формате JSON. как получить этот файл смотри в файле yandex_cloud.txt


ДАЛЕЕ.
вопрос вот у нас есть папка тераформа, какие файлы из нее нужно взять 
чтобы на основе их создать такую же папку. тоесть как пеереносить папку тераформа.
значит ответ такой, заходим в папку с тераформом и забираем оттуда только ТРИ файла.
этого достаточно:

main.tf  (здесь наш конфиг)
.terraform.lock.hcl  (здесь описаны провайдеры)
terraform.tfstate   (здесь стейт инфраструктуры на момент последнего apply)

копируем эти три файла в новую папку. далее
в этой папке запускаем команды

$  terraform  init
$  terraform validate 
$  terraform fmt
$  terraform plan -out "plan.txt"
$  terraform apply "plan.txt"

и вуаля мы восстановили папку тераформа в новом месте.


ДАЛЕЕ.
новая задача перенести файл "terraform.tfstate" из хранения в локальной папке на компе в яблоко .
зачем нам это надо.  это надо для того чтобы пользоваться тераформом можно было нескольким юзерам.
поскольку файл стейта очень важен то чтобы исключить человеческую ошибку будет класно если 
на лкальном компе ни у кого этого файла небудет а юзер будет обращаться за этим файлам куда то в сеть.
переносим файл стейта  в яблоко в его s3 хранилище в бакет. хранилище называется у яблока как "object storage"
для этого надо зайти на яблоко сервис object storage и создать там бакет. я покажу как это делать
позже. а пока  считай что мы создали бакет со всеми нужными штуками
далее в main.tf в раздел terraform в его конец вставляем кусок


terraform {
...

  backend "s3" {
    endpoint   = "storage.yandexcloud.net"
    bucket     = "имя_бакета"
    region     = "ru-central1-a"
    key        = "folder1/terraform.tfstate"
    access_key = "YCAJ..."
    secret_key = "YCM0..."

    skip_region_validation      = true
    skip_credentials_validation = true
  }

}


обьясняю поля
key        = "folder1/terraform.tfstate"
дело вот в чем. бакет это типа как папка. в этой папке будет создана подпапка с именем "folder1"
и в эту папку уже будет скопирован файл terraform.tfstate
возникает вопрс так что же в этом ключе прописывать. какой путь. 
самое смешеное что этот параметр может быть абсолютно любым. вот какой путь мы пропишем по такому 
пути тераформ и засунет наш стетй файл с локальной системы в этот бакет. вопрос а как засунуть то на практике ? ответ будет дальше.

а пока продолжаю про поля
access_key = "YCAJ..."
secret_key = "YCM0..."

это параметры так называемого static access key. откуда его взять? когда мы создали бакет (покажу как это делать ниже) то мы в ACL бакета указыаем кому в него можно лазить. в моем случае это serivce account
так вот идем в веб морде в яблоко folder. в ее свойствах заходим в наш сервис акаунт. и справа вверху 
есть кнопка "create new key" , там можно выбрать какой тип нового ключа мы хотим создать. 
есть "create authorized key" это наш старый знакомый RSA пара ключей который мы прописыаем в якли
либо в тераформ и они используя этот authorized key получают от iam сервиса iam токен. вобщем  это не то что 
нам надо на данынй момент потому что (хз знает почему) сервис object storage в котором сидит бакет 
он не аутентифицирует юезра как я понял через iam токен. он аутентифицирует юзера через "static aceesss key"
поэтому мы выбираем в том меню "create static access key" и он нам выдаст нам пару ключей. 
это и есть access_key и secret_key ! вот их то и надо указать в конфиге main.tf
так с этим разобрались. итак  с полями разобрадись в main.tf
итак еще раз поля access_key(это как бы id ключа) и secret_key(это сам ключ) = так вот эти поля это 
ключ доступа (метод аутнетификации) со стороны тераформа к яндекс бакету. тоесть эта хрень нужна 
в этой сеции чтобы тераформ смог получить доступ до бакета. 
и я тутже скажу что чтобы не харнить эти ключи в main.tf их можно сохраниьт в отдельном месте 
а именно например в профиле aws cli (смотри об этом где то дальше) и тогда в main.tf можно
указать вместо ключей название aws cli профилс тоесть

один вариант (ключи к бакету храним в самом main.tf )

  backend "s3" {
...
    access_key = "YCAJ..."
    secret_key = "YCM0..."
...
  }


и второй вариант ( ключи к бакету храним в отдельном месте - профиле aws cli)

 backend "s3" {
  ...
    profile  = "service-account-2"
  ...
  
  }



далее надо запустить команду

$ terraform init
она скопирует файл terraform.state в бакет на яблоко. 
бояться этой команды ненало. она не сотреть все инициализацией. она именно толтко
перенесет terraform.state в бакет на яблоко
именно этой командой мы и переносим стейт файл в облако.
также важно заметить что исходный стейт файл останется по прежнему
локальной папаке и надо тут же его стереть в папке руками !


далее  провеяряем что наш тераформ успешно делаем свои дела
подключаясь к удаленному стейт файлу

$ terraform validate
$ terraform fmt
$ terrafrom plan -out "plan.txt"
$ terraform aplly "plan.txt"

классно!

а сейчас я ввозращаюсь к вопросу как создать бакет:
идем в веб морду в object storage и создаем новый бакет. 
в его свойствах прописываем в ACL что нащ service account имеет права на чтение\запись 
в этот бакет.


все - теперь стейт файл хранится в яблоке в бакете. это просто отлично!

если мы потом хотим перенести наш стейт файл в новый бакет
или в этом же бакете но по новому пути то для этого надо
1) обновить сеекцию 
backend "s3" {
   ...
  }

2) скачать рукамт стейт файл из старого бакета и разместиь его 
в локальной папке под именем terraform.state

3) запустить 
$ terraform init -migrate-state

и все. наш стейт файл скопирван в новый бакет. также надо теперь 
удалить руками стейт файл из локальной папки.

также важно отметмть что если мы перенесли стейт файл в бакет то 
для переноса тераформ папки нам теперь надо не три файла а всего два:

main.tf  (здесь наш конфиг)
.terraform.lock.hcl  (здесь описаны провайдеры)

но это еще не конец мудежа. наща конечная задача это чтбы если один юзер запустил
команду на исполннеие тераформа то второй юзер должен немочь ее запукать чтобы 
неиспортить инфраструктуру. для этого длполенительно надо заюзать dynamo
для этого юзаю эту статью https://habr.com/ru/post/711982/









описать:
3.s надо чтобы все аспекты с нуля до конца были сделаны через командную строку.
например создание бакета и все такое. сейчас многое все делается руками а надо 
чтобы все было только через скрипт!!!!

тераформ работает от имени сервис акаунта в яблоке а сервис акаунт 
привязывается к конкретной папке яблока поэтому создавать папку яблока и 
сервис акаунт для этой папки через тераформ нельзя а можно только сторонниим методами
наприер через якли

переключаемся на якли профиль который раотает от имени веб юзера\яндекс юзера
$ yc config profile list
default
service_account_profile-krivosheev-test ACTIVE

актвируем профиль в котором прописан веб юзер
$ yc config profile activate default
Profile 'default' activated

проверяем что это рельно профиль от веб юзера
$ yc config list
token: y0_AgA...
cloud-id: b1g8s
folder-id: b1gobq
compute-default-zone: ru-central1-a


создаем новую яблокоо папку
$ yc resource-manager folder create test-folder-2
id: b1gop5honld
cloud_id: b1g8s2j
created_at: "2023-03-13T09:48:45.867259773Z"
name: test-folder-2
status: ACTIVE


создаем сервис акаунт для этой папки

$ yc iam service-account create --name service-account-2 \
  --description "сервис акаунт для папки test-folder-2 для тераформа" \
  --folder-name "test-folder-2"

id: ajec4h
folder_id: b1go
created_at: "2023-03-13T09:51:46.935971935Z"
name: service-account2
description: сервис акаунт для папки test-folder-2 для тераформа


создаем пару authorize keys для этого сервис акаунта.(подчеркиваюь что акаунт уже создан а мы создаем
щас для него ключи),  на основе этого атораайз key 
наш якли будет аутентифицироваться на иам сервисе и получать от него иам токен  который 
в свою очередь я кли будет предьявлять сервисам яблока для аутентиицикации.
таким макаром аторайз ки используется для аутентификации перед иам сервисом  но для доступа 
к другим сервисам яблока например vpc надо предьявлять иам токен который выдает иам сервис


$ yc iam key create --service-account-name service-account-2 --folder-name "test-folder-2" -o $HOME/.config/yandex-cloud/service-account-2-auth-keys.json

id: aje6rqernq
service_account_id: aje5n
created_at: "2023-03-13T10:05:51.187614866Z"
key_algorithm: RSA_2048


обращаю вниманеи что надо указывать папку в которой сидит сервис акаунт иначе  система небудет 
его находить.
файл с ключами мы сразу сохраняем в папку настроек якли.

даем этому сервис акаунту роль (права) на создание\редактирование виртуалок в этой папке
$ yc resource-manager folder add-access-binding test-folder-2 \
  --role compute.admin \
  --service-account-name service-account-2 \
  --folder-name test-folder-2


подставляем выше
$ yc resource-manager folder add-access-binding test-folder-2 \
     --role compute.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2

также добавляем роль чтобы акаунт мог сети в vpc создавать
$ yc resource-manager folder add-access-binding test-folder-2 \
     --role vpc.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2



также добавляем ему права на редактирование ydb баз "ydb.admin"
$ yc resource-manager folder add-access-binding test-folder-2  \
     --role ydb.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2


также добавляем роль чтобы этот акаунт мог создавать бакеты

$ yc resource-manager folder add-access-binding test-folder-2 \
     --role storage.admin \
     --service-account-name service-account-2 \
     --folder-name test-folder-2



генерируем для этого сервис акаунта его static access key ( аля aws static key)
$ yc iam access-key create \
   --service-account-name service-account-2 \
   --folder-name "test-folder-2" \
   --description "статик ключ для доступа на aws подобные ресурсы"

access_key:
  id: aje7nled85chli
  service_account_id: aje5na
  created_at: "2023-03-13T19:57:42.009502688Z"
  description: статик ключ для доступа на aws подобные ресурсы
  key_id: YCAJE...
secret: YCNuNIf...


устанавливаем aws cli чтобы туда записать полученный   access static key
потому что якли этот ключ использовать неможет а aws cli может (дебилизм конечно но я нашел что есть 
отдельный cli родной от ydb возможно он умеет делать то что надо. вот докумен8тация от родного cli https://ydb.tech/en/docs/getting_started/cli  но в данной инструкции мы будем использовать aws cli)
нам этот aws cli понадобится чуть ниже когда мы будем работать с YDB базой данных.

# apt-get install awscli
прописываем наш статический ключ который мы получили выше в aws cli конфиг используя aws cli профили  
для того чтобы мы имели возможность если нужно  записать
несколько стат ключей
$ aws configure --profile service-account-2
AWS Access Key ID [None]: YCAJE...
AWS Secret Access Key [None]: YCNuNIf...
Default region name [None]: ru-central1
default output: text


настройки будут сохранены в папку $HOME/.aws/

возвращаеися обратно к якли.
создаем профиль в якли с этим сервим акаунтом и его папкой в яблоке
$ yc config profile create service-account-2

далее подключаем  к текущему активному профилю наш ранее созданный файл с ключами RSA:
напомню что файл service-account-2-auth-keys.json должен лежать в папке $HOME/.config/yandex-cloud
$ yc config set service-account-key   service-account-2-auth-keys.json

далее полезно прписать в профиле cloud-id
сам id придется посмотреть через веб морду потому что как я понял с текушими ролями
наш сервис акаунт неможет его определить
$ yc config set  cloud-id b1g8sk79560kbvb6nr2j 

дале полезно добавит в профиль folder-id.
его мы уже можетм определить через cli
$ yc resource-manager folder list
+----------------------+---------------+--------+--------+
|          ID          |     NAME      | LABELS | STATUS |
+----------------------+---------------+--------+--------+
| b1gop5honldmvbpmg516 | test-folder-2 |        | ACTIVE |
+----------------------+---------------+--------+--------+


$ yc config set  folder-id  b1gop5honldmvbpmg516

в целом создание папки и сервис акаунта в яблоке закончена. также как бонус
мы создали профиль в якли с этим сервиc акаунтом.

теперь надо создать яблоко бакет (размером 50МБ) где мы будем хранить тераформ стейт для данной яблоко
папки

$ yc storage bucket create \
     --name bucket-2aw  \
     --default-storage-class standard \
     --max-size 52428800 \
     --grants grant-type=grant-type-account,grantee-id=aje5na0qfqnci747bgs5,permission=permission-write \
     --grants grant-type=grant-type-account,grantee-id=aje5na0qfqnci747bgs5,permission=permission-read

name: bucket-2aw
folder_id: b1gop5honldmvbpmg516
anonymous_access_flags:
  read: false
  list: false
default_storage_class: STANDARD
versioning: VERSIONING_DISABLED
max_size: "1"
acl: {}
created_at: "2023-03-13T20:53:02.424678Z"

причем --max-size 52428800  = это размер в БАЙТАХ указывается
grantee-id=aje5na0qfqnci747bgs5 = это id сервис акаунта которму мы даем права досутпа к бакету


теперь надо создать базу YDB  для того чтобы тераформ там ставил метку 
если тераформ начал чтот делать с инфраструктурой чтобы только один юзер в один момент времени
мог запускать тераформ. 

$ yc ydb database create terraform-state-lock-2 --serverless

проставляем пермишнсы для нашего сервис акаунта на эту базу
замечу что если сервис акаунт не имеет роли "ydb.admin" то команда успешно не пройдет.
я подчеркиваю я имею ввиде не про роль  "--role  ydb.editor" указанную в этой команде
а именно роль которую мы назначали акаунту выше.
$ yc ydb database add-access-binding  terraform-state-lock-2 \
     --role  ydb.editor \
     --service-account-id "aje5na0qfqnci747bgs5"


но базу создать мало надо создать таблицу в ней. и тут прикол в том что мы неможем этого сделтаь 
через якли. мы можем это сделать только через aws cli который мы поставили ранее. и прописали туда 
статик ключ от сервис акаунта.
для начала нам нужно опрееделить endpoint у нашей базы
$ yc ydb database get  --name   terraform-state-lock-2 
name: terraform-state-lock-2
document_api_endpoint: https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7

итак создаем таблицу в базе
$ aws dynamodb create-table \
   --profile service-account-2 \
   --table-name terraform-state-lock-2 \
   --attribute-definitions   AttributeName=LockID,AttributeType=S \
   --key-schema   AttributeName=LockID,KeyType=HASH \
   --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5  \
   --endpoint https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7

замечу что в этой команде я указал опцию --profile service-account-2 
это указан профиль самой утилиты aws cli чтобы aws cli стучался на яблоко под нужным ключом - под ключом именно сервис акаунта "service-account-2"



итак что сделано: 
создан сервис акаунт
ему выданы роли
создан бакет
создана база

это все нам дает теперь возможность прописать этот сервис акаунт в тераформ и он будет от его имени
стучаться на яблоко и создавать наконец виртуалки, также стейт тераформа будет хранися в бакете
и тераформ будет лочить свое состояние через YDB базу так что другой юзер не сможет запустить тераформ
если другой юзер уже запустил тераформ.

создаем папку
$ mkdir ~/terraform/01

создаем в папке два файла
$ cat .terraform.lock.hcl 
provider "registry.terraform.io/yandex-cloud/yandex" {
  version = "0.86.0"
  hashes = [
    "h1:hW7eHexpzEQXpun1kChUdfB+fFNepFxybhoxxbqOIjc=",
  ]
}




$ cat main.tf 


terraform {
  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
  }
  required_version = ">= 0.13"







  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }

}





provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/service-account-2-auth-keys.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gop5honldmvbpmg516"
  zone                     = "ru-central1-a"

}







resource "yandex_compute_instance" "vm-1" {
  name                      = "terraform1"
  platform_id               = "standard-v3"
  allow_stopping_for_update = true

  scheduling_policy {
    preemptible = true
  }


  resources {
    cores         = 2
    memory        = 1
    core_fraction = 20
  }


  boot_disk {
    initialize_params {
      image_id = "fd8haecqq3rn9ch89eua"
    }
  }

  network_interface {
    subnet_id = yandex_vpc_subnet.subnet-1.id
    nat       = false
  }

  metadata = {
    ssh-keys = "ubuntu:${file("~/.ssh/foxy.pub")}"
  }
}




resource "yandex_vpc_network" "network-2" {
  name = "network1"
}

resource "yandex_vpc_subnet" "subnet-1" {
  name           = "subnet1"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.network-2.id
  v4_cidr_blocks = ["192.168.10.0/24"]
}


output "internal_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.ip_address
}


output "external_ip_address_vm_1" {
  value = yandex_compute_instance.vm-1.network_interface.0.nat_ip_address
}



в этмом конфиг я хочу выделить блок

  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }


в нем мы прописваем

    bucket   = "bucket-2aw"                  # имя бакета созданного
    key      = "folder1/terraform.tfstate"   # куда в бакете класть стейт файл
    profile  = "service-account-2"           # имя AWS профиля на компе где искать static ключи для 
                                             # доступа к бакету
    dynamodb_endpoint=                       # точка доступа к базе 
    dynamodb_table =                         # имя таблицы в базе где хранить будем lock запуска тераформ



также хотел этот кусок выделить

provider "yandex" {
  service_account_key_file = file("~/.config/yandex-cloud/service-account-2-auth-keys.json")
  cloud_id                 = "b1g8sk79560kbvb6nr2j"
  folder_id                = "b1gop5honldmvbpmg516"
  zone                     = "ru-central1-a"

}

ключ service_account_key_file =  описывает где иcкать authorize key через который мы стучимся на iam сервис
который в свою очередь выдает вответ тераформу iam токен который он уже как паспорт предьявляет сервисам
яблока


в целом main.tf тераформ создаст +1 vpc  внем создаст подсеть. и в ней создаст одну виртулку

запускаем наконец тераформ

 $  terraform init
 $  terraform validate 
 $  terraform fmt
 $  terraform plan -out "plan.txt"
 $  terraform apply  "plan.txt"




далее.
как в main.tf  в свойствах виртуалки
прописывается ssh ключ
ответ

  metadata = {
    ssh-keys           = "ubuntu:${file("~/.ssh/foxy.pub")}"
    serial-port-enable = 0
  }

где ubuntu = это типа имя линук юзера которому будет этот ключ присунут. 
в данном случае это ubuntu. прикол в том что если мы вместо ubuntu напишем vasya то 
тераформ все равно добавить ключ к юзеру убунту. почему? потому что :

Regardless of the username specified, the key is assigned to the user given in the cloud-init configuration by default. In different images, these users differ. (об этом написано здесь https://cloud.yandex.com/en/docs/compute/concepts/vm-metadata)

далее.
как через один хост ssh зайти на другой хост по ssh
$ ssh -J ubuntu@51.250.14.188:22 ubuntu@192.168.10.16
где 51.250.14.188 = первый хост
и 192.168.10.16 = второй хост (конечный)

|далее. ИЗ ПРИКОЛОВ. вот у нас есть модуль создающий диски. и есть модуль создающий виртуалку.
и в тераформе прописана зависимость модуля виртулки от модуля дисков.  так вот если у нас в тераформе
активирован только модуль с дисками а виртуалка еще не создана (модуль виртуалки закомментирован)
то тогда если изменить имя диска или дескрипшн диска то тераформ покажет что он изменит свойства
диска без уничтожения диска.  а если уже есть виртуалка то тераформ покажет то он вначале удалит
виртуалку (неопонятно зачем) потом изменить параметры диска неуничтожая его и потом заново
создаст виртуалку. это поленый бред. потому что через веб морду можно изменить и имя диска
и его дескрипшн дажеесли диски уже прикреплены к виртуалке. и даже если этот диск это загрузоочный
диск. тоест это заеб именно тераформа. что же делать. --> надо удалить виртуалку из СТЕЙТА 
тераформа. потом спокойно с помощью тераофрма отредактировать диски , виртуалка при этом 
останется нетронутой. и потом просто импортировать ресурс виртуалки обратно.  тоесть

- $ terraform state rm module.vm_4.yandex_compute_instance.vm  # удалеям ресурс виртуалки из STATE
- меняем переемнные отвесвтенные за имя и декрипшн диска, и закоментируем модуль виртуалки в конфиге
- $ terraform validate && terraform fmt && terraform  plan -out "plan.txt"  # спокойно меняем параметры диска
- раскоментрруем в конфиге ресурс виртуалки
- terraform import  module.vm_4.yandex_compute_instance.vm  fhmasimn3j4hhmto8149   # импортруем ресурс виртуалки обратно в STATE



|далее.
как создать пустой доп диск и прикрепить его к ВМ.
вначале создаем пустой диск

resource "yandex_compute_disk" "vm-1---disk2" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---disk2"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}


а потом внутри настроек виртуалки добалвяем этот диск
resource "yandex_compute_instance" "vm-1" {
...
  secondary_disk {
    disk_id     = yandex_compute_disk.vm-1---disk2.id
    auto_delete = true
    mode        = "READ_WRITE"
  }
...
}


возникает вопрос  а как добавит третий черветрй диск на виртуалку?
ответ создаем еще один ресурс с диском
а потом в свойствах виртуалки просто добавляем еще одну секцию  secondary_disk
тоесть



resource "yandex_compute_disk" "vm-1---disk2" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---disk2"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}



resource "yandex_compute_disk" "vm-1---disk3" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---disk3"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}




resource "yandex_compute_instance" "vm-1" {
...
  secondary_disk {
    disk_id     = yandex_compute_disk.vm-1---disk2.id
    auto_delete = true
    mode        = "READ_WRITE"
  }

 secondary_disk {
    disk_id     = yandex_compute_disk.vm-1---disk3.id
    auto_delete = true
    mode        = "READ_WRITE"
  }


...
}


ура !
 



| далее
в яблоке есть такая хрень. 
диски можно менять размер через веб моорду только если ты оставнил виртулаку.
а через тераформ доблавялется доп хуйня состоящая в том что если менять доп диск
то он не уничтожается а именно расширяется. а если менять размер системного диска
то провайдер терформа от яндекса он уничтожает диск уничтожает инстанс и переоздает 
его заново. это просто отлично

|далее.
если у нас виртувлка выелючена то изменения накатаныне через тераформ оставят 
виртуалку выключенной. также нет таких опций у тераформа чтобы выключить или
включить виртуакдку. это тоже очень прекрасно


|далее. как делать импорт ресурсов в тераформ.
пример как импортировать vpc
скажем у нас есть vpc с названием "default" и id="enptqgmqsa4aa8sofj81"
в main.tf создаем кусок

resource "yandex_vpc_network" "default" {
  name = "default"
}

и в командной строке запускаем
$ terraform import yandex_vpc_network.default enptqgmqsa4aa8sofj81

готово.

также - импорт vpc он импортирует только свойства vpc и он не импортирует субнеты.точнее
он импортирует имена субнетов потому что это свойство vpc но свойства субнетов
он не импортирует.  импортировать субнеты надо отдельно руками. 

вот как импортировать субнет:
вот имеем описанный ресурс в main.tf

resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
}


импортируем этот субнет
$ terraform import "yandex_vpc_subnet"."private-1-a-subnet"  e9bc4ns5s3dhbbt8ksnb

смотрим результат (команда STATE SHOW)
$ terraform state show  "yandex_vpc_subnet"."private-1-a-subnet"
# yandex_vpc_subnet.private-1-a-subnet:
resource "yandex_vpc_subnet" "private-1-a-subnet" {
    created_at     = "2022-12-18T22:03:25Z"
    folder_id      = "b1gobq5rv8qm8qi"
    id             = "e9bc4ns5s3dhbbt"
    labels         = {}
    name           = "private-1-a-subnet"
    network_id     = "enptqgmqsa4aa8s"
    route_table_id = "enp1dtv89pussel"
    v4_cidr_blocks = [
        "172.16.1.0/24",
    ]
    v6_cidr_blocks = []
    zone           = "ru-central1-a"

    timeouts {}
}


кстати про разницу в командах
$ terraform state show
$ terraform show

первая команда покзыает инфо о конкетном ресурсе.
а вторая команда показывает  инфо обо всех ресурсах 

но в обоих случаях инфо черпается не из облака а из стейт файла.
поэтому перед этим полезно ввести
$ terraform resfresh
чтобы обновить инфо в стейт файле на основе реального состояния из облака



|далее. команды STATE и REFRESH
команда
$ terraform refresh
делает то что тераформ берет обрашается к облаку и скачиает состояние из облака для всех ресурсов
прописанных в main.tf и если на облоаке ресурс изменился то и в стейт файле тоже состояние будет 
обновлено. это очень хорошо. скажем через веб морду изменил размер диска и через рефреш это изменение 
подтягивается  в стейт файл. очень хорошо.

так теперь про команду state. она позволяет посмртеть свйоства записанные в стейт файл про некотрй ресурс
указанные в main.tf

например имеем в main.tf запись

resource "yandex_vpc_network" "network-1" {
  name = "network1"
}


тогда свойства этого ресурса в стейт файле можно посмотреть как

$ terraform state show yandex_vpc_network.network-1
# yandex_vpc_network.network-1:
resource "yandex_vpc_network" "network-1" {
    created_at = "2023-03-10T09:32:49Z"
    folder_id  = "b1gobq5rv8qm8qi76hig"
    id         = "enp4eaogkheinfabqcdu"
    labels     = {}
    name       = "network1"
    subnet_ids = [
        "e9bbko7j4em15tla0fbd",
    ]
}



чем полезна эта команда у нас в main.tf в свойствах ресурса может быть указана только
часть его пропертис. а вот стейт показывает как я понимаю все поля настроек которые у этого
ресурса по факту есть вот как это видно выше. в main.tf задано только name
а по факту в стейт гораздо больше полей определено.

перед тем как запускать STATE полезна запустить REFRESH чтобы мы увидели не просто записи в стейт
файле а именно то как оно есть на яблоке в реальности прям щас. тоесть

$ terraform refresh
$ terraform state show yandex_vpc_network.network-1



и тут приходит гениальная идея - а можно ли через команду terraform refresh воосстановить
стейт файл если мы его потеряли?
рассмаотрим этот вопрос:
значит я взял из рабочей терафор папки и скопировал только два файла.
main.tf
.terraform.lock.hcl

далее  я взял и вырезал из main.tf полностью весь блок про backend
 а именно вырезал блок

  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "ter1"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "test"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etnfjlci9"
    dynamodb_table    = "Terraform-backend-lock"

  }

теперь полуается у меня файл со стейтом будет хранится локально.
далее я инициализировал папаку

$ terraform init

далее я пытаюсь засосать состояние всех ресурсов из яблока в стейт файл. ибо стейт файла у меня
нет

$ terrafom refresh

однако облом, получаю воттакое

╷
│ Warning: Empty or non-existent state
│ 
│ There are currently no remote objects tracked in the state, so there is nothing to refresh.
╵

значит как я понял то  тераформ может делать refresh только если он знает ID ресурсов 
прописанных в main.tf а если стейт файла нет то и iD ресурсов неизвестен.
поэтому если унас нет стейт файла (мы его потеряли) то восстановить стейт файл через 
команду terraform refresh не получится! 
единсвтенный вариант восстановить стейт файл это мы имеем main.tf  и мы сидим и для каждого
ресурса делаем импорт 
$ terraform import имя_ресурса ID_ресурса.
имя_ресурса мы смотрим в main.tf а id_ресурса мы смотрим через веб морду в яблоке.

так прикол еще и в том что импорт тоже невсегда простое занятие как я почитал. 
поэтому лучше всего никогда нетерять стейт файл иначе будут проблемы большие.


|далее. очень ВАЖНЫЙ момент про КАВЫЧКИ.
по поводу использования кавычек в main.tf
пример

resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
  route_table_id = yandex_vpc_route_table.route-table-2.id
}

если у нас справа константа то заключаем ее в кавычки.
name           = "private-1-a-subnet"

а если у нас справа типа переменная то кавычки категорически нельзя
иначе переменная будет восприниматься тераформом как константа в буквальном смысле
поэтому правильно только вот так
  route_table_id = yandex_vpc_route_table.route-table-2.id

кстати вот еще вариант как это можно коректно прописать через кавычки
  route_table_id = "${yandex_vpc_route_table.route-table-2.id}"




| далее. надо научиться бэкапить стейт файл в бакете на облаке
потеря стейт файла это звездец.
для этого надо включить версионность


$ yc storage bucket  list
+------+----------------------+------------+-----------------------+---------------------+
| NAME |      FOLDER ID       |  MAX SIZE  | DEFAULT STORAGE CLASS |     CREATED AT      |
+------+----------------------+------------+-----------------------+---------------------+
| ter1 | b1gobq5rv8qm8qi76hig | 1073741824 | STANDARD              | 2023-03-12 14:09:35 |
+------+----------------------+------------+-----------------------+---------------------+

[vasya@lenovo Desktop]$ yc storage bucket  get --name ter1
name: ter1
folder_id: b1gobq5rv8qm8qi76hig
anonymous_access_flags:
  read: false
  list: false
  config_read: false
default_storage_class: STANDARD
versioning: VERSIONING_DISABLED
max_size: "1073741824"
created_at: "2023-03-12T14:09:35.619470Z"

$ yc storage bucket  update --name ter1 --versioning versioning-enabled
name: ter1
folder_id: b1gobq5rv8qm8qi76hig
default_storage_class: STANDARD
versioning: VERSIONING_ENABLED
max_size: "1073741824"
acl: {}
created_at: "2023-03-12T14:09:35.619470Z"

что дает активированная версионность - при каждой записи в бакет сохраняется предыдущая копия.
потом через веб морду можно просмотреть и восстановить одну и предыдущих версий. и последняя версия
и все преддыдущие хранятся в этом же бакете. 






1.1 про vpc
про его стркутуру. что в него входит из чего состоит:
* network
* subnet
* route-table
* gateway




* network

на самом верхнам уровне vpc находится network. их может быть несколько.

когда мы создаем network то из свойств  которые можно ей указать это 
только имя и все. 
$ yc vpc network create --help


однако по факту к нетворку крепятся другие элементы. тоесть нетворк внутри себя содержит 
такие элементы как:
subnet
route-table
security-group

просто их нельзя создать на стадии создания нетворка. они создаются отдельными командами
уже после того как нетворк создан.
еще этот нетворк его можно переместить в другую папку(так как все службы яблока они привязаны к папке)

как создать network через cli
$ yc vpc network create --name "network-1"

как создать network через terraform
resource "yandex_vpc_network" "network-1" {
  folder_id = "<folder ID>"
  name = "network1"
}


как импортировать network в тераформ
$ terraform import "yandex_vpc_network"."network-1"   ID_нетворка




* subnet 

субнет по сути это и есть та структура в которой прописаны IP адреса локальной сети 
которые и выдаются потом виртуалкам. тоеть в свойтсвах виртуалки прописывается субнет 


как создать через тераформ
resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
  route_table_id = yandex_vpc_route_table.route-table-2.id
}

значит видно что в свойствах субнета уазано: имя, зона, нетворк к которому субнет прикреплен,
блок ip адресов который закреплен за этим субнетом, и роут таблица.

если мы укажем id этого субнета в свойствах виртуалки то ей будет назначен IP адрес из блока 
этого субнета. вот пример как это пропсиывается в виртуалке



resource "yandex_compute_instance" "vm-3" {
...
network_interface {
    subnet_id = yandex_vpc_subnet.private-1-a-subnet.id
    nat       = false
  }
...
}

таким макаром мы создаем субнеты для описания в них блока Ip адресов чтобы потом через 
эти субнеты раздавать виртуалкам IP адерса.
и роут тейбл в свойтсвах субнета дает то что пакеты  проходя через рут тейбл будут раскидываться по 
разным выходным точкам. например часть пакетов будет уходить в интернет.




* роут тейбл

роут тейбл крепится к нетворку. тоесть при создании роут тейбла
ему нужно указать  к какому нетворку он относится. тоесть нетворк это как
коробка и внутри нее помещается игрушка под названием роут тейбл.

по сути у него такие свойства: имя, нетворк  к которому он прикреплен, набор статических маршрутов.
в статическом маршруте указывается имя гейтвея на который пуляют пакеты которые 
подходят под маршрут


создать через CLI
$  yc vpc route-table create    --help

создать через тераформ
resource "yandex_vpc_route_table" "route-table-2" {
  name       = "route-table-2"
  network_id = "yandex_vpc_network.default.id"

  static_route {
    destination_prefix = "0.0.0.0/0"
    gateway_id         = yandex_vpc_gateway.nat-2.id
  }
}


импортировать через тераформ
$ terraform import "yandex_vpc_route_table"."route-table-2"  enp1dtv89pusseldi3sb





* гетвеи

создать через CLI
$ yc vpc gateway create --help
$ yc vpc gateway create --name gate2
по сути это и все свойтства что есть у гейтвея - только имя
больше у этого ресурса и свойств то нет


создать в тераформ
описание ресурса в  тераформе  https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/vpc_gateway ) 
там написано что пофакту ресурс имеет только свойство "name" и свойство "shared_egress_gateway" который на данный момент неимеет никаких параметров. и еще свойство "descripton"


resource "yandex_vpc_gateway" "nat-2" {
  name = "nat-2"
  description = "шлюз в инет" -> null  
  shared_egress_gateway {}
}



импорт в тераформ
$  terraform import yandex_vpc_gateway.nat-2 enpkq1dil4o7b50hjhal

к ресурсу гейтвей  прикрепляется ресурc route-table. тоесть пакеты влетающие в роут табл 
пуляются на гейтвей.


на счет shared_egress_gateway {}  на что он влияет.
если мы его удалим из описания ресурса то при запуске тераформ он его удалить и на облаке 
но прикол в том что при следущем запуске тераформа он опять нарисует что он будет его удалять тоестьь 
как бутто как только мы удаляем это свойство на облаке оно его автоматом обратно доблавляет поэтому
это свойство получается должно быть must have.


хочу подытожить. гейтвей привязывается к роут-тейбл. роут-тейбл привязывается к субенету. субнет
привязывается  к нетворк.




| далее. outputs
если у нас в main.tf есть какие то outputs то как бы нам посмотеть
сейчас чему они равны. ответ

$ terraform output


| далее. про subnet 
значит вопрос вот у нас есть subnet.
у этого сабнета нет прикрепленного route-table который бы вел на gateway.
далее мы создаем в этом сабнете виртуалку без внешнего ip адреса.
вопрос можем ли мы пдключиться по ssh к этой виртуалке. ответ нет.
потому чтоу этой вирталки нет внешнего ip адреса замапленного.
ну окей. вопрос  если мы зайдем через веб морду через serial console то сможем 
ли мы хотя бы пинговать наружу интернет хосты например 8.8.8.8 
ответ = нет! неможем. почему? 
пускай у нас сабнет имеет диапазон 172.16.3.0\24
так вот на вирталке будет прописан марщурут по умолчанию на 172.16.3.1 
но проблема в том что очевидно далее на том яблочном гейтвее маршрута больше никуда никакого нет!
поэтому в так случае ни из интенета с такой вирталкой не свзяаться ни изнутри в инет связи нет!


привожу чеез тераформ пример такого сабнета и такой виртуалки

resource "yandex_vpc_subnet" "subnet3" {
  name           = "subnet3"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.3.0/24"]
}


resource "yandex_compute_instance" "vm-3" {
...
  network_interface {
    subnet_id = yandex_vpc_subnet.subnet3.id
    nat       = false
  }
...
}



далее. что будет если мы  к сабнету прикрепим route-table котоырй в свою очередь ведет на 
гейтвей.

resource "yandex_vpc_subnet" "private-1-a-subnet" {
  name           = "private-1-a-subnet"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.1.0/24"]
  route_table_id = yandex_vpc_route_table.route-table-2.id
}



resource "yandex_vpc_route_table" "route-table-2" {
  name       = "route-table-2"
  network_id = yandex_vpc_network.default.id

  static_route {
    destination_prefix = "0.0.0.0/0"
    gateway_id         = yandex_vpc_gateway.nat-2.id
  }
}



resource "yandex_vpc_gateway" "nat-2" {
  name        = "nat-2"
  description = "шлюз в инет"
  shared_egress_gateway {}
}



если мы прикрепим виртуалку к такому сабнету то он сможет изнутри пинговать 8.8.8.8
потому что пакеты уходят на  гейтвей нашего сабнета и натом гейтвее идет переброска уже на гейтвей
указанный у нас в тераформе. но по прежнему у нас не связи с виртуалкой из внешнего мира по ссш.
а как тогда мы можем проверить что из нее всеже можно пинговать 8.8.8.8 - ответ такой что 
можно зайти на вирталку через веб морду через serial интерфейс. еще на нее можно зайти 
через ssh прокси джамп. об этом ниже.  итак если у нас к сабнету прикреплен route-table который в свою
очередь ведет на гейтвей это дает то что  с виртуалки прикрелпенной к этому сабнету мы можем изунтри 
ходить наружу в интернет. инциииовать конектц из виртуалки в интернет хосты.


следущий случай. у нас есть субнет. у него нет прикрепленного route-table
но зато унас у виртуалки есть внешний IP. вот пример такого сабнета и такой виртуалки




resource "yandex_vpc_subnet" "dmz-1-a" {
  name           = "dmz-1-a"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.2.0/24"]
}



resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id = yandex_vpc_subnet.dmz-1-a.id
    nat       = true
  }
...
}


значит net=true нам дает то что наша виртуалка будет иметь примапленный снаружи интернет белый
адрес. это дает то что мы можем поключиться по ssh к нашей виртуалке из интернета. ура!
но также это дает то что изнутри виртуалки мы можем также успешно пинговать внешние серверы напрмиер 
8.8.8.8 хотя сабнет неимеет route-table



таким макаром я расмсмотрел все случай:
когда субнет неимеет рут тейбл
когда субнет имеет рут тейбл
когда виртуалка не имеет внешнего IP адрес или когда она его имеет.

таким макаром мы теерь знаем какие возможности у голого сабнете, что дает прикрпленнй рут-тейбл
к сабнету и что дает внешний IP адрес прикрепленный к виртуалке

тогд сумаарно логично в рамках одно network имет два subnet.
первый сабнет subnet-1 он неимеет роут-тейбл зато машины котоыре мы туда пихаем имеют прикрепленный
внешний IP адрес. а второй сабнет subnet2 имеет прикепрленный роут-тейбл который ведет на гейтвей но сами 
машины неимеют белых прикрелпнных IP адресов. тогда машины из subnet2 к ним нет прямого доступа из
интернета но они сами могут ходитьв интренрнет. а машины из subnet1 к ним есть прямой доступ из 
интернета и они могут ходить в интернет. и еше через них как через прокси можно обеспечить связь с машинами
из subnet2 потому что связь между subnet2 и subnet1 есть по локалке.
воттаакая архитектура!

| далее. статический IP адрес . как его прикрепить к ВМ.

как мы задаем сетевую карту на ВМ


resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = false
  }
}


resource "yandex_vpc_subnet" "dmz-1-a" {
  name           = "dmz-1-a"
  zone           = "ru-central1-a"
  network_id     = yandex_vpc_network.default.id
  v4_cidr_blocks = ["172.16.2.0/24"]
}

тоесть унас есть субнет. в нем есть блок адресов. и мы в свойствах вм 
указываем номер субнета. а уже яблоко само и карту создает и адрес из субнета назначает из его блока IP
адресов. тоесть subnet_id это не IP адрес. это номер субнета.



а как нам замапить внешний IP интернет адрес на этот локальный адрес

resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = true
  }
}


тоесть просто добавляем строку nat=true. 
есть только одна проблема то что при каждой перезагрузке виртуалки у нее будет меняться номер 
этого внешнего IP.  а как нам сделать статичекий внешний IP?

для начала надо создать этот статический внешний IP



resource "yandex_vpc_address" "ext-ip-1" {
  name = "ext-ip-1"

  external_ipv4_address {
    zone_id = "ru-central1-a"
  }
}


теперь надо этот внешний IP как прикрепить к ВМ
делается это вот так


resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = true
    nat_ip_address = ??
  }
}


вот осталось понять что же вставлять в поле "??"


чтобы это понять посмотрим как выглядит ресурс "yandex_vpc_address"."ext-ip-1"
для этого вставим в main.tf output

output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1
}

запускаем 
$ terraform refresh
получаем


external_debug_yandex_vpc_address_ext-ip-1 = {
  "created_at" = "2023-03-17T08:27:43Z"
  "deletion_protection" = false
  "description" = ""
  "external_ipv4_address" = tolist([
    {
      "address" = "51.250.3.188"
      "ddos_protection_provider" = ""
      "outgoing_smtp_capability" = ""
      "zone_id" = "ru-central1-a"
    },
  ])
  "folder_id" = "b1gobq5rv8qm8qi76hig"
  "id" = "e9bv1c385ddippqblidv"
  "labels" = tomap({})
  "name" = "ext-ip-1"
  "reserved" = true
  "timeouts" = null /* object */
  "used" = true
}

очевидно что нам надо добавить в наш путь external_ipv4_address чтобы мы стали ближе к получению ip адреса
в запросе
тоесть


output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1.external_ipv4_address
}



заупускаем
$ terraform validate && terraform fmt && terraform plan -out "plan.txt" && terraform apply
получаем

external_debug_yandex_vpc_address_ext-ip-1 = tolist([
  {
    "address" = "51.250.3.188"
    "ddos_protection_provider" = ""
    "outgoing_smtp_capability" = ""
    "zone_id" = "ru-central1-a"
  },
])


значит здесь мы видим что у нас стоит toist. это значит что у нас с точки зрения yaml мы 
имеем список . сосотоящий из одного элемента. как получить доступ к этому списку. доступ к элементам
списка идет через цифры. первый элемент это 0 .
значит добавляем ноль в путь
тоесть


output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1.external_ipv4_address.0
}

проверяем

external_debug_yandex_vpc_address_ext-ip-1 = {
  "address" = "51.250.3.188"
  "ddos_protection_provider" = ""
  "outgoing_smtp_capability" = ""
  "zone_id" = "ru-central1-a"
}


а теперт осталось доабвить address к пути
тоесть


output "external_debug_yandex_vpc_address_ext-ip-1" {
  value = yandex_vpc_address.ext-ip-1.external_ipv4_address.0.address
}


получаем

external_debug_yandex_vpc_address_ext-ip-1 = "51.250.3.188"

наконецто! мы смогли вычленить из ресурса yandex_vpc_address  параметры IP адреса.
вот эту штуку и подставляем в ВМ тоесть


resource "yandex_compute_instance" "vm-1" {
...
  network_interface {
    subnet_id      = yandex_vpc_subnet.dmz-1-a.id
    nat            = true
    nat_ip_address = yandex_vpc_address.ext-ip-1.external_ipv4_address.0.address
  }
}

вот это  и есть ответ на вопрос как нам примапить к ВМ статический ip адрес.
также приколв том что через веб интерфейс этого сделать совершенно нельзя.

таким макаром рассмотрено три случая - как виртуалке прописать локальный IP,как ей замапить динмический
IP внешний, как ей замапит стаический IP внешний


| далее. по поводу boot_disk

значит по дефолту загрузочный диск к ВМ прикрепляется вот так

resource "yandex_compute_instance" "vm-3" {
...
  boot_disk {
    initialize_params {
      image_id = "fd8haecqq3rn9ch89eua"
    }
  }
...
}

проблема тут в том что мы незаем какой у нас у диска id
и то что мы неможем ему задать имя.
это значит что если мы установим параметр чтобы яблоко неудаляло диск при удалении виртуалки то после
удаления вируалки мы в списке диско небудем знать какой же диск был у нас загрузочным для виртуалки

поэтому друой более правильный вариант как добавлять загрузочный диск к ВМ.



resource "yandex_compute_disk" "vm-1---bootdisk---disk1" { # название виртуалки к которой крепим диск и номер диска
  name       = "vm-1---bootdisk---disk1"
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "10"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
  image_id   = "fd8haecqq3rn9ch89eua"
}



resource "yandex_compute_instance" "vm-1" {
...
  boot_disk {
    disk_id     = yandex_compute_disk.vm-1---bootdisk---disk1.id
    auto_delete = false
    mode        = "READ_WRITE"

  }
...
}


output "vm_1_bootdisk" {
  value = yandex_compute_disk.vm-1---bootdisk---disk1.id
}


теперь если  удалим виртуалку то диск у нас останется. и его можно будет найти в списке 
дисков во первых по его id через вывод output а  во вторых даже глазами в веб морде по имени.


также важны момент. в main.tf надо размещать описание дисков перед описанием вм. иначе 
при создании вм тераформ будет писать ошибку что диск ненайден!


|далее. апдейт про BACKEND
выше я описал как настроить бекенд для тераформа.
так вот теперь апдейт этого вопроса.

значит возникла идея и задача о том тобы разделить код тераформа от данных тераформа.
чтобы кода осталвся тем же самым а данные были ли бы закоодированы через переменные 
а переменные  бы лежали в отдельном файле.

напомню что у меня в main.tf была секция

 backend "s3" {
    endpoint = "storage.yandexcloud.net"
    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"

    skip_region_validation      = true
    skip_credentials_validation = true

    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"

  }



я попробовал сюда начать всталвять перменные.

backend "s3" {
bucket = var.backend_bucket_name
...


и мне выдало ошибку


$ terraform init -reconfigure

Initializing the backend...
╷
│ Error: Variables not allowed
│
│ on main.tf line 19, in terraform:
│ 19: bucket = var.backend_bucket_name
│
│ Variables may not be used here.


таким образом в настройках бекенда нельзя использовать переменные. (об этом есть как раз 
подтверждение тут https://github.com/hashicorp/terraform/issues/13022 ) 


тогда я применил другой подход.
создаем отдельный файл backend.tfbackend в котором я буду хранит настройки бекенда
( о том как надо правильно называть написано тут https://developer.hashicorp.com/terraform/language/settings/backends/configuration#using-a-backend-block )


далее в основном файле main.tf мы удаляем почти все
и оставляем
$ cat main.tf 
...
  backend "s3" {
    endpoint = "storage.yandexcloud.net"
    skip_region_validation      = true
    skip_credentials_validation = true
  }
...

а остальные настройки мы переносим в backend.tfbackend

$ cat  backend.tfbackend

    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
    profile  = "service-account-2"
 
    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"


далее так как мы изменили настройки бекенда теперь надо бекенд переинцилизиаоровать.
и сделать это интересным способом

$  terraform init  -backend-config=./backend.tfbackend  -reconfigure

более того например в backend.tfbackend все параметры это константа кроме параметра "profile = service-account-2"
ибо это штука для каждого юзера индивидуальная. так вот так как в тераформ бекенде нельзя применять перменные
то эту штуку надо как то вынести наружу и иметь возможность как то задавать снаружи. и тут нам поможет 
вот такая штука (нашел в инете)

$ terraform init -backend-config="${backend_env}" \
                 -backend-config="key=global/state/${backend_config}/terraform.tfstate" \
                 -backend-config="region=us-east-1" -backend-config="encrypt=true" \
                 -backend-config="kms_key_id=${kmykeyid}" \
                 -backend-config="dynamodb_table=iha-test-platform-db-${backend_config}"


это дает нам такую вещь что мы можем из backend.tfbackend убрать параметр profile  а сам файл закинуть 
в гитлаб. 


$ cat  backend.tfbackend

    bucket   = "bucket-2aw"
    region   = "ru-central1-a"
    key      = "folder1/terraform.tfstate"
 
    dynamodb_endpoint = "https://docapi.serverless.yandexcloud.net/ru-central1/b1g8sk79560kbvb6nr2j/etn7rrapc9r6eoma53a7"
    dynamodb_table    = "terraform-state-lock-2"



а сам параметр когда мы скачаем этот файл из гитлаба задать через комндуню строку вот так

$ terraform init  -backend-config=./backend.tfbackend  -backend-config="profile=service-account-2"

это круто

важно еще отметить что нельзя вот взять и полностью всю секцию backend убрать из main.tf
тогда тераформ нас нахер пошлет.  в минимальном виде обязательно в main.tf должен быть 
вот хотя бы такой блок

$ cat main.tf
...
  backend "s3" {}

....


вот это очень важно знать . потому что я вначале полнотью убрал блок backend из main.tf
потом попробвал инциализировать и там одни только ошибки. поэтому backend "s3" {} обязательно
должен присутовать в main.tf


при таких настройках далее уже можно с терпформом работаь как обычно не указывыая  -backend-config каджый раз.
напрмиер 

$ terraform refresh


это очень важная инфо.

чего мы в итоге достигли.  -  так как переенные нельзя использовать в бекенд секции то 
мы эту секцию убрали из main.tf таким образом мы сделали main.tf более независимым от 
конкретных хардкод настроек бекенда. main.tf стало более универсальным более незавиимым от 
конкретики.


| далее. как импортировать в модуль
у меня был создан внешний стат IP адрес через ресурс в root модуле.
я хочу убрать ресурс из рут модуля при этом хочу чтобы сам IP адрес небыл удален
и импортировать его уже в модуль


в рут модуле ресурс был задан вот так


resource "yandex_vpc_address" "ext-ip-1" {
  name                = var.ext_ip_1
  deletion_protection = false

  external_ipv4_address {
    zone_id = var.zone
  }
}


в модуле он задан вот так

resource "yandex_vpc_address" "this" {
  count =  var.ext_ip != ""  ? 1 : 0
  name                = var.ext_ip
  deletion_protection = true

  external_ipv4_address {
    zone_id = var.zone
  }
}


значит как я делаю тогда:

я смотрю через равен id ресурса
$ terraform state show yandex_vpc_address.ext-ip-1

потом я удаляю ресурс из стейта
$ terraform rm  yandex_vpc_address.ext-ip-1
при этом на яблоке реруса конечно же остается

далее я комментирую описание ресурса в рут модуле

/*
resource "yandex_vpc_address" "ext-ip-1" {
  name                = var.ext_ip_1
  deletion_protection = false

  external_ipv4_address {
    zone_id = var.zone
  }
}
*/

далее я завожу в рут модуле наш новый модуль
resource "yandex_vpc_address" "this" {
  count =  var.ext_ip != ""  ? 1 : 0
  name                = var.ext_ip
  deletion_protection = true

  external_ipv4_address {
    zone_id = var.zone
  }
}


далее я его инициирую
$ terraform init

далее я импортирую наш ресурс из яблока в этот модуль

$ terraform import module.ext-ip-1.yandex_vpc_address.this[0]   e9bv1c385ddippqblidv

далее еще можно сделать рефреш
$ terraform refresh

далее я делаю 
$ terraform plan -out "plan.txt"
и проверяю вывод глазами что тераформ несобирается создавать ресурс module.ext-ip-1.yandex_vpc_address.this[0]
ибо если все правильно то он уже импортирован а значит создан

конец. готово.




#$#
0.a план:
  - понять какие ресурсы сделать неудаляемыми (отредактировать main.tf и модуль).
    и попробовать удалить виртуалку и заново ее создать и чтобы старые диски прикрутились.
  - описать настройки cloud-init, модуль засунуть в гит и оттуда скачивать. 
  - сделать variables.tfsf чтобы смена кода модуля ненаебнула настройки прода(даже незнаю стоит ли). 
  - импорт существующих виртуалок в контуре прод. раскатать туже инфраструрутуру что в проде на тестовой папке но с маленькими размерами. чтобы всегда была для теста стркутуру аналогичная тому что на проде 
  => после этого мы успешно запустили проект тераформ на проде. 
0.1 cloud-init
в main.tf прописано 
  metadata = {
    ssh-key = ...
    }
так вот на хосте служба cloud-init считывает из сети 
этот ключ через ссылку
http://169.254.169.254/2018-09-24/meta-data/public-keys/0/openssh-key (здесь лежит ssh-key)

а вот этот кусок в main.tf 
  metadata = {
    user-data           = data.template_file.cloud_init.rendered
}

его cloud-init счывтает по сети по ссылке
http://169.254.169.254/2018-09-24/user-data

если мы в main.tf поменяли user-data то 
накатываем новые настройки это херни через тераформ 
$ terraform validate
$ terrafrom plan
$ terraform aplly 
заходим на хост и мы сразу увидим эти настройки по ссылке 
http://169.254.169.254/2018-09-24/user-data

далее вопрос как накатить их на хост не перезагружая хост.
вначале нужно стереть файлы
# rm -rf /var/lib/cloud/*

а потом запутсить накат 
# cloud-init init

прикол в том что без стиарния файлов эта сука ненакатывает изменения. 
но и возникает проблема в том что из за стиаиния файлов на хосте меняется ssh_host_key
значит при заходе по ssh система напишет что ключ хоста сменился!

0.c добавить возможность прописывания стат. IP локального адреса. пригодится
при перетескивании имеющихся виртуалок в тераформ.(также протестировать импорт в модуль уже имеющеся виртуалки.
если полчится импортировать виртулку прям в модуль тогда это будет круто). еще  имя переменной в имени модуля. можно так? также понять в модулях вставлят константы или переменные из variable.tsvars ??
0.d реальные приколы и проблемы: 
- если назначить стат IP внешний  а потом его убрать из настроек тераформа
(перменаная ext-ip) то по факту этот стат IP все равно осатется прикрепленный  к этой вм. а чтобы
его открепить от этой вм надо отключить нат и обратно включить ( настройка isnat=true).
- еще одноврееннно создаем сетеовй ресурс и виртуалку описарбющуся на этот сет ресурс то 
тераформ эпплай напишет что данная виртуалка неможет быть создана так как остутстует сетевой ресурс. поэтому
приходиься вначале созадвать сетовой ресурс а потмо уже только виртуалку с этим сет ресурсом.
- далее клауд-инит вот мы задали настройки в cloud-init.cfg и раскатали виртуалку. а потом мы взяи 
и измнили файл cloud-iit.cfg и прикол в том что клауд инит больше настроки с эттго файла сука ненакатывает.
хоть перегружай хост хоть не прегржужай. возможно с клауд инит у меня изначально и неполучалось потому что
новые настройки работают только и только на абсолютно чистой системе при первой загрузке и все а ято 
пытался внести настройки на машине на которой уже было 100 перезагрузок и на которой клауд инит уже
отработал поэтому там ничего и не срабатывало
- удаление ресурсов лучше делать аккуратно и по частям
- массированное удаление с массированным созаданием ресурсов лучше разделить на части

1.15 terragrunt ???
1.2 aws instance teraform модуль на github посмотреь  с целью получения опыта 
0.0 переделать main.tf в test-folder и test-folder2 
чтобы вверху шли первостепенные ресурсы  а внизу  второстепенные
0.1  надо по взоможности те ресурсы которые надо создавать первыми размещать в main.tf вверху
а ресурсы котоыре создавать посоедними размещать внизу потому что если в файле намешано то тераформ
плохо понимает какое ресурс надо создавать первым  а потом пишет что немогу найти ресур.
например вверху main.tf надо рарзмешатть диски а виртулаки уже ниже чтобы неылло проблем.
0.2 надо локальный IP адрес создавать отдельно от виртуалки а потом уже к ней его клеить . почему.
потому что при пересоании виртуалки ей будет назначен другой локлаьный адрес и это может быть проблемой.
0.0 как правильно удалять ресурсы из тераформа?
убрал часть ресурсов из main.tf 
думал что он удалит после terfaorm aplly 
но нет. он выдал ошибку
тггда приходьистя руками удалять ресурсы их стет файла и потому уже руками из веба
удалять. 

$ terraform state list
yandex_compute_instance.vm-1
yandex_compute_instance.vm-2
yandex_compute_instance.vm-3
yandex_vpc_gateway.nat-2
yandex_vpc_network.default
yandex_vpc_route_table.route-table-2
yandex_vpc_subnet.dmz-1-a
yandex_vpc_subnet.private-1-a-subnet


и потом пользуя это можно убарть ресурс из стейт файла 
если тераформ немоет его удалитть сам

$ terraform state rm yandex_vpc_network.network-1

а так общий вывод что нужно ресурсы внаачалае закоменчивать. и только если уон удачно 
удален то уже можно удалять из конфига!!

конмет блока выгдяит вот так

/* 
....
....
*/



|далее. hostname в описании ВМ


разберем некотоыре поля в ВМ

resource "yandex_compute_instance" "vm-3" {
  name                      = "vm-3---terraform"
  hostname                  = "vm-3---terraform"
  description               = "тест машина. создана через тераформ"


name это название виртуалки которые видно через веб морду.
description это описание виртуалки в ввеб морде если на нее нажать.
тоесть эти два поля они вобще то для веб морды. и для якли

hostname это линукс хостнейм который нам будет виден когда мы войдем по ssh на виртуалку.

если мы хотим поменять name или description то тераформ это делает налету.
а вот если мы хотим поменяеть hostname то тераформ (внимание ) убивает виртуаку и пересоздает ее
заново!!!

какой огромный бонус есть от hostname - можно пинговать все виртуалки котоыре входят в состав network
по их hostname (он резволится через локальный яндекс dns сервер. для нас это все автоматически настроек делать руками ненадо)

|далее. STATE LIST

очень полезная комнанда

$ terraform  state list
yandex_compute_disk.vm-1---bootdisk---disk1
yandex_compute_disk.vm-1---disk2
yandex_compute_disk.vm-1---disk3
yandex_compute_instance.vm-1
yandex_compute_instance.vm-2
yandex_compute_instance.vm-3
yandex_vpc_address.ext-ip-1
yandex_vpc_gateway.nat-2
yandex_vpc_network.default
yandex_vpc_route_table.route-table-2
yandex_vpc_subnet.dmz-1-a
yandex_vpc_subnet.private-1-a-subnet
yandex_vpc_subnet.subnet3

показвыает все ресурсы

$ terraform show 
покажет другое


| далее. CONSOLE
в консоли можно посмтреть переменные. пример.
$ head variables.tf 
variable "cloud_id" {
  description = "cloud id"
  type        = string
  default     = "b1g8sk79560kbvb6nr2j"
}


запускаем консоль и смотрим чему равна переменная
$ terraform console
> var.cloud_id
"b1g8sk79560kbvb6nr2j"
> exit



| далее. в названии resource нельзя использовать переменные.
тоесть 

resource "yandex_vpc_address" "ext-ip-1" {
}

так вот в этой строке resource нельзя применять переменные а только константы . все.






| далее. DEBUG 
как активировать выполнение тераформа в режиме дебага

$ export  TF_LOG=TRACE
$ terrafrom ...

лог выполнения будет на экране прям. в лог файл лазить никакой ненадо.

а чтобы выключить дебаг то надо 
$ unset TF_LOG





| далее. ошибка failed to read provider configuration schema for ... provider
это значит что был установлен старый провайдер. надо его убрать из конфига и удалить через
$ terraform init
а потом хорошенько поискать как правильно поставить нужную версию провайдера






|далее. VARIABLES , MAP, LIST, CONSOLE
описание перменных. работа с ними. дебаг в консоли тераформа.

в тераформе переменные лучше всего обьявлять в спец файле
для этого variables.tf тогда тераформ автоматом засасывает этот файл,
обьявляется переменная  вот по такому правилу


variable "имя_переменной" {
  type = тип_переменной
  default = {
   
   значение переменной

  }
}


какие бывают типы перменных:
string
number
bool
list
map


самые интересные типы это list и map. они  в точности соотвестуют ямловским типа список и словарь тоесть
list и dictionary

как выглядит примеры с list и dictionary


variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [
    "10.0.101.0/24",
    "10.0.116.0/24"
  ]
}


соотсветсвенно можно тоже самое записать чуть по другому

variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [ "10.0.101.0/24", "10.0.116.0/24" ]
}


важно заметить что мы не просто пишем list а еще и указываем тип данныех внутри этого листа,
 в доках тераформа эти суки об этом ничего не пишут.
 значит по идее тогда таких типов list() может быть несколько

list(string)
list(number)
list(bool)

и еще может быть один спец тип списка
list(any)

заметим что значение описывается через квадратгые скобки [] ровно так как это и есть у ансибля в его ямль
в ансибле аналогичная переменная бы имела вид

private_subnet_cidr_blocks: [ "10.0.101.0/24", "10.0.116.0/24" ]

либо

private_subnet_cidr_blocks: 
    - "10.0.101.0/24"
    - "10.0.116.0/24" 






а вот пример dictionary

variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}



или вот в таком виде

variable "ami_image_id" {
  type = map(string)
  default = {  "ru-central1-a" = "fd8haecqq3rn9ch89eua",  "ru-central2-a" = "fd8haecqq3rn9chaaaaa"  }
}


а вот как эта бы перменная выгяедла бы в ансибле

ami_image_id: {  "ru-central1-a" = "fd8haecqq3rn9ch89eua",  "ru-central2-a" = "fd8haecqq3rn9chaaaaa"  }

либо

ami_image_id: 
   "ru-central1-a": "fd8haecqq3rn9ch89eua"
   "ru-central2-a": "fd8haecqq3rn9chaaaaa"  



тип map тоже не может быть просто типа map. он также должен быть прояснен какие виды значений
лежат в этом словаре тоесть
  type = map(string)

либо

  type = map(bool)

ну и так далее

а еще можно вотак

  type = map(any)



а теперь возникает очень важный вопрос - а как нам получать доступ к элементам внутри list
или dictionary. ответ такой - если это список то поиск идет по номеру элемента в списке ( по номеру строки
в списке) 

а если это dictionary то поиск идет по ключу ( так как дикшонари это ключ=значение то сама логика
просит нас делать поиск по ключу. у списка нет клюей поэтому там был вариант искать либо по номеру строки либо по значениею в итоге в списке ищут по номеру строки)

показываю на практике
имеем в variables.tf

$ cat variables.tf

variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}


variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [ "10.0.101.0/24", "10.0.116.0/24" ]
}



заходим  в консоль
$ terraform console


> var.ami_image_id
tomap({
  "ru-central1-a" = "fd8haecqq3rn9ch89eua"
  "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
})



> var.ami_image_id["ru-central1-a"]
"fd8haecqq3rn9ch89eua"



> var.private_subnet_cidr_blocks
tolist([
  "10.0.101.0/24",
  "10.0.116.0/24",
])


> var.private_subnet_cidr_blocks.0
"10.0.101.0/24"
> var.private_subnet_cidr_blocks.1
"10.0.116.0/24"



а теперь покажу что можно при вызове переменной исползовать другую переменную.


вот у нас есть две переменные

variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}


variable "zone" {
  description = "availability zone"
  type        = string
  default     = "ru-central1-a"
}



и тогда можно вот так

$ terraform console

> var.ami_image_id[var.zone]
"fd8haecqq3rn9ch89eua"


единственное надо постоянно помнить что мы когда вызываем переменную то надо приплюсовать var.
тоесть var.zone а не просто zone


итак еще раз если у нас есть переменная vasya у которой тип list это значит что внутри 
переменной есть списочек элементов
и можно получать доступ к элементам списка через вот так

var.vasya.0 или var.vasya.1

а если у нас переенная vasya имеет тип map то переменная состоит из элементов каждый из которых это 
ключ=значение. и доступ к элементу идет через значение ключа. тоесть 

var.vasya[ключ]


хочу еще раз рассмотреть момент определения сколько элементов имеет переменная list или map
в себе. потому что навсикду всегда непонятно сколько же элеметов имеет переменная типа list или map
еще раз рассмотрим переменные которые мы выше смотрели


variable "ami_image_id" {
  type = map(string)
  default = {
    "ru-central1-a" = "fd8haecqq3rn9ch89eua"
    "ru-central2-a" = "fd8haecqq3rn9chaaaaa"
  }
}


variable "private_subnet_cidr_blocks" {
  description = "Available cidr blocks for private subnets."
  type        = list(string)
  default = [ "10.0.101.0/24", "10.0.116.0/24" ]
}



еще раз посмотрим как выглядели бы эти переменные в ансибле


ami_image_id:
    ru-central1-a: "fd8haecqq3rn9ch89eua"
    ru-central2-a: "fd8haecqq3rn9chaaaaa"


private_subnet_cidr_blocks: 
    -  "10.0.101.0/24" 
    -  "10.0.116.0/24" 


тогда у первой переменной есть два элемента  'ru-central1-a: "fd8haecqq3rn9ch89eua"' и
'ru-central2-a: "fd8haecqq3rn9chaaaaa"''

у второй переменной есть два элемента  '"10.0.101.0/24"' и '"10.0.116.0/24"' 

надеюсь вопрос сколько элементов имеет list или map стал более понятным.


теперь рассмотрим еще один пример переменной

variable "project" {
  description = "Map of project names to configuration."
  type        = map(any)

  default = {
    client-webapp = {
      public_subnets_per_vpc  = 2,
      private_subnets_per_vpc = 2,
      instances_per_subnet    = 2,
      instance_type           = "t2.micro",
      environment             = "dev"
    },
    internal-webapp = {
      public_subnets_per_vpc  = 1,
      private_subnets_per_vpc = 1,
      instances_per_subnet    = 2,
      instance_type           = "t2.nano",
      environment             = "test"
    }

  }

}



посмотрим как выглядела бы эта переменная в ансибле


project: 
   client-webapp: 
      public_subnets_per_vpc: 2
      private_subnets_per_vpc: 2
      instances_per_subnet: 2
      instance_type: "t2.micro"
      environment: "dev"
   internal-webapp: 
      public_subnets_per_vpc: 1
      private_subnets_per_vpc: 1
      instances_per_subnet: 2
      instance_type: "t2.nano"
      environment: "test"


а теперь посмотрим как мы внутри этой переменной мы будем вычленять элементы и субэлементы


$ terraform console
> var.project
tomap({
  "client-webapp" = {
    "environment" = "dev"
    "instance_type" = "t2.micro"
    "instances_per_subnet" = 2
    "private_subnets_per_vpc" = 2
    "public_subnets_per_vpc" = 2
  }
  "internal-webapp" = {
    "environment" = "test"
    "instance_type" = "t2.nano"
    "instances_per_subnet" = 2
    "private_subnets_per_vpc" = 1
    "public_subnets_per_vpc" = 1
  }
})



> var.project.client-webapp
{
  "environment" = "dev"
  "instance_type" = "t2.micro"
  "instances_per_subnet" = 2
  "private_subnets_per_vpc" = 2
  "public_subnets_per_vpc" = 2
}


> var.project.internal-webapp
{
  "environment" = "test"
  "instance_type" = "t2.nano"
  "instances_per_subnet" = 2
  "private_subnets_per_vpc" = 1
  "public_subnets_per_vpc" = 1
}

> var.project.internal-webapp.environment
"test"

> var.project.client-webapp.environment
"dev"

> var.project.client-webapp.instance_type
"t2.micro"










1. почемуто output из модуля нихуя неработает сук бляь
1.3 conditinals

если нам надо if then else

locals {
  test = "${ condition ? value : (elif-condition ? elif-value : else-value)}"
}

(нашел здесь https://stackoverflow.com/questions/55555963/how-to-write-an-if-else-elsif-conditional-statement-in-terraform)


мой пример. в конфиге задается две переенные
vm_name
vm_hostname

моя задача : 
если незадан vm_name но при этом задан vm_hostnme то vm_name=vm_hostname инааче vm_name = random
если незадан vm_hostname но при этом задан vm_name то vm_hostname=vm_name иначе vm_hostname = random

решение

locals {

   check_name     = var.vm_name != "" ? var.vm_name : (var.vm_hostname != "" ? var.vm_hostname : "name-error")
   check_hostname = var.vm_hostname != "" ? var.vm_hostname : (var.vm_name != "" ? var.vm_name : "hostname-error")

   vm_name       = "${local.check_name}"
   vm_hostname   = "${local.check_hostname}---terraform"
   vm_desc       = "${local.check_hostname}  ${var.vm_desc}"

   disk1_desc    = "${local.check_name}  ${var.disk1_desc}"
   disk1_name    = "${local.check_name}---${var.disk1_name}"

}




0.0 как передать внутрт модуля наример параметр vpc network 
ответ надо передать через string имя vpc а внутри модуля создать через data этот vpc
прмиер
задаем переменную

variable "subnet" {
  description = "id сети "
  type        = string
  default     = "subnet3"
}


вот внутри модуля пишем

data "yandex_vpc_subnet" "this" {
  name = var.subnet

}

при том что сам vpc network созан не в модуле. мы лишь суда через data передали
его параметр

теперь внутри модуля можно обращаться е этому network vpc
  network_interface {
    subnet_id = data.yandex_vpc_subnet.this.id
    nat       = var.isnat
  }

  это круто!



0.9 terraform как скопироовать файл на вм
//EC2 Instances
resource “aws_instance” “my_web_servers” {
        ami = “ami-0d527b8c289b4af7f” //Ubuntu 20.04
 instance_type = “t2.micro”
 count = var.vm_count
 provisioner “file” {
  source = “./resolv.conf”
  destination = “/etc/resolv.conf”
 }
}
ссылка = https://www.cloudbolt.io/terraform-best-practices/terraform-template/


0.a variables.tf vs terraform.tfvars
https://stackoverflow.com/questions/56086286/terraform-tfvars-vs-variables-tf-difference

1.0 alien-terrafrom вопросы 
[ как подключить второй и третий диск через модуль?]
for_each (как юзать):
variable "vpc_name" {
  description    = "список vpc нетворков"
  type           = list(string)
  default        = [ 
                     "network-1",
                     "network-2"
                                 ]
}


resource "yandex_vpc_network" "this" {
  for_each = toset(var.vpc_name)
  name = each.value
}


результат

$ terraform show

# yandex_vpc_network.this["network-1"]:
resource "yandex_vpc_network" "this" {
    created_at = "2023-03-18T22:24:22Z"
    folder_id  = "b1gobq5rv8qm8qi76hig"
    id         = "enpf3vgubl2fpm6nmfg0"
    labels     = {}
    name       = "network-1"
    subnet_ids = []
}

# yandex_vpc_network.this["network-2"]:
resource "yandex_vpc_network" "this" {
    created_at = "2023-03-18T22:24:22Z"
    folder_id  = "b1gobq5rv8qm8qi76hig"
    id         = "enp19jihnisr82lad784"
    labels     = {}
    name       = "network-2"
    subnet_ids = []
}

а $ terraform state show yandex_vpc_network.this["network-1"]
неработает!

походу решение в другой области = https://github.com/yandex-cloud/terraform-provider-yandex/issues/43


витоге решение

variable "vasya" {
  type = list(string)
  default = [ "disk4", "disk5" ]
}





resource "yandex_compute_disk" "this" {
  for_each = toset(var.vasya)
  name       = each.value
  type       = "network-hdd"
  zone       = "ru-central1-a"
  size       = "5"
  block_size = "4096" # при таком блоке макс размер диска 8ТБ
}


resource "yandex_compute_instance" "vm-3" {


 dynamic "secondary_disk" {
    for_each     = var.vasya
    content {
     disk_id     =  yandex_compute_disk.this[secondary_disk.value].id
     auto_delete = true
     mode        = "READ_WRITE"
    }
 }


а если я не хочу ничего создавать тогда перменная должны выглядеть вот так

variable "vasya" {
  type = list(string)
  default = [ ]
}


далее в модуле пишем

data "yandex_vpc_subnet" "this" {
  name = var.subnet
}


(что за хрень  data . типа оно не создает ресурс а делает его рид онли копию.)

тогда в настройках вм пишем

resource compute_instance ...   {
...
  network_interface {
    subnet_id = data.yandex_vpc_subnet.this.id
    nat       = false
  }
...
}



далее вот у нас есть модуль и в нем переменная прописана. например 
вот кусок модуля

resource "yandex_compute_disk" "bootdisk---disk1" { # название виртуалки к которой крепим диск и номер диска
  description = var.disk1_desc
...
}

как нам передать эту переменную в  модуль из основного root модуля?
во первых в самом модуле есть файл variables.tf где эта переменная должна быть задана
например

variables "description" {
  type    = string
  default = "диск 1"
}

но нам то надо как то из root модуля перебить это значение. как это сделать.
ответ - через вызов в модуле а именно

module "vm_3" {
  source        = "./modules/create-vm"
  description   = "диск 31"
...
}

тогда "диск 31" перебьет "диск 1". тут еще важно отметить то что наш root модуль тоже имеет
свой variables.tf и в нем может иметь переменную description. так вот переменные в этом variables.tf они относится только к модулю root и никакого отгошения к нижележащим модулям не имеют.
таким образом еще раз.  файл variables.tf лежащий в модуле относится только к этому модулю и задает 
значения по умолчанию для модуля. файл variables.tf лежащий в root модуле относистя только к root модулю ( к файлу main.tf в рут модуле) и больше ни к чему, задает в нем дефолтовые значения. а если мы хотим из рут модуля из main.tf передать занчение в 
переменную в модуле то это делается  в рут модуле в секции модуль.  
$ cat main.tf (рут модуль)
module "vm_3" {
  source        = "./modules/create-vm"
  description   = "диск 31"
...
}

если мы в секции модуля  не передадим знаение в переменную в модуль то в модуле значение переменной
будет равно тому что указано  в модульном variables.tf и еще раз скажу что модулю насрать на variables.tf который 
относится к root модулю.




| далее. template тема. тема с темплейтами

имеем в модуле код

data "template_file" "cloud_init" {
  template = file("cloud-init.tftpl")

  vars = {
    ssh_user_name  = var.ssh_user_name
    ssh_public_key = var.ssh_public_key
  }
}



вопрос по какому пути тераформ будет искать файл cloud-init.tftpl 
ответ он будет его искать в той папке где лежит root модуль !
а если мы хотим чтобы файл искался в папке модуля?!
ответ - тогда надо записать вот так

data "template_file" "cloud_init" {
  template = file("${path.module}/cloud-init.tftpl")

  vars = {
    ssh_user_name  = var.ssh_user_name
    ssh_public_key = var.ssh_public_key
  }
}



| далее. как импортировать ВМ в модуль.
во первых импорт идет не всех ресурсов из модуля . нет!
импорт что в модуль что в root идет только индивидуальных ресурсов 
тоесть при импорте в модуль надо указать какой именно ресурс из модуля ты хочешь 
импортировать.  так вот виртуалку надо импортировать так:
1. вначале все ресурсы диски
2  и только потом ресурс виртуалку
например
$ terraform  import module.vm_3.yandex_compute_disk.bootdisk---disk1  fhmt5rbuqa8ds4j8fe3l
$ terraform  import  module.vm_3.yandex_compute_instance.vm fhm7ntr7ltgkl38thjs1

id подсматриваем в вею морде или якли

| далее. про защиту от удаления.
не все элементы имеют опцию защиты от удаления.
например "yandex_vpc_address" имеет защиту от удаления опция "delete_protection"
а какието элементы и нет. 
что касается виртуалки и дисков. сам ресурсдиск неимеет защиты от удаления.
а в ресурсе виртуалка есть свойство о том что при удалении именно ресурса виртуалка 
неудалять автоматом диск но это недает защиту от удаления если мы удаляем сам ресурс диск!
поэтому надо быть внимательным. а лучше всего при таких работах делать снэпшот дисков !

| далее . snapshot , image
разницу межд снэпшотом и имаджем я непонял.
по скорости снэпшот снимается быстрее. как я понял
имадж дольше. но в чем разница нпонятно.
у амазона есть сущесвтенная разница : у них снэпшот это снэпшот диска. а имадж еще включает некие параметры
операционки поэтому запустить новую виртуалку можно только  с имаджа а со снэпшота нельзя. со снэпшота можно
сделать диск но загрузиться с него будет нельзя. можно только с имаджа. у яндекса таких заморочек нет.
у яндекса запуск идет с диска. диск можно создать и из снжпшота и из имаджа. если в итоге исходник был загрузочным
то нет пролем с чего был сделан диск с имаджа или снэпшота. 
из того что я вижу пока это то что снэпшот делается быстрее но диск с него создается дольше.
имадж делется дольше но диск с него создатеся быстрее. и все.


| далее. как добавить +1 диск к виртуалке:  
  
дело втом что если добавит диск  в модуль создающий диски  
и потом накатить плейбук то тераформ (его недоделка) напишет   
что он сейчас удалит виртуалку , создась диск и потом пересоздаст виртулку.  
поэтому нам надо удалить виртуалку из стейте тераформа, дообавитт диск  
и импортировать обратно виртуалку.  
поехали:  
- удаляем из стейта тераформа модуль нашей виртуалки.  
  
$ terraform state rm module.vm_4.yandex_compute_instance.vm  
  
и коментируем этот модуль в main.tf  
  
- добавляем диск в модуль по созданию дисков  
  
- смотрим что будет создан диск и больше ничего не поменяется  
 2385  terraform validate && terraform fmt && terraform  plan -out "plan.txt"  
  
- если это так накатываем плейбук . то бишь в резулаате будет создан диск  
 2391  terraform  apply  "plan.txt"  
  
- далее раскоментируем модуль виртуалки в main.tf  
и проверяем что будет создана виртуалка только  
  
- импортируем виртуалку обрратно в стейт тераформа  
 2389  terraform import module.vm_5.yandex_compute_instance.vm  fhmed8q2vuse5it4g9b5  
  
- запускаем план чтобы убедиться что больше никаких имзенеий не будет  
  
- конец  


|далее суперважная тема про for_each
во первых если у нас есть модуль то типа того что мы не имеет доступа к переменным внутри модуля.
самое простое это настроить как бы экспорт нужных нам переменных. настройка идет через output
например скажем модуль create-disk имеет вот такие аутпуты:

$ cat ./modules/create-disk/outputs.tf 
output  "bootdisk_id"  {
   value = yandex_compute_disk.bootdisk.id
}


output "extra_disks" {
   value = yandex_compute_disk.this
}


положим в модуле root мы заюзали этот модуль вот так:
module "vm_4_disks" {
  source        = "./modules/create-disk"
...
}
output "vm_4_bootdisk_id" {
  value = module.vm_4_disks.bootdisk_id

}

тогда в другом модуле можно читать эти экспортрованные переменные вот так:
module "vasya" {
 ...
  bootdisk_id    = module.vm_4_disks.bootdisk_id
  extra_disks    = module.vm_4_disks.extra_disks
...
}

итак еще раз - если мы хотим из одног модуля всунуть некие данные в другой модуль
то надо настраивать outputs. это первый момент.

а теперь самое главное = предположим что переменная которую мы читаем и потом пихаем в другой 
модуль это непросто string а например list или map.
и положим что в модуле куда мы передаем на основе этой переменной создаются ресурсы в частности через команду for_each
например 


data "yandex_compute_disk" "extra_disk" { # название виртуалки к которой крепим диск и номер диска
    for_each     = var.extra_disks
     disk_id     =   each.value["id"]


}


так вот если мы передаем переменную типа list то тераформ пошлет нас нахер вот такой руганью:
 Error: Invalid for_each argument
│ 
│   on modules/cr-vm-2/main.tf line 67, in data "yandex_compute_disk" "extra_disk":
│   67:     for_each     = toset(var.extra_disks)
│     ├────────────────
│     │ var.extra_disks is list of string with 2 elements
│ 
│ The "for_each" set includes values derived from resource attributes that cannot be determined until apply, and so Terraform cannot determine the full set of keys that will identify the
│ instances of this resource.
│ 
│ When working with unknown values in for_each, it's better to use a map value where the keys are defined statically in your configuration and where only the values contain apply-time
│ results.
│ 
│ Alternatively, you could use the -target planning option to first apply only the resources that the for_each value depends on, and then apply a second time to fully converge.
╵


а вот обьясенние от одного толкового разраба тераформа:
Unfortunately what you observed here does seem to be Terraform working as designed, though I agree it's an annoying edge to the design.

The root problem here is that because map elements must have unique keys Terraform cannot know the number of elements in any map constructed with unique keys: Terraform has no way to prove that two of the keys won't end up having the same value.

Furthermore though, even ignoring the issue of number of elements for for_each in particular the map keys become part of the tracking address of each instance and so they must each individually be known during planning so Terraform can correlate with instances with the same keys during the apply phase.

So as commonly recommended in these situations, the answer is to set the map keys to include only known values and place unknown values such as a server-generated ID only in the values of the map elements. Terraform will then have the tracking keys it needs but can still take into account the unknown ID for use when populating arguments of the resource.



таким образом проблему можно решить двумя путями:  
это не передавать между модулями инфо через list а передавать через map. это мгновенно решает проблему.
и есть еще полуколхозный способ - если мы хотим передавать даные именно через list переменную то тогда 
надо вначале создать все ресурсы  через первый модуль. а уже толкьо потом запускать создание ресурсов 
на второй модуле. после первого шага так как ресурсы уже созданы то тераформ точно знает сколько и чего 
будет передано во второй модуль. но это колхоз решение а нормальеное решение я уже скааал - передвать  данные 
между модулями через map перемнную.





[ загрузка модуля из гитлаба ]
[ как заставить первым отрабатывать root модуь и только потом осталные модули????  ]
[ yandex security group? что за хрень? ]

1.3 null_provider и null_resurce
как поставить и юзать. для игры с переменными

1.4 list vs set ?
1.2 что за станный тип переменной?
variable "user_information" {
  type = object({
    name    = string
    address = string
  })
  sensitive = true
}
(https://developer.hashicorp.com/terraform/language/values/variables#set)


1.3  попробвать var = {} вместо var = ""
1.4 depends_on = ?
https://www.scalr.com/blog/terraform-locals#:~:text=How%20do%20locals%20differ%20from,more%20meaningful%20or%20readable%20result.

1.r There is also a concept of variables on Terraform which can be used to assign dynamic values, 
1.3 терраформ console ?
1.5 как дестроить ресурсы = https://spacelift.io/blog/how-to-destroy-terraform-resources

1.6 что такое локальные перменные 
( https://spacelift.io/blog/terraform-locals,
https://www.scalr.com/blog/terraform-locals#:~:text=How%20do%20locals%20differ%20from,more%20meaningful%20or%20readable%20result.)

1.1 импортировать полностью всю структуру в тераформ из папки test-folder
1.4 импортироровать структуру из папки prod 
3.4 проработать на практие опреацию рашинеия диска системного и загрузочного.
так чтбы четко видно что ничего нестирается при этом.
1.1 как создать стат internet IP и прикрепить его к виртулке в тераформ
1.3 стопим виртлку. через веб мрду расширяем размер бут диска. стартуем вирталку.
импортируем изменнеие в тераформ. убеждаемся что ничего не сломано.
1.4 создаем в тераформ бутовый диск отдельно через ресурс. потом уже его добавляем как 
бутовый в виртуалуй с опцией что нельяз удалять. если мы удалим эту вирталку то у нас останется id 
этого диска и мы сможем его перепокдлючить либо удалить черзе тераформ.
далее два варианта: первый это снимаем снэпшт или имадж с диска , и подключаем к другой виртулке ( это все
чере тераформ. ) правим этои диск , отключаем и создаем новую виртуалку. второй вариант удаляем 
виртуалку. берем этот диск пдключаем к дургой виртулке , правим его, отключаем, созаем новую виртуалку.
1.5 научится импортировать впц+субнеты+вм
2. два сервис акаунта для управления одной папкой
3. dynamo + запрет на одновременный модификация двух юзеров + проверить + описать
3.a [ создать кастом диск с lvm      ]
    [ импорт инфрастрктуры в main.tf ] 
3.1 записать сюда мой main.tf 
a.2 модули в тераформ ?
a.3 output что за хрень ?
0.0 как с переменными работать
variable "region" { default = "us-east-1" }
variable "availability_zone" { default = "us-east-1a" }
variable "instance_type" { default = "t2.micro" }
variable "instance_count" { default = "2" }
variable "key_name" {}
variable "public_key_path" {}
variable "connection_user" { default = "ec2-user" }
variable "name" { default = "web" }
variable "environment" { default = "production" }
provider "aws" {
  region = "${var.region}"
}
module "web" {
  source = "../modules/web"
  availability_zone = "${var.availability_zone}"
  connection_user = "${var.connection_user}"
  instance_count = "${var.instance_count}"
  instance_type = "${var.instance_type}"
  key_name = "${var.key_name}"
  public_key_path = "${var.public_key_path}"
  region = "${var.region}"
  web_security_groups = "${module.sg.security_group_id}"
  name = "${var.name}"
  environment = "${var.environment}"
}
module "sg" {
  source = "../modules/sg"
  name = "${var.name}"
  environment = "${var.environment}"
}
module "elb" {
  source = "../modules/elb"
  availability_zones = "${var.availability_zone}"
  name = "${var.name}"
  environment = "${var.environment}"
  web_instance_ids = "${module.web.web_instance_ids}"
}
output "elb_dns_name" {
  value = "${module.elb.dns_name}"
}
0.1 как же делать бэкап бакета? 



| далее. как поставить terragrunt
на убунту 18 его нет в пакетах. в snap он есть но как тосука глючить и неработает от имени не рута
поэтмоу ставим вот так

$ wget https://github.com/gruntwork-io/terragrunt/releases/download/v0.18.7/terragrunt_linux_amd64
$ mv terragrunt_linux_amd64 terragrunt
$ chmod +x terragrunt
$ sudo mv terragrunt /usr/local/bin


===
terraform

вот так импорт не прокатит

$ terragrunt import yandex_compute_disk.this["disk2"]  fhml8oqvisq4n97357f4

а вот так прокатит

$ terragrunt import yandex_compute_disk.this[\"disk2\"]  fhml8oqvisq4n97357f4
либо вот так

$ terraform import   module.vm_3_disks.yandex_compute_disk.this[disk2]  fhmvdn94c1rjr9rr8hhq

====
| как посмотреть подробности конкретного ресурса.
вначале смотрим название ресурса. 
$ terrafrom state list

а потом уже смотрим подробности ресурса
$ terraform state  show  yandex_compute_image.image-1

# yandex_compute_image.image-1:
resource "yandex_compute_image" "image-1" {
    created_at    = "2023-04-11T18:33:01Z"
    description   = "это заготовка шаблон болванка внутри lvm вольюм пустой. тип диска hdd"
    folder_id     = "b1gnaqbm3b48e5d97fjj"
    id            = "fd8s30e0ji2qahoga0kc"
    labels        = {}
    min_disk_size = 15
    name          = "image-distr-hdd-lvm"
    pooled        = false
    product_ids   = []
    size          = 0
    status        = "ready"

    timeouts {}
}


а вот еще прикол
если есть ресурс с именем
terraform state show   module.vm_12_disks.yandex_compute_disk.this["disk2"]

то "disk2" надо экранировать \"disk2\" при просмотреть стейта иначе выдаст ошибку
тоесть: 

$ terraform state show   module.vm_12_disks.yandex_compute_disk.this[\"disk2\"]
# module.vm_12_disks.yandex_compute_disk.this["disk2"]:
resource "yandex_compute_disk" "this" {
    block_size  = 4096
    created_at  = "2023-04-11T19:03:41Z"
    description = "fw-venlog-proxy   LVM(/)"
    folder_id   = "b1gnaqbm3b48e5d97fjj"
    id          = "fhm3ustqg1u98kvevqvg"
    image_id    = "fd8s30e0ji2qahoga0kc"
    name        = "fw-venlog-proxy---hdd-1-lvm"
    product_ids = []
    size        = 15
    status      = "ready"
    type        = "network-hdd"
    zone        = "ru-central1-a"

    disk_placement_policy {}
}


====
|plan 
|apply
проверить план
$ terraform validate && terraform fmt && terraform plan  -var-file=folder-specific-vars.tfvars  -out "plan.txt"
запустить план
$ terraform apply "plan.txt"


====
@@@@@@@@@@@@@@
@@@@@@@@@@@@@@

TERRAGRUNT

| mock_outputs

у него есть так называемые mock_outputs это типа  которые терагрунт будет использовать в модуле
если этот модуль зависит от другого модуля а тот модуь еще не установлен и от того модуля этот модуль использует outputs того модуля.
например есть модуль А = этот модуль создает диски. есть модуль  Б = он создает виртулку.  модуль Б исольщует id дисков которые
он считыает из outputs модуля А. если модуль А еще не был установлен то модулю Б считыать нечего и терграунт выдаст ошибку.
так вот можно либо вначале установит модуль А что неудобно так как у нас установка разбиватеся на две итерации либо 
в модуле Б пропиать mock_outputs которые терагрунт заюзает пока модуль А не установлен.  что по факту это дает = по факту это 
порсто костыльи полная параша. эта штука используется когда мы запускаем  terragrunt plan и вот чтобы план нам ошибку не выдал
то испольщуют mock_outputs. и тоода план нам невыдаст ошибку что модуль А ещне устанолвен и поэтому модуль Б не может быть коректно
устанолвен. вместо этого тераргрунт выдаст что все будет окей. пример


dependency "vpc" {
  config_path = find_in_parent_folders("vpc")
  mock_outputs = {
    vpc_id      = "temp-id"
  }
}
inputs = {
  vpc_id = dependency.vpc.outputs.vpc_id
}


в этом примере мы говорим что нашу модуль зависит от модуля "vpc". и так как ог еще не установлен то мы укзыаем
какиое фейковое значение для переменной  vpc_id надо юзать на стадии terragrunt plan.




то есть эта штука исполщуется для того чтобы план не выдал ошибку а на сталии уже apply будет устанолвен модуль А 
и модулю Б будут скормлены уже реальные переменные а не вот эти постановочные.

проблема этих мок аутпутс в том что план нам напиешт что будет сделано одно а по факту при  apply будет сделано чтото совсем 
другое.   поэтому этот костыль это полная параша и фундаментальня проблема терагрнутна. если в тераформе мы прорисали зависимые модули
и начнем их накатыать то тераформ на стаии плана нам ошибок писать не будет . он понимает что  модули еще не установлены и будут устанолвены позже. а терагрунт этого н епонимает и неуммет с этимкоректно работать.




===========
| for_each, for, dynamic 
супер тема.
ссылка которая поомгла разобоарться 
https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9

значит о чем речь.

во первых надо разобраться как работае цикл "for" в тераформе
для  этого поможет код:

$ cat main.tf 

#######

variable "vasya" {
    description = "1212"
    type = string
    default = "1"
}


output  "vasya" {
  value = var.vasya

}

#########


###

variable "hero_thousand_faces" {
  description = "map"
  type        = map(string)
  default     = {
    neo      = "hero"
    trinity  = "love interest"
    morpheus = "mentor"
  }
}

locals {

 bios = [for key, value in var.hero_thousand_faces : "key=${key}, value=${value}"]
}


output "bios" {
   value  = local.bios
}

output "bios2" {
  value = {for key, value in var.hero_thousand_faces : key => value}

}


####


variable "hero_thousand_faces2" {
  description = "map"
  type        = map(string)
  default     = {
    neo      = "hero"
    trinity  = "love interest"
    morpheus = "mentor"
  }
}



locals {
 bios3 ={ for key, value in var.hero_thousand_faces2 : key => value }
}

output  "bios3" {
   value = local.bios3
}


###

variable "p2" {
    description = "1212"
    type = map(any)
    default = {
                 disk2:{
                         "dpg_name": "123"
                         "size"    : "50"
                 },

                 disk3:{
                         "dpg_name": ""
                         "size"    : "10"
                 },
                 


    } 
}



output  "p2" {
  value = var.p2

}


output "p3" {
  value = { for key, value in var.p2: key => value if value["dpg_name"] != ""  } 

}



запускай смотри разбирайся.




далее следующий вспомогательный кусок


$ cat provider.tf 
###################################
##   general 
##################################


terraform {
  required_version = ">= 0.13"




required_providers {
    null = {
      source = "hashicorp/null"
      version = "3.2.1"
    }
  }



  backend "local" { }

}




$ cat main.tf

##################


variable "dict1" {
  description = "map"
  type        = map(string)
  default     = {
    key1      = "value1"
    key2      = "value2"

  }
}



resource "null_resource" "dict1" {

for_each = var.dict1


  triggers = {
    value = each.key
  }

}



resource "null_resource" "dict2" {

for_each = var.dict1


  triggers = {
    value = each.value
  }

}


##################################



variable "list1" {
  description = "set"
  type        = list(string)
  default     = [
      "value1",
      "value2"
  ] 
}



resource "null_resource" "list1" {

for_each = toset(var.list1)


  triggers = {
    value = each.key
  }

}



resource "null_resource" "list2" {

for_each = toset(var.list1)


  triggers = {
    value = each.value
  }

}







вывод на экран:

 # null_resource.dict1["key1"] will be created
  + resource "null_resource" "dict1" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "key1"
        }
    }

  # null_resource.dict1["key2"] will be created
  + resource "null_resource" "dict1" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "key2"
        }
    }

  # null_resource.dict2["key1"] will be created
  + resource "null_resource" "dict2" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "value1"
        }
    }

  # null_resource.dict2["key2"] will be created
  + resource "null_resource" "dict2" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "value2"
        }
    }

  # null_resource.list1["value1"] will be created
  + resource "null_resource" "list1" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "value1"
        }
    }

  # null_resource.list1["value2"] will be created
  + resource "null_resource" "list1" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "value2"
        }
    }

  # null_resource.list2["value1"] will be created
  + resource "null_resource" "list2" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "value1"
        }
    }

  # null_resource.list2["value2"] will be created
  + resource "null_resource" "list2" {
      + id       = (known after apply)
      + triggers = {
          + "value" = "value2"
        }
    }





чтоздесь важно заметить. если у нас в for_each передется переменна типа dictionary то
индексами будут ключи этого дикшонари. тоесть вот у нас переменная


variable "dict1" {
  description = "map"
  type        = map(string)
  default     = {
    key1      = "value1"
    key2      = "value2"

  }
}


у нее ключи   key1 и key2 
и тогда  у нас создаются ресурсы с индеками key1 и key2


 # null_resource.dict1["key1"] will be created
 # null_resource.dict1["key2"] will be created
 # null_resource.dict2["key1"] will be created
 # null_resource.dict2["key2"] will be created


а если у нас в for_each передается переменная типа set (это как list только с уникальными значениями, без повторяющихся)
то индексами ресурса будут уже value поля
тоесть

у нас есть пременная 



variable "list1" {
  description = "set"
  type        = list(string)
  default     = [
      "value1",
      "value2"
  ] 
}




и вот такие индексы будут у ресусов



  # null_resource.list1["value1"] will be created
  # null_resource.list1["value2"] will be created
  # null_resource.list2["value1"] will be created
  # null_resource.list2["value2"] will be created


далее что интересно. неважно передалимы в for_each дикошонари или set у нас обрашение к элементам
этого for_each будет одно и тоже

each.key
each.value


разница только в тмо что если мы преедали в for_each дикшонари то 

в each.key будет содержаться var.key
в each.value буде содержатся var.value

а в случае коогда мы в for_each передали set то будет вот так

each.key=var.value
each.value=var.value

это все очень хорошо видно из вывода сверху.


знаменательно то что когда мы преедали в for_each set то обращаться к элементам можно хоть через each.key
хоть через each.value

тоесть вот у нас есть 


variable "list1" {
  description = "set"
  type        = list(string)
  default     = [
      "value1",
      "value2"
  ] 
}


мы говорим

for_each = toset (var.list1)
each.key у нас будет равен [value1, value2]
 each.value у нас будет равено тожему же самому [value1, value2]

 запомни это ! это ниже пригодится!




ТЕПЕРЬ ФОРМУЛИРУЮ ПРОБЛЕМУ:  задача научиться создавать диски у которых будет прописан disk placement group
теперь как это на практике


в root модуле создается диск плейсмент группа

resource "yandex_compute_disk_placement_group" "disk_pgroup_1" {
  name        = "ssd-spread-group-1-a"
  folder_id   = var.folder_id
  description = "группа размещения дле нерплицируемых дисков"
  zone        = var.zone
}


далее в root модуле мы задаем  переменную 
в которой прописаны параметры доп дисков для виртуалки
в которых  в частности укаываем имя  плейсмент группы куда надо поместить создаваемые диски



variable "vm_12_extra_disks" {
  description = "доп диски виртуалки"
  type        = map(any)
  default = {
    disk2 = {
      description = "корневой раздел,  диск 1 lvm vgs"
      name        = "hdd-rootp-1-a" # имя в веб морде
      type        = "network-hdd"
      size        = 250
      block_size  = 4096
      image_id    = ""
      snapshot_id = "fd83924vj8b7vbes2dp6"
      dpg_name    = ""

    },
    disk3 = {
      description = "корневой раздел , диск 2 lvm vgs"
      name        = "hdd-rootp-2-a" # имя в веб морде
      type        = "network-hdd"
      size        = 50
      block_size  = 4096
      image_id    = ""
      snapshot_id = "fd8pq4r8eq5bqqu3l43i"
      dpg_name    = ""


    }

    disk4 = {
      description = "swap"
      name        = "ssd-swap-a" # имя в веб морде
      type        = "network-ssd"
      size        = 10
      block_size  = 4096
      image_id    = ""
      snapshot_id = "fd8v9rs8gqteku4q220e"
      dpg_name    = ""

    }


    disk5 = {
      description = "нереплицируемый диск под мускул"
      name        = "unssd-mysql-1-a" # имя в веб морде
      type        = "network-ssd-nonreplicated"
      size        = 279
      block_size  = 4096
      image_id    = ""
      snapshot_id = ""
      dpg_name    = "ssd-spread-group-1-a"


    }



    disk6 = {
      description = "нереплицируемый диск под мускул"
      name        = "unssd-mysql-2-a" # имя в веб морде
      type        = "network-ssd-nonreplicated"
      size        = 279
      block_size  = 4096
      image_id    = ""
      snapshot_id = ""
      dpg_name    = "ssd-spread-group-1-a"

    }


 у каждого диска есть суб переменная она описывает помщеен ли диск в плейсмент групп
 или нет

если
     dpg_name    = ""                        # не помещен

если
     dpg_name    = "ssd-spread-group-1-a"    # диск находится в плейсмент группе 


таким макаром диск5 и диск6 при создании надо поместить в плейсмент группу с именем "ssd-spread-group-1-a"


осталось создать эти диски:
переходимв в код модуля который создает диски:


первый кусок

data "yandex_compute_disk_placement_group"  "dpg_names" {
   for_each   = { for key, value in var.extra_disks: key => value if value["dpg_name"] != ""  }
       name    = each.value["dpg_name"]
}




он берет переменную var.extra_disks проходит по списку дисков , вычленяет для каждого диска его плейсмент 
группу по ключу "dpg_name" а вот дальше самое интересное - если имя плейсмент группы пустое тоесть dpg_name=""
то тераформ говорит что плейсмент группы с таким имеенем несуществует. я пытался по разному и "" и null подставялять
он все равно ругается. поэтому единсвенныеый выход это чтобы for_each пробегался не по всем дискам а только 
по тем у которых dpg_name != "" для этого и испольуеся супехитрый блок с 

{ for key, value in var.extra_disks: key => value if value["dpg_name"] != ""  }

дополнительно замечу что для for_each данные которые он жрет должны быть обяазтельно dictionary то есть  {}
что мы и делаем. тоесть список [] ему по моему неподходит. надо уточнять.

теперь когда мы для каждого диска  узнали параметры ресурса его плейсмент группы через 

data."yandex_compute_disk_placement_group"


теперь можно подставлять id этой группы в диск при его создании
для этого имеем код в этом же модуле


resource "yandex_compute_disk" "this" {
  for_each         = var.extra_disks
  ...

dynamic "disk_placement_policy" {
    for_each = each.value["dpg_name"] != "" ? toset([each.key]) : toset([]) 

     content {
             disk_placement_group_id =  data.yandex_compute_disk_placement_group.dpg_names[disk_placement_policy.value].id
    }

}



}




здесь много всего интресного насовано. 
во первых у нас есть for_each а внутри него   присунут dynamic блок.
внутри этого dynamic блока  есть свой for_each , причем значение для этого for_each мы подставляем из предыдущего for_each 
в виде его текущего значения each.value - по мне это просто пиздец как сложно. тоесть тераформ понимает что 
each.value относится к головному for_each а не текущему. пиздец.

значит чему будет равыен каждый each.value от головного for_each 
так как в головной for_each подставляется дикшонари то 
он будет равено вот такому


for_each = {   disk2: {
      description = "корневой раздел,  диск 1 lvm vgs"
      name        = "hdd-rootp-1-a" # имя в веб морде
      type        = "network-hdd"
      size        = 250
      block_size  = 4096
      image_id    = ""
      snapshot_id = "fd83924vj8b7vbes2dp6"
      dpg_name    = ""

    },
}


где

each.key = disk2
each.value =   {
      description = "корневой раздел,  диск 1 lvm vgs"
      name        = "hdd-rootp-1-a" # имя в веб морде
      type        = "network-hdd"
      size        = 250
      block_size  = 4096
      image_id    = ""
      snapshot_id = "fd83924vj8b7vbes2dp6"
      dpg_name    = ""

    }


еще прмимер

for_each = { disk6 : {
      description = "нереплицируемый диск под мускул"
      name        = "unssd-mysql-2-a" # имя в веб морде
      type        = "network-ssd-nonreplicated"
      size        = 279
      block_size  = 4096
      image_id    = ""
      snapshot_id = ""
      dpg_name    = "ssd-spread-group-1-a"

    }
}


где 

each.key = disk6
each.value =    {
      description = "нереплицируемый диск под мускул"
      name        = "unssd-mysql-2-a" # имя в веб морде
      type        = "network-ssd-nonreplicated"
      size        = 279
      block_size  = 4096
      image_id    = ""
      snapshot_id = ""
      dpg_name    = "ssd-spread-group-1-a"

    }




и вот в свою очередь уже этот each.value подсталяется в строку:

for_each = each.value["dpg_name"] != "" ? toset([each.key]) : toset([])


тогда этот второй for_each будет принимать значения и вот тут самое интересное
мы  проверяем что текущее значение each.value["dpg_name"] от головного for_each оно непустое != "" 
b если оно пустое то 

for_each = []  и блок dynamic несоздается  


а если each.value["dpg_name"] непустое то  в for_each подсталяется уже не переенная типа dictionary 
а типа set


for_each = [ each.key ] 

где each.key это хренота от головного for_each то есть (смотрим наверх) оно равно disk5 и disk6
таким образом наш второй for_each принимает такие значения

for_each =  [ "disk5" ]
for_each =  [ "disk6" ]


и dynamic модуль создается 

теперь возникает вопрос как нам обратится к этому элементу for_each чтобы получить disk5 и disk6.
как я говорило в самом верху. если у нас for_each это set то  можно обратться двумя способами.
они оба дадут одно итоже . можно обраться как each.key а можно как each.value
в обоих случаях поулчим disk5 и disk6

ну я выбрал обратиться через each.value
тут еще всопмнимаем что если у нас for_each исполуется в блоке dynamic то обращаться к нему надо через each.*
а через имя блока dynamic тоеств нашем случае это disk_placement_policy
итого чтовы вытащить из for_each его занчение надо обраться как disk_placement_policy.value  
охренеть и тогда

disk_placement_group_id =  data.yandex_compute_disk_placement_group.dpg_names[disk_placement_policy.value].id  


по сути у нас будет подставлсят во такие значения

disk_placement_group_id =  data.yandex_compute_disk_placement_group.dpg_names["disk5"].id
disk_placement_group_id =  data.yandex_compute_disk_placement_group.dpg_names["disk6"].id



вот такой пирог пиздеца.

а все поому что нам надо отфильтровать лишнее в data.


====

| null_resource


рассмотрим готовый плейбук


# cat main.tf

variable "list3" {
  description = "map"
  type        = map(any)

  default     = {
      "disk1" = { 
            "image_id" = "1212321dwfwe"
            size     = "20"
            type     = "network-hdd"
      },

      "disk2" = {
            image_id = "1212321dwfwe"
            size     = "10"
            type     = "network-ssd"

  }

}
}



resource "null_resource" "list3" {
  for_each = var.list3


  triggers = {
      value = 1

   }






}






# cat provider.tf
###################################
##   general 
##################################



terraform {
  required_providers {
    null = {
      source = "hashicorp/null"
      version = "3.2.1"
    }
  }



  backend "local" { }



}

provider "null" {
  # Configuration options
}






так вот рассмотрим null_resource


resource "null_resource" "list3" {
  for_each = var.list3


  triggers = {
      value = 1

   }






}



так вот в чем залупа. она состоит в том что конкретно в этом ресурсе 
нихуя неработает dynamic блок. именно по архитектуре.


а неработает dynamic потому что dynamic работает только для БЛОКОВ!!!
а truggers это сука не блок!

щас покажу что такое блок

resource "yandex_compute_instance" "vm" {
    allow_stopping_for_update = true
    created_at                = "2023-05-04T13:35:35Z"
    description               = "lxd-04  lxd сервер. "
   
...

  secondary_disk {
        auto_delete = false
        device_name = "fhma7n7geru4phf39m1c"
        disk_id     = "fhma7n7geru4phf39m1c"
        mode        = "READ_WRITE"
    }



так вот secondary_disk это блок и поэтому к нему можно применить dynamic





triggers = {
      value = 1

   }


это сука не блок!!! я не знаю что это но это не блок. потому что формально у него есть знак "=".
поэтому сука это неблок. и поэтому к нему dynamic неприменить. блядь

как гворится умей правильно определять блок перед тобой  или нет


| далее.

еще раз публикую супер поленую штуку.
по ней можно хорошо научтииться понимать

for_each 

for key in ...



[vasya@lenovo 119]$ cat main.tf 
#######

variable "vasya" {
    description = "1212"
    type = string
    default = "1"
}


output  "vasya" {
  value = var.vasya

}

#########


###

variable "hero_thousand_faces" {
  description = "map"
  type        = map(string)
  default     = {
    neo      = "hero"
    trinity  = "love interest"
    morpheus = "mentor"
  }
}

locals {

 bios = [for key, value in var.hero_thousand_faces : "key=${key}, value=${value}"]
}


output "bios" {
   value  = local.bios
}

output "bios2" {
  value = {for key, value in var.hero_thousand_faces : "key=${key}" => "value=${value}"}

}


####


variable "hero_thousand_faces2" {
  description = "map"
  type        = map(string)
  default     = {
    neo      = "hero"
    trinity  = "love interest"
    morpheus = "mentor"
  }
}



locals {
 bios3 ={ for key, value in var.hero_thousand_faces2 : key => value }
}

output  "bios3" {
   value = local.bios3
}


###

variable "p2" {
    description = "1212"
    type = map(any)
    default = {
                 disk2:{
                         "dpg_name": "123"
                         "size"    : "50"
                 },

                 disk3:{
                         "dpg_name": ""
                         "size"    : "10"
                 },
                 


    } 
}



output  "p2" {
  value = var.p2

}


output "p3" {
  value = { for key, value in var.p2: key => value if value["dpg_name"] != ""  } 

}





###############################################################################################



##################


variable "dict1" {
  description = "map"
  type        = map(string)
  default     = {
    key1      = "value1"
    key2      = "value2"

  }
}



resource "null_resource" "dict1" {

for_each = var.dict1


  triggers = {
    value = each.key
  }

}



resource "null_resource" "dict2" {

for_each = var.dict1


  triggers = {
    value = each.value
  }

}


##################################



variable "list1" {
  description = "set"
  type        = list(string)
  default     = [
      "value1",
      "value2"
  ] 
}





resource "null_resource" "list1" {
  for_each = toset(var.list1)


  triggers = {
    value = each.key
  }

}


output "list1" {
   value = null_resource.list1

}


output "list1_for" {
   value = [for key, value in null_resource.list1: "key=${key} , value=${value.id}"  ] 

}

output "list1_for2" {
   value = [for key, value in null_resource.list1: "key=${key} , value=${value.triggers.value}"  ] 

}




variable "list3" {
  description = "map"
  type        = map(any)

  default     = {
      "disk1" = { 
            "image_id" = "1212321dwfwe"
            size     = "20"
            type     = "network-hdd"
      },

      "disk2" = {
            image_id = "1212321dwfwe"
            size     = "10"
            type     = "network-ssd"

  }

}
}



resource "null_resource" "list3" {
  for_each = var.list3


  triggers  = {
      value = 1

   }






}






$ cat provider.tf 

###################################
##   general 
##################################



terraform {
  required_providers {
    null = {
      source = "hashicorp/null"
      version = "3.2.1"
    }
  }



  backend "local" { }



}

provider "null" {
  # Configuration options
}








