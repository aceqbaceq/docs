pod affinity antiaffinity


что мы хотим. чтобы на одном куб хосте незапускалось 
два пода из одной группы например два мастера эластика.

положим мы имеем stateFullSet для подов мастера эластика
привожу кусок его statefull сета

в целом все строится на тех label которые имеют наши поды которые мы хотим чтобы сидели раздельно.
в моем примере ниже я опираюсь на label
      app.pods-role: masters
я хочу чтобы поды с этим label несидели по два на куб хосте

и я требую от куба чтобы два пода с таким лейбелом категорически
запрещено запускать на одном куб хосте

чтобы это заработало на в statefullset вставить вот этот кусок

как видно в примере я юзаю antiaffinity

      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.pods-role
                operator: In
                values:
                - masters
            topologyKey: "kubernetes.io/hostname"

все остальное в портянке для statefullset остается нетронутым

значит в строке 
	- key: app.pods-role 
	мы указвыаем имя лейбела пода
а в строке 
	- masters 
	указываем чему лейбел равен.
ну а этот ключ 
	topologyKey: "kubernetes.io/hostname" 
	означает что два пода с одинаковым лейбелом недолжен встречаться
	именно там где есть этот ключ то естть на хосте куба. типа того.



ниже я его вставил



#
# master nodes
#
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-master
  namespace: es-cl-01
spec:
  serviceName: masters-headless
  replicas: 3
  selector:
    matchLabels:
      app.name: elastic
      app.pods-role: masters
  template:
    metadata:
      labels:
        app.name: elastic
        app.pods-role: masters
    spec:
# кусок отвечающих за affinity antiaffinity
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.pods-role
                operator: In
                values:
                - masters
            topologyKey: "kubernetes.io/hostname"
# конец такого куска
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.7.1
        resources:
            limits:
              cpu: 2000m
              memory: 370Mi
и там далее продолжается портянка стейтфуллсета

что я выяснил.
вот у нас уже есть развернутые работающие поды скажем вот этих мастеров.
мы изменили стейтфуллсет как указано выше чтобы поды мастеров сидели 
строго по одному на куб хосте.
мы применяем обнолвенный стейтфулл сет. куб начинает убивать по одному
поды мастеров и пытаться их рескудьюлить по одному на хочты куба.
при этом pvc у подов остались прежниими и (ТУТ ВАЖНО) если у нас pvc
были привязаны к к PV(local) тоесть сторадж у нас несетевой а налокальных
дисках то куб несможет перетащить под на другой хост потому что pvc его тянет на старый хост а на старом он неможет запуститься потому что там уже например работает еще один такой же под. (ну имеется ввиду если изначально на одном хосте работало два пода мастера). как бы в целом
это наверно хорошо. потому что куб он же неможет перетащить несетеовой Pv с одного хоста на другой. и у нас нет потери данных неожиданных.
мы например можем ну хотя бы для себя забэкапить данные с исходного pv.
 а так решение дальше такое. удаляем pvc после этого удаляем под который
 неможет рескудюлбнутся. и тогда куб запустит под на другом хосте.
 при этом конечно данный под запуститься на пустом PV на том хосте.
 
 отсюда кстати видно насколько усложняет гибкость с подами локальный сторадж.
 
 но в целом хотя бы очень очень и очень хорошо что поды оказываются просто приваренными к pvc и в случае коогда pvc связан с PV(local) мы можем быть 100% уверреными что если будут какието перебтрубации
 то куб незапустит наш под на пустом PV на другом хосте и мы не получим
 неоиданную потерю данных. под либо станет в стоп колом либо он будет запущен ровно там где его родной PV и никак иначе. с сетевым хранилищем
 конечно этих проблем нет.
 
 ===
 следущий момент
 
 вот эта строка
	requiredDuringSchedulingIgnoredDuringExecution:
	она жестко требует сделать либо так либо никак

 вот эта строка
	preferredDuringSchedulingIgnoredDuringExecution:
	она говорит что желательно но не строго
==
далее

выше описано как вобязательном порядке требовать такого или сякого 
размещения подов

а вот этот кусок незасталвяет а просит по возможности

podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone

	
тут присутствует новый параметр 
	      - weight: 100

он варьируется от 1 до 100 и вменяемого обьяснение я неувидел.
муть какаято.

==
далее
	ссылки где про это все херовенько но написано
	https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
	https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md
==

