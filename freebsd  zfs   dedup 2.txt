| freebsd
| zfs
| dedup
| lz5
| lz4
| zstd


влияние 
     алгоритмов сжатия (lz5\zstd) 
     коэфициентов сжавтия
     рекордсайза
     дедпуликации

на то место ктоторое файлы занимают на диске



имеем изначальный размер 5ГБ и  я его закачиваю на разные датасеты


вначале сжатие lz4 на датаесеты с разными рекордсазййми


POOL-01/ds-lz4-64K       1.92G   287G  1.92G  /POOL-01/ds-lz4-64K
POOL-01/ds-lz4-128K      1.85G   287G  1.85G  /POOL-01/ds-lz4-128K     100%
POOL-01/ds-lz4-256K      1.82G   287G  1.82G  /POOL-01/ds-lz4-256K      98%
POOL-01/ds-lz4-1024K     1.79G   287G  1.79G  /POOL-01/ds-lz4-1024K     97%


тоесть я бы сказал на рекордсайз 128-1024К пофиг при сжатии.
ну чем больше рекордсайз тем чуть получше
тогда  я беру самый класый рекодрсайз 1М итеперь 
меняю аолгоритм сжатия на zstd причем в нем меняю коэфиц сжатия


POOL-01/ds-zstd3-1024K   1.26G   287G  1.26G  /POOL-01/ds-zstd3-1024K   68%
POOL-01/ds-zstd9-1024K   1.19G   288G  1.19G  /POOL-01/ds-zstd9-1024K   64%  (сжимает в 2 раза медл)


тоесть я бы сказал что zstd-3 что zstd-9 сжимает одинаково.
тогда я беру уже zstd-3 потому что он раоатет гораздо быстрее
и еще раз сраниваю на двух рекордсайзах


POOL-01/ds-zstd3-1024K   1.26G   287G  1.26G  /POOL-01/ds-zstd3-1024K   68%
POOL-01/ds-zstd3-128K    1.34G   287G  1.34G  /POOL-01/ds-zstd3-128K    72%



из чего видно что чем бльше рекордсайз тем лучше сжимает.
также что zstd побеждает lz4 (про скорось сжатия не сранивал).
также что zstd-3 и zstd-9 существенно не отлиатся от степени сжатия.
поэтому  в иттге можно юзаать zstd-3/9 RS=128K/1MB 
тоесть  я выяснил что гавное это заюзать zstd3\9 а не lz4
а ккокй будет рекордсайз 128К или 1М неважно. и какой будет zstd3 или 9 неважно.
результат будет примрно одинаковый в плане занятого места
тогда далее я выбираю zstd-3 потому что он быстрый.



теперь играюсь с подклчением дедуликации и разными рекорсайзами.

dedup=on,  128K, zstd-3

# zpool status -D POOL-01

bucket              allocated                       referenced          
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    10.5K   1.31G    361M    361M    10.5K   1.31G    361M    361M
     2    2.29K    293M    106M    106M    5.97K    764M    284M    284M
     4    3.71K    474M    163M    163M    17.8K   2.23G    779M    779M
     8        3    384K    332K    332K       34   4.25M   3.79M   3.79M
 Total    16.5K   2.06G    629M    629M    34.3K   4.29G   1.39G   1.39G


размер на диске 629M  
тоесть добавление дедуп сжало в 2 раза контент. 
тоесть замена lz4 на zstd-3 уменьшило обьем с 100% до 72%. на 28%. 
а добавление дедупа сжало еще в  2 раза. 
а суммарно zstd-3 + dedup сжал в 3 раза.

щас я пробегусь по разным рекордсайзам


bs=64K
alocated dsize=843M
тоесть стало хуже

bs=256K
alocated dsize=664M
как ни странно то тоже хуже

bs=512K
allocated dsize=966M
стало хуже

bs=1024K
allocated dsize=1.00G
хуже


итого дедуп лучше всего на 128К срабатывает
еще раз посмтрим на таблицу


bucket              allocated                       referenced          
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    10.5K   1.31G    361M    361M    10.5K   1.31G    361M    361M
     2    2.29K    293M    106M    106M    5.97K    764M    284M    284M
     4    3.71K    474M    163M    163M    17.8K   2.23G    779M    779M
     8        3    384K    332K    332K       34   4.25M   3.79M   3.79M
 Total    16.5K   2.06G    629M    629M    34.3K   4.29G   1.39G   1.39G



хочу поясить здесь столбцы
referenced LSIZE 4.29G  это сколько бы занимали данные если их записать
на датасет без дедупиликации и без сжатия. я выше написал что на фс я копировал 
изначально 5Г. а тут 4.29G это значит что внутри файлов которые я копировал есть дырки
и при копировании на зфс он эти дырки схлопывает.

allocated LSIZE 2.06G это сколько бы занимало на диске блоков если бы мы только включили
дедупликацию. 

referenced PSIZE 1.39G это сколко бы занимало на диске если бы мы только сжали наши
изначальные данные. 

refrenced DSIZE равна колонке referenced PSIZE

allocated PSIZE 629M это обьем блоков которые были сжаты а потом из них выкинули дубликаты.
это и есть тот оьем который даныные занимаюь реально щас на диске

allocated DSIZE тоже самое что  allocated PSIZE


получается какая схема. я заквыаивал на диск 5Г. за счет дырок  файлах на диске он 
бы мог занять 4.29Г без всяикх фокусов просто засчет спарс фичи zfs.
если бы я просто сжал эти файлы то они бы заняли 1.39Г тоесть в 3 раза.
если бы я просто дедуплицирвоал данные то они бы заняли 2.06Г тоесть в 2 раза.
а если я сжал а потом еще дедуплицировал (как и работает алогоритм) то занимает 629М
тоесть в  7 раз. а если вспоинмить что данных было 5Г а не 4.29Г то суммарное сжатие в 8 раз.
если данные уже лежат на зфс датасете сжатые как lz7 либо как zstd то дедупликация
сожмет еще в 2 раза.
общий вывод - выгодно внаале сжать через zstd-3 а потом еще накатить дедупликцию.
сжатие в 3 раза может сжать. а дедуп потом еще в 2 раза.


а вот я еще подкопировал данных на датасет
и вот они стат данные

# zpool status -D POOL-01

bucket              allocated                       referenced          
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    31.4K   3.93G   1.56G   1.56G    31.4K   3.93G   1.56G   1.56G
     2    27.1K   3.38G   1.45G   1.45G    73.1K   9.14G   4.02G   4.02G
     4    46.2K   5.77G   3.66G   3.66G     190K   23.8G   14.9G   14.9G
     8    5.06K    648M    251M    251M    47.5K   5.93G   2.30G   2.30G
    16      313   39.1M   22.5M   22.5M    5.34K    684M    386M    386M
    32        1    128K     16K     16K       32      4M    512K    512K
 Total     110K   13.7G   6.94G   6.94G     348K   43.5G   23.2G   23.2G


для анализа нам нужна последняя строчка

bucket              allocated                       referenced          
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
 Total     110K   13.7G   6.94G   6.94G     348K   43.5G   23.2G   23.2G



здесь мы видим вот что сырые данные которые мы копировали на стораж это 43.5ГБ.
скажу сразу что это 50ГБ. просто получается внутри файлов есть спарс дырки.
если их просто сжать через zstd-3 мы получим на диске 23.2ГБ
а убирание из этих сжатых блоков повторов нам дает 6.94ГБ за счет дедупликации.
тоесть дедуликация еще в 3 раза выиграла нам места по сравнению с просто сжатием.


еще хотел про первуб колонку поовоговрить

bucket     
______   
refcnt   
------   
     1   
     2   
     4   
     8   
    16   
    32   


о чем она? она о том сколько раз на тот или  иной блок ссылаются блоки на фс.
тоесть у нас есть блоки которые встречаются только  1 раз, есть которые паоторяются  2 раза.
итак далее и до блоков котоыре встрятся 32 раза.
тоесть


bucket              allocated                       referenced          
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    31.4K   3.93G   1.56G   1.56G    31.4K   3.93G   1.56G   1.56G
     2    27.1K   3.38G   1.45G   1.45G    73.1K   9.14G   4.02G   4.02G
     4    46.2K   5.77G   3.66G   3.66G     190K   23.8G   14.9G   14.9G
     8    5.06K    648M    251M    251M    47.5K   5.93G   2.30G   2.30G
    16      313   39.1M   22.5M   22.5M    5.34K    684M    386M    386M
    32        1    128K     16K     16K       32      4M    512K    512K
 Total     110K   13.7G   6.94G   6.94G     348K   43.5G   23.2G   23.2G



у нас 31.4К блоков которые уникальные встрчатся 1 раз
      27.1К блоков которые встрются два раза итд 
до  1-го блка который встречается 32 раза. 
тоесть это вот такая статичстика. из сатсистикии видно что есть 27 тыш блоков котоыре встре
чаются 2 раза и 46тыщ блоков которые встрчатся аж 4 раза. вот за счет них дедуликация 
и сработала круто.


как погять солкьо памяти занимает таблица дедуплкиации.
берем общее число уникальных блоков это 110К и умножаем на 330 байт.

    110 000 * 392 = 41 МБ


    vm.uma.ddt_entry_cache.size: 392  (размер одной записи)

    vm.uma.ddt_entry_cache.stats.fails: 0  (хватает ли памяти. ответ да)



