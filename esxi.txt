чтобы в esxi создать виртуалку и поставит на нее esxi - nested esxi

посоле создания виртуалки нужно зайти через путти на фс и в свойствах имя-виртуалки.vmx
добавить две строки

vhv.enable = "TRUE"
hypervisor.cpuid.v0 = "FALSE"

--


DEVICE                                PATH/WORLD/PARTITION DQLEN WQLEN ACTV QUED %USD  LOAD   CMDS/s  READS/s WRITES/s MBREAD/s MBWRTN/s DAVG/cmd KAVG/cmd
mpx.vmhba1:C0:T0:L0                            -              32     -    0    0    0  0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00
mpx.vmhba1:C0:T1:L0                            -              32     -    0    0    0  0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00
mpx.vmhba32:C0:T0:L0                           -               1     -    0    0    0  0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00




клинируем флэшку с esxi установленным

~ # dd if=/dev/disks/mpx.vmhba1:C0:T0:L0 of=/dev/disks/mpx.vmhba1:C0:T1:L0 bs=1M conv=notrunc
24576+0 records in
24576+0 records out

--
esxi установленный на флэшку грущится через загрузчик syslinux

---

верзние две строки покаывзают только ven dev

чтоб узнать гораздо больше про иодеди флэшек надо заюзать

lspci -v


надо правильно тключичить usb arbirtaror.
перезагрузиться. 

~ # chkconfig --list | grep usb
usbarbitrator      on

# /etc/init.d/usbarbitrator stop
# chkconfig usbarbitrator off


и только потом можно на флэшку записать образ



как в esxi найти название диска который USB флэшка с которой он грузится

# esxcfg-info -s | grep -A10 "Diagnostic Partition"
         |----Name..................................................mpx.vmhba32:C0:T0:L0:9


значит путь к названию диска нашей флэшки /dev/disks/mpx.vmhba32:C0:T0:L0

когда мы  знаем имя диска 
то можем например посмотреть его размер

 # esxcli storage core device list
mpx.vmhba32:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba32:C0:T0:L0)
   Size: 7453
   

~ # esxcli storage core device list | grep -E "vmhba|USB|Size|Vendor"
   Size: 160219
   Queue Full Sample Size: 0
   Is USB: false
   Is Boot USB Device: false
mpx.vmhba33:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba33:C0:T0:L0)
   Size: 7453
   Devfs Path: /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0
   Queue Full Sample Size: 0
   Is USB: true
   Is Boot USB Device: true
   Size: 160219
   Queue Full Sample Size: 0
   Is USB: false
   Is Boot USB Device: false
mpx.vmhba32:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba32:C0:T0:L0)
   Size: 15064
   Devfs Path: /vmfs/devices/disks/mpx.vmhba32:C0:T0:L0
   Queue Full Sample Size: 0
   Is USB: true
   Is Boot USB Device: false

виден size в MB кстати обоих флэшек ( чтобы было понятно хватит ли флэшки для копирования )

~ # ls -1 /dev/disks/
mpx.vmhba32:C0:T0:L0
mpx.vmhba32:C0:T0:L0:1
mpx.vmhba33:C0:T0:L0
mpx.vmhba33:C0:T0:L0:1
mpx.vmhba33:C0:T0:L0:5
mpx.vmhba33:C0:T0:L0:6
mpx.vmhba33:C0:T0:L0:7
mpx.vmhba33:C0:T0:L0:8
mpx.vmhba33:C0:T0:L0:9


!!!!перед тем как клонироваь нужно в 
/bootbank/boot.cfg добавить ключ overrideDuplicateImageDetection 
и только потом клонируем.!!!!

# dd if=/dev/disks/mpx.vmhba33:C0:T0:L0 of=/dev/disks/mpx.vmhba32:C0:T0:L0 bs=1M  conv=notrunc

почемуто при копироании с диска на диск обязательно нужен с ключом conv=notrunc

two filesystems with the same UUID have been detected.
Make sure you do not have two ESXI installations

помогает что когда уже начинается заргрзука syslinux тыкаем Shift+o

внизц будет видна строка через которую syslinux грузит  esxi
и там надо дбавить слово overrideDuplicateImageDetection - его хватает только на  1 раз до перезагрузки.

надо понять где его можно вбить в загрузчик.

и еще хотя я гружусь с нровой флэшки но esxi в итоге монтирует себе старую флэшку.


---
полезная команда

 # esxcli storage vmfs extent list
Volume Name                               VMFS UUID                            Extent Number  Device Name                           Partition
----------------------------------------  -----------------------------------  -------------  ------------------------------------  ---------
local-0.104-156GB-ssd-r1-2dx159GB-first   4ce31cd8-b4afa840-4c52-00215ad4969e              0  naa.600508b1001c2463f2c7556ed2f0a7ae          1
iscsi-0.102-4.6TB-zfs-4hdd                5d650939-22ae5f1f-93ec-70106fb182b2              0  naa.6589cfc00000084e913aa61f9f14882d          1
local-0.104-156GB-ssd-r1-2dx159GB-second  4ce31cc2-3c75e0d0-ae46-00215ad4969e              0  naa.600508b1001c9c9f5e7f6e6d1e7f2c87          1
iscsi-0.202-4.3TB-zfs-4hdd                5e3fbe43-843521ba-b352-000af753f904              0  naa.6589cfc0000001b2475dddbe6c9464f8          1


----

посмотреть каеие флэшки воткнуты  хост

~ # lsusb
Bus 002 Device 003: ID 8564:1000
Bus 002 Device 002: ID 8564:1000
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
~ #

----

связь между устройство и именем раздела на volumes

~ # esxcfg-scsidevs -f
mpx.vmhba33:C0:T0:L0:5                                           /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0:5 cd284aef-38fd9052-5223-9a934dcd7dea
mpx.vmhba33:C0:T0:L0:6                                           /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0:6 73f22ee4-75bac953-b061-0a4f3e00df82
mpx.vmhba33:C0:T0:L0:8                                           /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0:8 5d5fd29a-dede4163-bed0-000af753f904
~ #


отсюда мы видим что раздел 5 на устройстве mpx.vmhba33:C0:T0:L0 смонтирован 
в папаку /vmfs/volumes/cd284aef-38fd9052-5223-9a934dcd7dea

freebsd неможет примонтировать fat раздел 5-ый и 6-ый
приходится грузится в линукс ubuntu rescue. в нем нет vi но есть nano

--
научитьс япользлваться syslinux чтоы понимать как его стартовать руками парамтеры после boot

-
тажке надо понять нужно ли GPT таблицу править у диска на который скопировали.
-
как я понялв esxi ты фиг подмоириуешь руками даже vfat 32 все что он находит он монтирует автомтаом
в папку /vmfs/vilumes/UUID

но типа можно скпировать без мтонтирования.

--
# gpart recover можно делать.
оно непортир татблицк разделов.
а просто в конце флэшки свободное место создает
--

# fstyp -lu /dev/ada1p5
msdosfs


# file -s /dev/ada1p5
/dev/ada1p5: DOS/MBR boot sector, code offset 0x3c+2, OEM-ID "MSDOS5.0",
sectors/cluster 8, reserved sectors 2, root entries 512, Media
descriptor 0xf8, sectors/FAT 250, sectors/track 32, heads 64, sectors
511968 (volumes > 32 MB), serial number 0x4e781016, label: "
", FAT (16 bit)

---
в общем я точно выяснил что проблема в самом фрибсд в его утилите mount_msdosfs 
она почемуто неможет смонтровать vfat разделы от esxi,

из по убунту все безрпоблем

--
надо попроавить gpt таблицу на 104-м на первой флешкек
--

разобраться можно ли как то по быстрому менять UUID на флэшке esxi
--


~ # esxcfg-scsidevs
esxcfg-scsidevs <options>
                           provides paths to.
-o|--offline-cos-dev       Offline the COS device corresponding to this vmkernel
                           device.
-n|--online-cos-dev        Bring online the COS device corresponding to this vmkernel
                           device.
---


No of outstanding IOs with competing worlds: 32
какая товажная опция как я помн=ю которуб можно анстрвиатьав

--


как идентифицировать флэшки
а именно связать номер mpx и название флэшки.
потому что копируем мы по mpx  пути. а потом в биосе надо будет выбирать флэшку по имени




~ # esxcfg-scsidevs -a
vmhba32 usb-storage       link-n/a  usb.vmhba32                             () USB
vmhba33 usb-storage       link-n/a  usb.vmhba33                             () USB
vmhba34 usb-storage       link-n/a  usb.vmhba34                             () USB


# lsusb -v

  iManufacturer           1 Generic
  iProduct                2 Ultra Fast Media Reader
  iSerial                 3 000002660A01

  iManufacturer           1 JetFlash
  iProduct                2 Mass Storage Device
  iSerial                 3 EN49NYR8

  iManufacturer           1 JetFlash
  iProduct                2 Mass Storage Device
  iSerial                 3 95WW3Z45F04MVNFS
 

~ # esxcli storage core device list

mpx.vmhba33:C0:T0:L0
   Size: 7452
   Vendor: JetFlash
   Model: Transcend 8GB
   
   
mpx.vmhba32:C0:T0:L0
   Size: 15064
   Vendor: JetFlash
   Model: Transcend 16GB
    


mpx.vmhba34:C0:T0:L0
   Size: 256
   Vendor: HP iLO
   Model: LUN 00 Media 0
   
   
текущая загрузочная флэшка
   # esxcfg-info -s | grep -A10 "Diagnostic Partition"
         |----Name..................................................mpx.vmhba33:C0:T0:L0:9

   
   значит сейчас мы грузится с той которая 7ГБ 
   а новая флэшка  та которая на 15ГБ
   
   
----------------
   
надо научитьс владеть uefi биосом
---

очень много подробной информации здест можно найти о хосте

 esxcfg-info -s |
 
 ====
 
надо разоабрстяь с uefi как им пользоваться.
как с его команднгой строкой рьроатта

и как ставить syslinux чо он умеет его boot строка итд

--
если режим загрузки UEFI  то когда мы потом смотрим в esxi
то в нем видно что для него загрузочная флэшка ровно та с которую мы указали
для загрузки в биосе.


====
чтото я сделал нетак во время перезагрузки esxi хостов.

в итоге у меня в вцентре появились виртуалки серые со статусом "orphaned"
также у меня нетолько в вцентре но и если на хост зайти появилась виртуалка серая без имени
а вместо имени unaccessible написано.


начну со второго. если вместо имени втиртуалки написано unaaccessible.
это проблема на самом хосте.
у него есть файл 

~ # cat /etc/vmware/hostd/vmInventory.xml

<ConfigRoot>
  <ConfigEntry id="0000">
    <objID>1</objID>
    <vmxCfgPath>/vmfs/volumes/4ce31cd8-b4afa840-4c52-00215ad4969e/core-dc-02/core-dc-02.vmx</vmxCfgPath>
  </ConfigEntry>
  <ConfigEntry id="0006">
    <objID>8</objID>
    <vmxCfgPath>/vmfs/volumes/4ce31cd8-b4afa840-4c52-00215ad4969e/fs-1/fs-1.vmx</vmxCfgPath>
  </ConfigEntry>
  <ConfigEntry id="0010">


в нем как видно прописаны vmx виртуалок которые на нем зарегистрированы.
хост когда перезагрузился то ему надо из этих vmx файлов прочитать параметры виртуалок. в том числе
их имена.  если vmx файл недоступен для чтения то в инвентори хоста вместо имени этой виртуалки
будет написано unknown.

получается причина этого это то что хост неможет получить доступ к конкретному vmx файлу.
(https://kb.vmware.com/s/article/2172 )

надо проверить что все датасторы доступны для этого хоста.
далее надо зайти в vmInventory.xml и методом исключения сравнивая с инвентори хоста понять 
к какой конкретно виртуалке vmx файлу нет доступа.


теперь про первую проблемы. orphan виртуалки это проблема на вцентре. в его базе
указано что на таком то хосте зарегистрированар такая то виртуалка. а по факту ее там нет.
написано что такая херня может быть если мы вцентр выключили .а на хостах с виртуалками 
чтото шаманили.

отсюда главный вывод - вцентр из всех виртуалок надо выключать самым последним а включать 
первым.

если вцентр выключил то акукратно и минимально чтот делать на самих хостах.

я точно выключил вцентр рановато. возможно както полчилось что на хостах
поемуто часть виртуалок была разрегистрированна.

-----

смена пароля на esxi

есть хосты esxi которые управляются через vcenter

заходим на хост esxi и через командную строку меняем пароль на root от хоста

при этом связь с вцентр у хоста нетеряется.

можно даже сделать в вцентр disconnect\connect для хоста и он приконнектит его неспрашивая новый пароль.
процедура disconnect\connect абсолютно неопасна. не потеряются ни пулы ничего при реконнекте.

насоклько я понимаю потому что vcenter коннектится к хостам не под рутом а под vpxuser чтоли.
либо dcui юзером. а  на них то пароль неменялся.

значит далее. кто то зашел через клиент на хост.

как нам это увидеть. ответ никак.

если человек зашел через вцентр то это видно в sessions.
есь человек зашел напрямую на esxi хост через клиент то мы это в вцентр невидим. облом и жопа.

но. можно гарантированно выкинуть всех с хостов esxi то есть убить их сессии.
как это сделть.

нужно зайти на каждый esxi хост и в строке 

#  services.sh restart

это выкинет всех кто зашел через клиент напрямую на esxi хост.
если заранее сменит на хосте пароль то больше никто незайдет. 
также надо зайти на хст черз клиент и посмотреть какие там есть локальные юзеры. а 
то могли завести и в вцентр этого невидно.

----
получается в целом как это надо клонировать usb флэшку

1. отлючить usb arbitrator 
2. перезагрузится чтобы все флэшки отвалились от юсбарбитратора 
и были видны в ls -1 /dev/disks
3. определить имя mpx.vmbha флэшки с которой грузится esxi

# esxcfg-info -s | grep -A10 "Diagnostic Partition"
         |----Name..................................................mpx.vmhba32:C0:T0:L0:9

4. на этой флэшке модиицировать через vi /bootbank/boot.cfg и /altbootbank/boot.cfg 
добавив overrideDuplicateImageDetection

5. склонировать эту флэшку на другую флэшку

# dd if=/dev/disks/mpx.vmhba33:C0:T0:L0 of=/dev/disks/mpx.vmhba32:C0:T0:L0 bs=1M  conv=notrunc


6. загрузиться с freebsd iso и исправить GPT разметку через 
# gpt recover имя-флэшки

готово.

---

задача.
надо вставить флэшку в сервер.
и на нее перенести esxi с бутовой флэшки.

сделать это максимально удобно

будем клонировать флэшку через виртуалку к которой приаттачим новую флэшку

1.  определяем какая флэшка является загрузочной. потому что флэшек
вставленных в сервер может быть миллион

# esxcfg-info -s | grep -A10 "Diagnostic Partition"

mpx.vmhba32:C0:T0:L0

1.5 на загрузочной флэшке на всякий случай прописываем опцию чтобы esxi
грузился даже если в сервер вставлено две одинаковые загрузочные флэшки

нужно в 
/bootbank/boot.cfg добавить ключ overrideDuplicateImageDetection 
и только потом клонируем.!!!!

и тоже самое прописать в  
/altbootbank/boot.cfg 

2. снимаем образ с загрузочной флэшки 

# dd if=/dev/disks/mpx.vmhba32:C0:T0:L0 of=/vmfs/volumes/iscsi-111.58-zfs-2xSSD-826GB/.backup-esxi-images/100-esxi_2020-0824 bs=1M  conv=notrunc

3. на сервере ставим freebsd

4. через scp копируем из esxi по сети снятный образ вовнутрь freebsd

5. вставляем новую флэшку в сервер.
 
6. на серверер должна быть запущена служба usbarbitrator.

$ /etc/init.d/usbarbitrator start

она делает вот что. если мы вставляем флэшку в сервер то эта флэшка
небудет доступна  на монтирования самой esxi (командная строка). эту 
новую флэшку перехватывает usbarbitrator както там изолирует. 
и она только доступна для того чтобы ее приаттачить к виртуальной машине.

если usbarbitrator остановлен то мы неможем приаттачить всталвенную флэшку
к втиртуальной машине

в целом пофиг вставить флэшку потом запустить службу или наоборот.
это все без разницы. просто в конечном итоге флэшка должна быть вставлена
и служба запущена.

если служба usbarbitrator выключена то приаттачить флэшку к виртуалке
неполучится.

7. идем в свойства вирт машины и добавляем usb контроллер типа "EHCI+UHCI"
у меня его было достаточно. 
там еще есть другой тип usb контроллера xHCI. он непонадобился.

8. еще раз заходим в свойства вирт машины причем виртуалка
должна быть выключена. и если все нормально то добавдляем устройство
"USB device".

по каким причинам для него может быть написано (unavailable).
что помогло мне. заходим на esxi командную строку и 

# services.sh restart

вот после этого "USB device" стал доступен. ура

все. новую флэшку мы приаттачили к виртуалке.

9. запускаем freebsd.

10. копируем через scp из esxi образ загрузочной флэшки.

# scp root@192.168.111.100:/vmfs/volumes/iscsi-111.58-zfs-2xSSD-826GB/.backup-esxi-images/110-esxi_2020-0824.img /img

10.5 идентифицируем во фрибсд устройство отвечающее за новую флэшку 

# camcontrol devlist

11. переносим на новую флэшку образ

# dd if=/img/110-esxi_2020-0824.img of=/dev/da1 bs=1M

12. поскольку флэшка другого размера то нужно у нее GPT таблицу отремонтировать

#  gpart recover da1

проверяем что таблица теперь нормальная

# gpart show da1

12.5
неплохо бы еще раз проверить что в boot.cfg есть опция overrideDuplicateImageDetection

на флэшке этот файл находится на 5 и 6 разделе. 
тип раздела это msdosfs. к сожалению во фрибсд какаято ошибка и она
немонтирует этот тип раздела. поэтому придется приаттачить флэшку
к линуксу.

13. по идее было бы класно протестировать что флэшка реально с нее 
можно грузиться. и зарузиться с нее на виртуалке.
просто так это сделать неполучится. но в целом это можно сделать.
нужно скачать plbt.iso 
далее мы на виртуалке грузимся с этого cd-rom iso. далее выскакивает меню
и в неи выбираем загрузку с USB флэшки.
и только тогда идет загрузка с приаттаченной флэшки.

---

посмотреть на каком устройстве лежит тот или иной vmfs массив

~ #  esxcli storage vmfs extent list
Volume Name                    VMFS UUID                            Extent Number  Device Name                           Partition
-----------------------------  -----------------------------------  -------------  ------------------------------------  ---------
iscsi-111.58-zfs-2xSSD-826GB   5e28f256-ec6bed36-98b6-009c029ff9ce              0  naa.6589cfc000000f658c2bd978f8e66bf8          1
111.120-850GB-ssd-r1-2dx950GB  5dfc35ca-48921add-5174-40a8f0287bf8              0  naa.600508b1001c8d8be9a24e20e9e5bd74          1
~ #


=====

как  найти связь между именем lun в hpssacli и esxi

в hpssacli

 # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0   ld 1  show

Smart Array P420i in Slot 0 (Embedded)

   array A

      Logical Drive: 1
         Size: 850.0 GB
         Fault Tolerance: 1
         Heads: 255
         Sectors Per Track: 32
         Cylinders: 65535
         Strip Size: 64 KB
         Full Stripe Size: 64 KB
         Status: OK
         Caching:  Enabled
         Unique Identifier: 600508B1001C8D8BE9A24E20E9E5BD74
         Logical Drive Label: 02E827EF0014380305E49A094E2
         Mirror Group 1:
            physicaldrive 1I:1:1 (port 1I:box 1:bay 1, Solid State SATA, 912.6 GB, OK)
         Mirror Group 2:
            physicaldrive 1I:1:2 (port 1I:box 1:bay 2, Solid State SATA, 912.6 GB, OK)
         Drive Type: Data
         LD Acceleration Method: Controller Cache

значит выяснилось что в esxi он будет иметь имя
         Unique Identifier: 600508B1001C8D8BE9A24E20E9E5BD74



=====

перенос esxi с флэшки на raid

1) overrideDuplicateImageDetection добавить 
vi /bootbank/boot.cfg
vi /altbootbank/boot.cfg


2) удалить старый массив
 # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2  delete  forced



3) создать новый массив 24ГБ под esxi
# /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 create type=ld drives=1I:1:3,1I:1:4 raid=1 size=24576 stripsize=default ssdsmartpath=disable

# /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2 modify  arrayaccelerator=enable

4) делаем наш новый lun загрузочным с точки зрения smart array контроллера
~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2 modify  bootvolume=primary
этого достаточно. потом в графический настройщик smart array вообще
заходить ненужно.
 # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 show | grep -i "primary boot volume"
   Primary Boot Volume: logicaldrive 2 (600508B1001CE9775BB12DF23B605083)



5) dd скопировать
вначале нам надо определить название устройства под которым esxi 
видит загрузочную флэшку
# esxcfg-info -s | grep -A10 "Diagnostic Partition"
получим
mpx.vmhba32:C0:T0:L0

далее надо определить название устройства под которым esxi 
видит новый lun

~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2 show |  grep Unique
         Unique Identifier: 600508B1001CE9775BB12DF23B605083

esxi увидит этот lun как /dev/disks/naa.600508b1001ce9775bb12df23b605083
тут важно отметить что у hpssacli все буквы большие 
а у esxi эти буквы маленькие. это имеет значение для esxi.


подставляем в dd
# dd if=/dev/disks/mpx.vmhba32:C0:T0:L0 of=/dev/disks/naa.600508b1001ce9775bb12df23b605083 bs=1M  conv=notrunc



7) partedUtil корректируем таблицу GPT
убеждаемся что на новом lun повреждена GPT таблица
 # partedUtil getptbl /dev/disks/naa.600508b1001ce9775bb12df23b605083
исправляем
 # partedUtil fix /dev/disks/naa.600508b1001ce9775bb12df23b605083
провеяем еще раз что сейчас все окей
 # partedUtil getptbl /dev/disks/naa.600508b1001ce9775bb12df23b605083



7.5) 
~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 modify cacheratio=0/100

7.6)
~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 modify drivewritecache=enable forced


8) грузимся чтобы окончательно проверить что с нового LUN со smart array контроллера мы успешно грузимся

9) по готовности отформатировать исходную  флэшку 

выяснилось. что если мы грузимся со smart array lun 
то esxi все равно както там в процессе грузится или чтото 
типа того с флэшки. это легко проверятеся через esxcfg-info -s | grep -A10 "Diagnostic Partition" после загрузки. возможно загрузчик какойто 
первичный на esxi ищет опреденный gpt раздел по guid и первым попадается
этот раздел с флэшки а не со smart array.
вывод - если мы грузиммся с raid1 lun smart array то мы неможем
иметь втоже время вставленную флэшку с таким же esxi загрузочным.

выяснисось что на 24ГБ отведенных на LUN под esxi оно реально
использует ~4GB места. остается ~ 20GB места на LUN
и это место доступно для создания стораджа из под vmware c# клиента.
пугаться ненадо. это нето место где лежит esxi это свободное 
место на том LUN так что датастор создавать можно.



===
оказалость что есть приложение для виндовс которое позволяет пожключаться
к ilo. это хорошо.э
==

прработать:
если есть usb с той же gpt а мы грузиммся с smart array то все равно
ESXI напишет что загрузился с usb

disabled external usb нельзя активровать 
ilo aplication standalone
textcons

у hp можно в биос деактивировать только внешние usb порты. 
внутрениие нельзя. в биосе путающее меню. 
деактивировать внешние usb называется "disable external usb ports"
а активировать их называется "usb enable".


походу вот еще в чем прикол. esxi блокирует запись на тот диск
с которого она загрузилась. только чтение разрешено
==
в биос есть "virtual install disk" опция.
как я понял это некий диск его бекенд мне непонятно где
он записан на мат плате сервера. но если его активировать в биос
то якобы на этом диске держат драйвера которые могут понадобиться
при установке ОС на сервер. главный вопрос как эти драйвера туда записать.
пока непонятно.
===

по поводу нумерации дисков LSI в сесии ssh и и в vcenter 

в команднрой строке esxi диски имеют нумерацию cтолбце EID
	4:0
	4:1
	итд



$ /opt/lsi/storcli/storcli  /c0 show

-----------------------------------------------------------------------
EID:Slt DID State DG     Size Intf Med SED PI SeSz Model            Sp
-----------------------------------------------------------------------
4:0       6 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:1       7 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:2      10 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:3       8 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:4      12 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:5      15 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:6      11 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U



но в вцентр в hardware status нумерация совсем другая.
первая цифра берется из EID тоесть 4.  а вторая цифра из DID, их 
вцентр слепляет и выводит в виде
	вместо 4:0 он выводит 4_6
	вместо 4:1 он выводит 4_7
	вместо 4:2 он выводит 4_10
	вместо 4:3 он выводит 4_8
	
вот такая связь. такая разница в выводе esxi и вцентр

---

есть такая задача. 
есть esxi
на нем стоит рейд контроллер
и в него воткнуты ssd диски.
надо узнать насколько износились ssd диски.

значит smart данные рейд контррллер невыдает 
поэтому smart хрень любая неработает.

а что работает.

в esxi можно посмотреть сколько байт было записано на lun
с момент старта сервера.
кога сервер будет перезагружен этот счетчик обнулится.

вот как это сделать

~ # esxcli storage core device stats get
naa.6589cfc000000f658c2bd978f8e66bf8
   Device: naa.6589cfc000000f658c2bd978f8e66bf8
   Successful Commands: 23061155
   Blocks Read: 248724493
   Blocks Written: 200529452
   Read Operations: 10296559
   Write Operations: 3218177
   Reserve Operations: 1
   Reservation Conflicts: 0
   Failed Commands: 117898
   Failed Blocks Read: 0
   Failed Blocks Written: 3376
   Failed Read Operations: 26031
   Failed Write Operations: 5244
   Failed Reserve Operations: 0

naa.600508b1001caf026adcbe0c1b91a50e
   Device: naa.600508b1001caf026adcbe0c1b91a50e
   Successful Commands: 10990299986
   Blocks Read: 45460020414
   Blocks Written: 119530291007
   Read Operations: 4608896091
   Write Operations: 6381012553
   Reserve Operations: 9140
   Reservation Conflicts: 0
   Failed Commands: 43413
   Failed Blocks Read: 0
   Failed Blocks Written: 0
   Failed Read Operations: 0
   Failed Write Operations: 0
   Failed Reserve Operations: 0

naa.600508b1001c2bdea2adb2c2515f744e
   Device: naa.600508b1001c2bdea2adb2c2515f744e
   Successful Commands: 5245128586
   Blocks Read: 65047388977
   Blocks Written: 358643072928
   Read Operations: 940332168
   Write Operations: 4304397066
   Reserve Operations: 13106
   Reservation Conflicts: 0
   Failed Commands: 43352
   Failed Blocks Read: 0
   Failed Blocks Written: 0
   Failed Read Operations: 0
   Failed Write Operations: 0
   Failed Reserve Operations: 0

mpx.vmhba32:C0:T0:L0
   Device: mpx.vmhba32:C0:T0:L0
   Successful Commands: 2684426
   Blocks Read: 99865084
   Blocks Written: 5771111
   Read Operations: 2101369
   Write Operations: 555853
   Reserve Operations: 0
   Reservation Conflicts: 0
   Failed Commands: 62040
   Failed Blocks Read: 0
   Failed Blocks Written: 0
   Failed Read Operations: 0
   Failed Write Operations: 0
   Failed Reserve Operations: 0

вот видно несоклько лунов.

берем лун
naa.6589cfc000000f658c2bd978f8e66bf8

нам нужен параметр

Blocks Written: 200529452

согласно вот этой статье https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.storage.doc/GUID-BB3B40D3-82D4-44D8-BD38-155129800B13.html
один блок = 512 байт.

поэтому умножаем Blocks Written: 200529452 на 512 байт 
и узнаем сколько байт было записано на lun  с момента старта сервера

в данном случае 95 ГБ.

аптайм этого сервера 534 дня.

таким образом можно оценить запиленность ssd дисков  штатными средствами

экспериментальным путем было выяснено
для конкретных серверов что 

sql база данных на raid10 из 4-ех дисков запиливает 1 ssd диск в год на 215 TB

флешка на которую установлен esxi и при этом логи льются не нафлэшку
а на другой лун , так вот такая флэшка в год запиливается на 2 GB\год

флэшку в списке лунов легко идентифицировать как чтото выглядящее как
mpx.vmhba32:C0:T0:L0

массив raid10 из 10 дисков на который постоянно пишутся картинки 
в нем 1 ssd диск в год запиливается на 35TB

для примера вот таблица endurance (tbw) дисков samsung

https://www.samsung.com/semiconductor/minisite/ssd/support/warranty/

860 pro 1TB имеет 1200 TBW

еще примеры endurance ssd дисков


SSDSC2KB96 (стали делать в 17 году это intel s4500 series)  = 1904 TBW

samsung 850 pro 1TB = 300 TBW входят в гарантию 

sSDSC2KG019T701 (серия s4600) = 11100 TBW = 37 730 ₽ 

SSDSC2KG019T801 (серия D3-S4610) = 9625 TBW = 38 850p

SDSC2KB019T801 (серия D3-S4510) = 6656 TBW = 29 370p

SEDC500M/1920G = 4555 TBW = 24 343p

XA1920LE10063 = 3500 TBW = 22 720p

SSDSC2KB019T701 (серия S4500) = 3348 TBW= 35 070 ₽

--

ssd Диски делаются на основе типпов ячеек
SLC, MLC, TLC, QLC

чем левее тем чип дороже быстрее и endurance больше.
чем правее то все гавнянее но дешевле.

еще эти ячейки они могут типа располагаться в 2D компоновке
и 3D компоновке. это влияет на endurance такого чипа.

то есть есть 2D MLC а есть 3D MLC.

зачем нам знать эту всю фигню. 
если производитель не привел сколько tbw для диска
то зная из каких ячеек сделан диск можно прикинуть вручную
сколько у него tbw.

вот здесь указано какие ячейки сколько циклов перезаписи выдерживают
https://searchstorage.techtarget.com/definition/TLC-flash-triple-level-cell-flash

вот еще табличка про количество циклов перезаписи ячеек
https://www.ni.com/ru-ru/support/documentation/supplemental/12/understanding-life-expectancy-of-flash-storage.html

но в целом ручной расчет endurance на основе типа ячеек лучше неделат
так как можно легко промахнуться а смотреть в spec у производителя
например 

sSDSC2KG019T701 (серия s4600) согласно спеку имеет 11100 TBW
согласно тому же спеку он сделан из 3D NAND TLC и должен 
иметь существенно меньший ресурс


еще пример 

sasmung 840 evo 1TB  для него непонятно какой tbw самсунг 
только дает что 3 года гарантия
https://www.samsung.com/semiconductor/minisite/ssd/support/warranty/

но мызнаем что он сделан на основе TLC как я понял просто TLC а не 3d TLC

для этих ячеек примерно они выдерживают 300 - 1000 циклов.
значит примерно samsung 840 evo 1TB  должен выдержать 500 tbw


еще пример.

вот пример берем диск SEDC500M/1920G = 4555 TBW
согласно спеку https://www.kingston.com/datasheets/dc500r-m_us.pdf 
он сделан из 3d TLC nand который типа имеет 3000 циклов перезаписи.
получается в теоррии его ресурс ~ 6000 tbw. 
однако согласно тому же спеку он равен 4555 tbw 


=======

esxi
vmware
iscsi
freenas
iops
satp

 есть такая проблема
 вот мы поставили freenas и через iscsi стали отдавать на esxi LUN.
 и еще мы отдаем по iscsi через multipath по round robin через несколько карточек.
 в чем проблема. по дефолту esxi забирает через карточку 1000 iops и только потом 
 переключается на следующую карточку на следующую 1000 iops.
 если мы хотим иметь много iops с низким лейтенси то такая схема гавно.
 гораздо лучше будет если esxi будет через карточку забирать 1 iops а следущий iops
 забирать через следующую карточку. замечу что такая схема это невина стораджа freenas 
 это вина клиента esxi
 
 вот этого и надо добиться. в интернете широко указано решение когда мы этого добиваемся
 но только для тех LUN которые уже подключены к esxi. такое решение гавно потому что 
 каждый раз когда мы будем подлкючать +1 новый LUN то для него нужно будет опять руками 
 запускать эту операцию.
 
 
 нам надо сделать чтобы для всех старых и новых LUN автоматом устанавливалась системе iops=1
 путем однократной настройки.
 
 
 во первых проверим через сколько iops на данный момент esxi переключается на следущую карточку
 
  # esxcli storage nmp  device list
  
  naa.6589cfc0000001b2475dddbe6c9464f8
   Device Display Name: FreeNAS iSCSI Disk (naa.6589cfc0000001b2475dddbe6c9464f8)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on; action_OnRetryErrors=off; {TPG_id=1,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=rr,iops=1000,bytes=10485760,useANO=0; lastPathIndex=3: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba34:C13:T1:L2, vmhba34:C1:T1:L2, vmhba34:C19:T1:L2, vmhba34:C7:T1:L2
   Is Local SAS Device: false
   Is USB: false
   Is Boot USB Device: false

  из вывод нам интересно вот это :
  
  {policy=rr,iops=1000,
  
  как видно автоматом esxi выставил iops=1000
  
  также для справки :

nmp = VMware Native Multipath Plugin (NMP). This is the VMware default implementation of the Pluggable Storage Architecture. 

satp = storage Array Type Plugins

 
 как же решить проблему. ответ - надо создать правило которое автоматом будет создавать уже нужный iops=1
 
 вот это правило:
 
 # esxcli storage nmp satp rule add -s "VMW_SATP_ALUA" -V "FreeNAS" -M "iSCSI Disk" -P "VMW_PSP_RR" -O "iops=1"
 
 здесь нужно знать что прописывать в параметрах
 
 -s
 -V
 -M
 -P
 
 узнаем что в них прописывать из вывода
 
   # esxcli storage nmp  device list
 
 
 
 -s  узнаем из строки  Storage Array Type:  
   
   Storage Array Type: VMW_SATP_ALUA
      
   поэтому -s = "VMW_SATP_ALUA"
   

 -P узнаем  из строки Path Selection Policy:

   Path Selection Policy: VMW_PSP_RR
   
   поэтому -P "VMW_PSP_RR"
   
   
 остается -V и -M. в целом оба эти параметра вместе прописаны в строке Device Display Name:
 ну а нам же надо их узнать по отдельности. для этого идем в с# клиент сферы - тыкаем сервер - 
 - configuration - storage adapters - rescan all
 потом заходим на сервер по ssh и ищем в логах
 
 # dmesg | grep -i freenas
 
 или 
 
 # cat /var/log/vmkernel.log | grep -i scsiscan | grep -i vendor
 
 в итоге мы увидим вот такие строки
 
 cpu4:33467)ScsiScan: 976: Path 'vmhba32:C0:T0:L0': Vendor: 'FreeNAS '  Model: 'iSCSI Disk      '  Rev: '0123'
 
 из которых мы узнаем что 
 
 
 Vendor: 'FreeNAS '  
 Model: 'iSCSI Disk      '
 
 
 из чего мы узнаем что 
 
 -V "FreeNAS" -M "iSCSI Disk"
 
 
 тут очередной прикол. невсегда пересканирование адаптеров и лунов дает нам строчки в логах.
 то дает то недает. поэтому возможный второй путь это вручную определять из строки
 
 Device Display Name: FreeNAS iSCSI Disk (naa.6589cfc0000001b2475dddbe6c9464f8)
 
 когда узнаю почему в логах невсегда можно найти Vendor и Model то напишу.
 
 
 далее еще один прикол и очень очень важный, я обжегся и потом нашел в инете то что несмотря на то что в логе названия идут с пробелами
 в командную строку эти проблемы вставлять категорически нельзя.
 
 поэтому правильно для подставновку в команду именно 
 
 -V "FreeNAS" -M "iSCSI Disk"
 
 без всяких лишних пробелов.
 
 единственная причина если в итоге правило несрабатывает это на практике только потому что Vendor
 и Model были указаны неправильно. и чаще всего изза лишних пробелов.
 итак я описал все ловушки.
 
  
 итого еще раз финальная строка
 
 
  
 # esxcli storage nmp satp rule add -s "VMW_SATP_ALUA" -V "FreeNAS" -M "iSCSI Disk" -P "VMW_PSP_RR" -O "iops=1"
 
 данное правило нужно вбивать на каждом хосте. 
 
 правило вступит в силу только после перезагрузки всего сервера.
 есть конешно теоритеический путь якобы как можно обойтись без перезагрузки esxi но 
 на практикето работает то нет. поэтому дальше надо перезагружать сервер.
 
 
 перезагрузились и запускаем 
 
 
 # esxcli storage nmp  device list
 
 
 и смотрим сколько там iops= указано для iscsi LUN-ов
 
 если все сработало то полуим вот такое
 
 
 naa.6589cfc0000001b2475dddbe6c9464f8
   Device Display Name: FreeNAS iSCSI Disk (naa.6589cfc0000001b2475dddbe6c9464f8)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on;{TPG_id=1,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=rr,iops=1,bytes=10485760,useANO=0; lastPathIndex=2: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba32:C8:T1:L2, vmhba32:C4:T1:L2, vmhba32:C0:T1:L2
   Is Local SAS Device: false
   Is Boot USB Device: false

нас интересует кусок

{policy=rr,iops=1

если мы перегрузились а вся эта шарманка несработала. 
то в каком то из параметров мы указали неверно. 

-s "VMW_SATP_ALUA"
-V "FreeNAS"
-M "iSCSI Disk" 
-P "VMW_PSP_RR"

чаще всего это -V и -M.

тогда чтобы это правило измениьт надо прежде всего удалить текущее правило.
без удаления новое правило недаст система вбить

удаляется оно вот так

# esxcli storage nmp satp rule remove -s "VMW_SATP_ALUA" -V "FreeNAS" -M "iSCSI Disk" -P "VMW_PSP_RR" -O "iops=1"


и после этого опять вбиваем новое правило, перегружаемся и проверяем сработало ли.

если все сработало то на этом все.

если нет то ,
далее чуть больше деталей.


как посмотреть наще правило и вообще какие правила уже есть на хосте.


~ # esxcli storage  nmp   satp rule list | grep -i freenas

VMW_SATP_ALUA      FreeNAS  iSCSI Disk     user          VMW_PSP_RR   iops=1            



физически правила хранятся на хосте в конфиге 
  
 /etc/vmware/esx.conf
 
 в каком то такое виде
 
 /storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/psp = "VMW_PSP_RR"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/pspOptions = "iops=1"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/vendor = "FreeNAS"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/model = "iSCSI Disk"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/ruleGroup = "user"
/storage/plugin/NMP/device[naa.6589cfc0000001b2475dddbe6c9464f8]/psp = "VMW_PSP_RR"
/storage/plugin/NMP/device[naa.6589cfc00000084e913aa61f9f14882d]/psp = "VMW_PSP_RR"
/storage/lun[naa.600508b1001c061702ee87cd7ffade8a]/fromUser = "false"
/storage/lun[naa.600508b1001c061702ee87cd7ffade8a]/displayName = "HP Serial Attached SCSI Disk (naa.600508b1001c061702ee87cd7ffade8a)"


============

