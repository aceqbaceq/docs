====
esxi 
план при установке

здесь будут описываться важные штучки которые надо обеспечить настроить
при установке нового хоста esxi



nic и hba:
	- надо чтобы версия драйвера соотвествовала версии firmware железки
и вобще nic hba это ядро перфоманса esxi

	
	
	
	
====
при создании VM надо помнить про нуму.
надо чтобы размер VM в плане цпу и RAM чтобы они влезали 
в одну NUMA структуру.



=========
чтобы в esxi создать виртуалку и поставит на нее esxi - nested esxi

посоле создания виртуалки нужно зайти через путти на фс и в свойствах имя-виртуалки.vmx
добавить две строки

vhv.enable = "TRUE"
hypervisor.cpuid.v0 = "FALSE"

и еще чтобы инсталлятор esxi увидел диск то надо в свойствах VM выставить 
	- дисковый контроллер: 'paravirtual'
	- scsi bus sharing: none
	
	
только с этим типом контроллера инсталятор esxi увидит диск куда его ставить.



--


DEVICE                                PATH/WORLD/PARTITION DQLEN WQLEN ACTV QUED %USD  LOAD   CMDS/s  READS/s WRITES/s MBREAD/s MBWRTN/s DAVG/cmd KAVG/cmd
mpx.vmhba1:C0:T0:L0                            -              32     -    0    0    0  0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00
mpx.vmhba1:C0:T1:L0                            -              32     -    0    0    0  0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00
mpx.vmhba32:C0:T0:L0                           -               1     -    0    0    0  0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00




клинируем флэшку с esxi установленным

~ # dd if=/dev/disks/mpx.vmhba1:C0:T0:L0 of=/dev/disks/mpx.vmhba1:C0:T1:L0 bs=1M conv=notrunc
24576+0 records in
24576+0 records out

--
esxi установленный на флэшку грущится через загрузчик syslinux

---

верзние две строки покаывзают только ven dev

чтоб узнать гораздо больше про иодеди флэшек надо заюзать

lspci -v


надо правильно тключичить usb arbirtaror.
перезагрузиться. 

~ # chkconfig --list | grep usb
usbarbitrator      on

# /etc/init.d/usbarbitrator stop
# chkconfig usbarbitrator off


и только потом можно на флэшку записать образ



как в esxi найти название диска который USB флэшка с которой он грузится

# esxcfg-info -s | grep -A10 "Diagnostic Partition"
         |----Name..................................................mpx.vmhba32:C0:T0:L0:9


значит путь к названию диска нашей флэшки /dev/disks/mpx.vmhba32:C0:T0:L0

когда мы  знаем имя диска 
то можем например посмотреть его размер

 # esxcli storage core device list
mpx.vmhba32:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba32:C0:T0:L0)
   Size: 7453
   

~ # esxcli storage core device list | grep -E "vmhba|USB|Size|Vendor"
   Size: 160219
   Queue Full Sample Size: 0
   Is USB: false
   Is Boot USB Device: false
mpx.vmhba33:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba33:C0:T0:L0)
   Size: 7453
   Devfs Path: /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0
   Queue Full Sample Size: 0
   Is USB: true
   Is Boot USB Device: true
   Size: 160219
   Queue Full Sample Size: 0
   Is USB: false
   Is Boot USB Device: false
mpx.vmhba32:C0:T0:L0
   Display Name: Local USB Direct-Access (mpx.vmhba32:C0:T0:L0)
   Size: 15064
   Devfs Path: /vmfs/devices/disks/mpx.vmhba32:C0:T0:L0
   Queue Full Sample Size: 0
   Is USB: true
   Is Boot USB Device: false

виден size в MB кстати обоих флэшек ( чтобы было понятно хватит ли флэшки для копирования )

~ # ls -1 /dev/disks/
mpx.vmhba32:C0:T0:L0
mpx.vmhba32:C0:T0:L0:1
mpx.vmhba33:C0:T0:L0
mpx.vmhba33:C0:T0:L0:1
mpx.vmhba33:C0:T0:L0:5
mpx.vmhba33:C0:T0:L0:6
mpx.vmhba33:C0:T0:L0:7
mpx.vmhba33:C0:T0:L0:8
mpx.vmhba33:C0:T0:L0:9


!!!!перед тем как клонироваь нужно в 
/bootbank/boot.cfg добавить ключ overrideDuplicateImageDetection 
и только потом клонируем.!!!!

# dd if=/dev/disks/mpx.vmhba33:C0:T0:L0 of=/dev/disks/mpx.vmhba32:C0:T0:L0 bs=1M  conv=notrunc

почемуто при копироании с диска на диск обязательно нужен с ключом conv=notrunc

two filesystems with the same UUID have been detected.
Make sure you do not have two ESXI installations

помогает что когда уже начинается заргрзука syslinux тыкаем Shift+o

внизц будет видна строка через которую syslinux грузит  esxi
и там надо дбавить слово overrideDuplicateImageDetection - его хватает только на  1 раз до перезагрузки.

надо понять где его можно вбить в загрузчик.

и еще хотя я гружусь с нровой флэшки но esxi в итоге монтирует себе старую флэшку.


---
полезная команда

 # esxcli storage vmfs extent list
Volume Name                               VMFS UUID                            Extent Number  Device Name                           Partition
----------------------------------------  -----------------------------------  -------------  ------------------------------------  ---------
local-0.104-156GB-ssd-r1-2dx159GB-first   4ce31cd8-b4afa840-4c52-00215ad4969e              0  naa.600508b1001c2463f2c7556ed2f0a7ae          1
iscsi-0.102-4.6TB-zfs-4hdd                5d650939-22ae5f1f-93ec-70106fb182b2              0  naa.6589cfc00000084e913aa61f9f14882d          1
local-0.104-156GB-ssd-r1-2dx159GB-second  4ce31cc2-3c75e0d0-ae46-00215ad4969e              0  naa.600508b1001c9c9f5e7f6e6d1e7f2c87          1
iscsi-0.202-4.3TB-zfs-4hdd                5e3fbe43-843521ba-b352-000af753f904              0  naa.6589cfc0000001b2475dddbe6c9464f8          1


----

посмотреть каеие флэшки воткнуты  хост

~ # lsusb
Bus 002 Device 003: ID 8564:1000
Bus 002 Device 002: ID 8564:1000
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
~ #

----

связь между устройство и именем раздела на volumes

~ # esxcfg-scsidevs -f
mpx.vmhba33:C0:T0:L0:5                                           /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0:5 cd284aef-38fd9052-5223-9a934dcd7dea
mpx.vmhba33:C0:T0:L0:6                                           /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0:6 73f22ee4-75bac953-b061-0a4f3e00df82
mpx.vmhba33:C0:T0:L0:8                                           /vmfs/devices/disks/mpx.vmhba33:C0:T0:L0:8 5d5fd29a-dede4163-bed0-000af753f904
~ #


отсюда мы видим что раздел 5 на устройстве mpx.vmhba33:C0:T0:L0 смонтирован 
в папаку /vmfs/volumes/cd284aef-38fd9052-5223-9a934dcd7dea

freebsd неможет примонтировать fat раздел 5-ый и 6-ый
приходится грузится в линукс ubuntu rescue. в нем нет vi но есть nano

--
научитьс япользлваться syslinux чтоы понимать как его стартовать руками парамтеры после boot

-
тажке надо понять нужно ли GPT таблицу править у диска на который скопировали.
-
как я понялв esxi ты фиг подмоириуешь руками даже vfat 32 все что он находит он монтирует автомтаом
в папку /vmfs/vilumes/UUID

но типа можно скпировать без мтонтирования.

--
# gpart recover можно делать.
оно непортир татблицк разделов.
а просто в конце флэшки свободное место создает
--

# fstyp -lu /dev/ada1p5
msdosfs


# file -s /dev/ada1p5
/dev/ada1p5: DOS/MBR boot sector, code offset 0x3c+2, OEM-ID "MSDOS5.0",
sectors/cluster 8, reserved sectors 2, root entries 512, Media
descriptor 0xf8, sectors/FAT 250, sectors/track 32, heads 64, sectors
511968 (volumes > 32 MB), serial number 0x4e781016, label: "
", FAT (16 bit)

---
в общем я точно выяснил что проблема в самом фрибсд в его утилите mount_msdosfs 
она почемуто неможет смонтровать vfat разделы от esxi,

из по убунту все безрпоблем

--
надо попроавить gpt таблицу на 104-м на первой флешкек
--

разобраться можно ли как то по быстрому менять UUID на флэшке esxi
--


~ # esxcfg-scsidevs
esxcfg-scsidevs <options>
                           provides paths to.
-o|--offline-cos-dev       Offline the COS device corresponding to this vmkernel
                           device.
-n|--online-cos-dev        Bring online the COS device corresponding to this vmkernel
                           device.
---


No of outstanding IOs with competing worlds: 32
какая товажная опция как я помн=ю которуб можно анстрвиатьав

--


как идентифицировать флэшки
а именно связать номер mpx и название флэшки.
потому что копируем мы по mpx  пути. а потом в биосе надо будет выбирать флэшку по имени




~ # esxcfg-scsidevs -a
vmhba32 usb-storage       link-n/a  usb.vmhba32                             () USB
vmhba33 usb-storage       link-n/a  usb.vmhba33                             () USB
vmhba34 usb-storage       link-n/a  usb.vmhba34                             () USB


# lsusb -v

  iManufacturer           1 Generic
  iProduct                2 Ultra Fast Media Reader
  iSerial                 3 000002660A01

  iManufacturer           1 JetFlash
  iProduct                2 Mass Storage Device
  iSerial                 3 EN49NYR8

  iManufacturer           1 JetFlash
  iProduct                2 Mass Storage Device
  iSerial                 3 95WW3Z45F04MVNFS
 

~ # esxcli storage core device list

mpx.vmhba33:C0:T0:L0
   Size: 7452
   Vendor: JetFlash
   Model: Transcend 8GB
   
   
mpx.vmhba32:C0:T0:L0
   Size: 15064
   Vendor: JetFlash
   Model: Transcend 16GB
    


mpx.vmhba34:C0:T0:L0
   Size: 256
   Vendor: HP iLO
   Model: LUN 00 Media 0
   
   
текущая загрузочная флэшка
   # esxcfg-info -s | grep -A10 "Diagnostic Partition"
         |----Name..................................................mpx.vmhba33:C0:T0:L0:9

   
   значит сейчас мы грузится с той которая 7ГБ 
   а новая флэшка  та которая на 15ГБ
   
   
----------------
   
надо научитьс владеть uefi биосом
---

очень много подробной информации здест можно найти о хосте

 esxcfg-info -s |
 
 ====
 
надо разоабрстяь с uefi как им пользоваться.
как с его команднгой строкой рьроатта

и как ставить syslinux чо он умеет его boot строка итд

--
если режим загрузки UEFI  то когда мы потом смотрим в esxi
то в нем видно что для него загрузочная флэшка ровно та с которую мы указали
для загрузки в биосе.


====
чтото я сделал нетак во время перезагрузки esxi хостов.

в итоге у меня в вцентре появились виртуалки серые со статусом "orphaned"
также у меня нетолько в вцентре но и если на хост зайти появилась виртуалка серая без имени
а вместо имени unaccessible написано.


начну со второго. если вместо имени втиртуалки написано unaaccessible.
это проблема на самом хосте.
у него есть файл 

~ # cat /etc/vmware/hostd/vmInventory.xml

<ConfigRoot>
  <ConfigEntry id="0000">
    <objID>1</objID>
    <vmxCfgPath>/vmfs/volumes/4ce31cd8-b4afa840-4c52-00215ad4969e/core-dc-02/core-dc-02.vmx</vmxCfgPath>
  </ConfigEntry>
  <ConfigEntry id="0006">
    <objID>8</objID>
    <vmxCfgPath>/vmfs/volumes/4ce31cd8-b4afa840-4c52-00215ad4969e/fs-1/fs-1.vmx</vmxCfgPath>
  </ConfigEntry>
  <ConfigEntry id="0010">


в нем как видно прописаны vmx виртуалок которые на нем зарегистрированы.
хост когда перезагрузился то ему надо из этих vmx файлов прочитать параметры виртуалок. в том числе
их имена.  если vmx файл недоступен для чтения то в инвентори хоста вместо имени этой виртуалки
будет написано unknown.

получается причина этого это то что хост неможет получить доступ к конкретному vmx файлу.
(https://kb.vmware.com/s/article/2172 )

надо проверить что все датасторы доступны для этого хоста.
далее надо зайти в vmInventory.xml и методом исключения сравнивая с инвентори хоста понять 
к какой конкретно виртуалке vmx файлу нет доступа.


теперь про первую проблемы. orphan виртуалки это проблема на вцентре. в его базе
указано что на таком то хосте зарегистрированар такая то виртуалка. а по факту ее там нет.
написано что такая херня может быть если мы вцентр выключили .а на хостах с виртуалками 
чтото шаманили.

отсюда главный вывод - вцентр из всех виртуалок надо выключать самым последним а включать 
первым.

если вцентр выключил то акукратно и минимально чтот делать на самих хостах.

я точно выключил вцентр рановато. возможно както полчилось что на хостах
поемуто часть виртуалок была разрегистрированна.

-----

смена пароля на esxi

есть хосты esxi которые управляются через vcenter

заходим на хост esxi и через командную строку меняем пароль на root от хоста

при этом связь с вцентр у хоста нетеряется.

можно даже сделать в вцентр disconnect\connect для хоста и он приконнектит его неспрашивая новый пароль.
процедура disconnect\connect абсолютно неопасна. не потеряются ни пулы ничего при реконнекте.

насоклько я понимаю потому что vcenter коннектится к хостам не под рутом а под vpxuser чтоли.
либо dcui юзером. а  на них то пароль неменялся.

значит далее. кто то зашел через клиент на хост.

как нам это увидеть. ответ никак.

если человек зашел через вцентр то это видно в sessions.
есь человек зашел напрямую на esxi хост через клиент то мы это в вцентр невидим. облом и жопа.

но. можно гарантированно выкинуть всех с хостов esxi то есть убить их сессии.
как это сделть.

нужно зайти на каждый esxi хост и в строке 

#  services.sh restart

это выкинет всех кто зашел через клиент напрямую на esxi хост.
если заранее сменит на хосте пароль то больше никто незайдет. 
также надо зайти на хст черз клиент и посмотреть какие там есть локальные юзеры. а 
то могли завести и в вцентр этого невидно.

----
получается в целом как это надо клонировать usb флэшку

1. отлючить usb arbitrator 
2. перезагрузится чтобы все флэшки отвалились от юсбарбитратора 
и были видны в ls -1 /dev/disks
3. определить имя mpx.vmbha флэшки с которой грузится esxi

# esxcfg-info -s | grep -A10 "Diagnostic Partition"
         |----Name..................................................mpx.vmhba32:C0:T0:L0:9

4. на этой флэшке модиицировать через vi /bootbank/boot.cfg и /altbootbank/boot.cfg 
добавив overrideDuplicateImageDetection

5. склонировать эту флэшку на другую флэшку

# dd if=/dev/disks/mpx.vmhba33:C0:T0:L0 of=/dev/disks/mpx.vmhba32:C0:T0:L0 bs=1M  conv=notrunc


6. загрузиться с freebsd iso и исправить GPT разметку через 
# gpt recover имя-флэшки

готово.

---

задача.
надо вставить флэшку в сервер.
и на нее перенести esxi с бутовой флэшки.

сделать это максимально удобно

будем клонировать флэшку через виртуалку к которой приаттачим новую флэшку

1.  определяем какая флэшка является загрузочной. потому что флэшек
вставленных в сервер может быть миллион

# esxcfg-info -s | grep -A10 "Diagnostic Partition"

mpx.vmhba32:C0:T0:L0

1.5 на загрузочной флэшке на всякий случай прописываем опцию чтобы esxi
грузился даже если в сервер вставлено две одинаковые загрузочные флэшки

нужно в 
/bootbank/boot.cfg добавить ключ overrideDuplicateImageDetection 
и только потом клонируем.!!!!

и тоже самое прописать в  
/altbootbank/boot.cfg 

2. снимаем образ с загрузочной флэшки 

# dd if=/dev/disks/mpx.vmhba32:C0:T0:L0 of=/vmfs/volumes/iscsi-111.58-zfs-2xSSD-826GB/.backup-esxi-images/100-esxi_2020-0824 bs=1M  conv=notrunc

3. на сервере ставим freebsd

4. через scp копируем из esxi по сети снятный образ вовнутрь freebsd

5. вставляем новую флэшку в сервер.
 
6. на серверер должна быть запущена служба usbarbitrator.

$ /etc/init.d/usbarbitrator start

она делает вот что. если мы вставляем флэшку в сервер то эта флэшка
небудет доступна  на монтирования самой esxi (командная строка). эту 
новую флэшку перехватывает usbarbitrator както там изолирует. 
и она только доступна для того чтобы ее приаттачить к виртуальной машине.

если usbarbitrator остановлен то мы неможем приаттачить всталвенную флэшку
к втиртуальной машине

в целом пофиг вставить флэшку потом запустить службу или наоборот.
это все без разницы. просто в конечном итоге флэшка должна быть вставлена
и служба запущена.

если служба usbarbitrator выключена то приаттачить флэшку к виртуалке
неполучится.

7. идем в свойства вирт машины и добавляем usb контроллер типа "EHCI+UHCI"
у меня его было достаточно. 
там еще есть другой тип usb контроллера xHCI. он непонадобился.

8. еще раз заходим в свойства вирт машины причем виртуалка
должна быть выключена. и если все нормально то добавдляем устройство
"USB device".

по каким причинам для него может быть написано (unavailable).
что помогло мне. заходим на esxi командную строку и 

# services.sh restart

вот после этого "USB device" стал доступен. ура

все. новую флэшку мы приаттачили к виртуалке.

9. запускаем freebsd.

10. копируем через scp из esxi образ загрузочной флэшки.

# scp root@192.168.111.100:/vmfs/volumes/iscsi-111.58-zfs-2xSSD-826GB/.backup-esxi-images/110-esxi_2020-0824.img /img

10.5 идентифицируем во фрибсд устройство отвечающее за новую флэшку 

# camcontrol devlist

11. переносим на новую флэшку образ

# dd if=/img/110-esxi_2020-0824.img of=/dev/da1 bs=1M

12. поскольку флэшка другого размера то нужно у нее GPT таблицу отремонтировать

#  gpart recover da1

проверяем что таблица теперь нормальная

# gpart show da1

12.5
неплохо бы еще раз проверить что в boot.cfg есть опция overrideDuplicateImageDetection

на флэшке этот файл находится на 5 и 6 разделе. 
тип раздела это msdosfs. к сожалению во фрибсд какаято ошибка и она
немонтирует этот тип раздела. поэтому придется приаттачить флэшку
к линуксу.

13. по идее было бы класно протестировать что флэшка реально с нее 
можно грузиться. и зарузиться с нее на виртуалке.
просто так это сделать неполучится. но в целом это можно сделать.
нужно скачать plbt.iso 
далее мы на виртуалке грузимся с этого cd-rom iso. далее выскакивает меню
и в неи выбираем загрузку с USB флэшки.
и только тогда идет загрузка с приаттаченной флэшки.

---

посмотреть на каком устройстве лежит тот или иной vmfs массив

~ #  esxcli storage vmfs extent list
Volume Name                    VMFS UUID                            Extent Number  Device Name                           Partition
-----------------------------  -----------------------------------  -------------  ------------------------------------  ---------
iscsi-111.58-zfs-2xSSD-826GB   5e28f256-ec6bed36-98b6-009c029ff9ce              0  naa.6589cfc000000f658c2bd978f8e66bf8          1
111.120-850GB-ssd-r1-2dx950GB  5dfc35ca-48921add-5174-40a8f0287bf8              0  naa.600508b1001c8d8be9a24e20e9e5bd74          1
~ #


=====

как  найти связь между именем lun в hpssacli и esxi

в hpssacli

(кстати hpsacli считается устаревшим и его заменяем новый ssacli
  якобы hpssacli для esxi 5.5 а ssacli для esxi 6.5)

 # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0   ld 1  show

Smart Array P420i in Slot 0 (Embedded)

   array A

      Logical Drive: 1
         Size: 850.0 GB
         Fault Tolerance: 1
         Heads: 255
         Sectors Per Track: 32
         Cylinders: 65535
         Strip Size: 64 KB
         Full Stripe Size: 64 KB
         Status: OK
         Caching:  Enabled
         Unique Identifier: 600508B1001C8D8BE9A24E20E9E5BD74
         Logical Drive Label: 02E827EF0014380305E49A094E2
         Mirror Group 1:
            physicaldrive 1I:1:1 (port 1I:box 1:bay 1, Solid State SATA, 912.6 GB, OK)
         Mirror Group 2:
            physicaldrive 1I:1:2 (port 1I:box 1:bay 2, Solid State SATA, 912.6 GB, OK)
         Drive Type: Data
         LD Acceleration Method: Controller Cache

значит выяснилось что в esxi он будет иметь имя
         Unique Identifier: 600508B1001C8D8BE9A24E20E9E5BD74



=====

перенос esxi с флэшки на raid

1) overrideDuplicateImageDetection добавить 
vi /bootbank/boot.cfg
vi /altbootbank/boot.cfg


2) удалить старый массив
 # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2  delete  forced



3) создать новый массив 24ГБ под esxi
# /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 create type=ld drives=1I:1:3,1I:1:4 raid=1 size=24576 stripsize=default ssdsmartpath=disable

# /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2 modify  arrayaccelerator=enable

4) делаем наш новый lun загрузочным с точки зрения smart array контроллера
~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2 modify  bootvolume=primary
этого достаточно. потом в графический настройщик smart array вообще
заходить ненужно.
 # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 show | grep -i "primary boot volume"
   Primary Boot Volume: logicaldrive 2 (600508B1001CE9775BB12DF23B605083)



5) dd скопировать
вначале нам надо определить название устройства под которым esxi 
видит загрузочную флэшку
# esxcfg-info -s | grep -A10 "Diagnostic Partition"
получим
mpx.vmhba32:C0:T0:L0

далее надо определить название устройства под которым esxi 
видит новый lun

~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 ld 2 show |  grep Unique
         Unique Identifier: 600508B1001CE9775BB12DF23B605083

esxi увидит этот lun как /dev/disks/naa.600508b1001ce9775bb12df23b605083
тут важно отметить что у hpssacli все буквы большие 
а у esxi эти буквы маленькие. это имеет значение для esxi.


подставляем в dd
# dd if=/dev/disks/mpx.vmhba32:C0:T0:L0 of=/dev/disks/naa.600508b1001ce9775bb12df23b605083 bs=1M  conv=notrunc



7) partedUtil корректируем таблицу GPT
убеждаемся что на новом lun повреждена GPT таблица
 # partedUtil getptbl /dev/disks/naa.600508b1001ce9775bb12df23b605083
исправляем
 # partedUtil fix /dev/disks/naa.600508b1001ce9775bb12df23b605083
провеяем еще раз что сейчас все окей
 # partedUtil getptbl /dev/disks/naa.600508b1001ce9775bb12df23b605083



7.5) 
~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 modify cacheratio=0/100

7.6)
~ # /opt/hp/hpssacli/bin/hpssacli ctrl slot=0 modify drivewritecache=enable forced


8) грузимся чтобы окончательно проверить что с нового LUN со smart array контроллера мы успешно грузимся

9) по готовности отформатировать исходную  флэшку 

выяснилось. что если мы грузимся со smart array lun 
то esxi все равно както там в процессе грузится или чтото 
типа того с флэшки. это легко проверятеся через esxcfg-info -s | grep -A10 "Diagnostic Partition" после загрузки. возможно загрузчик какойто 
первичный на esxi ищет опреденный gpt раздел по guid и первым попадается
этот раздел с флэшки а не со smart array.
вывод - если мы грузиммся с raid1 lun smart array то мы неможем
иметь втоже время вставленную флэшку с таким же esxi загрузочным.

выяснисось что на 24ГБ отведенных на LUN под esxi оно реально
использует ~4GB места. остается ~ 20GB места на LUN
и это место доступно для создания стораджа из под vmware c# клиента.
пугаться ненадо. это нето место где лежит esxi это свободное 
место на том LUN так что датастор создавать можно.



===
оказалость что есть приложение для виндовс которое позволяет пожключаться
к ilo. это хорошо.э
==

прработать:
если есть usb с той же gpt а мы грузиммся с smart array то все равно
ESXI напишет что загрузился с usb

disabled external usb нельзя активровать 
ilo aplication standalone
textcons

у hp можно в биос деактивировать только внешние usb порты. 
внутрениие нельзя. в биосе путающее меню. 
деактивировать внешние usb называется "disable external usb ports"
а активировать их называется "usb enable".


походу вот еще в чем прикол. esxi блокирует запись на тот диск
с которого она загрузилась. только чтение разрешено
==
в биос есть "virtual install disk" опция.
как я понял это некий диск его бекенд мне непонятно где
он записан на мат плате сервера. но если его активировать в биос
то якобы на этом диске держат драйвера которые могут понадобиться
при установке ОС на сервер. главный вопрос как эти драйвера туда записать.
пока непонятно.
===

по поводу нумерации дисков LSI в сесии ssh и и в vcenter 

в команднрой строке esxi диски имеют нумерацию cтолбце EID
	4:0
	4:1
	итд



$ /opt/lsi/storcli/storcli  /c0 show

-----------------------------------------------------------------------
EID:Slt DID State DG     Size Intf Med SED PI SeSz Model            Sp
-----------------------------------------------------------------------
4:0       6 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:1       7 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:2      10 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:3       8 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:4      12 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:5      15 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U
4:6      11 Onln   0 3.637 TB SAS  HDD N   N  512B ST4000NM0023     U



но в вцентр в hardware status нумерация совсем другая.
первая цифра берется из EID тоесть 4.  а вторая цифра из DID, их 
вцентр слепляет и выводит в виде
	вместо 4:0 он выводит 4_6
	вместо 4:1 он выводит 4_7
	вместо 4:2 он выводит 4_10
	вместо 4:3 он выводит 4_8
	
вот такая связь. такая разница в выводе esxi и вцентр

---

есть такая задача. 
есть esxi
на нем стоит рейд контроллер
и в него воткнуты ssd диски.
надо узнать насколько износились ssd диски.

значит smart данные рейд контррллер невыдает 
поэтому smart хрень любая неработает.

а что работает.

в esxi можно посмотреть сколько байт было записано на lun
с момент старта сервера.
кога сервер будет перезагружен этот счетчик обнулится.

вот как это сделать

~ # esxcli storage core device stats get
naa.6589cfc000000f658c2bd978f8e66bf8
   Device: naa.6589cfc000000f658c2bd978f8e66bf8
   Successful Commands: 23061155
   Blocks Read: 248724493
   Blocks Written: 200529452
   Read Operations: 10296559
   Write Operations: 3218177
   Reserve Operations: 1
   Reservation Conflicts: 0
   Failed Commands: 117898
   Failed Blocks Read: 0
   Failed Blocks Written: 3376
   Failed Read Operations: 26031
   Failed Write Operations: 5244
   Failed Reserve Operations: 0

naa.600508b1001caf026adcbe0c1b91a50e
   Device: naa.600508b1001caf026adcbe0c1b91a50e
   Successful Commands: 10990299986
   Blocks Read: 45460020414
   Blocks Written: 119530291007
   Read Operations: 4608896091
   Write Operations: 6381012553
   Reserve Operations: 9140
   Reservation Conflicts: 0
   Failed Commands: 43413
   Failed Blocks Read: 0
   Failed Blocks Written: 0
   Failed Read Operations: 0
   Failed Write Operations: 0
   Failed Reserve Operations: 0

naa.600508b1001c2bdea2adb2c2515f744e
   Device: naa.600508b1001c2bdea2adb2c2515f744e
   Successful Commands: 5245128586
   Blocks Read: 65047388977
   Blocks Written: 358643072928
   Read Operations: 940332168
   Write Operations: 4304397066
   Reserve Operations: 13106
   Reservation Conflicts: 0
   Failed Commands: 43352
   Failed Blocks Read: 0
   Failed Blocks Written: 0
   Failed Read Operations: 0
   Failed Write Operations: 0
   Failed Reserve Operations: 0

mpx.vmhba32:C0:T0:L0
   Device: mpx.vmhba32:C0:T0:L0
   Successful Commands: 2684426
   Blocks Read: 99865084
   Blocks Written: 5771111
   Read Operations: 2101369
   Write Operations: 555853
   Reserve Operations: 0
   Reservation Conflicts: 0
   Failed Commands: 62040
   Failed Blocks Read: 0
   Failed Blocks Written: 0
   Failed Read Operations: 0
   Failed Write Operations: 0
   Failed Reserve Operations: 0

вот видно несоклько лунов.

берем лун
naa.6589cfc000000f658c2bd978f8e66bf8

нам нужен параметр

Blocks Written: 200529452

согласно вот этой статье https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.storage.doc/GUID-BB3B40D3-82D4-44D8-BD38-155129800B13.html
один блок = 512 байт.

поэтому умножаем Blocks Written: 200529452 на 512 байт 
и узнаем сколько байт было записано на lun  с момента старта сервера

в данном случае 95 ГБ.

аптайм этого сервера 534 дня.

таким образом можно оценить запиленность ssd дисков  штатными средствами

экспериментальным путем было выяснено
для конкретных серверов что 

sql база данных на raid10 из 4-ех дисков запиливает 1 ssd диск в год на 215 TB

флешка на которую установлен esxi и при этом логи льются не нафлэшку
а на другой лун , так вот такая флэшка в год запиливается на 2 GB\год

флэшку в списке лунов легко идентифицировать как чтото выглядящее как
mpx.vmhba32:C0:T0:L0

массив raid10 из 10 дисков на который постоянно пишутся картинки 
в нем 1 ssd диск в год запиливается на 35TB

для примера вот таблица endurance (tbw) дисков samsung

https://www.samsung.com/semiconductor/minisite/ssd/support/warranty/

860 pro 1TB имеет 1200 TBW

еще примеры endurance ssd дисков


SSDSC2KB96 (стали делать в 17 году это intel s4500 series)  = 1904 TBW

samsung 850 pro 1TB = 300 TBW входят в гарантию 

sSDSC2KG019T701 (серия s4600) = 11100 TBW = 37 730 ₽ 

SSDSC2KG019T801 (серия D3-S4610) = 9625 TBW = 38 850p

SDSC2KB019T801 (серия D3-S4510) = 6656 TBW = 29 370p

SEDC500M/1920G = 4555 TBW = 24 343p

XA1920LE10063 = 3500 TBW = 22 720p

SSDSC2KB019T701 (серия S4500) = 3348 TBW= 35 070 ₽

--

ssd Диски делаются на основе типпов ячеек
SLC, MLC, TLC, QLC

чем левее тем чип дороже быстрее и endurance больше.
чем правее то все гавнянее но дешевле.

еще эти ячейки они могут типа располагаться в 2D компоновке
и 3D компоновке. это влияет на endurance такого чипа.

то есть есть 2D MLC а есть 3D MLC.

зачем нам знать эту всю фигню. 
если производитель не привел сколько tbw для диска
то зная из каких ячеек сделан диск можно прикинуть вручную
сколько у него tbw.

вот здесь указано какие ячейки сколько циклов перезаписи выдерживают
https://searchstorage.techtarget.com/definition/TLC-flash-triple-level-cell-flash

вот еще табличка про количество циклов перезаписи ячеек
https://www.ni.com/ru-ru/support/documentation/supplemental/12/understanding-life-expectancy-of-flash-storage.html

но в целом ручной расчет endurance на основе типа ячеек лучше неделат
так как можно легко промахнуться а смотреть в spec у производителя
например 

sSDSC2KG019T701 (серия s4600) согласно спеку имеет 11100 TBW
согласно тому же спеку он сделан из 3D NAND TLC и должен 
иметь существенно меньший ресурс


еще пример 

sasmung 840 evo 1TB  для него непонятно какой tbw самсунг 
только дает что 3 года гарантия
https://www.samsung.com/semiconductor/minisite/ssd/support/warranty/

но мызнаем что он сделан на основе TLC как я понял просто TLC а не 3d TLC

для этих ячеек примерно они выдерживают 300 - 1000 циклов.
значит примерно samsung 840 evo 1TB  должен выдержать 500 tbw


еще пример.

вот пример берем диск SEDC500M/1920G = 4555 TBW
согласно спеку https://www.kingston.com/datasheets/dc500r-m_us.pdf 
он сделан из 3d TLC nand который типа имеет 3000 циклов перезаписи.
получается в теоррии его ресурс ~ 6000 tbw. 
однако согласно тому же спеку он равен 4555 tbw 


=======

esxi
vmware
iscsi
freenas
iops
satp

 есть такая проблема
 вот мы поставили freenas и через iscsi стали отдавать на esxi LUN.
 и еще мы отдаем по iscsi через multipath по round robin через несколько карточек.
 в чем проблема. по дефолту esxi забирает через карточку 1000 iops и только потом 
 переключается на следующую карточку на следующую 1000 iops.
 если мы хотим иметь много iops с низким лейтенси то такая схема гавно.
 гораздо лучше будет если esxi будет через карточку забирать 1 iops а следущий iops
 забирать через следующую карточку. замечу что такая схема это невина стораджа freenas 
 это вина клиента esxi
 
 вот этого и надо добиться. в интернете широко указано решение когда мы этого добиваемся
 но только для тех LUN которые уже подключены к esxi. такое решение гавно потому что 
 каждый раз когда мы будем подлкючать +1 новый LUN то для него нужно будет опять руками 
 запускать эту операцию.
 
 
 нам надо сделать чтобы для всех старых и новых LUN автоматом устанавливалась системе iops=1
 путем однократной настройки.
 
 
 во первых проверим через сколько iops на данный момент esxi переключается на следущую карточку
 
  # esxcli storage nmp  device list
  
  naa.6589cfc0000001b2475dddbe6c9464f8
   Device Display Name: FreeNAS iSCSI Disk (naa.6589cfc0000001b2475dddbe6c9464f8)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on; action_OnRetryErrors=off; {TPG_id=1,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=rr,iops=1000,bytes=10485760,useANO=0; lastPathIndex=3: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba34:C13:T1:L2, vmhba34:C1:T1:L2, vmhba34:C19:T1:L2, vmhba34:C7:T1:L2
   Is Local SAS Device: false
   Is USB: false
   Is Boot USB Device: false

  из вывод нам интересно вот это :
  
  {policy=rr,iops=1000,
  
  как видно автоматом esxi выставил iops=1000
  
  также для справки :

nmp = VMware Native Multipath Plugin (NMP). This is the VMware default implementation of the Pluggable Storage Architecture. 

satp = storage Array Type Plugins

 
 как же решить проблему. ответ - надо создать правило которое автоматом будет создавать уже нужный iops=1
 
 вот это правило:
 
 # esxcli storage nmp satp rule add -s "VMW_SATP_ALUA" -V "FreeNAS" -M "iSCSI Disk" -P "VMW_PSP_RR" -O "iops=1"
 
 здесь нужно знать что прописывать в параметрах
 
 -s
 -V
 -M
 -P
 
 узнаем что в них прописывать из вывода
 
   # esxcli storage nmp  device list
 
 
 
 -s  узнаем из строки  Storage Array Type:  
   
   Storage Array Type: VMW_SATP_ALUA
      
   поэтому -s = "VMW_SATP_ALUA"
   

 -P узнаем  из строки Path Selection Policy:

   Path Selection Policy: VMW_PSP_RR
   
   поэтому -P "VMW_PSP_RR"
   
   
 остается -V и -M. в целом оба эти параметра вместе прописаны в строке Device Display Name:
 ну а нам же надо их узнать по отдельности. для этого идем в с# клиент сферы - тыкаем сервер - 
 - configuration - storage adapters - rescan all
 потом заходим на сервер по ssh и ищем в логах
 
 # dmesg | grep -i freenas
 
 или 
 
 # cat /var/log/vmkernel.log | grep -i scsiscan | grep -i vendor
 
 в итоге мы увидим вот такие строки
 
 cpu4:33467)ScsiScan: 976: Path 'vmhba32:C0:T0:L0': Vendor: 'FreeNAS '  Model: 'iSCSI Disk      '  Rev: '0123'
 
 из которых мы узнаем что 
 
 
 Vendor: 'FreeNAS '  
 Model: 'iSCSI Disk      '
 
 
 из чего мы узнаем что 
 
 -V "FreeNAS" -M "iSCSI Disk"
 
 
 тут очередной прикол. невсегда пересканирование адаптеров и лунов дает нам строчки в логах.
 то дает то недает. поэтому возможный второй путь это вручную определять из строки
 
 Device Display Name: FreeNAS iSCSI Disk (naa.6589cfc0000001b2475dddbe6c9464f8)
 
 когда узнаю почему в логах невсегда можно найти Vendor и Model то напишу.
 
 
 далее еще один прикол и очень очень важный, я обжегся и потом нашел в инете то что несмотря на то что в логе названия идут с пробелами
 в командную строку эти проблемы вставлять категорически нельзя.
 
 поэтому правильно для подставновку в команду именно 
 
 -V "FreeNAS" -M "iSCSI Disk"
 
 без всяких лишних пробелов.
 
 единственная причина если в итоге правило несрабатывает это на практике только потому что Vendor
 и Model были указаны неправильно. и чаще всего изза лишних пробелов.
 итак я описал все ловушки.
 
  
 итого еще раз финальная строка
 
 
  
 # esxcli storage nmp satp rule add -s "VMW_SATP_ALUA" -V "FreeNAS" -M "iSCSI Disk" -P "VMW_PSP_RR" -O "iops=1"
 
 данное правило нужно вбивать на каждом хосте. 
 
 правило вступит в силу только после перезагрузки всего сервера.
 есть конешно теоритеический путь якобы как можно обойтись без перезагрузки esxi но 
 на практикето работает то нет. поэтому дальше надо перезагружать сервер.
 
 
 перезагрузились и запускаем 
 
 
 # esxcli storage nmp  device list
 
 
 и смотрим сколько там iops= указано для iscsi LUN-ов
 
 если все сработало то полуим вот такое
 
 
 naa.6589cfc0000001b2475dddbe6c9464f8
   Device Display Name: FreeNAS iSCSI Disk (naa.6589cfc0000001b2475dddbe6c9464f8)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on;{TPG_id=1,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=rr,iops=1,bytes=10485760,useANO=0; lastPathIndex=2: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba32:C8:T1:L2, vmhba32:C4:T1:L2, vmhba32:C0:T1:L2
   Is Local SAS Device: false
   Is Boot USB Device: false

нас интересует кусок

{policy=rr,iops=1

если мы перегрузились а вся эта шарманка несработала. 
то в каком то из параметров мы указали неверно. 

-s "VMW_SATP_ALUA"
-V "FreeNAS"
-M "iSCSI Disk" 
-P "VMW_PSP_RR"

чаще всего это -V и -M.

тогда чтобы это правило измениьт надо прежде всего удалить текущее правило.
без удаления новое правило недаст система вбить

удаляется оно вот так

# esxcli storage nmp satp rule remove -s "VMW_SATP_ALUA" -V "FreeNAS" -M "iSCSI Disk" -P "VMW_PSP_RR" -O "iops=1"


и после этого опять вбиваем новое правило, перегружаемся и проверяем сработало ли.

если все сработало то на этом все.

если нет то ,
далее чуть больше деталей.


как посмотреть наще правило и вообще какие правила уже есть на хосте.


~ # esxcli storage  nmp   satp rule list | grep -i freenas

VMW_SATP_ALUA      FreeNAS  iSCSI Disk     user          VMW_PSP_RR   iops=1            



физически правила хранятся на хосте в конфиге 
  
 /etc/vmware/esx.conf
 
 в каком то такое виде
 
 /storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/psp = "VMW_PSP_RR"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/pspOptions = "iops=1"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/vendor = "FreeNAS"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/model = "iSCSI Disk"
/storage/plugin/NMP/config[VMW_SATP_ALUA]/rules[0000]/ruleGroup = "user"
/storage/plugin/NMP/device[naa.6589cfc0000001b2475dddbe6c9464f8]/psp = "VMW_PSP_RR"
/storage/plugin/NMP/device[naa.6589cfc00000084e913aa61f9f14882d]/psp = "VMW_PSP_RR"
/storage/lun[naa.600508b1001c061702ee87cd7ffade8a]/fromUser = "false"
/storage/lun[naa.600508b1001c061702ee87cd7ffade8a]/displayName = "HP Serial Attached SCSI Disk (naa.600508b1001c061702ee87cd7ffade8a)"


============
storcli

как создать raid10

 # /opt/lsi/storcli/storcli  /c0  add vd type=raid10 name=SSD-R1-2xssd drives=62:4,62:5,62:6,62:7 pdcache=on wt nora strip=64 pdperarray=2

Controller = 0
Status = Success
Description = Add VD Succeeded

 в чем была прикол. начал создавать этот массив.
 а оно пишет ошибку
 
 Controller = 0
Status = Failure
Description = operation not possible for current RAID level, Invalid RAID level


оказалос что сторкли небудет создавать raid10 если неуказать параметр 

	pdperarray=
	
ради этого эта запись и сделана


======================
esxi 
версия = VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso 

на этой версии  при Vmotion сфера выпадала в пураурный экран смерти

а на версии VMware-VMvisor-Installer-5.5.0.update03-3568722.x86_64-Dell_Customized-A06.iso 

такой проблемы нет

===

выяснилась неожтданная подлянка от HP\HPE smart array

значит в кои то веки я создал массив через встроенный в биос веб интерфейс 
утилиту работы с контроллером smart array а обычно я это делаю уже после устновки 
esxi через hpssacli установленный в esxi. в этот раз пошел по другому потому что 
ставил esxi на массив а не на флэшку.

так вот. запускаю hpssacli из под esxi и получаю вот такой отлуп

APPLICATION UPGRADE REQUIRED: This controller has been configured with
 a more recent version of software.

и все ничего не могу конфигурировать с массивом.

а логика какая -  встроенный в пролиант версия программы управления чтото там запимсывает 
в контрроллер какую то версию . и если версия hpssacli меньше то уже нифига 
конфиг на таком контроллере непомнпеть.

либо стирать массив и создавать заново, либо на esxi поставить 
"современную" версию утилиы по работе с smart array.


значит выяснилось что hpssacli он типа ушел в вечность.
вмест него сейчас ssacli . походу таже штука только с дрнуим именем.

в итоге поставил некую версию ssacli и наконец стало можно конфингурировать 
смарт эррей из под esxi.



===
пытаемся отформатировать LUN в esxi datastor
а он выдает ошибку

esxi dd '/dev/disks/naa function not implemented

решение надо на конце dd добавить conv=notrunc

# dd if=/dev/zero of=/vmfs/devices/disks/naa..  bs=1M count=10 conv=notrunc

===

столкнулся на esx 5.5 pass through
при проброске диска nvme напрямую в вирт машину
при попытке запуска ВМ вылезает ошибка


Failed to register the device pciPassthru0  due to unavailable hardware or software support

в HP рекомендуют применить патч на esxi

побыстрому лечится тем что виртуальной машине нужно выставить меньше обьем RAM
тогда она без проблем запустится.



===

esxi 
как заресетить пароль root на флэшке.

грузимся с linux rescue cd

потом подмонтируем 5 и 6 раздел с флэшки.
там находится bootbank и его запасная копия.

так вот пароли записаны в state.tgz

ресетим пароль до пустого как нарисано тут

https://www.altaro.com/vmware/reset-esxi-root-password/

тоесть надо разжаь state.tgz и внутри отредактировать файл shadow


mkdir /boot /temp
mount /dev/sda5 /boot
cd /boot
cp state.tgz /temp
cd /temp
tar -xf state.tgz
tar -xf local.tgz
rm *.tgz
cd etc

редактируем shadow чеерез vi
удаляем в строке с root все что во втором столбике нахоодится ( между двумя двоеточиями)
таким макаром пароль к root становится пустым.
дальше надо обратно сжать всю эту папку до state.tgz и обратно положить на флэшку

cd ..
tar -cf local.tgz etc/
tar -cf state.tgz local.tgz
mv state.tgz /boot
umount /boot
reboot

перегруэаемся через vmware c# клиент мы можем зайти на хост.
в ssh зайтинеможет он юзеров без пароля непускает.



====

vid
did

https://kb.vmware.com/s/article/1027206


вот это позволет увидеть какой дрвйвер обслуживаем картуочку и какой vmnic у нее

 # esxcli network nic list
 
 зная  это можно узнать vid\did
 
 # vmkchdev -l |grep vmnic0
0000:05:00.0 8086:1521 103c:337f vmkernel vmnic0

 и узнать какие опции ест у драйвера по настройуке
 
 # vmkload_mod -s имя_драйвера
 
 ====
 про sr-iov
 
 мало того что его :
 должна железка поддерживаеть
 биос
 ОС
 
 так еще и драйвер должен поддерживать.
 
 для i350 ( 
 HP Ethernet 1Gb 2-port 361i Adapter - NIC)  драйвер igb а он sr-iov неподдерживает.
 
 ура товарищи
 
 вобщем как писали человеки в интернете что в редко какая карта 1Gb поддверживает 
 SR-IOV а я добавлю что редко какой драйвер от этой карты в esxi его поддверживает.
 
 361i типа поддвеживает. но дрйвер от нее igb его неопддерживает.
 так что если карта меньше 10Gb то про sr-iov можно успокоиться.
 
 ==
 esxi
 vcenter
 супепролены совет.
 
 если собираешься чтото серьезное делать с хостом 
 который в кластере да ивобще то лучше с него вобще все вм убрать 
 и даже те которые выключены.
 
 потому что если что - все эти ВМ будут в состоянии потерянные или unknown
 
 так что правило - если выводишть хост из кластера то мигиируй нафиг с него
 все ВМ и даже выключенные. и тока потом его выключай.
 щас я получил кучу ВМ которые даже непонятно как назвыаются ибо они обозначены как unknown.
 где теперь их искать на дисках.
 
 =====
 
 посмотреть список FC контроллеров
 и названия их драйверов
 
 # esxcli storage core adapter list
 
 либо список контроллеров без названия драйверов
 
 # esxcli  storage san fc list
 
 
 например 

 # esxcli storage core adapter list
 
 vmhba64   qlnativefc  link-n/a    fc.50014380186a0445:50014380186a0444 
 (0:132:0.0) QLogic Corp ISP2532-based 8Gb Fibre Channel to PCI Express HBA

 
 значит драйвер у кулоджика qlnativefc
 
 когда мы знаем название драйвера то мы можем посмотреть 
 какие опции он предоставляет для ручного подкручивания
 


 
 а эта команда покажет какие ручные опции подкручивания активированы 
 на этом сервере для этого драйвера


# esxcli system module parameters list -m qlnativefc

# esxcfg-module --get-options qlnativefc
qlnativefc enabled = 1 options = ''

в данном случае видно что никакие.

 
 =====
 
 установка esxi на хост который по цпу который невходит в compatbiltylist,
 как это выглидт.
 
 установочный esxi полностью загружается. 
начинается процесс установки.
мы выбираем флэшку на которую собираемся устанавливаться и тут [наконец!]
esxi пишет что мол процессор невходит в список разрешенных.

как говрится непррошло и пол года

====

vcenter 7 поддерживает esxi 6.5 
а все что меньше уже не поодерживает.
а esxi 6.5 он поддерживает далеко не все процессоры

=====

новые версии esxi их установка на старые цпу

esxi:
	- 6.5u3(13932383)
 

оно все еще успешно ставится на эти процессоры:

	- intel E5-2680v4
	- intel E5-2620v4
	- intel X5650

никаких доп манипуляций делать ненадо.
едиснветнное непосредственно перед тем как начать ставится ( это сразу после
того как ввел два раза пароль новому админу) инсталлятор напишет
что мол внгимание этот цпу неподдерживается на будущих версиях esxi. 
если я правильнро запомнил смысл. тоесть эта версия еще поддерживает
а будущие уже нет.

а вот уже 
esxi:
	- 7
	
уже неставится по дефолту на цпу:
	- X5650
	
помогает волшебный прием
надо на стадии загрузки установщика когда на экране мигает обраный отсчет 5 секунд 
то нужно нажать Shift+O и прописать в параметры загрузчика

	allowLegacyCPU=True



Тогда перед самым последним моментом перед установкой , тоесть 
когда мы вводим два раза пароль нового админа появляется надпись что данный цпу
данная esxi неподдерживает , но! при это позволяется установку продолжить.
на свой страх и риск понятно.
самая здлесь подстава то что путает  это что появляется надпись что цпу неподдерживается
но установка дальнейшая при этом разрешена.

значит после установки esxi с таким трюком мы перегружаемся и 
esxi несмоожет загрузиться он напишет что цпу неподдерживатеся
поэтому надо опять входить в параметры загрузчика и там
руками опять вставялть 

	allowLegacyCPU=True

ну и дальше когда загрузтдились надо редактировать файлы на флэшке чтобы вставить этот
ключ перманентно.

еще полезная полезняшка. прежде чем пробвать постаить новыф esxi 
удобно на этом же сервере на текущем esxi попробваоб поставить 
VM с nested esxi на основе нового esxi. сразу будет видно поставиться он на 
текущий процессор или нет. это гораздо быстрее чем доблавться на живую
на голом физ сервере.


====
 
как зайти на vcenter 7

можно зайти на него как на эпплианс это 

https://ip:5480

но нам надо так чтобы видеть наши хосты. это другой URL

https://ip/ui

====

esxi , vcenter лицензии

лицензии на хост esxi 
разница между esxi standard vs esxi enterprize+ 
по мне единственная ощутимая фича который нет 
в esxi standard это то что sr-iov опция доступна толтько в enterprize+

также лицензия на сам vcenter. они есть vcenter essential (гавнецо), f
vcenter foundation - до 4-ех хостов esxi max
vcenter stadard - до 2000 хостов .

esxi лицнзия цена идет с учетом на сколько  сокетов цпу 
есть на сервере.  лицензия на сокет включает цпу с 32 ядер макс.
а если у него ядер больше то нужно доп лицензию покупать.

также нужно покуптать поддержку. 
минимальный срок на 1 год. 
поддержка покупается тоже с учетом числа сокетов.
как я понял регулярно поддржку продлевать ненадо.

расчет цен - http://www.v-grade.ru/doku.php?id=vmware-vsphere-5-licensing

====

триальыне версии esxi и vcener которые доступны на сайте 
они полноценные просто по сроку ограниченные. чтобы их сделать
полноценными нужно просто добавить ключи

===
про PCI-E

ver.1 дает 0,25 GB\s на 1 линию как туда так и в обратнуб сторону
ver.2 дает 0,5 GB\s на 1 линию
ver.3 дает 1,0 GB\s на 1 линию
=====

разберем тех характеристики карты BCM5719 

x4 PCI Express v2.1 at 5 GT/s or 2.5 GT/s

значит она имеет разьем pci-e v2 и имеет 4 линии.
значит она сидит на разьеме скорость которого 4 х 0.5 = 2GB/s

что с головой хватает для 4 портов по 1Gb 



===
тема перед тем как начать говорить про сетевые карточки.
это тема прерываний.

про MSI и MSI-X

[C]


===
 
 про технолгии сетевых карточек NIC
 
 тут будет большой пост.
 
 вобще как я понял мудота начинается с сетевыми картами настройками 
 при переходе от 1Gb на 10Gb и далее.
 на 1Gb типа все работает хорошо по дефолту.
 
 
 во первых про драйверы: есть inbox драйверы это драйвер который написала сама
 vmware,  а есть async драйвер это который написал проивзоводитель железки например
 сетевой карты. считается что inbox он надежнее а async в нем больше опций фич 
 для настройки  железки.
 драйвер это модуль ядра. его можно подгружать выгружать и во время загрузки там
 можно прописывать какие опции этого модуля чему равны. список доступных опций
 можно посмотреть через команду. обо всем этом ниже.
 
 как узнать драйвер такойто у  нас inbox или async.
 походу только по названию установочного файла
 
 # esxcli software vib list | grep tg3
net-tg3                        3.131d.v60.4-2vmw.650.0.0.4564106     VMW      VMwareCertified   2021-06-27
ntg3                           4.1.3.2-1vmw.650.2.75.10884925        VMW      VMwareCertified   2021-06-27

если видим в имени vmw - значит он inbox

 # esxcli software vib list | grep tg3
net-tg3                        3.137l.v55.1-1OEM.550.0.0.1331820      Broadcom         VMwareCertified   2021-05-20

если видим в названии OEM значит он async.

теперь вопрос как узнать какой драйвер используется для скажем сетевой карты

# esxcli network nic list
Name    PCI Device    Driver  Admin Status  Link Status  Speed  Duplex  MAC Address         MTU  Description                                         
------  ------------  ------  ------------  -----------  -----  ------  -----------------  ----  -------------------------------------------------------
vmnic0  0000:05:00.0  igb     Up            Up            1000  Full    98:4b:e1:5f:58:66  1500  Intel Corporation 82576 Gigabit Network Connection  
vmnic1  0000:05:00.1  igb     Up            Up            1000  Full    98:4b:e1:5f:58:67  1500  Intel Corporation 82576 Gigabit Network Connection  
vmnic2  0000:07:00.0  ntg3    Up            Up            1000  Full    00:0a:f7:53:f9:04  1500  Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet
vmnic3  0000:07:00.1  ntg3    Up            Up            1000  Full    00:0a:f7:53:f9:05  1500  Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet
vmnic4  0000:07:00.2  ntg3    Up            Up            1000  Full    00:0a:f7:53:f9:06  1500  Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet
vmnic5  0000:07:00.3  ntg3    Up            Up            1000  Full    00:0a:f7:53:f9:07  1500  Broadcom Corporation NetXtreme BCM5719 Gigabit Ethernet

здесь мы видим что для vmnic2 драйвер ntg3. 
драйвер и название модуля ядра это одно и тоже кстати.

теперь вопрос  а как называется vib файл из которого этот драйвер родом.
как нам понять из какого vib файлы этот драйвер.

# esxcli network nic get -n vmnic2
         Driver: ntg3
         Firmware Version: bc 1.33 ncsi 1.1.19.0
         Version: 4.1.3.2
 
отсюда мы видим что ntg3 имеет версию Version: 4.1.3.2

теперь ищем все vib которые похожи по названию на tg3

#  esxcli software vib list | grep tg3
net-tg3                        3.131d.v60.4-2vmw.650.0.0.4564106     
ntg3                           4.1.3.2-1vmw.650.2.75.10884925    

и мы видим что вот этот  файл 4.1.3.2-1vmw.650.2.75.10884925 имеет в себе 4.1.3.2

таким образом мы научились для сетевой карты vmnic2 находит 
какой драйвер ей рулит. какая версия этого драйвера. и из какого vib файла 
он был поставлен.


помимо async\inbox типа драйвера они делятся еще по признаку native драйвер
или linux драйвер.

если драйвер имеет тип linux драйвера то схема его работы такая

железка-> линукс драйвер -> vmklinux модуль дляра -> vmkernel (само ядро esxi)

если драйвер имеет тип native то схема такая

железка->  драйвер -> vmkernel (само ядро esxi)

плюс нативного драйвера что путь короче и скорость работы получается быстрее. меньше
накладных расходов.

как определить нативный драйвер или линукс тип у него.

определяется по тому в какой папке он лежит.


нативные драйвера лежат в папке  /etc/vmware/default.map.d/
линукс драйвера лежат в папке /etc/vmware/driver.map.d/

например ранее мы определили что vmnic2 работает на tg3
ищем его в этих папках

 # ls -1 /etc/vmware/driver.map.d/ | grep tg3
tg3.map

таким образом tg3 это линукс тип драйвер

как я понял вмвейр безусловно рекомендует пользоваться нативными драйверами.
а от линукс дррайверов отказывться.

посмотрим другой известный драйверы такие как igb это драйвер интел карт и hpsa
это драйвер smart array контроллеров

# ls -1 /etc/vmware/driver.map.d/ | grep igb
igb.map

получается igb это тоже линукс драйвер

 # ls -1 /etc/vmware/driver.map.d/ | grep hpsa
hpsa.map

hpsa тоже это линукс драйвер

это был esxi 5.5

а вот как есть у esxi 6.5

[root@localhost:~] ls -1 /etc/vmware/default.map.d/ | grep igb
igbn.map
[root@localhost:~] ls -1 /etc/vmware/default.map.d/ | grep hpsa
nhpsa.map

тоест igbn и nhpsa  у него уже имеют нативный тип.

еще раз скажу что линукс драйверы они работают через посредника - модуль в ядре
под названиеем vmklinux

в esxi 7 его полностью выпилили из esxi. так что с esxi 7 только 
нативные драйверы. говоря рускими словами старое железо которое поддеживает esxi 7 
его мало. ибо под него нужны нативные драйверы которые никто писать небудет уже.

дальеш прикол в том что как бы вмвейр пишет что  чтобы драйвер с железкой работал
хорошо должно быт соотвесвтие между номером драйвера и номером firmware
в железке.
так что находим на основе того что написано выше какой firmware версия у нашей 
железки а потом для данной firmware надо найти соовествующую версию драйвера 
чтобы было все по фэншуй.

тоесть должна быть связка железка firmware (такая то версия) + драйвер ( такая то версия)
тоесть драйвер с его версия работает в связке с firmware такойто версии.

как выглядит установка\обновление драйвера для сетевой:
	значит по первых определимся с драйвером который щас обеспечивает карту
	и версией firmware:
	# esxcli network nic get -n vmnic2
         Bus Info: 0000:07:00:0
         Driver: ntg3
         Firmware Version: bc 1.33 ncsi 1.1.19.0
         Version: 4.1.3.2
 
	еще это мжоно сделаь и по другому
	но второй способ невсегда срабатвыает
	например на esxi 6.5 и ntg3 оно напишет
	# ethtool -i vmnic2
    Can not get control fd: No such file or directory

	а вот просто другой esxi и другой драйвер когда сработало
	 # ethtool -i vmnic2
	driver: tg3
	version: 3.137l.v55.1
	firmware-version: 5719-v1.46 NCSI v1.5.18.0

		значит текущий драйвер ntg3, Version: 4.1.3.2
        текущий Firmware Version: bc 1.33 ncsi 1.1.19.0 , попросут говоря 1.33
		
		поймем это нативный или vmklinux драйвер
		# ls -1 /etc/vmware/default.map.d/ | grep ntg3
        ntg3.map
		
		значит это нативный драйвер. что типа хорошо для скорости
		
		найдем из какого файла он был установлен
		# esxcli software vib list | grep ntg3
         ntg3  4.1.3.2-1vmw.650.2.75.10884925
		 
		еще ксати узнать из какого файла установлен драйвер можно и другим способом
		# vmkload_mod -s ntg3
         vmkload_mod module information
         input file: /usr/lib/vmware/vmkmod/ntg3
         Version: 4.1.3.2-1vmw.650.2.75.10884925
		 
		также мы видим что это inbox драйвер , тоесть от самой vmware  а не от броадкома
		
		
		посмотрим какие опции позводляет настривать этот драйвер
		
		#  esxcli system module parameters list -m ntg3
        Name             Type    Value  Description
        ---------------  ------  -----  --------------------------------------
        initRingSzRxJmb  ushort         RX Jumbo Ring Size  
        initRingSzRxStd  ushort         RX Standard Ring Size
        intrMode         ushort         Interrupt mode: 0=IntX, 1=MSI(Default)

        прямо скажем негусто.

        тоже самое еще можо получить через другую команду

		# vmkload_mod -s ntg3
         vmkload_mod module information
         input file: /usr/lib/vmware/vmkmod/ntg3
         Version: 4.1.3.2-1vmw.650.2.75.10884925
         Build Type: release
         License: BSD
         Required name-spaces:
         com.vmware.vmkapi#v2_4_0_0
         Parameters:
         initRingSzRxJmb: ushort
         RX Jumbo Ring Size
         initRingSzRxStd: ushort
         RX Standard Ring Size
         intrMode: ushort
         Interrupt mode: 0=IntX, 1=MSI(Default)


		а теперь выясним а какие из возможных параметров установлены
		# esxcfg-module -g ntg3
         ntg3 enabled = 1 options = ''

		как видно никакие. все по дефолту.
		прикол в том что непонятно откуда узнать а чему равно дефолтные значения
		
		значит нормальный производитель сетевых карт выпускает доки к своим 
		драйверам где описвыает какие опции у них доступны в драйвере для настройки
		
		например async драйвер для карт broadcom это tg3 , поэтому ищем в инете
		broadcom tg3 doc
		и находим pdf - Broadcom® NetXtreme® BCM57XX User Guide
		единственное что он плохо написан.
		а вот emulex для своих драйверов пишут хороший док.
		
		все таки остается непонятно как посмотреть текущие значения на драйвере
		тоесть часть значений может быть дефолтовая а часть кастомная и интересует
		так что там на выходе получилось. как это просмотреть непонятно.
		
кстати полезная функция как определить визуаьно какой vmnic соотвествует 
какой дырке на карте сервера. для этго можно заставить индикатор мигать

# ethtool -p vmnic0 2
где 2 это время в секундах сколько будет мигать лампочка


список модулей которые скажем так знает esxi 

# esxcli system module list

как активировать модуль

# esxcli system module set --enabled=true --module=native_driver_name

как деактииивировать модуль при загрузке

# esxcli system module set --enabled=false --module=native_driver_name
		
это потому что на система может одновременно могут быть установлены модули 
и нативные и линуксовые для одного и того же железки.

потом надо перезагрузиться.








значит какой важный момент. 
модули ставятся в систему из vib пакетов. это понятно.
и далее они сразу автоматом видны  в системе модулей. через команду

# esxcli system module list

если поставлен пакет  а модуль в этой команде невиден. значит какаято проблема
с этим модулем. можно попробоват команду типа задизейблить этот модуль


# esxcli system module set --enabled=false --module=имя_невидимого_модуля

тогда он может появвится в списке со статусом FALSE FALSE

например модуль igbn установлен из пакета но он невиден почемуто в списке.
тогда

# esxcli system module set --enabled=false --module=igbn

и о чудо он появляется в списке

# esxcli system module list
Name                           Is Loaded  Is Enabled
-----------------------------  ---------  ----------
igbn                               false       false

значение полей непонятно если четсно.

но факт есть факт если такой модуль активироат то он исчезнет в списке
и даже перезарузка ничего недаст.  я считаю что просто имеетмся какая то 
проюблему с запуском этого модуля этой версии драйвера вообще в целом.






приколно что сервер начинает пинговаться кактолько esxi загрузило модуль при загрузке.
а сама esxi при этом еще может и дальге продолжпть грузиться

 





про SR-IOV
рассмотрю карту HP NC362 она на основе intel I350
так вот вцентр в свойствах карты пишет что она якобы sr-iov совместимая.
однако это наебалово. я нашел об этом в редите запись. это типа баг как говорит делл
суппорт.
а как на практике это всегда проверить. надо посмотреть список доступных 
опций которые драйвер дает.

:~] esxcli system module  parameters list -m igb
Name                   Type          Value  Description
---------------------  ------------  -----  ----------------------------------------------------------------------------------------------------
DMAC                   array of int         Disable or set latency for DMA Coalescing ((0=off, 1000-10000(msec), 250, 500 (usec))
EEE                    array of int         Enable/disable on parts that support the feature
IntMode                array of int         Change Interrupt Mode (0=Legacy, 1=MSI, 2=MSI-X), default 2
InterruptThrottleRate  array of int         Maximum interrupts per second, per vector, (max 100000), default 3=adaptive
LLIPort                array of int         Low Latency Interrupt TCP Port (0-65535), default 0=off
LLIPush                array of int         Low Latency Interrupt on TCP Push flag (0,1), default 0=off
LLISize                array of int         Low Latency Interrupt on Packet Size (0-1500), default 0=off
MDD                    array of int         Malicious Driver Detection (0/1), default 1 = enabled. Only available when max_vfs is greater than 0
Node                   array of int         set the starting node to allocate memory on, default -1
QueuePairs             array of int         Enable Tx/Rx queue pairs for interrupt handling (0,1), default 1=on
RSS                    array of int         Number of Receive-Side Scaling Descriptor Queues (0-8), default 1, 0=number of cpus
VMDQ                   array of int         Number of Virtual Machine Device Queues: 0-1 = disable, 2-8 enable, default 0
debug                  int                  Debug level (0=none, ..., 16=all)
heap_initial           int                  Initial heap size allocated for the driver.
heap_max               int                  Maximum attainable heap size for the driver.
skb_mpool_initial      int                  Driver's minimum private socket buffer memory pool size.
skb_mpool_max          int                  Maximum attainable private socket buffer memory pool size for the driver.


как видно никаких признаков поддержки sr-iov
это и есть конечная инстанция.











		


мы определяем ее vid\did хрени. они нам нужны чтобы найти эту карту на сайте vmwar

# vmkchdev -l | grep vmnic2





кстати помотреть статистику работы порта

# esxcli network nic stats get  -n vmnic0
NIC statistics for vmnic0
   Packets received: 741
   Packets sent: 0
   Bytes received: 50778
   Bytes sent: 0
   Receive packets dropped: 0
   Transmit packets dropped: 0
   Multicast packets received: 739
   Broadcast packets received: 0
   Multicast packets sent: 0
   Broadcast packets sent: 0
   Total receive errors: 0
   Receive length errors: 0
   Receive missed errors: 0








 
 1. TOE tcp offload engine
 
 
 
 
 
 2.  далее посомтереть драйвер карты и ее фирмвейр 
 
  # esxcli network nic get -n vmnic0
   Advertised Auto Negotiation: true
   Advertised Link Modes: 10baseT/Half, 10baseT/Full, 100baseT/Half, 100baseT/Full, 1000baseT/Half, 1000baseT/Full
   Auto Negotiation: true
   Cable Type: Twisted Pair
   Current Message Level: 4260095
   Driver Info:
         Bus Info: 0000:02:00.0
         Driver: tg3
         Firmware Version: 5719-v1.46 NCSI v1.5.18.0
         Version: 3.137l.v55.1
   Link Detected: true
   Link Status: Up
   Name: vmnic0
   PHYAddress: 1
   Pause Autonegotiate: true
   Pause RX: true
   Pause TX: true
   Supported Ports: TP
   Supports Auto Negotiation: true
   Supports Pause: true
   Supports Wakeon: true
   Transceiver: internal
   Wakeon: MagicPacket(tm)

увидели вот это полезное

Driver: tg3
         Firmware Version: 5719-v1.46 NCSI v1.5.18.0
         Version: 3.137l.v55.1
		 
после этого можно узнать какие полезные 
опции поддерживает драйвер карты 

# esxcli system module parameters list -m tg3

disable_fw_dmp      int                  For debugging purposes, disable firmware dump feature when set to value of 1                                
disable_tso         int                  Setting the variable to 1 to disable HW TSO functionality on the driver level. Value of 0 will enable HW TSO which is the default.
force_netq          array of int         Force the maximum number of NetQueues available per port (NetQueue capable devices only)                    
heap_initial        int                  Initial heap size allocated for the driver.                                                                 
heap_max            int                  Maximum attainable heap size for the driver.                                                                
psod_on_tx_timeout  int                  For debugging purposes, crash the system  when a tx timeout occurs                                          
skb_mpool_initial   int                  Driver's minimum private socket buffer memory pool size.                                                    
skb_mpool_max       int                  Maximum attainable private socket buffer memory pool size for the driver.                                   
tg3_debug           int                  Tigon3 bitmapped debugging message enable value                                                             
tg3_disable_eee     int                  Disable Energy Efficient Ethernet (EEE) support       

далее можно псмотртеь что из возможных опция подкручено на карте

 #  esxcfg-module -g tg3
tg3 enabled = 1 options = ''

видим что ничего не подкручено. все по дефолту.





3.  NetQueue

термины:
vNic - это сетевая карта виртуальной машины

технологии вспомогательные чтобы NetQueue работало
vmdq (непонятно обящетально ли эта штука)
msi-x

вобще эта хрень изначально делалась только для карт 10Gb нениже.
для 1Gb якобы оно неимеет смысла большого.

что оно дает - пока неочень понятно.


провермть что netqueue принципиально разрешен на хосте.	 
# esxcfg-advcfg -j netNetqueueEnabled
netNetqueueEnabled = TRUE

еще можно вот так проверить
~ # esxcli system settings kernel list | grep -i netqueue
netNetqueueEnabled     Bool    Enable/Disable NetQueue support.    TRUE   TRUE   TRUE

кстати список всех переменных ядра
# esxcfg-advcfg -l

в этой стьате https://kb.vmware.com/s/article/2035701  написано 
что netqueue неимеет смысла на картах 1Gb. 
и типа одно ядро может тянуть на себе 3Gb. (непонятно какого конкрентно цпу)
однако получается что эту хрень надо настраивать если карта 10Gb.
иначе поток попадая якобы всего на одно ядро будет захлебывать 
это ядро. и надо поток разбивать.
а если на сервере 4 карты по 1Gb ? ну получается если esxi бдует 
плоток с порта кидать на разные ядра то потоки с карт небудут
упираться в цпу. а если он кинет все 4 потока на одно ядро
то сетеовй потрок упрется в цпу.
в общем это надо тестировать. нагружать 4 карты по полной. и смотреть
как там esxi служебные нитки нагружают ядра. 
потом настраивать распалеливание (как указано ниже) и смотреть 
есть ли разница.

так вот к  практике :
то что netqueue активирован на хосте этого мало. это как бы
только в принципе мы разрешеаем это делать. а надо еще прлписать конкретику.
надо на карточке настроить так на сколько цпу разбрасывать поток.

# esxcli system module parameters set -m tg3 -p force_netq=8,8
или вот еще так
# esxcfg-module -s force_netq=8,8 tg3

число восьмерок равно числу портов которые драйвер tg3 в данном случае
обслуживает на хосте. в данном случае две восьмерки означают 
что на серерер два tg3 порта.
8 означает что поток с карты разбивать по 8 ядрам.

если мы хотим чтобы поток неразбивался по ядрам то можно в целом
отключить на хосте ( знаю чтобы включить надо перегружать хост
а чтобы выключить незнаю надо или нет.) 

# esxcfg-advcfg -k FALSE netNetqueueEnabled

а можно деактивирватб для конкретного драйвера (ведь карты могут быть
от разных драйверов на хосте)

# esxcfg-module -s force_netq=0,0,0,0 tg3

для четырех портов tg3 разбивака потока по ядрам будет отменена.
я так это сработает мгнровеенно.

как я понял активация этой функции порой очень даже приводит 
к глюкам на сетевой карте. так что это прикол.

но подлоянки незаканчиваются. как я понял разбивать поток на 
неколько цпу можно непроизволльно. надо знать сколько RX поток
поддрживает порт. 

bcm5719 1Gb поддверживает 17 receive queues. это означает что
поток можно разбивать на 17 ядер. но небольше.

поэтому для 4 портов это будет

# esxcfg-module -s force_netq=17,17,17,17 tg3








про intel VMDQ.
как работает обработка сетевого трафика на сфере.
как я пока это понял.
пакет прилетает в NIC. при этом NIC запускает interrupt.
цпу видя что есть интеррапт понимает что сетевая карта получила  пакет.
из карты пакет попадает в standard switch сферовский функциаонал которого 
обрабатывается програмным кодом в виде процесса на сфере (какой это процесс
пока непонятно) , назовем этот процесс esxi-switch. этот процесс напрягая цпу 
сервера начинает разбираться а к какой ВМ направляется этот пакет. разбираясь
напрягается цпу сервера. когда свич понял к какой вм относится пакет этот 
пакет копируется (путем напрягания цпу сервера) в память виртуалки. 
и далее уже процесс который отвечает за виртуалку начинает напрягаться обрабатывася
данный пакет на своей vNIC.тут походу важно понимать что вроде как vswitch у него 
в esxi свой\свои процессы в системе висят. у виртуалки свои процессы.в итоге 
при обработке пакета напрягается вначале процесс от vswitch а потом уже 
процессы виртуалки. в итоге напрягается процессор два раза.
процесс (имя которго пока неизвестно) который отвечает за vswitch сильно грузит
цпу потому что ему приходится сортировать весь поток от всех виртуалок.
типа разбираясь какой пакет какой ВМ предназначен. незнаю что там трудного . наверно
трудного мало. просто сам поток по срупуту очень большой то обьем большой 
этому процессу приходится на себе обарабатывать каждый пакет влетающий в сервер 
обработка идет путем нагрузки цпу.
что делает vmdq. процесс сортировки сетевого потока переносится с vswitch в недра 
( ятак понял именно внедра) сетевой карты. внутри карты как бы появляется вирт 
свич в который влетают все пакеты и он их сам сортирует и на выходе карта выдает 
несколко очередей где находятся (тут я точно непонял) то ли в каждлоый очереди
находятся пакеты строго для отдельной ВМ , то ли группу ВМ. и пакеты в vswitch 
поступают уже через эти очереди а не скопом , поэтому по карйней мере в случае когда
очередль предназначена для отдельной ВМ то всвич ему ничего ненужно сортировать
ему нужно сразу эти пакеты копировать в память виртуалки на ее Vnic.
поэтму плюс vmdq в том что нет больше сортировки на vswitch. но все равно процесс 
всвич путем напрягу цпу должен копировать каждый пакет из всвич в память ВМ. это тоже 
плохо в плане того что грузит цпу но лучше чем было до этого.
я видел эксперимент. до активации vmdq сервер мог тянуть трафик суммарно чтото типа 3Гбит.
а после активации уже 9.8 Гбит с карточки.

тутже добавлю про sr-iov
что он дает.он убирает в сетевом пути компонент vswitch.
там тоже очереди заволятся на карте. но данные из очередей копируются сразу в память
виртуалки минуя vswitch. копирование происходит через механизм DMA. который
позволяет общаться устроуствам на компе обмеиваясь данными минуя участие 
цпу. что прекрасно. цпу начнет напрягаться уже на процессах которые образуют 
данную вм когда пакет от карты до vnic через DMA прилетит в ВМ.
главнрое что мы полностью исключили vswitch процесс из процесса транспорта
пакета от карты до ВМ.





про сетевой трафик.
как я понял отправкой и приемом сторадж i\o и сетевых пакетов 
у виртуалки отвечает процессы vmm ( сколько у виртуалки vCPU столько у нее
штук процессов vmm).  грубо говоря как я понимаю этот процесс обслуживает 
сторадж контроллер и сетевую карту vNIC виртуалки. что непонятно это какой 
или какие процессы в esxi обслуживают vswitch.

потому что между vNIC и реальной железной сетевой картой сервера еще 
лежит vswitch который обслуживается софтом сферы и некоторым ее служебным 
процессом.

значит я зпустил iperf3 между двумя VM в первом случае обе ВМ сиядт на одном
сервере. поэтому между ними как бы 10ГБ свич у них 10ГБ vnic.
и я смотрел какие проецссы в esxtop грузят цпу. 

вот как выглядят загрузка процессов одной ВМ 
напоминаю трафик идет на скорсти 10Gb

      ID      GID NAME                            NWLD   %USED    %RUN    %SYS   %WAIT %VMWAIT    %RDY   %IDLE  %OVRLP   %CSTP  %MLMTD  %SWPWT
 1229340  2287030 vmx                                1   79.99    0.11   79.90   99.59       -    0.01    0.00    0.00    0.00    0.00    0.00
 1229342  2287030 vmast.1229341                      1    0.02    0.03    0.00   99.68       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229344  2287030 vmx-vthread-5:test-iperf-1         1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229345  2287030 vmx-vthread-6:test-iperf-1         1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229346  2287030 vmx-mks:test-iperf-1               1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229347  2287030 vmx-svga:test-iperf-1              1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229348  2287030 vmx-vcpu-0:test-iperf-1            1   23.23   21.93    0.00   77.67    0.09    0.10   77.59    0.04    0.00    0.00    0.00
 1229349  2287030 vmx-vcpu-1:test-iperf-1            1   32.17   31.04    0.00   68.54    0.83    0.12   67.71    0.30    0.00    0.00    0.00
 



во втором случае обе виртуалки на разных серверах. на серерах стоят 1Gb железные карты.
что касается загрузки процессов одной из виртулок оно выглядит так
напоминаю что трафик идет на 1Gb


      ID      GID NAME                            NWLD   %USED    %RUN    %SYS   %WAIT %VMWAIT    %RDY   %IDLE  %OVRLP   %CSTP  %MLMTD  %SWPWT
 1229340  2287030 vmx                                1    4.33    0.06    4.26   99.64       -    0.01    0.00    0.00    0.00    0.00    0.00
 1229342  2287030 vmast.1229341                      1    0.02    0.02    0.00   99.68       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229344  2287030 vmx-vthread-5:test-iperf-1         1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229345  2287030 vmx-vthread-6:test-iperf-1         1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229346  2287030 vmx-mks:test-iperf-1               1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229347  2287030 vmx-svga:test-iperf-1              1    0.00    0.00    0.00   99.70       -    0.00    0.00    0.00    0.00    0.00    0.00
 1229348  2287030 vmx-vcpu-0:test-iperf-1            1    0.06    0.05    0.00   99.64    0.00    0.01  100.00    0.00    0.00    0.00    0.00
 1229349  2287030 vmx-vcpu-1:test-iperf-1            1   17.14   15.71    0.00   83.98    0.57    0.01   83.42    0.21    0.00    0.00    0.00

  

как видно vmx-vcpu загружен. он выполняет код самой виртуалки.
условно говоря это рабтает код драйвера сетевой карты на ОС
и загружен процесс vmx который как понимаю (так как загружен именно %SYS) это
работает уже код vmkernel который пихает трафик в ядро а может даже в железную
сетевую карту. 

в обоих случаях я не увидел в esxtop какойто служебный esxi процесс который бы
жрал цпу. таким образом я ненашел название процесса который обслуживает vswitch
возможно vswitch это не какойто один процесс. это группа процессов. 
каждый из которых имеет имя vmx и он запущен для каждой vm обслуживая ее часть
трафика.

значит еще раз emx-vcpu обслуживает код драйвера сетевушки ОС гостевой
а vmx обслуживает код ядра vmkernel пихая трафик либо в кернел либо в уже в железную
сетевушку но скорей всего в vmkernetl потому что в случае 10Gb так вм обе на 
одном сервер и трафик ни в какую железную сетевушку непопдает.

вспоминаем что user = run + sys
run это когда процесс выполнял код гостевой ос а sys это когда процесс 
вызывал код ядра и ядро vmkernel его код исполнялся.  хотя процессы vmm ядерные они в ps
видны но esxtop их не рисует. тоесть esxtop ккак я понял никакие ядерный процессы
не отображает их загрузку. вобщем темный лес.
но будем считать что если мы хотим гонять сетевой трафик от одной ВМ неважно куда 
на другую вм на этом сервер либо на другой физ сервер то нужно помнить 
что этот сетевой трафик требует 4.33 + 17.14 = 22% мощности от одного ядра X5650  
в случае 1Gb трафика. тоесть он вклюает в себя и сколько виртуалка будет загружена
и скольк ядро vmkernel потребует в себя мощности. получается 10 виртуалок 
если они все жрут по 1Gb трафика то это сожрет 0,22 * 10 = 2 ядра цпу X5650
это только для обеспечения сетевого трафика. 
а если у нас виртуалка херачит на 10Gb то это требует 1,3 ядра cpu X5650
если у нас 10 виртуалок и все херачат 10Gb поток то это сожрет 13 ядер X5650
ну или 5 ядер E5-2680v4 , это только обслуживание сетевого трафика.
поэтому тут надо уже подключать SR-IOV чтобы исключить из всей этой сжемы vswitch.

 я все больше прихожу к мысли что vswitch это не какойто один процесс.
а это группа процессов vmx каждый из которых запущен для каждой виртуалки. 
это такой как бы вирт порт от vnic в ядро vmkernel.
но непонятно сколько еще само ядро жрут цпу когда оно обрабатыавет трафик 
который пришел от всех виртуалок внутрь ядра. 




====================================

если на сторадже есть vmkdump
то хрен этот датастор можно атмаунтить.

лекарство - https://vnote42.net/2015/03/13/delete-vmkdump-files-on-esxi-hosts-and-hidden-vmkfstools-options/#:~:text=Remove%20vmkdump%2Dfiles,dumpfile

====

esxi

storage migtration 
идет на таких параметрах

bs=64KB, q=32, w=1

вот сколько может сторадж выжать из себя на чтение
с такой скоростью миграция и будет идти

прием esxi  и читает на таких параметрах и записывает при storage vmotion

=====

esxi 
smart array

если один из LUN мы сделали загрузочным через BIOS контроллера то он 
имеет в свойствах строчку

         Boot Volume: Primary


 Array C

      Logical Drive: 3
         Size: 151.36 GB
         Fault Tolerance: 0
         Heads: 255
         Sectors Per Track: 32
         Cylinders: 38901
         Strip Size: 256 KB
         Full Stripe Size: 256 KB
         Status: OK
         Caching:  Enabled
         Unique Identifier: 600508B1001CD2516C04E66BE8C55EF4
         Boot Volume: Primary
         Logical Drive Label: proxmox
         Drive Type: Data
         LD Acceleration Method: Controller Cache


еще это мжоно сделать через ssacli

# ssacli ctrl  slot=1 ld 1 modify bootvolume=primary


===========================
на esxi storage vomtion идет 
на параметрах w=1, q=32 , bs=64KB
таким образом эта скорость существенно ниже будет чем прилинейном чтении\записи
на тех же стораджах. потому что обычная линейное чтение\запись 
идет на режимах таких как w=1, q=32, bs=1-2MB, может даже bs и больше.
поэтому ненадо удвилсятся что миграция идет не такой скорости как макс 
линейная сокрость чтения\записи массива.

==============================
тема про то какие процессы образуют виртуалку с точки зрения esxi

процессы vmkernel зовутся у них как миры wordls

каждая виртуалка состоит из нескольких процессов( акак миров) 
каждый процесс имеет свой ID = World ID = WID, 
и все они принадлежат 
одной группе котрорая имеет свой group IP = GID

еще есть Cartel ID = CID = Leader World ID = LWID 

важно отметит что CID и GID это разное

забавно то что эти суки насоздавали кучу сущностей с кучей синонимов
согласно esxtop мы имеем

  LWID = Leader World Id (World Group Id)

LWID = World Group ID(WGI) = Cartel ID(CID) 

~ # vmdumper -l
wid=208166      pid=-1  
cfgFile="/vmfs/volumes/5d650939-22ae5f1f-93ec-70106fb182b2/_test_-kub-14/_test_-kub-14.vmx"
uuid="42 23 0a 3d ca ec b6 23-34 97 6a e1 34 c4 d6 f7"       
displayName="_test_-kub-14"     
vmxCartelID=208165

из этого выводы мы имеем 
wid - это просто world id, как pid  в линуксе
vmxCartelID = CID => то что нам нужно

теперь когда мы знаем CID=LWID=Cartel ID 
то  можно  получить полный список процессов\миров 
которые образуют виртуалку.

vmxCartelID=208165

через ps

~ # ps  | grep 208165
208165 208165 vmx                  /bin/vmx
208185 208165 vmx-vthread-7:_test_-kub-14 /bin/vmx
208186 208165 vmx-vthread-8:_test_-kub-14 /bin/vmx
208187 208165 vmx-vthread-9:_test_-kub-14 /bin/vmx
208188 208165 vmx-mks:_test_-kub-14 /bin/vmx
208189 208165 vmx-svga:_test_-kub-14 /bin/vmx
208190 208165 vmx-vcpu-0:_test_-kub-14 /bin/vmx
208191 208165 vmx-vcpu-1:_test_-kub-14 /bin/vmx
208192 208165 vmx-vcpu-2:_test_-kub-14 /bin/vmx
208193 208165 vmx-vcpu-3:_test_-kub-14 /bin/vmx

но это еще не все. есть еще процессы

~ # ps  | grep vmm | grep kub | grep kub-14
208166      vmm0:_test_-kub-14
208182      vmm1:_test_-kub-14
208183      vmm2:_test_-kub-14
208184      vmm3:_test_-kub-14

или другой способ

~ # ps -j | egrep "GID|kub-14"
WID     CID    World Name            		GID  	Command
208166         vmm0:_test_-kub-14   		208165
208182         vmm1:_test_-kub-14   		208165
208183         vmm2:_test_-kub-14   		208165
208184         vmm3:_test_-kub-14   		208165
208185 208165  vmx-vthread-7:_test_-kub-14 	208165 	/bin/vmx
208186 208165  vmx-vthread-8:_test_-kub-14 	208165 	/bin/vmx
208187 208165  vmx-vthread-9:_test_-kub-14 	208165 	/bin/vmx
208188 208165  vmx-mks:_test_-kub-14 		208165 	/bin/vmx
208189 208165  vmx-svga:_test_-kub-14 		208165 	/bin/vmx
208190 208165  vmx-vcpu-0:_test_-kub-14 	208165 	/bin/vmx
208191 208165  vmx-vcpu-1:_test_-kub-14 	208165 	/bin/vmx
208192 208165  vmx-vcpu-2:_test_-kub-14 	208165 	/bin/vmx
208193 208165  vmx-vcpu-3:_test_-kub-14 	208165 	/bin/vmx


внимательный глаз заметит что ps  | grep 208165
невыдал полный набор процессов а ps -j выдал
это потому что просто ps он показывает толко столбцы WID, CID
а ps -j также показывает столбец GID
так вот vmx процессы они выполняются в юзер спейсе (по аналогии с ядром линукса)
а vmm процессы выполняются в vmkernel (kernel) спейсе и они неbмеют CID
поэтому ps | grep 208165 их не грепает . так как 208165 это CID которого 
повторюсь vmm процессы неимеют.

что еще непонятно esxtop показвыает свой GID 
а ps показывает свой GID

как видно для виртуалки test_-kub-14 ps показывает что ее GID = 208165
а ее CID=LWID=208165

а esxtop показывает что ее GID=333943

#esxtop
     GID   	LWID 		NAME            
  333943 	208165 		_test_-kub-14      


тоесть CID=LWID и ps и esxtop показывают одинаковый а GID разный.
непонятно.


еще пример список миров\процессов для виртуалки с 16 vCPU
полученный через esxtop


     GID   LWID NAME                            
   12067  39189 vmx                                    
   12067  39189 vmast.39190                        
   12067  39189 vmx-vthread-19:mk-kub2-07          
   12067  39189 vmx-vthread-20:mk-kub2-07          
   12067  39189 vmx-vthread-21:mk-kub2-07          
   12067  39189 vmx-vthread-22:mk-kub2-07          
   12067  39189 vmx-mks:mk-kub2-07                 
   12067  39189 vmx-svga:mk-kub2-07                
   12067  39189 vmx-vcpu-0:mk-kub2-07             
   12067  39189 vmx-vcpu-1:mk-kub2-07              
   12067  39189 vmx-vcpu-2:mk-kub2-07              
   12067  39189 vmx-vcpu-3:mk-kub2-07              
   12067  39189 vmx-vcpu-4:mk-kub2-07              
   12067  39189 vmx-vcpu-5:mk-kub2-07              
   12067  39189 vmx-vcpu-6:mk-kub2-07              
   12067  39189 vmx-vcpu-7:mk-kub2-07              
   12067  39189 vmx-vcpu-8:mk-kub2-07              
   12067  39189 vmx-vcpu-9:mk-kub2-07              
   12067  39189 vmx-vcpu-10:mk-kub2-07             
   12067  39189 vmx-vcpu-11:mk-kub2-07             
   12067  39189 vmx-vcpu-12:mk-kub2-07             
   12067  39189 vmx-vcpu-13:mk-kub2-07             
   12067  39189 vmx-vcpu-14:mk-kub2-07             
   12067  39189 vmx-vcpu-15:mk-kub2-07             


тот же список полученный через ps -j зная СID=LWID=39189 виртуалки

# ps -j | grep 39189
39189 39189 vmx                  39189 /bin/vmx
39190      vmm0:mk-kub2-07      39189
39191      vmast.39190          39189
39192      vmm1:mk-kub2-07      39189
39193      vmm2:mk-kub2-07      39189
39194      vmm3:mk-kub2-07      39189
39195      vmm4:mk-kub2-07      39189
39196      vmm5:mk-kub2-07      39189
39197      vmm6:mk-kub2-07      39189
39198      vmm7:mk-kub2-07      39189
39199      vmm8:mk-kub2-07      39189
39200      vmm9:mk-kub2-07      39189
39201      vmm10:mk-kub2-07     39189
39202      vmm11:mk-kub2-07     39189
39203      vmm12:mk-kub2-07     39189
39204      vmm13:mk-kub2-07     39189
39205      vmm14:mk-kub2-07     39189
39206      vmm15:mk-kub2-07     39189
39207 39189 vmx-vthread-19:mk-kub2-07 39189 /bin/vmx
39208 39189 vmx-vthread-20:mk-kub2-07 39189 /bin/vmx
39209 39189 vmx-vthread-21:mk-kub2-07 39189 /bin/vmx
39210 39189 vmx-vthread-22:mk-kub2-07 39189 /bin/vmx
39211 39189 vmx-mks:mk-kub2-07   39189 /bin/vmx
39212 39189 vmx-svga:mk-kub2-07  39189 /bin/vmx
39213 39189 vmx-vcpu-0:mk-kub2-07 39189 /bin/vmx
39214 39189 vmx-vcpu-1:mk-kub2-07 39189 /bin/vmx
39215 39189 vmx-vcpu-2:mk-kub2-07 39189 /bin/vmx
39216 39189 vmx-vcpu-3:mk-kub2-07 39189 /bin/vmx
39217 39189 vmx-vcpu-4:mk-kub2-07 39189 /bin/vmx
39218 39189 vmx-vcpu-5:mk-kub2-07 39189 /bin/vmx
39219 39189 vmx-vcpu-6:mk-kub2-07 39189 /bin/vmx
39220 39189 vmx-vcpu-7:mk-kub2-07 39189 /bin/vmx
39221 39189 vmx-vcpu-8:mk-kub2-07 39189 /bin/vmx
39222 39189 vmx-vcpu-9:mk-kub2-07 39189 /bin/vmx
39223 39189 vmx-vcpu-10:mk-kub2-07 39189 /bin/vmx
39224 39189 vmx-vcpu-11:mk-kub2-07 39189 /bin/vmx
39225 39189 vmx-vcpu-12:mk-kub2-07 39189 /bin/vmx
39226 39189 vmx-vcpu-13:mk-kub2-07 39189 /bin/vmx
39227 39189 vmx-vcpu-14:mk-kub2-07 39189 /bin/vmx
39228 39189 vmx-vcpu-15:mk-kub2-07 39189 /bin/vmx

как видно esxtop непоказывает все миры относящиеся к виртуалке 
а только vmx часть этих миров. тоесть только те процессы которые работают в user space.


причем как я понимаю 
именно процесс vmx занимается обработкой сетевого трафика виртуалки

       ID      GID NAME                                      NWLD   %USED    %RUN    %SYS 
   39189    12067 vmx                                          1    8.08    0.10    7.99  
   39191    12067 vmast.39190                                  1    0.12    0.16    0.00  
   39207    12067 vmx-vthread-19:mk-kub2-07                    1    0.00    0.00    0.00  
   39208    12067 vmx-vthread-20:mk-kub2-07                    1    0.00    0.00    0.00  
   39209    12067 vmx-vthread-21:mk-kub2-07                    1    0.00    0.00    0.00  
   39210    12067 vmx-vthread-22:mk-kub2-07                    1    0.00    0.00    0.00  
   39211    12067 vmx-mks:mk-kub2-07                           1    0.00    0.00    0.00  
   39212    12067 vmx-svga:mk-kub2-07                          1    0.00    0.00    0.00  
   39213    12067 vmx-vcpu-0:mk-kub2-07                        1   28.97   29.77    0.00  
   39214    12067 vmx-vcpu-1:mk-kub2-07                        1   28.94   29.35    0.00  
   39215    12067 vmx-vcpu-2:mk-kub2-07                        1   28.61   28.72    0.00  
   39216    12067 vmx-vcpu-3:mk-kub2-07                        1   27.92   28.33    0.00  
   39217    12067 vmx-vcpu-4:mk-kub2-07                        1   30.11   30.35    0.00  
   39218    12067 vmx-vcpu-5:mk-kub2-07                        1   31.24   31.36    0.00  
   39219    12067 vmx-vcpu-6:mk-kub2-07                        1   28.66   28.81    0.00  
   39220    12067 vmx-vcpu-7:mk-kub2-07                        1   29.41   29.58    0.00  
   39221    12067 vmx-vcpu-8:mk-kub2-07                        1   30.94   32.18    0.00  
   39222    12067 vmx-vcpu-9:mk-kub2-07                        1   28.96   29.66    0.00  
   39223    12067 vmx-vcpu-10:mk-kub2-07                       1   31.36   31.83    0.00  
   39224    12067 vmx-vcpu-11:mk-kub2-07                       1   30.64   31.66    0.00  
   39225    12067 vmx-vcpu-12:mk-kub2-07                       1   29.34   29.42    0.00  
   39226    12067 vmx-vcpu-13:mk-kub2-07                       1   30.39   31.80    0.00   
   39227    12067 vmx-vcpu-14:mk-kub2-07                       1   32.06   32.43    0.00   
   39228    12067 vmx-vcpu-15:mk-kub2-07                       1   28.45   28.62    0.00   


видно что для vmx id=39189 %sys=7,99
я считаю что именно этот процесс для данной виртуалки обрабатывает сетевой
ее трафик



итак мы получили список всех процессов\миров составляющих VM

опишем типы этих процессов:

vmm  = как я понял это эмулятор ядра цпу . работает в кернел спейсе
vmast = это непонятно что делает процесс
vmx  = это непонятно. только я заметил что при сетевой нагрузке на виртуалку этот процесс заружен по цпу причем в %SYS
vmx-vthread  = это непонятно что такое
vmx-mks		 = отвечает за эмуляцию mouse\keyboard  а вот с поледним непонятно это screen. что за скрин и чем отличется от mks-svga
vmx-svga  = вроде бы эта штука эмулирует видеокарту как железку
vmx-vcpu = насколько  я понял это юзерский процесс по одному на каждый vCPU виртуалки который служит как бы прокси для уже ядерых модулей vmm


vmm = насколько я понимаю это либо модули ядра. либо код работающий
в режиме ядра и обменивающийся с ядром vmkernel инфо без посредников 
и лишних переключений.
в книжке сказано что vmm это процесс работающий внутри ядра. но это такой маркетинговый
булшит а что на техническом уровне пока непонятно.
vmm переводится как virtual machine monitor.
при запуске VM их для этой vm создается столько сколлько vCPU назначено 
виртуалке. насколько я понимаю vmm процесс как раз обеспечивает эмуляцию одного
ядра цпу для гостевой ОС. тоесть гостевая ос работая с ядром цпу который она "видит"
и запускает какуюто команду на этом цпу. и вот трансляцию того что хочет гостевая ОС
сделать на своем виртуальном цпу vmm транслирует на реальное ядро реального цпу 
железного.
пока непонятно все таки так какой процесс заведует обработкой сетевого трафика 
от ВМ. на видео от интел они рисуют что VMM это делает. 

=====================
есть утилита 
ssaducli

раньше она называлась SmartSSD Wear Gaug

которая может показать статус изнощености ssd на smart array контроллере
но она на esxi неустановить.толтко виндовс и линукс.

там же утилиита управлением смарт эрреем уже третий раз менятеся.
была hpacucli потом hpssacli теперь ssacli

так вот в другом месте я нашел что  в hpssacli уже мол и ненадо пользоваться
ssaducli мол теперь достаточно написать

# ssacli controller slot=3  show config detail

и это мол и есть тот самый отчет

у хп нашел вот что

To take advantage of SMARTSSD Wear Gauge™, 
Smart Array Firmware version 5.0 or greater is required

========================

hp smart array 
контрроллер.

если посмотреть на свойства ld
то увидим такую хрень как MirrorGroup


 # /opt/smartstorageadmin/ssacli/bin/ssacli controller slot=3  ld 1 show

Smart Array P840 in Slot 3

   Array A

      Logical Drive: 1
         Size: 16.00 GB
         Fault Tolerance: 1+0
         Heads: 255
         Sectors Per Track: 32
         Cylinders: 4112
         Strip Size: 256 KB
         Full Stripe Size: 1024 KB
         Status: OK
         Unrecoverable Media Errors: None
         MultiDomain Status: OK
         Caching:  Enabled
         Unique Identifier: 600508B1001C5D7A9B3A15FDDD59AA72
         Boot Volume: Primary
         Logical Drive Label: ESXI
         Mirror Group 1:
            physicaldrive 1I:1:5 (port 1I:box 1:bay 5, SATA HDD, 3 TB, OK)
            physicaldrive 1I:1:6 (port 1I:box 1:bay 6, SATA HDD, 2 TB, OK)
            physicaldrive 1I:1:8 (port 1I:box 1:bay 8, SATA HDD, 1.5 TB, OK)
            physicaldrive 1I:1:1 (port 1I:box 1:bay 1, SATA HDD, 2 TB, OK)
         Mirror Group 2:
            physicaldrive 1I:1:2 (port 1I:box 1:bay 2, SATA HDD, 2 TB, OK)
            physicaldrive 1I:1:3 (port 1I:box 1:bay 3, SATA HDD, 2 TB, OK)
            physicaldrive 1I:1:4 (port 1I:box 1:bay 4, SATA HDD, 1.5 TB, OK)
            physicaldrive 2I:2:1 (port 2I:box 2:bay 1, SATA HDD, 1.5 TB, OK)
         Drive Type: Data
         LD Acceleration Method: Controller Cache


что обьединяет диски входящие в Mirror Group 1.
то что это такой набор дисков на котором информация\данные
 записаны в полном обьеме. тоесть это рейд0. а на другом Mirror Group 2 
 находится вторая копия данных. тоемсть второй рейд0.
 поэтому: если мы вытащим диск из миррор груп то этот рейд0 развалится и нас
 спасает только то что у нас есть второй миррор груп.
 с другой стороны можно вытащить за раз все диски входящие в миррор груп
 и нам ниченго небудет потому что у нас есть полная копия информации
 на втором миррор груп.
 таким образом еще раз миррор груп это по факту = Raid0 наших данных.
 
 таким образом хьюлет пакардовский raid10 по факту это на самом деле 
 raid01. потому что это куча дисков собранных в рейд0. и потом от него построили 
 зеркало.
 
 для сранения raid10 это куча парочек дисков каждая пара это миррор. и эти парочки 
 собрали в рейд0.
 
 =====================
 
 пока короткое описание темы
 
 https 
 certs
 certificates
 
 на хосте esxi ест сертификат котрый он пользует когда мы 
 ломимся на хост по https
 
 лежит он в файлах:
 
# ls -1 /etc/vmware/ssl/
rui.crt
rui.key

эти два файла то бишь сертификат можно пересоздать 
и засунуть обратно на хост. перекопировать заменив.

как сгенерировать по быстрому

# openssl req  -nodes -new -x509  -keyout server.key -out server.cert

причем это можно как я понял сделать даже на самом esxi

потом просто пеерименовываем файлы в rui.crt , rui.key

далее надо перезпустить вспомогательные сервисы

# services.sh restart

если сертификаты как то там неопнравятся esxi то максимум что будет 
это 

1) потеря связи между вцентр и хост
2) небдует работать web https:// доступ на хост

тоесть херня.

===

еще тема пока коротко.

мы хотим с хостов esxi снятть подробный отчет о дисках которые крутсятн
на smart array контроллере.


на данный момент этот функционал встроен в ssacli
но под esxi создание отчета неработает. если мы запустим команду
создай отчет под esxi то ssacli пошлет

~ # /opt/smartstorageadmin/ssacli/bin/ssacli ctrl slot=3 diag

Error: An ADU report cannot be generated in VMware ESXi using the diag command
       due to limitations of the operating system. You must use the SSADUESXI
       utility executed from a remote machine in order to obtain the report.

       Obtain and install the Smart Storage Diagnostics Utility (SSADU) CLI.

       Please see the VMware Utilities User Guide for more information.



значит что надо сделать.

надо поставить на компе с виндовсом vmware cli

конфигурация компонентов такая:
на хостах стоит esxi 5.5
на них стоят контроллеры:
	p410i + ssacli-2.60.18.0-5.5.0.1391871
	p840ar + ssacli-3.30.14.0-6.7.0.7535516
	
значит надо на компе с винловсом в данном случае это win2012r2
поставить 
	- VMware-vSphere-CLI-6.5.0-4566394.exe 

и надо поставить 
	- HPE Smart Storage Administrator Diagnostic Utility (HPE SSADU) 
		CLI for Windows 64-bit
		3.47.6.0 (9 сен 2019)
		https://support.hpe.com/hpesc/public/swd/detail?swItemId=MTX_5a99cfc5a2dc4997b1b61c615f
	
вот с такими компонентами все срослось.
подчеркиваю что надо ставить именно diagnostic utility а не storage administrator


ксати найти на диске эту программу Diagnostic Utility  не так 
просто так как ее более старые версии то водну папку их HP ставит то в другую
конкретно эту версию на диске надо искать в папке 
	- "c:\Program Files\Smart Storage Administrator\ssaducli\bin\ssaduesxi.exe"

как вся эта шарманка работает.

ssaduesxi.exe через esxcli конектится к esxi хосту . там запускает ssacli 
и закачивает на виндовс комп отчет подробный с контроллера.

поэтому здесь важные все эти версии.
например что я словил на более старой версии ADU. она на хосте пыталась найти и 
запустить hpacucli тоесть старую версию ssacli , ее там конечно небыло 
поэтому ADU писало ошибку. причем тупейшую мол немогу найти esxcli.
да софт у HP всегда был гавно конченое то еще. и тольков логах ADU 
можно было найти причину что ей ненравится.
поэтому если унас на esxi стоит ssacli то надо на компе с виндовсом юзать 
современую версию ADU . чтобы она искаала на esxi хосте именно ssacli.

отчет забирается на виндовс комп так:

> c:\Program Files\Smart Storage Administrator\ssaducli\bin>ssaduesxi --server=10.0.0.105 
--user=root --password=vasya123
--file=esxi-105.zip

если мы хотим сокращенный очет только о состоянии изнощенности ssd дисков так назвыаемый 
HPE Wear Gauge то вот так

> c:\Program Files\Smart Storage Administrator\ssaducli\bin>ssaduesxi --server=10.0.0.105 
--user=root --password=vasya123
--file=esxi-105.zip  --ssd

но тут подьебка HPE Wear Gauge работает только если мы юзаем фирменные ssd диски 
от самой HP. на колхозных обычных ssd дисках ничего неполучим. никакого 
статуса изношенности.

опять же подчеркну что на обычных дисках мы статус об изношеннсои не получим.
а общий статус (который без ключа --ssd ) мы прекрасно получим. этот общий
отчет очень огромный. там вчастности есть информаци сколько byte было записано
на ssd с момента ресета сервера и  с момента рождения диска.

но прежде чем мы сможем успешно запустить ssaduesxi
нужно провести подготовитеьную работу (да ибо подьебки незакончилисб).

вначале нужно убедиться что мы можем пдключатся к esxi через vmware cli = esxcli
на винде.

во первых нужно esxcli путь к ней вставить в %PATH% .
чтобы введя в cmd esxcli система ее сразу находила. ибо именно так 
будет ее искать ADU

если ADU ненайдет esxcli то оно напишет

HPADUESXI requires the use of the vSphere CLI (esxcli) client
application. Make certain that this client has been installed
and is available from this directory.

но невсегда эта фраза значит то что на значит.
надо всегда смотреть текстовый лог ADU где уже написана реальная правда
что ей нравится
лог находится в той же папке
"c:\Program Files\Smart Storage Administrator\ssaducli\bin\esxcli.log" 




ибо напомню еще раз цепочеку как раюбоает ADU

ADU(ssaduesxi) -> windows vmware cli(esxcli) -> esxi(ssacli)

когда esxcli уже находится через cmd 
то надо проверить конект к хосту

> esxcli -s 10.0.0.105

оно српосит логин и пароль. а дальше выскочит подьебка

Connect to 10.0.0.105 failed. Server SHA-1 thumbprint: 62:67:6F:6F:86:84:2B:23:8
5:C5:0C:AF:65:B4:B3:4D:96:40:B8:8B (not trusted).

итак esxcli ненравится thumbprint от сертификата который он получает.
что делать. сразу скажу что бесполезно добавлять сертификат от esxi хоста
в Trusted Authorities в mmc сертификаты. для esxcli это совершенно похеру не смотря на 
обычную здоровую логику - мол если винда доверяет сертификату то и я доверяю.
нет esxcli это похер. обходится это по другому

идем вот сюда

"c:\Program Files (x86)\VMware\VMware vSphere CLI\Perl\apps\general\credstore_admin.pl" 

и нужно с помощью этой проги 
добавить в некое безопасное хранилище escli ip сервера и его thumbprint
которму мы будем доверять

водим такую команду

c:\Program Files (x86)\VMware\VMware vSphere CLI\Perl\apps\general>credstore_admin.pl 
add --server 10.0.0.105 
--thumbprint F2:C5:2:99:EE:DB:50:7A:2D:97:10:77:03:39:56:07:C9:18:14:0E
New entry added successfully
	

тут правда выяснится что чтобы команда отарботала надо чтоды на винде стоял перл(очередная
подьебка).  я поставил strawbwrry perl x64


теперь когда мы введем esscli для 10.0.0.105 
> esxcli -s 10.0.0.105

он больше небудет ругаться что ему ненравится thumbprint

когда esxcli успешно конектится к хосту esxi то 
можно запускать ADU. теперь должно все сработать.

еще раз почеркну что бесполезно сертификат от esxi хоста всатвлять в доверренные
сертификаты виндовса. esxcli на это глубоко похер.

еще также подчеркну что общий отчет о дисках он работает 
для любых ssd от HP и от любых сторонних . а отчет о изноеннности дисков HPE Wear Gauge
тот который в ADU гененируется через ключ --ssd он работает тлько для фирменных 
дисков HP


===========================================
