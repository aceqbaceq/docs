теперь добавляем дата ноды в к8


>> выяснилась странная но сверхважная вещь. про сеть на дата ноде.

у меня на дата ноде три IP разбиты на две карты

ens160
192.168.1.56
gateway 192.168.1.1

ens192
192.168.0.243
192.168.7.243


так вот все началось с того что я стучу с компа 0.141 а 1.56 и ssh сесссия поработает минуту и замирает. путем тыканья  выяснил что походу
пьесы пакет влетает в ens160 как и положено а обратный пакет вылетает 
не из ens160 а из ens192 ну наверное пока точно непонятно потому что так как мой комп 0.141 то типа до него более прямой маршрут из 0.243 который на ens192 хотя я всегда считал что на линуксу обратный пакет всегда вылетает из той карты в которую он влетел. получается как бы несимметричный роутинг.
 в общем путем тыканья чтобы вылечить эту херню надо: 
	создать таблицу 
		echo 200 reth0 >> /etc/iproute2/rt_tables
	имя reth0 абсолютно неважно. можно другое заюзать
	и потом надо дать команды
	ip route add 192.168.1.0/24 dev ens160 src 192.168.1.56 table reth0
	ip route add default via 192.168.1.1 dev ens160 src 192.168.1.56 table reth0
	ip rule add from 192.168.1.56 table reth0
 
 и тогда сейчас я поканепонимаю почему но обратные пакеты прекращаюи идти через ens192 и начинаюи идти как положено через ens160  и соотсвственно и сессия ssh больше не замирает.
 чтобы не вводить эти команду после перезагурзки их включают в /etc/netowrk/interfaces
	конечный вариант сетевого конфига смотри в ("interfaces сетевой конфиг  дата ноды.txt") самая фишка это post-up команды там


lvm разделы готовим  с помощью "lvm.txt"

на дата ноде в итоге монтируем отрезаные lvm в папки вида

/var/lib/kubernetes-PV-folder/1G/06

где 1G - это характерный размер отрезанного вольюма

далее вводим ноду в куб об этом ниже. а в самом низу как накатить local provisioner для которого мы готовили вольюмы.



- (для дата ноды)добавить доп сет карту под сеть 192.168.7.0 (эластик транстпорт)

на дата нодах монтируем бекенд диски локальные
	обьединяем из в один LVM-VG  а дальше интересно отрезаем
	от VG небольшие LV (100GB) к примеру и монтируем как отдельные
	партишены в /mnt/k8s-pv-folder это нам дает гибкоу управление
	местом под pv куба. и мы знаем что если мы выделили 100ГБ под кусок
	то столько и есть.не будет переоплнения.
	когда мы примонтировали LV то ставим на куб local provisioner (код yaml пихаем в гит). он автоматом создаем PV в кубе. все система готова
	для разворачивания программ.


дата ноды ставим как уже написано выше
замечу что flannel на дата ноду ставить НЕНУЖНО! (куб сам его туда накатит)
тажке замечу что интерфейс docker0 172.17.0.1\16 появляется на ноде 
при установке докера. до установки кубернетеса. это чисто докеровский
интерфейс




на дата нодах вводим 

# kubeadm join 172.16.102.31:6443 --token ho9qay.07wqqjjeuypa5i0p  --discovery-token-ca-cert-hash sha256:95e4e1db5e638587e3cee2d662e1d55ab48a04d9d2564e8e1bad6db6d6c85500


эту строчку берем из вывода после установки кубернетеса на мастер ноде.
если мы ее прохлопали то ее можно сгенерировать заново НА МАСТЕР НОДЕ

запускаем НА МАСТЕР НОДЕ

# kubeadm token generate
# kubeadm token create <generated-token> --print-join-command --ttl=0

(тут у меня на HA кластере вылезла ошибка, причина неясна.но команда отработала.
$ kubeadm token create 7f549s.y9b2bh4vm7mj23nt   --print-join-command  --ttl=0
kubelet.go:200] cannot automatically set CgroupDriver when starting the Kubelet: cannot execute 'docker info -f {{.CgroupDriver}}': exit status 2
)


и после это мастер нода выдаст строку вида

>kubeadm join 172.16.102.31:6443 --token s59914.k4ku8hhl6ityqpj2     --discovery-token-ca-cert-hash sha256:485f93d02e587d9286b2c3a439c59fbc7277d7181644f48099a459abeb53e22f

вот ее то и нужно запускать НА ДАТА НОДЕ!


если мы добавляем второй контро плейн то на мастере

# kubeadm init phase upload-certs --upload-certs
[upload-certs] Using certificate key:
cbec82f4e3eed8f5ac7522cfb4137baf38c1f4c2d2bd4f6b6db27714cd126fd9

тогда строка


# kubeadm join 172.16.102.100:6440 --token isinbt.lecpp3t1nenpqlt3     --discovery-token-ca-cert-hash sha256:3b2a317f993d7ebeace52eb0d72de6a489133ac6180765a9e21787f1aa29ddf8 
--control-plane --certificate-key cbec82f4e3eed8f5ac7522cfb4137baf38c1f4c2d2bd4f6b6db27714cd126fd9

далее если мы прям сразу посмотрим статус этой ноды
то будет видно что нода notready.
это потому что надо подождать минутку чтобы мастер-нода развернула  на дата ноде
под с фланнелем. вот когда фланнель будет мастером установлен на дата ноде. вот тогда
статус дата ноды станет ready. еще раз отмечу что руками разворачивать фланнель на 
дата ноде ненужно. мастер поставит это сам автоматом.


через пару минут если все окей то дата нода напишет

проверяем статус этой ноды в кластере

#
root@test-kub-01:~# kubectl get nodes
NAME          STATUS   ROLES    AGE     VERSION
test-kub-01   Ready    master   88m     v1.16.0
test-kub-02   Ready    <none>   3m35s   v1.16.0

видно что статус kub-02 = ready.

замечу еще раз что kubectl команда будет рабоать только если в $HOME на компе на котором 
мы хотим эту комаду запустить лежит config от кластера. иначе будет выдавать ошибку.

как тока мы убедились что куб увспешно завелся на 
дата ноде надо сразу еще раз проверить и докер и кубелет
на то какой они используют драйвер cgroup

# docker info | grep -i cgroup
# journalctl -u kubelet | grep -u cgroup

он должен быть systemd для обоих. если это не так
то ищи выше как это исправить.
это надо делать сразу пока мы не запустили на дата нодах контейнеры.

также сразу поймем то что при вводе дата ноды в кластер
куб ставим на дату ноду системные поды. 
посмотрим какие системыне поды куб поставил на дата ноду

# kubectl get pods --all-namespaces -o wide

kube-flannel-ds-t8md8      1/1          172.16.102.33   test-kub-03   
kube-proxy-tch8f           1/1          172.16.102.33   test-kub-03   

итого видно что куб мастер поставил на дата ноду 
сист поды фланнель и прокси.

еще раз замечу что по умолчанию к8 публиукует пользовательские поды только на дата нодах и не публикует на мастер ноде (системные поды есть на всех 
нодах)

если на дата нодах есть локлаьные диски то можно на куб 
поставить local provioner со сторадж классом и накатывать поды
на эти локалные диски такжу удобно как на сетевые стораджи
 (см. local provisioner.txt)
==
