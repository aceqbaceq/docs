теперь добавляем дата ноды в к8

настройки сети:

- (для дата ноды)добавить доп сет карту под сеть 192.168.7.0 (эластик транстпорт).
так как мы добавили дополниельный IP на дата ноду то надо исправить сразу все проблемы которые из за этого возникают 
	(см. "interfaces сетевой конфиг дата ноды.txt" )

настройки по дискам:
lvm разделы готовим  с помощью "lvm.txt"

теперь когда в целом мы вспомнили как нарезать lvm
то перейдем к конкретике

на дата ноде нам надо нарезать lv и смонтировать в папки чтобы потом 
скажем local provioner или какойто другой провижионер который умеет 
готовить из локальных дисков на дата ноде уже PV в кубе.

на дата нодах монтируем бекенд диски локальные
	обьединяем их в один LVM-VG  а дальше нарезаем
	 небольшие LV (100GB) к примеру и монтируем как отдельные
	партишены в /mnt/k8s-pv-folder это нам дает гибкоу управление
	местом под pv куба. и мы знаем что если мы выделили 100ГБ под кусок
	то столько и есть.не будет переоплнения.
	когда мы примонтировали LV то ставим на куб local provisioner. он автоматом создаем PV в кубе. все система готова
	для разворачивания программ.

для local provisioner 
монтируем отрезаные lvm скажем 100гигабайтовые в папки вида

/var/lib/kubernetes-PV-folder/100G/00
/var/lib/kubernetes-PV-folder/100G/01
/var/lib/kubernetes-PV-folder/100G/02
...
/var/lib/kubernetes-PV-folder/100G/XX

где 100G - это характерный размер отрезанного вольюма

смонтировали вольюмы - отлично.

далее вводим ноду в куб. а в самом низу как накатить local provisioner для которого мы готовили вольюмы. еще позже мы поставим keepalived на дата ноде.

сразу замечу что flannel на дата ноду ставить НЕНУЖНО! (куб сам его туда накатит)
тажке замечу что интерфейс docker0 172.17.0.1\16 появляется на ноде 
при установке докера. до установки кубернетеса. это чисто докеровский
интерфейс. куб его неиспользует.


итак вводим дата ноду в куб  

идем на НА МАСТЕР НОДУ

запускаем НА МАСТЕР НОДЕ

$ sudo kubeadm token generate
$ sudo kubeadm token create <generated-token> --print-join-command --ttl=0

замечу что эта пара команд она одноразовая. если ты уже ввел 
какую то ноду с их помощью то для новой ноды 
надо генерировать заново (проверил на себе)

и после это мастер нода выдаст строку вида

>kubeadm join 172.16.102.31:6443 --token s59914.k4ku8hhl6ityqpj2     --discovery-token-ca-cert-hash sha256:485f93d02e587d9286b2c3a439c59fbc7277d7181644f48099a459abeb53e22f

вот ее то и нужно запускать НА ДАТА НОДЕ. причем неважно подключаем
мы дата ноду к кубу у котрого один контрол плейн или их несколько в HA кластере. строка одна и таже.


далее если мы прям сразу посмотрим статус этой ноды
то будет видно что нода notready.
это потому что надо подождать минутку чтобы мастер-нода развернула  на дата ноде под с фланнелем. вот когда фланнель будет мастером установлен на дата ноде. вот тогда статус дата ноды станет ready. еще раз отмечу что руками разворачивать фланнель на 
дата ноде ненужно. мастер поставит это сам автоматом.

через пару минут если все окей то дата нода напишет

проверяем статус этой ноды в кластере

#root@test-kub-01:~# kubectl get nodes
NAME          STATUS   ROLES    AGE     VERSION
test-kub-01   Ready    master   88m     v1.16.0
test-kub-02   Ready    <none>   3m35s   v1.16.0

видно что статус kub-02 = ready.

замечу еще раз что kubectl команда будет рабоать только если в $HOME на компе на котором 
мы хотим эту комаду запустить лежит config от кластера. иначе будет выдавать ошибку.

как тока мы убедились что куб увспешно завелся на 
дата ноде надо сразу еще раз проверить и докер и кубелет
на то какой они используют драйвер cgroup

# docker info | grep -i cgroup
# journalctl -u kubelet | grep -u cgroup

он должен быть systemd для обоих. если это не так
то (см. "установка docker на хост.txt" ) как это исправить.
это надо делать сразу пока мы не запустили на дата нодах контейнеры.

также сразу поймем то что при вводе дата ноды в кластер
куб ставим на дату ноду системные поды. 
посмотрим какие системыне поды куб поставил на дата ноду

# kubectl get pods --all-namespaces -o wide

kube-flannel-ds-t8md8      1/1          172.16.102.33   test-kub-03   
kube-proxy-tch8f           1/1          172.16.102.33   test-kub-03   

итого видно что куб мастер поставил на дата ноду 
сист поды фланнель и прокси.

еще раз замечу что по умолчанию к8 публиукует пользовательские поды только на дата нодах и не публикует на мастер ноде (системные поды есть на всех 
нодах)

далее нужно на дата ноду добавить свою кастомную службу в systemd
которая будет делать нужные добавки для сетевых настроек.
(см. "systemd сервис для iproute cni0.txt" )


И НАКОНЕЦ если на дата нодах есть локлаьные диски то можно на куб 
поставить local provisioner со сторадж классом который автоматом 
из наших локальных подмонтированных lvm создать на кубе PV
на эти PV можно будет удобно устанавливть поды через сторадж класс также удобно как это делается на  сетевые стораджи. 
	(см. local provisioner.txt)

==
