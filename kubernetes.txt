
УСТАНОВКА KUBERNETES



общая схема установки kubernetes
	(далее будет детальная схема)
	

1. ставим git+gitweb
1.5 глобальная сетевая проблема с кубом.
2. подготовительная работа на ноде прежде чем на нее ставить куб
3. ставим контрол панель куба. 
	возможно два варианта 
		3.а ставим одну контрол панель
		3.б ставим несколько контрол панелей в HA кластере
4. ставим дата ноды
	на дата нодах монтируем бекенд диски локальные
	обьединяем из в один LVM-VG  а дальше интересно отрезаем
	от VG небольшие LV (100GB) к примеру и монтируем как отдельные
	партишены в /mnt/k8s-pv-folder это нам дает гибкоу управление
	местом под pv куба. и мы знаем что если мы выделили 100ГБ под кусок
	то столько и есть.не будет переоплнения.
	когда мы примонтировали LV то ставим на куб local provisioner (код yaml пихаем в гит). он автоматом создаем PV в кубе. все система готова
	для разворачивания программ.
	

установка с подробностями:

1. ставим git+gitweb
	(смотри git+gitweb+nginx.txt )

1.5 глобальная сетевая проблема с кубом.
	(см. "kubernetes глобальная сетевая проблема.txt" )

2. подготовительная работа на ноде прежде чем на нее ставить куб
	эта часть общая и для мастер ноды и для дата ноды. и там 
	и там ее надо делать.
	надо:
	   - если нода уже является частью другого куба то ее оттуда нужно удалить ( см. "kubectl drain ноду.txt"  )
	   - миниум 2цпу и 2ГБ
	   - часовой пояс !!!!!!!
	     $ sudo dpkg-reconfigure tzdata 
	   - удалить unattebded upgrades пакет
			$ sudo apt -y remove unattended-upgrades
	   - (если ставим на виртуалку) то потсавить в свойствах чтобы вся память была зарерривана чтобы файл подкачки несоздвался для виртулки на сфере
	   - если ставим ненавиртуалку то настроить ntp клиент
	   - отчключить свап в линуксе?.
	   - (для дата ноды)добавить доп сет карту под сеть 192.168.7.0 (эластик транстпорт)
	   - надо чтото делать с ext4 чтоб она неломалась при power loss. юзаем
		 опцию data=journal для всех разделов кроме корневого в том числе для LVM (см "ext4 data journal.txt"). 
		 если есть возможность то под журнал юзаем отдельный диск
		 корневой раздел нельзя монтировать с data=journal там какая то мудота при загрузке и в итоге корневой раздел будет примонтирован как RO
	   - примонтировать несколько дисков.прчием если это виртуалка необязательно добавлять на каждый диск отдельный диск сферы. можно добавить один большой а потом его порезать на куски через LVM (см lvm.txt)
			/var/lib/docker = 100G		 root.root		711
			/var/log = 2G				 root.syslog	775
			(на дата нодах):
			еще создаем папку  под pv куба 
			/var/lib/kubernetes-PV-folder  root.root		711
			и внутрь этой папки в подпапки монтируем lvm куски по 50-100ГБ  в зависимости какого размера PV мы будем отдавать подам (при такой системе мы можем гибко управлять lvm кусками и заполнение одного PV никак не влияет на другие PV)
					/var/lib/kubernetes-PV-folder/pv-01
					/var/lib/kubernetes-PV-folder/pv-02
		
		- записать в fstab примонтированные диски, 
			записывать lvm   через  /dev/mapper/...   
			пример
				/dev/mapper/vg01-lvol0  /mnt/lvol0   ext4  errors=remount-ro,data=journal   0    2
				( про fstab см fstab.txt)
			запрещено монтировать lvm разделы через UUID (почему см "mount lvm.txt")
		- перезагрузиться и убедиться что точки монтирования примонтировались по нужным путям
		- лишние сервисы линукса выключить
		- dns несколько прописать
		-forward актививровать
		-на  dns сервере прописать ноды куба тот ip который мы будем
		юзать чтоб на него по ssh заходить
		-сделать чтобы sudo без пароля для mkadm
		- несколько  сетевых карта связать вместе в L2 домен (bonding)
		-удаление следов установки предыдущего кубернетеса
			("удаление следов установки предыдущего кубернетеса.txt")
		- ставим docker на систему 
			("установка docker на хост.txt" )
		- установка пакетов кубернетеса на хост
			(установка пакетов кубернетеса на хост.txt)

3. ставим контрол панель куба. 
	возможно два варианта 
	3.а ставим одну контрол панель
		(установка единственной контрол панели куба.txt)
	3.б ставим несколько контрол панелей в HA кластере
         (смотри файл kubernetes-cluster.txt
		 начиная со слов  "ставим кипэлавдэ")
	3.в при желании можно установить coredns autoscale 
		(см. coredns autoscale.txt)

4. ставим дата ноды
		("установка куба на дата ноду.txt" )
5. на дата нодах ставим keepalived.
	чтобы через кластерные IP обращасять к проброшенным наружу портам подов куба (см. "keepalived.txt"
		со слов "секция - конкретные команды как  установить")

УСТАНОВКА KUBERNETES - КОНЕЦ

6. может быть забэкепить etcd или контрол панель целиком

=========================================================

МЫСЛИ



>>
идея. дата нода куба должны иметь просто IP это через который контрол панель куба связывается с кубелетом дата нод.

а для сервисов куба для обращения к ним снаружи категорически нельзя 
использовать вот те ip. потому что если дата нода легла то для приложения 
будет пропаадание связи. надо вместо этого на дата нодах поднматть keeplaived  и использоваеть его cluster ip при обращении снанружи к сервисам куба.



>>
мегановсть

продукт openebs 

что он дает.

он ползволяет исопльзвать локаьные диски на хостах с кубом
для ДИНАМИЧЕСКОГО ПРОВИЖИОНИНГА!!!! ЭТО МЕГАБОМБА.

как при этом можно гарантировать что убитый под будет создан на том же
же хосте на том же PV.

это гарантрванно! рабтает это так.

у нас есть опубликованный statefullset.
он создает pvc+pv. если мы при этом убиваем pod то pvc+pv все равно
осатется потому что statefullset то живой! тоесть схема такая
в поде жестко прописано какой у него pvc. а этот pvc жестко уже связан с 
pv. поэтому когда новый pod создаетс вместо убиттго у нкго тоже имя и тот же имя pvc в нем указан. так что супер! значит пока жив statefullset
будут живы все pvc (и pv) и поскольку поды персозадются с тем же именем
и постьоку в поде жестко прописано имя pvc то переознаый под будет 100% попопадать в тот же pv!

это в итоге позвояет ( open ebs) создать мегадешевый и мегабыстрый стррадж с динамическим провижионингом при этом не имея сетевого стораджа!
и не имея быстрой сети между хостами!

 


----
непонятно где докер хранит данные о контейнерах 
ведь их же нужно перезапускать после перезагрузки компа
----

~# kubectl -n kube-system get cm kubeadm-config -oyaml
apiVersion: v1
data:
  ClusterConfiguration: |
    apiServer:
      extraArgs:
        authorization-mode: Node,RBAC
      timeoutForControlPlane: 4m0s
    apiVersion: kubeadm.k8s.io/v1beta2
    certificatesDir: /etc/kubernetes/pki
    clusterName: kubernetes
    controlPlaneEndpoint: 172.16.102.100:6440
    controllerManager: {}
    dns:
      type: CoreDNS
    etcd:
      local:
        dataDir: /var/lib/etcd
    imageRepository: k8s.gcr.io
    kind: ClusterConfiguration
    kubernetesVersion: v1.19.2
    networking:
      dnsDomain: cluster.local
      podSubnet: 10.253.0.0/16
      serviceSubnet: 10.96.0.0/12
    scheduler: {}
  ClusterStatus: |
    apiEndpoints:
      test-kub-04:
        advertiseAddress: 172.16.102.34
        bindPort: 6443
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: ClusterStatus
kind: ConfigMap
metadata:
  creationTimestamp: "2020-10-10T14:07:32Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:ClusterConfiguration: {}
        f:ClusterStatus: {}
    manager: kubeadm
    operation: Update
    time: "2020-10-10T14:07:32Z"
  name: kubeadm-config
  namespace: kube-system
  resourceVersion: "158"
  selfLink: /api/v1/namespaces/kube-system/configmaps/kubeadm-config
  uid: b870385a-7546-4b16-bea4-d402c7e975ee




далее я нашел что нужно : 
отключение SELinux командой setenforce 0 необходимо для корректного доступа 
контейнеров к файловой системе хоста, что, в свою очередь, требуется для работы сети у подов (pod networks).
как это конкретно делать я пока пишу так как в ubuntu 16 selinux
по дефолту просто неустановлен

так. тут в сколько то слов как же устроен запуск  кубернетеса. 
об этой подноготной. на первый взгляд кубернетс control plane состоит
из подов. возникает вопрос каже стартуют поды если кубернетес еще
не стартовал. вроде как парадокс. на самом деле нет.
во первых что такое контейнер. контейнер в линуксе это всего навсего процесс.
да да. обычный процесс. и ничего более. вообще говоря это может быть группа
процессов (в LXC) но скажем докер предпочитает иметь один процесс.
так вот контейнер это процесс который по сравнению с "обычными" процессами
имеет ряд изоляций. он изолирован от остальной системы по ряду параметров.
а так это просто процесс. процесс с элементами изоляции от "обычных" процессов. что это за изоляции. процесс сидит в своем pid неймспейсе, сетевом
неймспейсе, своем mount неймпейсе, и во всех остаьных неймпейсах он у него свой. ( число индивидуаьных неймспейсов можно выбирать самые частые индивиду
альные неймспейсы это сетевой и mount неймспейс). 
если посмотреть в целом то "обычные" процессы все сидят в одном и том же
неймспейсе. в одном и том же сетевом неймспейсе, одном и том же маунт неймспейсе итп. в этом смысле "обычные" процессы образуют один большой
контейнер. так что "обычные" процессы это тоже контейнер.
раньше до появления неймспейсов весь линукс был один большой контейнер.
с появлением неймспейсов появилась возможность разные процессы сажать в разные
неймспейсы.
но самое главное это то что контейнер это тот же процесс. просто иза за
ряда изоляций один процесс может невидеть другой процесс либо полностью
либо его части. например процессы сидящие в разных маунт нейммпейсах 
невидят файловую систему друг друга. 
но еще раз главное самое то что контейнер это процесс. 
докер является интсрументом который позволяет создвавать процессы в своих
индивидульных неймпейсах не руками а быстренько освобождает нас от 
обезьянней работы. точ но также как /etc/ftsab освбобождает нас от обязанности
руками монтировать разделы при загрузке.
окей. вот запустили мы с помошью докера несколько контейнеров в системе.
значит когда докер создает контейнер то он его точно сажает в свой pid name
space , net namespace, mount namespace.
возникает важный вопрос - почему когда мы наберем 
# ps aux
мы увидим в нашем дефолтовом неймспейсе все процессы которые запущены в контей
нерах.
например
# ps aux

2890  0.2  2.1 747680 44968 ?        Ssl  Sep27  25:52 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-ku

2948  2.7 17.3 1098060 354560 ?      Ssl  Sep27 307:55 kube-apiserver --advertise-address=172.16.102.31 --allow-privileged=true --authorization-mod

вот я вижу из дефолтового неймспейса процессы 2890 и 2948 которые запущены
в контейнерах. это части куба.

ведь если у нас эти процессы запущены в своих pid namespacах то по логике
наш процесс работающий в своем pid неймспйсе недолжен видеть процессы
работающие в своих pid namespaсах.

на самом деле это неверно. из man pid_namespaces узнаем:
в мане написано что каждый новый pid namespace получается путем 
клонироавния какого то другого pid namespace.
и написано что процессы крутящиеся в дочернем неймспйсе ВИДНЫ в неймспейсе
родительском. (из этого вытекает что pid процесса это величина неуникальная,
и что процесс имеет по несколько pid-ов - в родительском он один
а в "своем родном" у него другой). есть также самый первый корневой
pid namesapce в котором получается видны абсолютно все процессы
компа крутящиеся во всех неймсейсах. из этого получается что в обычном
неймспейсе init видны абсолютно все процессы крутящиеся в контейнерах.
в каждом неймспейсе процесс видит процессы своего неймспейса и процессы
всех дочерих неймспейсов. процессы родительского неймспейса 
процесс невидит. один процесс видит другой это например значи что 
один процесс может другому послать сигнал используя $PID того другого процесса. то есть один процесс может влиять на другой процесс зная его $PID в этом неймспейсе. вернемся к /proc
именнно это и обьясняет почему мы сидя в bash в обычной
сесиии видим процессы кубернетесного control plane через ps aux. 
тут надо немного уточнить что ps как это происходит. ps читает данные из /proc. а  /proc(procfs) является отражением статистики процессов с точки зрения текущего pid namespace и его дочерних.
раз наш процесс видит все процессы дочерних неймспейсов 
значит и статистика процессов лежжащих в дочерних неймспейсах отражена в /proc . что значит процесс видит другой процесс. если процесс с точки зрения
данного неймспейса имеет некоторый pid то наш процесс сидящий вданном неймспейсе можеть через syscall например подать сигнал SIGTERM на тот процесс
указав его pid. то есть один процесс может влять на другой процесс зная его pid в текущем неймспейсе. это и значит что он его видит. еще это значит что 
в данном нейсспейсе процессы имеют свой pid. если один процесс А сидит в одном
неймспейсе а процесс Б сидит в другом неймспейсе и процесс А невидит процесс Б это значит что процесс Б неимеет своего pid в немспейсе первого процесса.
ну а раз процесс имеет в некотором неймспейсе pid то для этого процесса обязана записываться статистика на procfs в /proc/pid это я прочитал 
вдругом месте а именно . я прочитал что когда создается процесс то обязательно в ядре 
появлятеся запись в /proc/pid. как я понял эту запись делает само ядро без участия процесса. поскольку у нас процесс в каждом pid namespace может иметь pid отличный от его pid в другом pid namespace 
то логично что в каждом pid namespace /proc свой. раз pid процесса в разных
неймспесах разный то статистика пишется в разные /proc/pid/.
из за природы procfs и из за того что в родительских неймспейсах видны 
процессы дочерних неймспейсов мы в нашем Init неймспейсе видим процессы
куба.

таким образом получается также что если процессы сидят в разных pid namespace
это еще незначит что они друг друга невидят не значит что они изолированы.
дочерний невидит родителский. а родительский видит дочерний. 
односторонняя связь есть.

таким образом теперь стало понятно почему процессы куба запущены в контейнерах
а мы их видим из консоли хоста. парадокс разрешен.

что попутно интересного выяснилось про /proc

вот мы запускаем bash в новом pid неймспейсе

также. надо быть осторожным с утилитами которые читают данные из /proc
у нас так может быть что pid namespace один а /proc примонтирован другой.
осторожным имеется ввиду что у нас тогда мы будем видет статистику по процессам которые  к нашему неймспейсу процессов могут быть 
вобще не видны в нашем pid неймспейсе. пример утилит ps или lsof
они читают данные из /proc . если этот /proc к нашейму текущему неймспейсу 
неимеет отношения то запуск ps и lsof неимеет смысла. мы получим статистику
работы процессов от другого pid namespace
когда такое может получиться. 
сейчас покажу.
создаем руками как бы контейнер с новым pid и mnt неймспесами
~# unshare --pid  --mount --propagation private -f bash
наш новый mnt неймспейс склонирован с предыдущего. и это значит что 
все точки монтироавния которые были там теперь здесь.(ну не они сами 
а их клоны с новыми mount-id но бекенд у них тотже).
значит наш текущий /proc ровно такой же как был тотже.
проверим
~# ls -1 /proc
1
10
101
102
1053
1056
1061
1064
1067
1072
...

видим что куча процессов. а в нашем "контейнере" такого быть неможет.
также проверим с другой стороны
смонтируем /proc в отдельную папку

~# mkdir /mnt/A
root@test-kub-02:~# mount -t proc proc /mnt/A
root@test-kub-02:~# ls -1 /mnt/A/
1
14
acpi


теперь всего два процесса. это уже похоже на правду.

теперь мы точно знаем что в /proc мы видим procfs от предыдущего pid неймспейса а в /mnt/A от текущего. 

соотсветсвенно если мы щас запустим ps aux то увидим что у нас типа запущена
куча процессов. но это не так. это в том родитеьском pid неймспейсе так.
также lsof покажет кучу открытых файлов процессами. это тоже не у нас 
а у родительскго pid неймспейса.

наша задача отмаунтить /proc и примаунтить туда актуальный
прежде всего нам надо проверить открыты ли какието файлы на нашими процессами
на  текущем /proc но мы этого неможет делать потому что мы незнаем 
pid id наших текущих процессов в том pid неймспейсе. поэтому еслимы запустим
lsof то это нам ничего недаст так как мы незнаем пиды наших процессов здесь 
там. окей. просто доверимся что наши процессы на /proc ничего не открыли.
второй момент надо  проверить может у /proc есть субточки монтирования.
это мы уже можем проверить

# cat /mnt/A/1/mountinfo | grep '/ /proc'
424 281 0:4 / /proc rw,nosuid,nodev,noexec,relatime - proc proc rw
425 424 0:35 / /proc/sys/fs/binfmt_misc rw,relatime - autofs systemd-1 
426 425 0:97 / /proc/sys/fs/binfmt_misc rw,relatime - binfmt_misc 

binfmt_misc - это такая штука которая позволяет когда мы запускаем 
исполняемый файл из шелла то линукс читает первые байты этого файла 
и по ним распознает какой прогой исплонять файл. 
в общем мы можем без проблем эти субточки отмонтировать. 
а потом если что примонтировать после.

итак

# umount /proc/sys/fs/binfmt_misc

теперь можем успешно отмонтироавть /proc

# umount /proc

и терь наконце мы можем примонтировать актуальный для нашего pid неймспейса
procfs

~# mount -t proc proc /proc

проверяем

~# ls -1 /proc
1
24
acpi
...

все ОК.

теперть можем воспользваться ps aux и lsof

~# ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0  22624  5264 pts/0    S    01:00   0:00 bash
root        27  0.0  0.0  37364  3348 pts/0    R+   01:15   0:00 ps aux

все ОК

итак еще раз когда можно атмаунтить точку. 
то есть в текущем pid namespace на этой точке недолжно быть открытых файлов.
но этого мало. может быть что с этой точки неоткрыт ни один файл.но еще 
 надо чтобы у этой точки монтирования небыло субточек монтиорвания.
 только тогда ее можно атмаунтить

еще скажу то что какого то простого способа определить какой pid имеет
наш процесс вдругом неймспейсе нет. 
есть только статьи где програмисты указыают на syscalls с помощью
которых это можно вычислить


расписать вот мы сверху текузей точки примаунтили другую - какие файлы 
в итоге будут видны старой точки или новой

надо выяснить если я создал мнт спейс. и анмаунчу точку монтирования
она же в родительском  неймспейсе останется примаунченной . ответ - да.
размаунчивание точки монтирования в дочернем маунт неймспейсе неразмаунчивает 
точку с таким же путем в родительском маунт неймспейсе. это и логично
потому что наша точка и родительская это разные точки с разным mount-id

еще раз повторюсь про pid неймспесы. в новом pid неймспейсе недолжно 
быть изначально никаких процессов кроме того с которым мы его создали.
этот процесс имеет pid=1.
как это проверить надо в произвольную папку смонтирвать procfs
#mount -t proc proc /mnt/folder
и посмоотреть сколько процессов видит наш procfs
# ls -1 /mnt/folder
если там один два то кул. если больше то чтото пошло нетак.
если при этом в /proc много процессов то это значт что в /proc показан procfs
от предыдущего pid неймспейса.
еще момент что родительский pid неймспейс видит процессы дочерныих неймспейсов. а дети невидят процессы родителей. зная Pid процесса в одном
спейсе нет простых средств узнать его pid в другом неймспейсе. искать надо 
по строке запуска процесса его параметры. 

еще момент. проверил на практике. вот у нас есть точка монтирования.
и с нее даже могут читаться и писаться файлы. мы можем по тому же пути
примонтировать новую точку монтирования. и линукс это спокойно дает делать.
при этом мы будем видеть бекенд новой точки монтирования. 
при этом те файлы которые читали либо писались в старый бекенд  они будут 
продолжать работать. видимо до того момента когда мы их закроем.
если мы отмаунтим нашую новую точку монтирования то пояявится старый бекенд.

 
что еще выяснил. если отмаунтиить а потом примаунтить то это неодно и тоже если выполнить ремаунт. ремаунт маунтит старый бекенд.
поэтому скажем /proc нужно отмаунтить а потом примаунтить.

оказщалось что есть очень полезный ключ  --mount-proc у unshare если мы создаем новый pid неймспейс, 
с этим ключчом unshare перед запуском процесса в новом неймспейсе монтирует новый procfs
в /proc. соотсвтеенно этот ключ также создает новый mnt неймспейс и делает 
/proc приватным чтоб наш ноый /proc немешал исходному /proc.
удобно и ненужно делать это руками.
единственное что хочу сказать я проерил как работает эта опция
~# cat /proc/self/mountinfo | grep '/ /proc'
892 810 0:4 / /proc rw,nosuid,nodev,noexec,relatime - proc proc rw
961 892 0:189 / /proc rw,nosuid,nodev,noexec,relatime - proc proc rw

и видно что наш новый procfs был примаунчен сверху старого.
тоесть старый у нас неотмаунчен. то есть не так качественно сделано как
я делал это руками

чтобы узнать команда встроена в shell или это внешняя по отношению к нему команда юзаем type
# type cd
cd is a shell builtin
# type echo
echo is a shell builtin

chvt n -?

что еще я узнал. что когда мы в shell запускаем команду 
и эта команда является встроенной команду шелла например команда for 
или echo ( а провреить что это встроенная команда шелла можно через type echo)
то текущий процесс запускает fork() системный вызов
который создает новый процесс с новым pid.
этот новый процесс это точный клон старого процесса.
едиснвтенное то что старая память стоит в режиме на чтение и если 
память новый процесс меняет то новые куски пишутся в другое место.
старый процесс создается новый процесс и как  понял он команду которую 
мы вбили передает как параметр и вот этот новый процесс ее уже непострественно
исполняет.
возникает вопрос зачем для исполнения команды надо было создавать свой
клон. пока неясно. ведь старый процесс нихуя неделает он как я понял 
ждет окончания работы нового процесса. также непонятно если создался
новый процесс пусть он и клон старого то почему у нас на экране 
текстовая консоль не клонируется? вот интересно открыли мы консоль bash
вот этот черное окошко оно представлено одним процессом его реализация
или это 100 процессов каждый из которых чтото там делает чтобы это окошко висело. через ps aux видно что на одной tty висит сразу несколько 
процессов.
так.... как я примерно понял чтото новое и важное. = сейчас я рассмотрю более
базовый вариант. не сеанс по ssh с линуксом а то что я установил линукс
на физ комп и сижу за клавой и монитором перед ним.
и вот линукс грузитсяи и по экрану пошли строчки и появилось окно
ввода логина и пароля. я ввел и попал в так называемую сраную консоль.
что это за хрень. как она работает. какая ее архитектура.
вначале зайдем сдругой стороны. 
есть процесс в линукс.
и этот процесс как я понимаю всегда ли может не всегда но часто
имеет такой фукционал который ему предоставляется от рождения под названием
stdin , stdout , stderror.
код процессам может через сисколл в ядре наверно запросить данные
из своего stdin. а может выплюнуть во внешний мир какието данные в stdout.
так вот куда уйдут эти данные зависит далее от ядра. куда он их пиханет.
получается процесс он слепой как крот. и от всего отрезан. 
все что у него есть для связи с внешним миром это stdin stdout stderror.
идем далее. 
к процессу может быть прикреплен /dev/tty1
это типа как медаль на груди у процесса или то что ему выдали телефон.
так вот если к процессу прикреплен tty1 то тогда как я понимаю ядро направляет stdout процесса в устройство tty1 которое связано с 
драйвером консоли а это в конечном итоге значит то что все что высрал 
из себя процесс в свой stdout оно в конечном итоге отобразится на мониторе.
это как скажем испекли блины в казахстане в подвале и выдал их в окно из своего подвала а в итоге эти блины оказались на столе в нью йорке. вот 
в этом и есть глубокая трагедия процессов линукса. в том числе и bash.
это не сам баш рулит консолью и экраном компа. нет нихуя. 
монитор экрана в линуксе это нью йорк и олигарх на яхте.
а баш это таджик в подваел в казахстане который печет блины. 
баш без понятия куда блины уходят . она их только через окно выдает. 
а куда они дальше уплывают хуй знает. но у пользователя за компом 
создется впечатление что это баш рулит экраном.
получается примерно схема такая . тыкаешь кнопку  . этот тык уходит в
некий драйвер линукса. это драйвер берет эти данные и направляет их из клавиатуры (ньюйорка) в stdin
процесса (подвал в казахстане ) например bash. и он баш их обрабатывает.
получается экраном рулит драйвер в линуксе. а баш от этого экрана в 1 000 000километров сидит. между башем и экраном такая же пропаст как между 
человеком который запустил ssh клиент и сервером с ssh демоном на луне.
а человеку кажется что вот все рядом на экране черный квадратик.
получается черный квадрат на мониторе и подпись

home-computer#

это не баш. это деятельность драйвера линукса который рулит экраном.
баш там далеко в недрах линукса.а на экране драйвер tty отображает 
то последнее что баш когда то там высрал на свой stdout. он высрал и забыл
а tty продолжает отрисовывать на экране.
скажем баш это художник в барнауле. он нарисовал картину и выдал 
в окно. ее взяли и привезли на самолете в нью йорк и выставли в магазине
. художник уже давно своими делами занимается а картина стоит и стоит  в
магазине. поэтому огромная разница и пропасть между тем что нарисовано
в консоли и что делает баш и чем она занимается.

теперь понятно как могут жить несколько процессов у которых один tty.
да хоть мильон процессов с одним tty.
суть просто в том что если эт процессы начнуть срать в свой stdout
то это будет ядром нправлятпся в порядке общей очереди в дрйвер
консоли монитора и он драйвер будет это отображат в черном окне.

пример

процесс1 посылает "1"-> stdout > ядро -> драйвер консоли мониторра -> монитор-"1"

далее процесс2 посылает "2"-> stdout > ядро -> драйвер консоли мониторра -> монитор-"1 2"

то ест мы видим что символ 1 бует на экране результатом сранья первого 
процесса а символ 2 будет резултатом сранья процесса 2.

теперь понятно почему когда bash клонирует себя через fork у нас на экране
неудваивается число запущеных "черных окон". потому что это вообще разные вещи.

процессы это условно говоря призывники приписанные к одному военкомату
от того что число призывников увеличивается от этого вонкомат не клонируется
он остается один. просто увличивается число бабла которое втекает в 
военкомат.

процессы это как гавно. а консоль на экране это унитаз. 
при клонировании процессов унитаз никак не меняется. 
может поменять лишь количество говна котррое из него начнет вылиываться.

в какойто степени еще такая аналогия . 
браузер это фронтенд сайта. но это не его бекенд. бекенд находистя за три дветья земель это во первых а во вторых он нахоится на серверах.
а нам кажется что сайт он здесь рядом и  сайт это конечная картинка в браузере.
так вот брайзер это черный экран консоли. а bash процесс это сервера в америке которые срут потоком https на мой браузер. 

возвращаемся к тому что делаешь bash когда начинает исполнять команду.
рассмотрим вообще как работает ssh сеанс.

мы ввели пароль и демон ssh создает эту хитрую виртуальную консоль
и запускает процесс bash который имеет связсь  с этой консолью.
 связь состоит в том что кнопки из консоли летят в stdin баша 
 а когда процесс bash срет на stdout то это прилетает в итоге
  в черный экран.
  также ssh постоянно следит что если баш помер как процесс то он 
  вырубает этот черный экран.
  естественно ssh неявляется сам черным экраном он такой же тупой процесс
  экраном управляет драйвер линукса экрана.
  что мне немножко непонятно почему исчезает приглашение на ввод новых
  команд в bash когда мы запустил команду на выполнение. 
  что там конкретно случается.
  
  например.
  
  мы сидим в баше. и запускаем комаду for посчитай нам от 1 до бесконечности
  с сумму с шагом единица. и тыкнули enter
  
  совершенно понятно что наш процесс от ядра консоли получает на stdin 
  задание с параметрами. процесс баща начинает его исполнять. а именно
  он идет в ядро и делает syscall fork с параметрами. таким образом
  ядро создает +1 процесс. и этот процесс встает в очередь на исполненеие.
  потом наконец шедулер его всосывает на процессор и этот проецесс начинает 
  уже непосредственно исполняться считать эту сумму. 
  что мне пока непонятно что в это время твориться с исходным процессом.
  совершенно понятно что он переходит в состояние sleep. 
  но что это значит по сути непонятно. 
  возможно просходит вот какая хрень. наш исхдоный процесс он говорит ядру
  ты меня небуди пока новый процесс незакончит работу. таким образом 
 код исходного процесса перестает пихаться на процессор шедулером.
 и естественно понятно что сама черная консоль при этом продлжает жить 
 потому что ее обслуживает не сраный процесс баш а драйвер ядра. 
 который живет и здравтсвует каждую секунду. и это в первую очередб 
 драйвер ядра консоли обслуживает кнопки на экране.
 ну то что пока новый процесс считает и у нас нет приглашение на ввод команды
 но мы тыкаем кнопки и они русуются на экране. 
 это как я понимаю обслуживает драйвер консоли экрана а не тот сраный 
 исходный баш и не новый процесс. новый процесс в процессе работы
 если будет срать в stdout то это будет рисоваться на экране потому что 
 он тоже прикреплен же к tty. 
 потом он досчитает. сдохнет. шедулер увидит что новый процесс сдох
 и оживит исхоный процесс как он и просил.
 наверно так ?
 такая еще добавка. 
 если баш запускает встроенную команду то он пользует форк.
 а если баш запускает сторонний файл то он пользщует как я понял
 fork + execv.
 
 так вот это все была присказка.
 к сказке о том что 
 если я запускаю команду 

# unshare --pid --propagation private bash

то получаю ошибку
-bash: fork: Cannot allocate memory

а чтоб ее небыло надо  добавлять  ключ -f

вот с этим щас будем разбираться.

значит как по деталям это все работает.
мы сидим в баше. это процесс. мы даем команду unshare и тыкаем enter.
баш создает новый процесс который уже непосредственно
запускает unshare а наш баш засыпает.
новый процесс запускает unshare.
как я понял unshare использует системный колл ядра который тоже назвыается
unshare. так вот в чем ебанизм этого сисколлла как я понял он работает.
этот сиколл создает новые неймспейсы в том числе и новый pid namespace
далее unshare делает свой клон путем создания нового процессе в котором он уже непосредственно запускает bash но этом баш запускается во всех новых неймспейсах кроме pid неймспейса. а если сам баш уже создаст из себя новый процесс вот только тогда ядро поместит его в новый pid namespace.
ну не ебанизм? мы зачем новый неймспейс создали и bash указали? чтобы в него
баш и поместить. но я так понял это ебнутая реализация толи Unshare команды
то ли unshare сисколла.и короче наш бащ небудет запущен в новом неймспейсе. 
но! баш когда он стартует он делает сам по себе всегда как я прочитал 
парочку новых тредов или новых процессов дочерних чтобы они по быстрому сделали для него пару каких то грязных дел. сделали и сдохли. 
это тоже важная деталь. так вот они то и засовываются в новый pid namespace 
но они сделав свои дела подыхают и вот это  и вызвает проблему. хотя
точно какую неочень понятно. по мне я просто этого недолжен был заметить
а может проблема в том что дочерний процесс должен был головному башу 
чтото передать при смерти но так как дочерний процесс оказался в другом 
пид неймспейсе то головной чтото неполучает и падает с ошибкой.
так вот что дает ключ -f он дает то что unshare команда насильно пихает 
в новый пид неймспейс наш основной баш а не запускает его здесь.
тогда в новом неймспейсе запускается и основной и его потомошние дочерние
и так как они там все сидят в одном неймспейсе то без проблем даже подыхая
делятся данными друг с другом и все в шоколаде.
по мне основная дебилистика состоит в изначальной архитектуре - если мы заказали новый неймспейс почему программа запускается в этом а толко ее 
дети в новом . это же дебилизм.
пока хватит про это


полезняшки про procfs
узнать какой командой был запущен процесс
# cat /proc/2890/cmdline
kube-scheduler--authentication-kubeconfig=/etc/kubernetes/scheduler.conf--authorization-kubeconfig=/etc/kubernetes/scheduler.conf--bind-address=127.0.0.1--kubeconfig=/etc/kubernetes/scheduler.conf--leader-elect=true--port=0root@test-kub-01:~#

мегавещь /proc/pid/root - можно смотреть и копировть файлы из контейнера 
сидя в нашем init нейсмпейсе. 
главно что в mc это вобще неработает .  а в команднйо строке запросто!
проблема переноса файлов  между хостом и контейнером решена !
огонь !

папка /proc/self = proc/pid где pid это пид нашего текущего процесса
под которым мы лезем на procfs



     опишу как руками создать контейнер 
и чтобы он после презагурзкки его воссстановить:
рассказываю. одно дело создать контейнер а другое дело его сохранить 
и после пеоезагрузки восстановить. прежде всего нужен какойто каталог 
в нашем дефолтовом маунт неймспейсе. этот каталог лекго превратить либо через
overlayfs либо через mount --bind в точку монтирования.
сам запуск контейнера мы прописываем как службу в systemd.
далее нам нужно создать новый Pid, mount,net namespaces пока без
процесса запущенного внутри. где то нам надо сохранить ссылку в виде
файла на эти неймспейсы. далее вспоминаем что в новом сетевом неймспейсе
нет сетевых интервейсов кроме lo. значит создаем пару veth. 
один прокидываем внутрт созданного net неймспейса. назначаем ему IP адрес и дефолт гейтвей.
на хосте создаем свич и втыкаем veth в свич. теперь когда у нас все подготовлено мы запускаем unshare

# unshare --net=/путь к файлу mount==/путь к файлу pid=/путь к файлу путь_к_сприпту

в новых неймспейсах запускаемый скрипт должен сделать pivot_root в 
подготовленную точку монтирования где лежит структура папок похожих на корень 
на хосте, этот скрипт должен смонтровать в /proc актуальный для того pid 
неймспейса procfs и скрипт должен запустит в итоге тот бинарник который 
собсвтенно и должен рабоать в контейнере.
вот так я думаю работает запуск контейнера котторый мы запускали до этого 
до перезагрузки.

и я это рассказад потому что сейчас мы возвращаемся к вопросу как
стартует установленный control plane куба

вот мы поставили пакеты куба. далее мы их запускаем.
в нашем маунт неймспейсе есть папки на которых располагается будущая
корневая система контейнеров.
контрол плейн куба состоит из подов. 
под это группа контейнеров у которых ряд общих неймспейсов. 
посмотрим какие у них общие

                                cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 ipc -> ipc:[4026532486]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 mnt -> mnt:[4026532494]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 pid -> pid:[4026532495]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 uts -> uts:[4026531838]
root@test-kub-01:~#
root@test-kub-01:~# ls -1al /proc/2776/ns

dr-x--x--x 2 root root 0 Oct  7 00:41 .
dr-xr-xr-x 9 root root 0 Oct  4 04:04 ..
lrwxrwxrwx 1 root root 0 Oct  7 00:41 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 ipc -> ipc:[4026532486]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 mnt -> mnt:[4026532484]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 pid -> pid:[4026532487]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Oct  7 00:41 uts -> uts:[4026532485]

у них общие = cgroup, ipc, net, user неймспейсы

а в доке от куба пишут что у подовских контейнеров "shared storage/network resources" . брешут

значит это группа контейнеров в итоге .

значит про сохраннеие и запуск контейнеров после перезагрузки 
я уже описал. так что нет никакой проблемы стартанут контрол
плейн куба пока куб еще незапущен.

так что нет никаких технических проблем это делать.

то что мы видим в дефолтовом неймспейсе потом запущенные контейнеры
в списке процессов тоже уже необьяснимым неявляется.

в доках дополнитеьно пишут что сам kubelet отслеживает 
 здоровье  так назвыаемых статических подов. 
 статические поды это манифесты к которым указаны у кубелета
 в папке /etc/kubernetes/manifests
 
 вот какие манифесты там лежат
etcd.yaml
kube-apiserver.yaml
kube-controller-manager.yaml
kube-scheduler.yam

посмотрим дополнительно какие службы в systemd 
прописаны из тех что контрол плейн составляют

значит я посмотрел  в systemd сервисах нет ниодного 
сервиса по названию похожему на компоненты контрол плейна.

только кубелет стартует из systemd.
понятно. значит схема такая.

systemd запускает -> kubelet а он запускает контейнеры докера которые 
составляют поды компонентов контррол плейна куба -> докер контейнеры

поды котоые запускает куб назвыабтся как уже написал статические поды.

вот так сам себя запускает куб.   

приколтьно что на дата нодах в /etc/kubernetes/manifests
нет ничего.

возникает вопрос кто после перезагрузки дата нод на них пинает 
запуск etcd и фланнель подов. если в systemd их нет и в манифестах кубелета
их там троже нет. получается с мастер ноды  шедулер ?



---
кстати можно забиндить нетолько папку но и файл.
только для этго нужно вначале создать дестинейшн файл пустышку
~# mkdir /mnt/D
root@test-kub-01:~# touch /mnt/D/hosts
root@test-kub-01:~# mount --bind /etc/hosts /mnt/D/hosts

огонь !
интересно какая разница с хардлинками  к файлам
---
двигаем дальше.

публикуем pod 

# kubectl run vasya --image=quay.io/openshiftlabs/simpleservice:0.5.0 --port=9876

  а у него статус pending

~# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
vasya   0/1     Pending   0          7s

нам надо понять почему

root@test-kub-01:~# kubectl describe pods vasya
Name:         vasya

...

Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.
root@test-kub-01:~#

вот и причина 
нет дата нод на которых можно развернуть этот под.

посмотрим на этой мастер ноде 
еще раз на поды которые составляют структуру голого кубернетеса

 ~# kubectl get pods --all-namespaces
     NAME                               READY   STATUS    
   coredns-f9fd979d6-6h9dc               1/1     Running   
   coredns-f9fd979d6-cwn55               1/1     Running   
   etcd-test-kub-01                      1/1     Running   
   kube-apiserver-test-kub-01            1/1     Running   
   kube-controller-manager-test-kub-01   1/1     Running   
   kube-flannel-ds-pgqq2                 1/1     Running   
   kube-proxy-ltz7l                      1/1     Running   
   kube-scheduler-test-kub-01            1/1     Running   
 
 теперь посмотрим а сколько контейнеров составляет один под,
 возьмем первый под  coredns-f9fd979d6-6h9dc
 
 поищем чтото такое в контейрах
 
# docker ps тратата | grep coredns-f9fd979d6-6h9dc

60fc4275f411:     "/coredns -conf /etc…"            k8s_coredns_coredns-f9fd979d6-6h9dc_kube-system_f62ca5bc-6eee-4da9-b9ed-dcdb0c0c50c4_7

eff1cee12617:     "/pause"            k8s_POD_coredns-f9fd979d6-6h9dc_kube-system_f62ca5bc-6eee-4da9-b9ed-dcdb0c0c50c4_25
 
получается что один под составляет два контейнера для данного пода


про статические поды.
есть поды за которыми следит не apiserver а сам кубелет.
если поды эти останавливаются он их перезапускает.
это статические поды.
в /var/lib/kubelet/config.yaml 
прописано
staticPodPath: /etc/kubernetes/manifests

манифесты что лежат в этой папке кубелет считает статическими
и он их оттуда запускает.

в этой папке на мастер ноде лежат манифесты
etcd
kube-apiserver
kube-controller-manager
kube-scheduler

как видно все они являются core компонентами самого кубернетеса.

вот эта штука со статик подами как бы решает рекурсивную задачу - чтобы
запустить под нужен запущещенный кубернетес который тоже состоит из
подов.

тоесть кубелет помимо аписервера тоже умеет ухаживать за подами

далее.
по поводу маунт поинтов которые появляются в системе
после установки кубернетеса

во первых это несколько вот таких
/var/lib/docker/overlay2/5d32b...

смысл их уже понятен. говоря об первичном их смысле - это так выглядит
точки монтирования когда мы используем overlayFS.
говоря о вторичном смысле - зафига они сдались. когда 
оверлейфс смонтирован в нашем неймспейсе далее кубернетес создает под
а это в свою очередь создает докер контейнеры которые в свою очередь 
создают новый маунт неймспейс на основе нашего неймспейса и далее докер
делает pivot_root в новом неймспейсе в новую точку монтирования 
исходной точкой котрого является наша точка  монтирования оверлейфс.
таким образом каждая такая точки монтирования создается при создании
нового контейнера.

/var/lib/docker/containers/5bc7 = здесь хранятся логи контейнера

и вот еще такие точки монтирования появляются

/var/lib/kubelet/pods/f62ca
заметим что это подпапка кубелета.
смысл таких  папок.

возьмем одну конкретную

/var/lib/kubelet/pods/f62ca5bc-6eee-4da9-b9ed-dcdb0c0c50c4/volumes/kubernetes.io~secret/coredns-token-nf7kz

видно что она както связана с coredns.

если мы сделаем docker inspect контейнера с coredns
то мы увидим что данная папка используется чтобы из фс хоста пробросить
папку внутрь контейнера. 
вот и весь смысл папки /var/lib/kubelet/pods
оттуда пробрасывают папки внутрь контейнеров, но не всех попало контейнеров
нет  а которые относятся к системным
подам которые составляют сам кубернетес. 
 
 итак после установки кубернетеса у нас появится туча 
 новых точек монтирования. вот их лист
 
 /var/lib/docker/overlay2/5d32b...
 /var/lib/docker/containers/5bc7
 /var/lib/kubelet/pods/f62ca
 
теперь мы знаем их физичемский смысл
если мы заговоили про точки монтирования упомняем такую точку монтирования как
/dev/shm 
это точки монтирования tmpfs файловой системы. и как японял этот рам 
диск может быть использован чтобы передваавть данные между 
процессами.


далее.  важно помнить и понимать. вот мы заходим в папку вида merged 
например 

/var/lib/docker/overlay2/5d32b7b13a798972478/merged

так вот важно понимать и помнить  что мы при этом непопали
внутрь ФС контейнера. нет нет и нет. 
это исходная точка монтирования а контейнер работатет с ее клоном
. мы как бы зашли на зеркало которое отражает фс контейнера.
но еще раз говорю что важно помнить что контейнер работает не с этой
точкой монтирования а сдругой которая лежит в другом маунт 
неймспейсе. да мы видим многое но мы невидим субточки монтирования 
внутри фс контейнера. мы видим только простые файлы и папки
внутри контейнера. 

>>>>
 
остановился здеесь!!!!!!!!!

далее.

полезная команда

root@test-kub-01:~# kubeadm config view
(более новая версия этой команды  # kubectl get cm -o yaml -n kube-system kubeadm-config)
apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  dnsDomain: cluster.local
  podSubnet: 10.252.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}

здес четки видно какие ip будут иметь поды и какие сервисы

проверим . опубликум несколько подов . и убедимся что 
их ip лежат в сети 10.252.0.0

пробуем в кластере опубликовать под. 

для этого создадим файл nginx.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd

и создадим под

# kubectl apply -f nginx-pod.yaml

погвоорим о полезной информации которую можно почерпнуть из 

# kubectl describe pods nginx

сразу посмотрим надата ноде а сколько контейнеров у нас появилось
в связи с публикацией пода. ксати узнать на какой ноде крутится pod можно
через 

# kubectl get pods -o wide

так вот  под состоит из двух контейнеров. один наш контейнер
и второй контейнер запаузенный. так обычно под и состоит из двух
контейнеров.

когда мы введем 

# kubectl  describe pods nginx

мы увидим Container ID, пока что я незнаю что он значит 
потому что на дата ноде где крутится сам под его контейнеры 
не имеют этот container id. поэтому физ смысл этой штуки пока непонятен

поговорим про IP

посморим какой у пода IP


# kubectl describe  pods nginx | grep IP
IP:           10.252.1.4

если мы отмотаем наверх к команде kubeadm config view

то мы увидим

podSubnet: 10.252.0.0/16

очевидно что под имеет IP из сети podSubnet

возникает вопрос откуда мы можем пинговать этот 10.252.1.4
это маршуртизируемая сеть или нет или как



опубликуем еще один под

для этого в yaml файле просто имя подправим 
name: nginx
на
name: nginx2

публикуем наш второй под

# kubectl apply -f nginx-pod.yaml

смотрим какой у него IP

# kubectl describe  pods nginx2 | grep IP
IP:           10.252.1.5

очевидно что он входит в podSubnet: 10.252.0.0/16

окей. это мы прояснили.

>>>>>>>>>>>>>>>>>>>>>>>>>>

напоминаю конфигурацию кластера по нодам

root@test-kub-01:~/kube-samples# kubectl get nodes
NAME          STATUS   ROLES    AGE    VERSION
test-kub-01   Ready    master   108m   v1.18.0
test-kub-02   Ready    <none>   94m    v1.18.0

пробуем  пинговать под на мастере и дата ноде.


на мастер ноде(test-kub-01) это ничего недаст
а вот на дата(test-kub-02) ноде все успешно

попробуем понять почему

сравним IP на мастер ноде

root@test-kub-01:~/kube-samples# ip a | grep inet | grep -v 'inet6'
    inet 127.0.0.1/8 scope host lo
    inet 172.16.102.31/24 brd 172.16.102.255 scope global ens160
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    inet 10.252.0.0/32 scope global flannel.1
    inet 10.252.0.1/24 scope global cni0


и на дата ноде

root@test-kub-02:~# ip a | grep inet | grep -v 'inet6'
    inet 127.0.0.1/8 scope host lo
    inet 172.16.102.32/24 brd 172.16.102.255 scope global ens160
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    inet 10.252.1.0/32 scope global flannel.1
    inet 10.252.1.1/24 scope global cni0


берем IP нашего второг пода = IP:           10.252.1.5

видим что этот IP входит в сеть 10.252.1.1/24 на дата ноде.
это и обьясняет почему он успешно пингуется на дата ноде
и почему непигуется на мастер ноде.

еще раз напомню также podSubnet: 10.252.0.0/16

тоже самое касается и IP:           10.252.1.4 для первого пода.
будет пинговаться только на дата ноде.

причем еще замечу что если мы будет пинговать под с другой дата ноды
где его нет то тоже мы до него несможем достучаться.
итого 10.252.0.0\16 это адрес пода но достучаться до него 
по этому IP можно только непосредственно на хосте на котором 
он крутится. сугубо локально. ивсе.

>>>

теперь надо заглянгуть внутрь обоих контейнеров которые относятся 
к поду и посмотреть а какие ip они имеют. 
потому что контейнеры это не поды. конейтенеры это бекенд пода.

берем под

(куб мастер)# kubectl get pods -o wide
nginx    1/1     Running   1          92m   10.252.1.4   test-kub-02 

видим что nginx крутится на test-kub-02
заходим на него и ищем контейнеры которые образуют этот под

(test-kub-02)# docker ps | grep -E 'POD_nginx_|nginx_nginx_'

99d1c5baa423  "/docker-entrypoint.…"  k8s_nginx_nginx_default_7
1c644fb82abc  "/pause"                k8s_POD_nginx_default_7bd29f95


два контейнера. один из которых  /pause

ищем pid-ы этих контейнеров


test-kub-02:~# docker inspect  99d1c5baa423 | grep  '"Pid":'
            "Pid": 3148,

test-kub-02:~# docker inspect  1c644fb82abc | grep  '"Pid":'
            "Pid": 2853,

            
зная пиды может войти в их нетворк неймспейс. и увидеть 
их контейнерные IP


test-kub-02:~#  nsenter --net=/proc/3148/ns/net ip -c -f  inet address show
3: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> 
    inet 10.252.1.4/24 brd 10.252.1.255 scope global eth0
       
test-kub-02:~#  nsenter --net=/proc/2853/ns/net ip -c -f  inet address show eth0
3: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> 
    inet 10.252.1.4/24 brd 10.252.1.255 scope global eth0
     
итого мы видим что оба контейнера имеют во первых один и тот же IP,
во вторых он вточности равен IP самого пода.

это удобно. что контейнеры и под имеют один ip. а не то что
внутри контейнера один ip а потом он там как то хитропреобразывается
через ip tables в ip пода.
теперь можно смело понимать что ip пода это иесть в конечном итоге
ip внутри контейнера. едиснвтенно что таких контейнеров неодин.
по факту просто мы имеем два процесса которые сидят в одном нетворке
неймспейсе. а в этом неймспейсе сидит сетевой интерфейс с ip = 10.252.1.4

&&&&
(тут надо прочитать про pause контейнери про связь 10.252.1.4 
с интерфесом docker0 есть ли она). увязать также с docker networks ls

&&&&




замечу что сеть 172.17.0.1 от  docker0 неимеет вообще никакого смысла связи с кубернетесом.
сеть 10.252.0.0\24 на мастере и 10.252.1.0\24 на дата ноде. 
это сети под поды. то есть сидя на этой ноде. можно пингануть под этой ноды. ip будет указан 
в его свйойствах. 

таким образом еще раз подведем значению ip адресов на нодах на серверах

    inet 127.0.0.1/8 scope host lo
    inet 172.16.102.32/24 brd 172.16.102.255 scope global ens160   ( адрес ноды в локалке  который был до установки k8)
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0   (смысла нет практического)
    inet 10.252.1.0/32 scope global flannel.1                    (смысла нет)
    inet 10.252.1.1/24 scope global cni0                    ( сеть под поды )


сеть под поды - это еще раз значит что смотрим в свйоствах пода какой у него ip через kubectl describe pod nginx
и под этим IP зайдя на эту дата ноду на которой он лежит ( с другой дата ноды его непропинговать) он будет доступен.
забавно что пока мы неувидели нигде вот эту группу ip serviceSubnet: 10.96.0.0/12 под сервисы.
но двигаем дальше.




теперь сделаем к нжинкс запрос по 80 порту.

# curl 10.252.1.4
опять же на мастер ноде неудачно
а на дата ноде все окей.

4/8/2020


пользуясь случаем также замечаю что serviceSubnet: 10.96.0.0/12
для нее нет никаких IP ни на дата ноде ни на мастер ноде.



4/10/2020
я закончил на том что мне надо понять вот мы взяли pod
и надо понять к какому деплойменту сервису итд он принадлежит.
дальнейший ход понять что такое деплоймент сервис
и какие IP им будут назначены.

ибо низшая структура это pod, потом видимо реплика сет, потом дейлоймент, потом сервис.
как то так...


оказалось что узнать куда входит в под ( в какую replicaset) можно через kubectl describe pods 
и ищем строку controlled by

пример

# kubectl describe pods nginx-f89759699-kcp84 | grep -i Controlled
Controlled By:  ReplicaSet/nginx-f89759699

отсюда мы узнаем что под nginx-f89759699-kcp84 не сам по себе болтается а входит в определунный
replicaset


далее таким же образмо мы можем узнать а куда входить данная replicaset

 kubectl describe  replicaset nginx-f89759699 | grep -i controlled
Controlled By:  Deployment/nginx

ага она входит в деплоеймент nginx


обратим внимание на системыные поды. и посмотрим про них информацию


~# kubectl get pods --namespace=kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-5cm7d           

посмотрим куда входит под coredns-66bff467f8-5cm7d

Controlled By:  ReplicaSet/coredns-66bff467f8


ага в реплику сут такую то.

смотрим еще один системный под куда он входит

 kubectl describe pod --namespace=kube-system etcd-test-kub-01 | grep -i controlled
Controlled By:  Node/test-kub-01

ага уже входит невреплику а в Node. что такое node ?

но я смотрю пока дальше а куда входят другие системыные поды

root@test-kub-01:~# kubectl describe pod --namespace=kube-system kube-apiserver-test-kub-01 | grep -i controlled
Controlled By:  Node/test-kub-01

root@test-kub-01:~# kubectl describe pod --namespace=kube-system kube-controller-manager-test-kub-01 | grep -i controlled
Controlled By:  Node/test-kub-01

ага опять загадочный тип Node

root@test-kub-01:~# kubectl describe pod --namespace=kube-system kube-flannel-ds-amd64-b5wxf | grep -i controlled
Controlled By:  DaemonSet/kube-flannel-ds-amd64

демонсет это уже более знакомый тип контроллера

root@test-kub-01:~# kubectl describe pod --namespace=kube-system kube-proxy-dgdqq | grep -i controlled
Controlled By:  DaemonSet/kube-proxy

root@test-kub-01:~# kubectl describe pod --namespace=kube-system kube-scheduler-test-kub-01 | grep -i controlled
Controlled By:  Node/test-kub-01


итак осталось понять что это за загодный тип контроллера Node. просмотрел поискал - пока загадка что это значит.
Controlled By:  Node/test-kub-01

такого контроллера как Node нет в доках.


а так суммарно получается что системные поды входят либо  в

ReplicaSet
DaemonSet
Node

ради интереса посмотрим какие IP имеют системные поды

вот системные поды

root@test-kub-01:~# kubectl get pod --namespace=kube-system

coredns-66bff467f8-5cm7d	      
coredns-66bff467f8-6v6fj 	             
etcd-test-kub-01          	               
kube-apiserver-test-kub-01            
kube-controller-manager-test-kub-01   
kube-flannel-ds-amd64-b5wxf           
kube-flannel-ds-amd64-v5l4l           
kube-proxy-dgdqq                      
kube-proxy-mj7hp                      
kube-scheduler-test-kub-01            


а вот их IP и в какой контроллер они входят
# kubectl get pod --namespace=kube-system  | grep -v NAME | awk '{print $1}' |  kubectl describe pod --namespace=kube-system | grep -E -i 'controlled|IP:'


IP:                   10.252.0.2
  IP:           10.252.0.2
Controlled By:  ReplicaSet/coredns-66bff467f8

IP:                   10.252.0.3
  IP:           10.252.0.3
Controlled By:  ReplicaSet/coredns-66bff467f8

IP:                   172.16.102.31
  IP:           172.16.102.31
Controlled By:  Node/test-kub-01

IP:                   172.16.102.31
  IP:           172.16.102.31
Controlled By:  Node/test-kub-01

IP:                   172.16.102.31
  IP:           172.16.102.31
Controlled By:  Node/test-kub-01

IP:           172.16.102.32
  IP:           172.16.102.32
Controlled By:  DaemonSet/kube-flannel-ds-amd64

IP:           172.16.102.31
  IP:           172.16.102.31
Controlled By:  DaemonSet/kube-flannel-ds-amd64

IP:                   172.16.102.31
  IP:           172.16.102.31
Controlled By:  DaemonSet/kube-proxy

IP:                   172.16.102.32
  IP:           172.16.102.32
Controlled By:  DaemonSet/kube-proxy

IP:                   172.16.102.31
  IP:           172.16.102.31
Controlled By:  Node/test-kub-01

что мы видим

если под вхоим в репликасет то его IP входит в сеть выделенную под поды 10.252.0.0\16
если под входит в Node контроллер то его ip  = 172.16.102.31 , это LAN IP мастер ноды как сервера
если под входит в демонсет то его ip = 172.16.102.31 | 172.16.102.32 , то есть это LAN IP мастер ноды как сервера и дата 
ноды как сервера.

я вверху в списке подов подрисовал руками куда они входят

(имя-пода)				(куда-входит)		(IP-пода)		(хост-где-лежит)

coredns-66bff467f8-5cm7y		(ReplicaSet)		10.252.0.2		test-kub-01
coredns-66bff467f8-6v6fj 	        (ReplicaSet)     	10.252.0.3		test-kub-01
etcd-test-kub-01			(Node)			172.16.102.31		test-kub-01
kube-apiserver-test-kub-01		(Node)  		172.16.102.31          	test-kub-01
kube-controller-manager-test-kub-01	(Node) 			172.16.102.31		test-kub-01
kube-flannel-ds-amd64-b5wxf           	(DaemonSet)		172.16.102.32		test-kub-02
kube-flannel-ds-amd64-v5l4l           	(DaemonSet)		172.16.102.31		test-kub-01
kube-proxy-dgdqq			(DaemonSet)         	172.16.102.31           test-kub-01  
kube-proxy-mj7hp			(DaemonSet) 		172.16.102.32           test-kub-02          
kube-scheduler-test-kub-01            	(Node)			172.16.102.31		 test-kub-01


мы видим что все поды лежат на одном хосте - на мастер ноде, кроме одного фланнеля и одного кубпрокси
мы видим coredns входит в репликусет
мы видим что фланнэль и кубпрокси входят в демонсет
а остальное входит в Node

навскидку делаем себе зарубки. если под входит в репликусет то его IP лежит в сети выделенной под поды. 10.252.0.0\16
если под входит в Node то его ip = IP мастер ноды
если под входит в демонсет то его ip = IP нод входящих в кластер ( мастер нода и дата нода )

получается что все системные поды они пингуются со всех нод.
а coredns поды нет. они только на мастер-ноде видны.


вспоминаем исходную задачу -->

-->я закончил на том что мне надо понять вот мы взяли pod
и надо понять к какому деплойменту сервису итд он принадлежит.
дальнейший ход понять что такое деплоймент сервис
и какие IP им будут назначены. <--


я научился понимать как узнать куда входит под.
если в описании пода в поле "controlled by" ничего нестоит значит это одинокий под сам себе предоставленный.

еще я дополняю задачу -->
-->надо понять если убить под то автоматом он будет восстановлен системой или нет
и сохранит ли свой IP и все это в зависимости куда он входит посмотреть<--


на данном этапе мы рассмотрели поды. . выяснили их IP и куда они входят.
 поднимемся выше. посмотрим куда свою очередь входят реплики сет , демонсет, Node.
и какие полезные параметры они имеют.


вот они системные репликисет и демонсет

root@test-kub-01:~# kubectl get replicaset --namespace=kube-system
NAME                 DESIRED   CURRENT   READY   AGE
coredns-66bff467f8   2         2         2       4d1h

root@test-kub-01:~# kubectl get daemonset --namespace=kube-system
NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-flannel-ds-amd64     2         2         2       2            2           <none>                   4d1h
kube-proxy                2         2         2       2            2           kubernetes.io/os=linux   4d1h


посмотрим куда входит сисемная репликасет

~# kubectl describe replicaset --namespace=kube-system coredns-66bff467f8  | grep -i controlled
Controlled By:  Deployment/coredns

 в деплоймент

других полезных параметров в свойствах репликасет  я ненашел


посмотрим куда входят демонсеты

# kubectl describe daemonset --namespace=kube-system kube-flannel-ds-amd64 | grep -i control
root@test-kub-01:~#

~# kubectl describe daemonset --namespace=kube-system kube-proxy | grep -i control
root@test-kub-01:~#

то есть демонсет невходит уже нивочто. он является высшей структурой.
в свйоствах демонсета я интересного пока ненашел.


 я заглянул в свойства деплоймента.
пока чтотоже интересного там ненашел.


ни репликасет , ни демонсет , ни деплоймент = неимеют никаких IP в своих свойствах.

вот так мы рассмотрели системыне поды их свойства куда они входят.
уже какоето представление.


coredns поды это ничто иное как днс серверы запущенные на подах. а это IP адреса
по которым к ним можно обратиться.
самое опять же смешное что они оба сидят на мастер ноде. поэтому доступ к ним есть только с этой
мастер ноды. а с дата ноды уже никакого доступа нет.

 
вспоминаю поставленные задачи , убираю то что сделали
и оставляю то что осталось

задача-->
понять что такое деплоймент, сервис
и какие IP им будут назначены. <--

-->надо понять если убить под то автоматом он будет восстановлен системой или нет
и сохранит ли свой IP и все это в зависимости куда он входит посмотреть<--
<--задача

то что я щас буду делать - это убивать системные поды и смотреть что получится   12\04\2020

я удалил coredns контейнер 

# kubectl delete pods --namespace=kube-system coredns-66bff467f8-n8q28


и ( упс!) возникла проблема.  под автоматом новый создается но почему то он создается 
нена мастер ноде как это должно быть а на дата ноде. и в итоге неможет запуститься с ошибкой

Readiness probe failed: HTTP probe failed with statuscode: 503

то есть явно налицо корявая конфигурация на стороне к8 по развороту своих системных подов.

это ужасно. 

покачто как самому развернуть сломанный под coredns на мастер ноде незнаю.

кстати часть yaml-ов по развороту системных подов лежит на мастер ноде в папке

/etc/kubernetes/manifests

но там опять же манифесты только на уровне подов , нет ни манифестов репликасет ни  демонсет итд.

где это лежит непонятно

причем имеет место один еще очень важный вопрос. если даже система могла бы 
успешно развернуть coredns что там с данными. это голый бесполезный днс
или там каким то образом будут данные от кластера актуальные.

это касается всех системных подов. 
непонятно - если даже убитый под пересоздан потеряна там информация или нет.

если да. то спасением от убийства системных подов является только бэкап.


далее я успешно удалил 

kubectl delete pod --namespace=kube-system etcd-test-kub-01

система его  пересоздала и он рапортует что у него все успешно.
это хорошо. только остается вопрос он пустой или данные в нем есть. пока непонятно

витоге я поудалял все системные поды. и все они успешно пересоздались и завелись
то есть статус

READY

1/1

кроме coredns. потому что как уже сказал он почему то его пересоздает на дата ноде
а не на мастере.

далее я проверил а как там с IP адресами у системных подов после их пересоздания.
все IP адреса неизменны кроме того coredns который не смог завестись

итак осталась задача
задача-->
понять что такое деплоймент, сервис
и какие IP им будут назначены. <--


из того что я увидел. я увидел что часто более выской уровень после пода 
это репликасет. посмотрим на нее что это такое.

вопрос который вылез чем label отичается от label selector.
как я понял label - это свойство пода.
а label selector это свойство контроллера и мы в нем прописываем как раз лейбелы тех подов которые 
принадлежат данному контроллеру . пример

под
label=vasya

под
label=petya


контроллер
label selector = vasya

это значит что  "контроллер" ищет среди подов тот который имеет label=vasya и прибирает его к совим рукам.

таким образом label это характеристика обьекта которому он назначен
а label selector описывает не свойтсво обьекта в котором он прописан а свойства 
других обьектов которые данный обьект будет искать.

скажем русский это свойство человека. а РФ это обьект который в себя вмещает всех человеков которые русские.

человек
label=русский

РФ
label selector = русский.


--> есть такая проблема.
задаче - поднять под. поработать. забить данными.
потом поднять на другом дата ноде и чтобы данные были на месте.

поды это в конце концов докер контейнеры.
контейнер базиурется на локальной файловой системе.
проблеима в сохраннеии настроек из за файлового стораджа.
сетвая файловый сторадж.

к8 можно изучать снизу. то есть голове железо.
на него к8. на него сетевой iscsi харнилище.
и потом уже возимся с подами.

 а можно сверху. пофиг на сетевое хранилище.
вначале разобраться с контроллерами и ингрессами

<--


значит что я выяснил

root@test-kub-01:~# kubectl get svc --namespace=kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   9d

мы видим сервис=kube-dns и у него тип=ClusterIP
данный IP ни на какой ноде непингуется. окей это и не подразумевается по архитектуре сервисов.
а срабатывает вот что.

на мастер ноде работает curl 10.96.0.10:9153
на дата ноде это несработает.

и ( важный момент) как вроде подтвердид эксперимент и как написано в доке (https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/) данный способ ( clusterIP) он неделает сервис доступным извне кластера.

таким образом поды coredns которые в итоге сходятся вверх на сэрвисе kube-dns - они доступны только
на самих нодах кластера. а из вне кним необратиться.

 а вот способ NodePort  пробрасывает порт сэрвиса  на порт внешнего IP ноды.
и таким макаром сервис становится доступен извне. надо будет стучат на IP ноды : порт

щас будет пример.

дело в том что в кубернетесе есть вэб морда (web ui ). 
она кстати неустанавливается поумаолчанию.
ее нужно ставить отдельно.

так вот.

если мы поставим его согласно официальной доке. 
то это нам вообще недаст то что нужно.
потому что по умолчанию сервис для вэб морды будет создан типа ClusterIP

это значит что зайти мы в вэб морду снаружи несможем. мы сможем зайти на вэб морду k8 
только установив графическую оболочку на мастер ноде и введя в браузере адрес localhost

это полный ужас.бесполезная штука.


поэтому мы будем ставить вэб морду по другому.
как написано здесь


https://habr.com/ru/company/southbridge/blog/443658/

а именно берем указанный там "правильный" yaml. и применяем его

в итоге мы получим сервис типа LoadBalancer

root@test-kub-01:~/kube-samples# kubectl get svc --all-namespaces
NAMESPACE     NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
kube-system   kubernetes-dashboard   LoadBalancer   10.109.45.26   <pending>     443:31596/TCP            4s

кстати замечу очень полезный ключ --all-namespaces
он показывает все сущности во всех неймспейсах. это очень удобно когда мы незнаем в каком
неймспейсе мы ищем сущность  в данном случае сервис. а лезить в yaml файл и там смотреть нам лень.

значит я подтверждаю что данный сервис доступен 
если заходит на мастеере через сокет curl -k https://10.109.45.26:443
если заходит снаружи через https://172.16.102.31:31596

где 172.16.102.31 - это IP мастер ноды как сервера.

нам важно чтобы сервис был доступен снаружи.

то есть когда сервис типа LoadBalancer такой сервис доступен снаружи для запросов.
с дата ноды сервис доступен тоже только через внешний сокет https://172.16.102.31:31596
через внутренний недоступен

эти штуки сетевые работают через iptables.
навскидку я в них неочень разобрался. хотя там несупер сложно. но и нелегко.


можно поменять yaml  https://habr.com/ru/company/southbridge/blog/443658/
изменив в нем

LoadBalancer на NodePort

переэплайить yaml

и мы тогда получим сервис вот такого вида

root@test-kub-01:~/kube-samples# kubectl get svc --all-namespaces
NAMESPACE     NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
kube-system   kubernetes-dashboard   NodePort    10.109.45.26   <none>        443:31596/TCP            10m


соотвесвтенно такой сервис тоже доступен снаружи через https://172.16.102.31:31596
просто external-ip неподразумевается ибо Nodeport не подразумевает внешний балансировщий
а loadbalancer подразумевает.

что такое внешний балансировщик.

если у нас две ноды имеют IP

мастер=1.1.1.1
дата=1.1.1.10

и тип сервиса = NodePort , 31596

то сервис доступен снаружи через сокеты

1.1.1.1 : 31596
1.1.1.10 : 31596

если же у нас в сети есть некий внешний балансировщик ( типа нжинкс , амазоновский балансировщик итп) у которого ip = 3.3.3.3

то можно указать что сервис имеет тип = LoadBalancer
 и тогда наш сервис будет доступен снаружи по ЕДИНОМУ IP = 3.3.3.3 :31596 
а не по двум как это есть когда сервис имеет тип NodePort


хотя при этом также подчеркну что кодга сервис=loadbalancer то сервис доступен снаружи нетолько сокету 
внешнего балансировщика 3.3.3.3 :31596 но и по двум другим сокетам самих нод

1.1.1.1 : 31596
1.1.1.10 : 31596

окей. морду поставили. 
достучаться до нее снаружи можем через браузер.

далее. надо пройти аутентифкацию.

4\18\2020
закончил на том что надо через bearer token войти в вэб морду к8

теперь надо зайти в вэб морду.
сделать это непросто.


из той же самой статьи https://habr.com/ru/company/southbridge/blog/443658/
делаем

( что это малопонятно но разбирать щас небудем)

control# vi kube-dashboard-admin.yaml
apiVersion: v1 
kind: ServiceAccount 
metadata: 
  name: admin-user 
  namespace: kube-system 
--- 
apiVersion: rbac.authorization.k8s.io/v1beta1 
kind: ClusterRoleBinding 
metadata: 
  name: admin-user 
roleRef: 
  apiGroup: rbac.authorization.k8s.io 
  kind: ClusterRole 
  name: cluster-admin 
subjects: 
- kind: ServiceAccount 
  name: admin-user 
  namespace: kube-system

control# kubectl create -f kube-dashboard-admin.yaml
serviceaccount/admin-user created 
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

получаем токен ( тоже малопонятно но тоже щас небудем это разбирать )

# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')


берем этот токен и вставляем в браузере

в

https://172.16.102.31:31596

в итоге я зашел в этот душбоард но с кучей приколов

по первых только через файрфокс потому что хром пишет проблема с сертификатом.
во вторых зайдя в файрфокс толком графики непоказываются множественные ошибки 

\\\configmaps is forbidden: User "system:anonymous" cannot list resource "configmaps" in API group "" in the namespace "default" \\\


и вроде как в форуме разрабов дашбоарда написано что так и есть типа новый к8 неподдерживает старый дашбоард (незабыавем
что мы ставили дашбоард из кастомного манифеста).

и еще проблема как сделать чтобы куб отдавал через дашбоард другой сертификат.

это все куча вопросов. пока все это я оставляю.

пока что достаточно для дашбоарда и на этом.

вот вроде как здесь указано как сертификат создать - https://itnext.io/how-to-expose-your-kubernetes-dashboard-with-cert-manager-422ab1e3bf30 но пока что там куча непонятного.

поэтому едем дальше.
есть понимание что модель аутентифкации авторизации пока что в к8 тоже остается за бортом


интерсно в чем разница между фланнель и kubeproxy. и тот и тот обеспечивает "сетевые дела" кубернетеса.

понять бы разницу.


уже понятно вот что. что flannel сеть =  CNI ( незнаю что это)  = overlay network


overlay network термин означает что сеть посроена поверх существующей.

типа у нас по дефолту на серверах стоят сетевые карточки с реальными ip адресами в локалке нашей.


172.16.102.31

172.16.102.32

мы ставим софт на эти серверы. и внутри серверов поднимаем некую новую логическую IP 10.252.0.0\16 сеть.
через которую мы можем связываться между нодами по этим новым IP адресам.
но важно две вещи. видимость по новым IP адресам работает только между нодами.
и вторая вещь при передаче между нодами используется как прикрытие существующая "реальная" IP сеть 172.16.102.0\24

суть оверлейной сети термина стал понятен. оверлей обеспечивает у нас фланнэль.
насолько я понимаю с точки зрения компонетов к8  оверлейная сеть дает IP адреса ПОДАМ.

связь между нодами по IP оверлейной сети обеспечивается маскированием через iptables (postrouting prerouting SNAT DNAT) под
ip адреса 172.16.102.0\24

конкретно какую ip сеть через какую карточку  дает фланнель на мастер ноде

# ip a 

10.252.0.1/24 scope global cni0


на дата ноде

10.252.1.1/24 scope global cni0

при этом конфиг к8 также показывает

podSubnet: 10.252.0.0/16
serviceSubnet: 10.96.0.0/12

откуда еще раз видно что сетевые карточки cni0 созданные фланнелем образуют сети
которые как подсети входят в сеть выделенную в к8 для ПОДОВ.

карточки фланнелевские на нодах виртуальные ( типа как vpn карточки бывают )

без фланнеля к8 неможет выдавать подам IP адреса. 
зачем нужен фланнель. возникает такой вопрос. надо чтобы поды получали IP адреса назависящие от 
IP адреса сервера в сети. к8 же незнает о всех других IP участниках сети. поэтому для 
подов должны быть IP совершенно независящие от IP сервера и других IP серверов в локалке. 
можно было бы без фланеля както выдать IP подам на ноде и спрятать эти IP за натом 
но надо чтобы между подами была связь между нодами. поэтому нужен фланель который создает
вирт сет  карточку на нодах на каждой.  

10.252.0.1/24
10.252.1.1/24

и к8 уже выдает подам IP из этой сети. и плюс фланнель занимается междунодовй комуникацией.
чтобы для к8 и для подов это было все прозрачно.

типа фланель занимается впн между нодами. а к8 этим незнаимается для него это прозрачно.

поканепонятно но возможно междунодовая связь средствами фланеля идет через iptables. но это неточнопока.


кстати еще момент. далеко не все ПОДЫ получают IP из фланелевской сети( вверху это было показано).
поды системные зачастую имеют IP не из фланелевской сети.

далее,

первая нода фланель

cni0, 10.252.0.1/24





вторая нода фланель

cni0, 10.252.1.1/24




получается к8 берет сетть прописанную в kubeadm confi view = podSubnet: 10.252.0.0/16

делит ее на подсети \24 и назначает уже по нодам. 

на первую 

10.252.0.0\24
на вторую
10.252.1.0\24
на третью ( просто ее нет )
10.252.2.0\24
на четвертую
10.252.3.0\24

таким образом траифик  между подами одной ноды невылезает за пределы ноды ( широковещательный).
а в целом все сети входят в более широкую сеть \16.



посмотрим теперь на счет между нодовой связи. посмотрим таблицы роутинга


первая нода

10.252.0.1/24 scope global cni0

Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         172.16.102.1    0.0.0.0         UG    0      0        0 ens160
10.252.0.0      *               255.255.255.0   U     0      0        0 cni0
10.252.1.0      10.252.1.0      255.255.255.0   UG    0      0        0 flannel.1



вторая нода

10.252.1.1/24 scope global cni0

Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         172.16.102.1    0.0.0.0         UG    0      0        0 ens160
10.252.0.0      10.252.0.0      255.255.255.0   UG    0      0        0 flannel.1
10.252.1.0      *               255.255.255.0   U     0      0        0 cni0


я нашел что согласно к8 сетевой модели каждый под может достучаться до другого пода
без ната и других спец усилий. другими словами путем прямого пинга.
также к8 сетевая модель обязывает чтобы с ноды (сервера самого) можно достучаться
до любого пода путем прямого пинга
еще к8 сетевая дока требует чтобы под сам себя и другие поды видят его под
одним IP (тем же самым).


в итоге вышли на то что надо в к8 понять сеть!!!!

4\19\2020

начинаем с veth понять что это и как

cgroups - задает лимит цпу памяти и прочего 
namespaces - позволяет ограничиить видимость ресурсов друг от друга.

есть несколько типов неймспейсов.

pid неймспейс делает вот что. процесс он в линуксе принадлежит какомуто конкретному pid неймспейсу.
и процесс который сидит в каком то неймспейсе он видит только процессы в своем неймспейсе.
по умолчанию процесс принадлжеит дефолтовому неймспейсу.

сетевой неймспейс. каждый сетевой интерфейс входит в некоторый сетевой неймспейс. это дает то что
сетевой обмен возможен только между интерфейсами которые сидят в одном неймспейсе.
связь   между интерфейсами разных неймспейсов невозможна. только если между неймспейсами не сделан
специальый мостик.
полуается такая фигня. если зайдем в первый сетевой  неймспейс и опубликуем тамошние интерфейсы то увидим
условно

eth0
eth1

если войдем во второй неймспейс сетевой и запросим список сетевых интерфейсов то увидим

eth10
eth20

соотсвтеенно в каждом неймспейсе своя таблица маршрутизации.

и  таблица маршрутизщации неймспейс 1 ничего незнает о таблице маршрутизации из неймспейс2.
и как уже сказал связь между ними возможно только через спец гейтвей.

каждый сетевой нейспейс это как отдельная виртуалка на хосте только она касается не вообще всех ресурсов
завиртуальзированных а тока сетевые интерфейсы ограничивает в группы.

итак сетевой неймспейс позволяет разбить сетевые интерфейсы на компе на отдельные незавимые невидящие
друг друга группы. ( а вместе с ним в кажом нейспейсе своя таблица маршрутизации и iptables правила но 
главное это именно интерфейсы от них все пляшет)

тут возникает очень интересный вопрос. окей мы распределили по независимым ящикам интерфейсы в линуксе
это считай как на свиче мы разделили порты по разным виланам. только на свиче мы порты
разделаем по L2 приниципу. а на линуксе мы их разделяем на L1 уровне. так что ли..

так вот. в эти порты ктото должен посылать пакеты. очевидно что это должен делать некий процесс в линуксе.
как я понимаю это работало раньше.  имеем процесс в линуксе.
этот процесс это терминальная сессия. в ней пишем команду ping 127.0.0.1
команда полетела в ядро линукса. ядро смотрим что на компе есть такой интерфейс 127.0.0.1 
за интерфейс отвечает сокет , спецустройство или какая то там софтовая хрень в ядре.
в итоге запрос от ping летит в эту софтовую хрень. она его обрабатыавет и посылает обратно
ответ в ядро а ядро переправляет в ping а он выводит обратно в терминал что ответ получен.

насколько я понимаю с приходом неймспейсов. сейчас каждый процесс линукса входит в состав
одного ( а может и нескольких ) сетевых неймспейсов.
поэтому чтобы из термианальной сессии мы смогли пингануть  некий интерфейс ethX  который входит 
в сетевой неймспейс "вася" надо чтобы  процесс через который мы посылаем пинг  входил в 
сетевой неймспейс "вася".

то есть теперь каждый процесс в линуксек имеет свойство которое говорит в какое или какие 
сетевые неймспейсы он входит.

если сессия команднйо строки входит в сетевой неймспейс "петя" и пингует сетвой интерфейс
который входит в сетевой неймспейс "вася" то пинг не пройдет. это тоже самое 
как если мы будем пинговат сетевой интерфейс на компе северного полюса 
сидя в африке. ( имеется ввиду что между компами нет сетевой связи).

получается чтоб какая то программа в линуксе могла пингануть некий IP который принадлежит
сетвой карте eth0 (который входит в сетевой неймспейс "вася") надо чтобы программа 
 имела в своих свойствах свойство что она входит в сетевой неймспейс "вася".

полуается процесс это процесс а сетевой стек это отдельная штука.

получается елси мы подняли loopback1 = 172.16.10.1 в сетевом неймспейсе "петя"
то пингануть его просто так мы несможем. 
мы сможем его пингануть только из процесса "командная строка"  который имеет доступ
к сетевому неймспейсу "петя". процесс должен видеть сетевой неймспейс "петя".

создадим сетевой неймспейс.
поднимем там интерфейс
дадим ему IP

попробуем его пингаунть просто так. а потом с процесса который имеет доступ к сетевому
неймспейсу этому.

про неймспейсы легко понять через гипервизор esxi и виртуалки.
процессы одной виртуалки невидят процессы другой виртуалки. 
это получается аналог pid namespace. гипервизор это ядро линукса типа.
и процессы в одном pid namespace невидят процессы в другом pid namespace
тоже самое и про net namespace. гипервизор заводит кучу
вирт карточек на хосте. и карточки которые сидят на одном вирт свиче 
они не имеют связи с вирт сет карточками которые прикреплены к другому вирт свичу на esxi хосте 
(только если эти свичи не связаны друг с другом спец образом).
и каждая виртуалка прикрепляется к какой то вирт сет карточке или карточкам.
я бы сказал что если мы проводим аналогию между linux net namespace и esxi 
то аналогом net namespace на esxi является vSwitch. потому что

несоклько процессов в линукс могут иметь доступ к неймспейсу "вася"
поэтому все эти процессы могут пинговать все карточки входящие в сетевой
неймспейс "вася".
тоже самое в esxi . вирт машина которая  имеет карточку которая входит в состав vSwitch0
может пинговать все сет карточки которые входят в состав этого vSwitch0 (а не только карточки 
которые входят в состав этой вирт машины). поэтому аналогом net namespace на esxi является vSwitch (а не 
виртуалка).

команда ip очень полезная. ее синтаксис

ip (опции) обьект

самые частые обьекты = link | address

LINK это добавить удалить интерфейс и друная работа с интерфейсом на уровне L2
address это  работа с ip адресом интерфейса

кстати ее вывод может быть цветным что супер удобно.

кстати ip a = ip addr а не ip all как я думал.

полезные примеры ее использования

# ip -c -f inet addr show

# ip -f inet -c link show

здесь addr и link это обьекты а остальное опции

команда очень большая по опциям обьектам и использованию поэтому хочегь нехочешь
надо читать 

ip help
man ip


вот привожу полный списко обьектов

 OBJECT := { link | address | addrlabel | route | rule | neigh | ntable | tunnel | tuntap | maddress | mroute | mrule | monitor | xfrm | netns | l2tp |
               tcp_metrics }


так вот что еще интересно.
можно читать общую справку

# man ip

а можно читать справку по каждому обьекту в отдельной справке

для link это

# man ip-link

для address это

# man ip-address

для route это

# man ip-route

то есть это всегда ip-обьект

это удобно.


начнем рассмтривать детально что же значат те или иные параметры в выводе команды  ip


расмотрим вывод этой команды типичный

root@test-kub-01:~# ip -c link show

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00


я сейчас обращаю внимание на 
<LOOPBACK,UP,LOWER_UP>

эти же штуки можно видеть и через ifconfig

root@test-kub-01:/sys/class/net/ens160# ifconfig lo
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1

что этотакое.

в общем это некоторые флаги. которые показывают особенность режима работы интерфейса.

вот здесь написано какие есть флаги и что  они значат

https://github.com/torvalds/linux/blob/master/include/uapi/linux/if.h

вот это написано что значат флаги.

 * @IFF_UP: interface is up. Can be toggled through sysfs.
 * @IFF_BROADCAST: broadcast address valid. Volatile.
 * @IFF_DEBUG: turn on debugging. Can be toggled through sysfs.
 * @IFF_LOOPBACK: is a loopback net. Volatile.
 * @IFF_POINTOPOINT: interface is has p-p link. Volatile.
 * @IFF_NOTRAILERS: avoid use of trailers. Can be toggled through sysfs.
 *	Volatile.
 * @IFF_RUNNING: interface RFC2863 OPER_UP. Volatile.
 * @IFF_NOARP: no ARP protocol. Can be toggled through sysfs. Volatile.
 * @IFF_PROMISC: receive all packets. Can be toggled through sysfs.
 * @IFF_ALLMULTI: receive all multicast packets. Can be toggled through
 *	sysfs.
 * @IFF_MASTER: master of a load balancer. Volatile.
 * @IFF_SLAVE: slave of a load balancer. Volatile.
 * @IFF_MULTICAST: Supports multicast. Can be toggled through sysfs.
 * @IFF_PORTSEL: can set media type. Can be toggled through sysfs.
 * @IFF_AUTOMEDIA: auto media select active. Can be toggled through sysfs.
 * @IFF_DYNAMIC: dialup device with changing addresses. Can be toggled
 *	through sysfs.
 * @IFF_LOWER_UP: driver signals L1 up. Volatile.
 * @IFF_DORMANT: driver signals dormant. Volatile.
 * @IFF_ECHO: echo sent packets. Volatile.

состояние этих флагов для каждого интерфейса хранится тут
# cat /sys/class/net/ens160/flags

в hex виде

0x1003

вот еще раз как про это написано в инете

/sys/class/net/<iface>/flags 

Indicates the interface flags as a bitmask in hexadecimal. See
		include/uapi/linux/if.h for a list of all possible values and
		the flags semantics.


в этоже if.h 
указано какой бит означает какой флаг

	IFF_UP				= 1<<0,  /* sysfs */
	IFF_BROADCAST			= 1<<1,  /* volatile */
	IFF_DEBUG			= 1<<2,  /* sysfs */
	IFF_LOOPBACK			= 1<<3,  /* volatile */
	IFF_POINTOPOINT			= 1<<4,  /* volatile */
	IFF_NOTRAILERS			= 1<<5,  /* sysfs */
	IFF_RUNNING			= 1<<6,  /* volatile */
	IFF_NOARP			= 1<<7,  /* sysfs */
	IFF_PROMISC			= 1<<8,  /* sysfs */
	IFF_ALLMULTI			= 1<<9,  /* sysfs */
	IFF_MASTER			= 1<<10, /* volatile */
	IFF_SLAVE			= 1<<11, /* volatile */
	IFF_MULTICAST			= 1<<12, /* sysfs */
	IFF_PORTSEL			= 1<<13, /* sysfs */
	IFF_AUTOMEDIA			= 1<<14, /* sysfs */
	IFF_DYNAMIC			= 1<<15, /* sysfs */
	IFF_LOWER_UP			= 1<<16, /* volatile */

	IFF_DORMANT			= 1<<17, /* volatile */
	IFF_ECHO			= 1<<18, /* volatile */





в выводе команд ifconfig ili ip эти флаги показываются без @IFF_
то есть @IFF_BROADCAST = BROADCAST

пример


root@test-kub-03:~# ifconfig lo
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:312 errors:0 dropped:0 overruns:0 frame:0
          TX packets:312 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:26260 (26.2 KB)  TX bytes:26260 (26.2 KB)

root@test-kub-03:~#
root@test-kub-03:~#
root@test-kub-03:~# ip link show lo
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00


ха! причем видно что ifconfig показыает флаг RUNNING а ip его непоказывает. зато показывает флаг LOWER_UP
то есть каждая команда част флагов показывает а часть нет. причем каждая свои.


возьмем несколько интерфейсов. посмотрим какие у них флаги есть. 
и какие из них показыавают ifconfig и ip

root@test-kub-03:~# ifconfig lo
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:312 errors:0 dropped:0 overruns:0 frame:0
          TX packets:312 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:26260 (26.2 KB)  TX bytes:26260 (26.2 KB)


показывает флаги
UP LOOPBACK RUNNING

root@test-kub-03:~# ip link show lo
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

показал флаги
<LOOPBACK,UP,LOWER_UP>

смотрим через sysfs какиеже флаги реально есть у lo 
# cat /sys/class/net/lo/flags
0x9

0x9 = 1001

види что нулевой и третий бит выставлены. смотрим в таблицу вверх и дешифруем
0 = IFF_UP = UP
3 = IFF_LOOPBACK = LOOPBACK

итого

UP,LOOPBACK

интересно что все три показали разные флаги.


смотрим еще интерфейс ens160 это живой настоящий ethernet интерфейс

root@test-kub-03:~# ifconfig ens160
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1

итого показал
UP BROADCAST RUNNING MULTICAST

# ip link show ens160
2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 

итого показал
<BROADCAST,MULTICAST,UP,LOWER_UP>


смотрим через sysfs

# cat /sys/class/net/ens160/flags
0x1003

0x1003 = 1000000000011 (младший бит самый правый) = 1 0000 0000 0011

получаем выставлены нулевой первый и двенадцатый бит.
смотрим на таблицу вверх

0 =  IFF_UP
1 = IFF_BROADCAST
12 = IFF_MULTICAST

в итоге

UP,BROADCAST,MULTICAST

хм.. опять все разное. но в целом теперь есть понимание откуда команды ifconfig
или ip команда  берут данные для этих флагов.

поговорим про смысл этих флагов

эти флаги как написано некоторые можно менять через sysfs
но большей частью все же это флаги. то есть они предназначены 
не для изменения со стороны пользователя а для индикации со стороны
ядра ( оно их меняет) что интерфейс себя чуствует так то и такто.
например что выдернут провод из порта.
то есть в целом не мы на эти флаги влиять должны. а 
их высталвяет драйвер или ядро. и они нам в итоге сообщают 
специфические детали как себя чуствует или что умеет делать
этот интерфейс.


флаг LOOPBACK - это признак что это лупбэк интерфейс. что примечательно что такой флаг 
есть только у системного лупбэкап.
если мы создадим свой лупбэк то линукс такой флаг ненарисует. пример. я создал лупбэк lo2
и вот его флаги

11: lo2: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000

флаг LOWER_UP устаналивает драйвер сетевого интерфейса если драйвер умеет распознавать что 
в интерфейс вставлен провод и в него подается ток . то есть что на уровне L1 есть 
необхдоимые сигналы из провода в интерфейсе. вот что значит этот флаг.

расмотрим флаги уже не лупбэкап а нормального классического ethernet живого 
интерфейса

2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000


UP - значит что административное состояние порта = включено.
административное состояние это значит что мы на уровне линукса включили порт.
через команду
ifconfig up
ip link ... up
что мы хотим его юзать.

но это совсем незначит что порт готов к работе. незначит что в нем ток есть итп.
реальная готовность к работе порта в линуксе называется операционное состояние порта.

через флаги  операционную  реальную готовность порта к работе можно разглядеть через сумму двух флагов


UP,LOWER_UP

первый показывает что мы интерфейс включили в линуксе в настройках
второй что ток в порт поступает.

вообщето как видно выше есть спец флаг которые покаывает операционную готовность
IFF_RUNNING который в линуксе должен был бы быть виден как 

RUNNING

но его почему то неопоказывают. вместо него покаывают только 

UP,LOWER_UP

а вот если   порт административно включен а провод в порт невставлен то есть его 
операционная готовнсть неготова . то флаги будут

UP,NO-CARRIER

NO-CARRIER это отсутсивие флага RUNNING

меня изначально эта тема заинтересовала изза флага LOWER_UP у lo что он значит

менять биты флагов через sysfs я не пробовал.
через ifconfig или ip эти флаги напрямую неменяются.
некоторые меняются через ifconfig или  ip ненапрямую а через определенные опции
ну скажем указали мы что нельзя  arp на интерфейсе через опцию ip .... noarp
и флаг поменяет уже сам драйвер.

продолжаем рассматривать и дешифровавывать вывод команды ip


root@test-kub-03:~# ip -c -d address  show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:a3:56:a7 brd ff:ff:ff:ff:ff:ff promiscuity 0
    inet 172.16.102.33/24 brd 172.16.102.255 scope global ens160
       valid_lft forever preferred_lft forever
    inet6 fe80::250:56ff:fea3:56a7/64 scope link
       valid_lft forever preferred_lft forever


---->\\\начало темы про scope

можно заметить вот такой параметр непонятный

scope host
scope global

поговорим про scope.

параметр scope встречается как в маршрутах в таблице маршрутизации
14.14.14.14 dev vasya2  scope link

так и в настройках IP сетевой карты.
14: vasya2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state     
    inet 172.22.0.1/24 scope host vasya2
    inet 172.21.0.1/24 scope link vasya2
    inet 172.20.0.1/24 scope global vasya2

видим три IP на одной карте и каждый IP лежит в другом scope

скоупов три = global, link, host

про них в документации буквально почти ноль.
пришлось только из эксперимента вытаскивать как оно работает.


на что влияет scope. он влияет на то что какой src ip линукс
будет подставлять в ip пакеты которые он будет выплевывать из сетевой
карточки.

скоуп в рутах и скоуп в IP карточки работают друг с другом. сейчас обьясню как.

в настройках карты scope указывать необязательно. если его 
неуказать система его установит сама. 
когда мы смотрим ip настройки карты то линукс scope для IP адреса
указывает всегда чему он равен
 
# ip address show
2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:a3:41:00 brd ff:ff:ff:ff:ff:ff
    inet 172.16.102.35/24 brd 172.16.102.255 scope global ens160

в таблице маршруатизации scope указан для маршрута невсегда.
вот как в этом примере
~# ip r s
default via 172.16.102.1 dev ens160 onlink

если он неуказан то он равен scope=global.

таким образом в таблице маршрутизации scope каждму маршруту прописан всегда.
просто еще раз если его нет он равен global
при добавлении маршрута руками scope указывать необязательно
система его назначит сама если что.


еще раз скажу на что влияет scope. 
когда линукс выплеывает ip пакет из карточки в сеть то ему надо принять 
решение какой src ip назаначить пакету.

как он это решает.(судя по книжке)
1). если приложение само скажет линуксу какой scr ip использовать то он 
будет использоват указанное src ip
2). если пакет влетел снаружи то понятно какой для него использовать src ip
надо переставить местами dst ip и src ip и готово
3). если оба пункта неподходят то начинается котовасия.
котовасия). если в маршруте прописан параметр src то (внимание) линукс абсолютно забыавает про такую хрень как scope игнорирует чему он равен в маршруте и в настройках IP адресов на карточке, забывает игнорирует  абсолютно и сразу использует src ip = src указанный в маршруте.
пример
90.90.90.0/24 dev petya0  proto kernel  scope link  src 90.90.90.1 
мы имеем маршрут. у него указано src 90.90.90.1
все линукс будет использоват в качестве src ip = 90.90.90.1 
для всех пакетов из этого маршрута.

если src в маршруте неуказан то начинается дополнительная котовасия.
линукс смотрим как scope прописан в маршруте. как мы помним если даже в маршруте не прописан scope то это значит что scope = global.
посмотрим вот на этом примере

вот маршрут
182.32.0.8  scope link dev vasya3

вот карточка
17: vasya3: <BROADCAST,NOARP,UP,LOWER_UP> 
    inet 172.30.0.1/24 scope host vasya3

в маршруте scope=link
дальше линукс лезет на карточку vasya3
и начинает смотреть есть ли на карточке IP c scope=link
если есть то все отлично. он будет использовать этот IP как src ip.
в нашем случае такого IP у карточки нет. у нас есть ip с scope=host

то есть scope у маршрута и у доступного IP несовпадают. 
что делать. что делает линукс.

через IP scope=host может пройти только маршрут у которого scope=host
через IP scope=link может пройти только маршрут у котрого scope=link, scope=host
через IP scope=global может пройти маршрут с любым scope

согласно этой таблице мы видим что наш IP не может через себя пропустить
этот маршрут. а других IP у карточки нет. что делате линукс.
опять же здесь котовасия. как я понял что будет дальше зависит от драйвера
карточки. если это реальная физ карточки то в качестве src ip будет 
использован src ip = 0.0.0.0 
да прям буквально именно 0.0.0.0 будет подставлен в ip пакет.
ну и такой пакет понятное дело он невернется и связи небудет но это уже 
другой вопрос. по факту в сеть будет выплевыаться пакет у которого src ip = 0.0.0.0
а если карта типа dummy то в качестве src ip будет подставлят IP у которго scope=global с другой карты. то есть линукс возьме IP адрес с другой карты
в системе и подставит как src ip в пакет выплевываемый через эту карту.
IP обязательно будет иметь scope=global
вот эту фигню что для физ карты 0.0.0.0 одно а для dummy  ip с другой карты
я это проверил несколько раз на нескоольких виртуалках.


еще раз если у маршрута указан src то про scope можно сразу забыть.
он неважен ни у маршрута ни у карточек. линукс сразу будет исполтьзовать 
для пакетов данного маршрута src указанный в маршруте.

вот маршрут
172.32.0.0/24 dev vasya5  proto kernel  scope link  src 172.32.0.1
у него указан src. все нам посрать какой scope у маршрута и у карточек

если же src у маршрута неуказан то будет все то что описано выше.

полезно рассмореть маршрут по умолчанию

default via 172.16.102.1 dev ens160 onlink

у него src неуказан. так что будет задействована вся та математика что
я описал.
у него неуказан scope значит у него scope=global

паметр onlink что он значит. 
когда мы вбиваем маршрут руками в таблицу маршрутизации то линукс
сразу проверяем все паарметры маршрута насколько они реалистичны.
и если ему чтото ненравится он непримет маршрут.
не скажу что линукс все параметры проверяет или круто это проверяет
но проверяет и часто маршрут от балды добавить в таблицу неполучится.
так вот если мы для маршрута указываем гейтвей то линукс смотрит 
сидит ли карточка ее IP и гейтвей в одной сети. тоесть проверят "доступность"
гейтвея с карточки. если они в разных сетях то он непримет такой маршрут
так вот параметр onlink он заставляет линукс непроверять доступность 
гейтвея а принять маршрут как есть. 
еще раз доступность это нето что линукс пингует гейтвей . нет. 
он сморит входит ли ip с карточки в одну сеть с гейтвеем.
в ряде случаев это делать ненадо. в ряде случаев гейтвей может лежать в 
другой IP сети и это нормлаьно. как я понял этов случае впн тоннелей.
но на практике пока мне мало понятно как так может быть.

когда мы добавляем карточку в систему и даем ей IP и поднимаем эту карточку
то линукс автоматом пропсываем ряд маршрутов.
маршруты которые линукс прописал автомтом в таблицу маршрутиазции имеют параметр proto kernel
пример
172.20.0.0/24  proto kernel  scope link  src 172.20.0.1

любой маршрут можно удалит из таблицы. втом числе и proto.
зачем линукс сам добавляем маршруты. 
он облегчает нам жизнь.
иначе бы нам его прищшлось самому добавлять.
дело в том что когда мы дали карточке ip то мы несможем
пирнговать даже хосты в локальной сети .
для этого тоже нужен маршрут.
без маршрута ни один пакет неможет быть отправлен с компа.

так вот линукс автоматом прописывает маршрут для компов сидящих
в локалке с карточкой.

так вот можно легко проверить что мы скажем назначим для карты IP
и укажем для него scope=host
а линукс пропишет маршрут  и в маршруте укажет scope=link
пример
172.20.0.0/24  proto kernel  scope link  src 172.20.0.1
спрпашивается что за бред. 
а на самом деле пофиг потому что линукс прописывает маршрут и указывает
src так что для этого маршрута посрать какой scope прописал линукс в 
маршруте и какой scope мы прописали для IP карточки.

scope имеет отношение именно к L3 IP а не к L2 карты. это айпишный
параметр

при добавлении маршрута мы указываем карту через которую мы хотим
этот маршрут чтобы тек. так вот при добавлении маршрута руками 
можно успешно добавить марошрут с любым scope

14.14.14.14  scope host dev ens160
172.19.0.16  scope link dev ens160
172.19.0.17  dev ens160

линукс добавит проглотит примет маршрут с любым указанным нами scope
и линуксу посрать есть ли на карте IP с такимb scope. то есть scope
при добавлении маршрута линук непроверяет.

пример 
карта имеет IP с scope=host

17: vasya3: <BROADCAST,NOARP,UP,LOWER_UP> 
    link/ether a2:e4:80:54:65:56 brd ff:ff:ff:ff:ff:ff
   
для нее мы добавляем три маршрута
# ip route add 182.32.0.7/32 scope host dev vasya3
# ip route add 182.32.0.8/32 scope host link vasya3
# ip route add 182.32.0.9/32 scope host global vasya3

вот как будут выглядет маршруты
# ip r s
182.32.0.7  scope host
182.32.0.8  scope link
182.32.0.9
   
тоесть все было принято. несмотря на то что не все из них могут 
быть обработаны картой.

то что третий маршрут неимет scope это занчит что он у него = global

на счет темы когда у нас несколько IP на одной карте.
абсолютно спокойно можно сделать несколко IP адресов из одной сети
,но неполучится сделать чтобы они при этом были из разных scope.
недаст система.ip адреса из разных scope должны лежать в разных ip сетях.

в таблице маршрутизации в качестве src можно указать нетолько 
адрес карточки которая в маршруте прописана но вообще любой адрес
который есть на компе от любой карты.
пример

182.32.0.10 dev vasya3  src 172.30.0.1

карта vasya3 она неимеет 172.30.0.1 . это ip от другой карты.
главное чтобы указвыаемый ip вообще существовал на компе.

система недаст добавить маршрут с src ip которого нет на компе

пример
у меня есть вот такая карта
172.19.0.1/24 scope link vasya2

я попытался добавить вот такой маршрут

 # ip route add 7.7.7.7  dev ens160 scope global src  172.19.0.1
 
 и вот такой итог
 
 7.7.7.7 dev ens160  src 172.19.0.1
 
 как видим scope отсутсвует в маршруте. это потому что как я уже
 говорил scope=global неотображается в таблице маршрутизации. 
 но именно когда его нет это и означает что scope=global
   
  разница между отсылкаой локлаьного IP пакета и интернет пакета.
  для лроакального пакета комп запрашивает MAC с IP в пакете.
  для интенет пакета он заприваетМАК компа с IP не из пакета
  а с IP гейтвея. о!
  
  
если в маршруте указан src но tcpdump показывает что используется неон 
а каокйто другой ip то 100% дело в iptables. там стоит правило NAT
которое подменяет src ip.то есть вся теория верная что выше просто еще 
вмешивается iptables и среди его правил надо искать правило.



наличиец в маршрруте гейвейя невлияет на src ip и dest ip
оно влияет только на процесс arp выяснения dst MAC адреса.
если гейтвея в марршруте нет то arp будет запраштвати mac для IP пакета
а если гейтвей есть то будет запрпивать MAC для IP гетвейя. но при этом еще 
раз src ip и dst ip никак не меняются.

получается когда мы имеем дефолт маршрут 

default via 172.16.102.1 dev ens160 onlink

как рассждет линукс.
линукс берет dst ip , смотрит какой марршрут соотвстветует ему в таблицк
такого ненаходит. тогда остаетс ятолко default.


если на карте два IP scope link , и маршрут scope link.
в итоге испольщуется src ip первого из двух вбитого IP карты.
один ip помечается как основной global второй как secondary global 

что еще обнаружил. бывает что src IP в arp запросе отличается
от src ip уже в финальном запросе когда известен dst MAC.
может это иза iptables. вмешивание iptables непроверял в этом случае.
то что я описал выше всю математику она точно работает для финального
пакета. для ARP может иногда несрабатывает незнаю.

про маршруты если scope неуказан то он = global.

у нас три типа scope=global,link,host
пишут что можно еще добавить типы в файл 
 /etc/iproute2/rt_scopes. 
 но как это будет на практие обрабатвыаться непонятно.
 
 очень мудежно по архитетуре придумали
 

далее. при удалении последнего IP  с карты  ядро удаляет все связанные с ним
марштуры в таблице

 
 
 ---
 полезняшка
 
 узнать через какуб карту пойдет пакет на адрес 8.8.8.8.
 
 ip route get 8.8.8.8
 
 --
 полезняшка

узнат все маршурты через карту vasya2
 
 # ip -c route show  dev vasya2
9.9.9.9
10.10.10.10  scope link  src 10.5.98.1
14.14.14.14  scope link
172.19.0.16  scope link
172.20.0.0/24  proto kernel  scope link  src 172.20.0.1
---
 

comp1 mac1 ip1 хочет доступатьчася до компа 2 с ip2,
arp request летит src mac = mac1 dst mac = броадкаст
src ip = ip1,

обратный arp replay леити как  src mac=mac2 dst mac=mac1
src ip = ip2, dst ip = ip1.
как я понмю кроме всего прочего mac2 указываеться ещщ и явно где 
то там повыше мак уровня. может в каком то пейлоад.

важно понять что программа она знает только IP того далекого компа 8.8.8.8
и программа ничего незнает и нехочет знать про всякие там гейтвеи.

нижележащая сетевая подсистема она никак обсолютно неменяем dst IP от программы.

а как же гейтвей. сетевая система исользуя IP гетйвей узнает мак адрес
гейтвея чтобы создать фрейм  dst mac = мак гейтвея. 
вот чтобы  мак гетйвея узнать нужен его IP.
но он нужен только для внешней обертки фрейма. 
внутри же IP пакета от программы там никакого IP гетйвей в помине нет. 
там IP конечного хоста.

условно говоря программа это человек который хочет послать посылку
в ростов. сетевой стек это его дворецкий который знает что посылки 
доставляет DHL и гейтвей это номер телефона водителя DHL

человек отдает дворецкому и говои отошли в ростов.
дворейций зовнит в DHL чтобы они приехали и забрали и отослали в ростов.
DHL забирает довозит до ближайщего своего пункта. там сбрасывает 
и посылка в ростов уже через другой гейтвей кочует на следущий пункт.

говоря другими словами некст-хоп для мааршрутата в таблице маршутизации
нужен толкодля того чтобы узнать мак адрес гейтвея чтобы создать времнный
фрейм вокруг IP пакета.

остется вопрос что значит scope уже в таблице маршутизации
а не в настройках ip


ip -f ***** addr show

адрес интерфейса может принадлежать

ipv6
ipv4
ipx
итд


вот полный список
 inet, inet6, bridge, ipx, dnet, mpls or link

link - значит невошло ничего из списка.

странно что этот списко входит bridge. не могу понять как это L3 протокол = bridge бред какойто. потому что

ipv4 = l3 osi layer
ipv6 = l3 osi
bridge  = ????? что ???
ipx = l3 osi
mpls = между l2 и l3

непонятно поэтому как туда затесялся bridge . о чем это загадка.




посмотреть bridge L2 кто есть

3:~# ip -c link  show type bridge
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:ee:20:30:71 brd ff:ff:ff:ff:ff:ff
4: bridge_name: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 66:6f:bd:b1:36:ee brd ff:ff:ff:ff:ff:ff





посмотрим теперь на вывод команды ip addr show

~# ip -c -f inet a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    inet 172.16.102.33/24 brd 172.16.102.255 scope link ens160
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever


теперь мы знаем что значать все поля

про параметр group кстати

Link groups are similar to port ranges found in managed switches. You can add network interfaces to a numbered group and perform operations on all the interfaces from that group at once.


это чисто удобняшка. для человека. на работу невлияет.

loopback интерфейс на линуксе это не то что лупбек на циске.
более близкий к цисковскому лупбеку как пишут это dummy на линуксе.

loopback обладает множеством нелогичных и особых штук как на нем
все работает. по мне лучше его не трогать. 
чего стоит та штука что если мы пингуем на компе любой его локальный IP
то весь трафик идет на лупбек. это и через tcpdump видно.
значит как по идее работает классический роутинг L3.
мы хотим послать пакет на dst ip , софт смотрит в таблицу маршрутизации
находит маршрут. в маршруте прописано через какую карту выплевывать пакет наружу. и софт это делает. так вот прикол с looback в том что пакеты на него кидаются не в соотвествии с таблицей маршрутизации.  а как исключение
из правил.

посмотрим на примере 
будем пинговать LAN адрес компа.

# ping 172.16.102.35

строка в таблице маршрутизации
которая по идее отвечает за маршурутизацию этого IP

172.16.102.0/24 dev ens160  proto kernel  scope link  src 172.16.102.35

как видно по идее данный пакет должен направляться через карту ens160
но он так небудет направлен.
во первых это подтверждает tcpdump 
во вторых всегда можно проверить как на самом деле будет обработан пакет

# ip route get 172.16.102.35
local 172.16.102.35 dev lo  src 172.16.102.35

как видно пакет будет выплюнут через карту lo
это отправная на нашей стороне.
соотственно пакет через lo войдет в ядро и там будет сформирован ответный
он будет брошен обратно в lo.
поэтому уже из этого примера видно что lo он особый.
поэтому лучше на него никакой кастомный ip адрес ненавешивать и нетрогать.
а то необерешься непотных маршрутов с ним.



---->\\\конец темы про scope




-->\\\вехи

список вопросов
причем есть еще вопросы и выше недоделанные

k8 сеть.



kube-proxy
flannel сеть
надо понять в чем разница

сеть для подов
сеть для сервисов

связность между подами на разных нодах

veth


возрващается обратно высоко вверх узнаем инфо про veth интерфейс

# ip  -c -d  link show vethedebf689
9: vethedebf689@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
    link/ether 82:d4:2a:8e:87:48 brd ff:ff:ff:ff:ff:ff link-netnsid 1 promiscuity 1
    veth
    bridge_slave state forwarding priority 32 cost 2 hairpin on guard off root_block off fastleave off learning on flood on addrgenmode eui64


видим что он бридж slave

создать два неймспейса и две проги чтобы они связь наладили в рамках
одного неймспейса а потом между неймспейсами через veth

сетевые неймспейсы

утилита ip



создать loopback интерфейс


разобраться с выводом ip

разобраться со scope


\\<--



я узнал секрет про вот такой кусок brd + который часто встречается в интернете. например
ip addr add 192.168.100.199/24 brd + dev eth10

brd + = broadcast +  и означает две вещи
1) мы разрешаем на L3 интерфейс принимать  IP пакеты  которые имеют броадкастный L3 IP  адрес
2) мы говорим системе ( через знак + ) чтобы она сама вычислила броадакастный IP адрес на основе IP интерейса и маски 
что мы указали 

в случае знака + броадскастный IP адрес будет вычислен путем установки в 1 всех битов которые отвечают за хостовую
часть адреса.

пример.

ip интерфейса = 172.16.0.1 маска 24. значит броадкастный IP = 172.16.255.255

частично это описано в  man ip-address

 broadcast ADDRESS
              the broadcast address on the interface.
              It is possible to use the special symbols '+' and '-' instead of the broadcast address. In this case, the broadcast address is derived by setting/resetting the host bits of the interface prefix.

насколько я понимаю если указать brd - то броадкастный ip будет вычислен как 172.16.0.0
зачем он такой нужен непонятно.


создадим loopback интерфейс

вот такой синтаксис для этого

# ip link add [ link DEVICE ] [ name ] NAME
               [ txqueuelen PACKETS ]
               [ address LLADDR ] [ broadcast LLADDR ]
               [ mtu MTU ] [ index IDX ]
               [ numtxqueues QUEUE_COUNT ] [ numrxqueues QUEUE_COUNT ]
               type TYPE [ ARGS ]

где  TYPE := [ bridge | bond | can | dummy | hsr | ifb | ipoib | macvlan | macvtap | vcan | veth | vlan | vxlan | ip6tnl | ipip | sit | gre | gretap | ip6gre |
               ip6gretap | vti | nlmon | ipvlan | lowpan | geneve ]

поскольку у нас будет лупбэк то его type = dummy


в итоге хочется написать вот такую команду

# ip link add link lo3 name lo3 type dummy

но система выдаст ошибку что она незнает такое спец устройство как lo3
и как его создать руками непонятно. остается только одно, убрать эту опцию из команды

# ip link add name lo3 type dummy

и вот такая команда сработает.

непонятно если link add неможет само создать сетевое устройство зачем тогда эту опцию включать в команду.
с другой стороны когда мы вообще опускаем имя сетевог спец устройства команда проходит. 
значи в итоге ip link add все же создает это сетевое устройство... в общем загадка.

смотрим что получилось.


~# ip link show lo3
6: lo3: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 96:e7:51:82:7c:45 brd ff:ff:ff:ff:ff:ff

видим флаг BROADCAST
что он значит несовсем понятно. согласно таблице что выше @IFF_BROADCAST: broadcast address valid. Volatile.
но я нашел в другом месте что якобы это флаг означает что толи интерфейс толисеть поддерживает отправку прием броадкастов.
то есть интерфейс может отправлят и принимтаь броадкасты. и под броадкастами я понимаю броадкаст фреймы L2
собсвтенно и показан мак такого фрейма brd ff:ff:ff:ff:ff:ff
таким образом флаг BROADCAST не имеет никакого отношения к L3 броадкастам. он имеет отношение только к L2 броадкастам.
это важно.
второй флаг его значение @IFF_NOARP: no ARP protocol. тоже несовсем понятно.


попробуем сменить какието параметры уровня L2 для lo3


синтаксис

 ip link set { DEVICE | group GROUP } { up | down | arp { on | off } |
               promisc { on | off } |
               allmulticast { on | off } |
               dynamic { on | off } |
               multicast { on | off } |
               protodown { on | off } |
               txqueuelen PACKETS |
               name NEWNAME |
               address LLADDR | broadcast LLADDR |
               mtu MTU |
               netns PID |
               netns NETNSNAME |
               alias NAME |
               vf NUM [ mac LLADDR ] [ vlan VLANID [ qos VLAN-QOS ] ] [ rate TXRATE ] [ max_tx_rate TXRATE ] [ min_tx_rate TXRATE ] [ spoofchk { on | off } ] [state { auto | enable | disable} ] |
               master DEVICE |
               nomaster  |
               addrgenmode { eui64 | none }
               link-netnsid ID  }


# ip link set lo3 up arp on promisc off allmulticast off multicast off 

смотрим что получилось

~# ip link show lo3
6: lo3: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 96:e7:51:82:7c:45 brd ff:ff:ff:ff:ff:ff

интересно что исчез NO_ARP из флагов

странно что для создаваемого вручную лупбэка нет возмжоности установить флаг LOOPBACK как это есть для l0 
которй система создает

и еще странно что для лупбэков state = UNKNOWN

окей с L2 разобрались. поехали на L3 настройки

назначим ему ip адрес

синтаксис

ip address { add | change | replace } IFADDR dev IFNAME [ LIFETIME ] [ CONFFLAG-LIST ]

из интересных параметров

 IFADDR := PREFIX | ADDR peer PREFIX [ broadcast ADDR ] [ anycast ADDR ] [ label LABEL ] [ scope SCOPE-ID ]

PREFIX в данном случае это ip + prefix  

# ip address add 127.1.0.1/16 broadcast +  label lo3:vasya scope host dev lo3

прописал я scope = host так как этот ip точно доступен только на хосте. так что здесь с scope все просто.
label он связан с алиасом может даже это одно и тоже. но всегда у них связь такая

label = name:alias

если у нас name=lo3 то label=lo3:alias
alias любой.  я выбрал vasya

итого label = lo3:vasya

label должен быть короче 16 символов иначе будет ошибка.



смотрим что получилось

# ip -c -f inet  address  show lo3
6: lo3: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.1.0.1/16 brd 127.1.255.255 scope host lo3:vasya
       valid_lft forever preferred_lft forever

рассмотрим строку
    inet 127.1.0.1/16 brd 127.1.255.255 scope host lo3:vasya

inet - значит что L3 адрес относится к ipv4 протоколу. то есть ipv4 адрес обозначается линуксом как inet адрес.
тоесть inet 127.1.0.1/16 = ipv4 address 127.1.0.1/16

то есть словом inet линукс хочет сказать что за ним щас будет указан адрес протокола ipv4
если бы наш интерфейс имел l3 ipx адрес то строка бы выглядела как

ipx ........

а у нас

inet .....

scope host - понятно. то что данный ip адрес для чужих компов недоступен. ибо он доступен только внутри хоста.

lo3:vasya  - это LABEL . но лейбл чего непонятно пока.




добавим еще один IP на lo3

~# ip address add 127.2.0.1/16 broadcast +  label lo3:petya scope host dev lo3

смотрим что в итоге получилось

# ip -c -f inet  address  show lo3
6: lo3: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.1.0.1/16 brd 127.1.255.255 scope host lo3:vasya
       valid_lft forever preferred_lft forever
    inet 127.2.0.1/16 brd 127.2.255.255 scope host lo3:petya
       valid_lft forever preferred_lft forever



root@test-kub-03:~# netstat -i
Kernel Interface table
Iface   MTU Met   RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
ens160     1500 0    551190      0     19 0        961353      0      0      0 BMRU
lo        65536 0       170      0      0 0           170      0      0      0 LRU
lo3        1500 0         0      0      0 0             7      0      0      0 BRU
lo3:petya  1500 0       - no statistics available -                        BRU
lo3:vasya  1500 0       - no statistics available -                        BRU


я создал еще один лупбэк вот такой

root@test-kub-03:~# ip ad show loopback-04
7: loopback-04: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether e6:88:96:fa:7f:c3 brd ff:ff:ff:ff:ff:ff
    inet 127.3.0.1/16 brd 127.3.255.255 scope host loopback-04
       valid_lft forever preferred_lft forever
    inet 127.4.0.1/16 brd 127.4.255.255 scope host loopback-04
       valid_lft forever preferred_lft forever
    inet6 fe80::e488:96ff:fefa:7fc3/64 scope link
       valid_lft forever preferred_lft forever


теперь вот что сравниваем



lo3: 
    inet 127.1.0.1/16  lo3:vasya
      
    inet 127.2.0.1/16  lo3:petya
     
  
loopback-04: 
    inet 127.3.0.1/16  loopback-04
       
    inet 127.4.0.1/16  loopback-04
       
   для lo3 мы задавали label при добавлени ip адресов
   для lopback-04 я не задавал label вообще.



пытаюсь понять суть поля LABEL.
lo3:vasya  - это LABEL . но лейбл чего непонятно пока. какой смысл что обозначает.

чуть в сторону. я экспериментально установил что name и dev в линуксе это одно итоже для сетевых устройств.

потому что 
я создал интефейс с name=lo5 без указания dev

~# ip link add name lo6 type dummy

далее вот как выгляди синтаксис ip link show

 ip link show  DEVICE

то есть после show мы указываем не name а dev

я выполнил

~# ip link show lo6
10: lo6: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether ba:86:d4:58:f5:e3 brd ff:ff:ff:ff:ff:ff
  
прошло успешно. ну окей. значит при создании интерфейса он создает dev который равен name.

далее я переименовал name  для dev=lo6  согласно этому синтксису

ip link set  DEVICE name NEWNAME 

~# ip link set lo6 name loopback-06

то есть я dev неменял я менял только name

но в итоге либо поменялся сразу и dev потому что 

~# ip link show loopback-06
10: loopback-06: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether ba:86:d4:58:f5:e3 brd ff:ff:ff:ff:ff:ff

~# ip link show lo6
Device "lo6" does not exist.

либо name и dev это одно  и тоже.

я буду счиать что name и dev это одно  и тоже для линукса.

поэтому в выводе команды ip link show первый столбец это DEV=NAME

получается это имя интерфейса. причем имя L2 интерфейса как я понимаю.

потому что имя есть у интфейса еще до присвоения ему L3 адреса.
а LABEL появляется только при присвоении интерфейсу L3 адреса. 
если label явно непрописывать то он автоматом = имени L2 интерфейса.

тогда наверно понятен теперь суть LABEl.

все  интерфейсы в линуксе должны иметь имя.
причем интерфейс имеет четко принадлежность к L2 либо к L3

скажем eth0 это L2 интерфейс

а eth0:vasya это L3 интерфейс.


и показаны они будут в линуксе раздельно

пример

# ifconfig

lo3       Link encap:Ethernet  HWaddr 96:e7:51:82:7c:45
          inet6 addr: fe80::94e7:51ff:fe82:7c45/64 Scope:Link
          UP BROADCAST RUNNING  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:7 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:570 (570.0 B)

lo3:vasya Link encap:Ethernet  HWaddr 96:e7:51:82:7c:45
          inet addr:127.1.0.1  Bcast:127.1.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING  MTU:1500  Metric:1
                
то есть они представлены в линуксе как совершенно разные незавииисмые интерфейсы.

почему. ну наверно потому что функционал подход обработка работа с L2 интерфейсом и L3 она разная.


до этого момента мы прописывали L3 параметры через командную строку.
которая исчезнет настройка после перезагрузки.

чтоб после перезагузки настройки сохранились надо вписать это в конфиг.
в конфиге мы будем юзать именно label

auto lo3:vasya
iface lo3:vasya inet static
        address 172.16.102.33
        ...

или как мы там будем ?????
все таки суть label не совсем понятка зачем она нужна и что дает.

вот интересный вопрос что означает в выводе ip show  первая колонка 
скажем вот пример

lo3:vasya Link encap:Ethernet  HWaddr 96:e7:51:82:7c:45
          inet addr:127.1.0.1  Bcast:127.1.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING  MTU:1500  Metric:1

и я говорю про lo3
я раньше думал что это название спецустройства типа /dev/lo3
но сейчас я полагаю что это нетак. и что это не имя устройства а имя интерфейса которое необязано
совпадать с именем устройства.
я нашел в интернете по адресу линукс кернела что ip alias это средство чтобы на одном физ интерфейсе
иметь несклоько IP адресов.

как я понимаю изначальная архитектура такова была что один физ интерфейс должен иметь один IP
адрес. поэтому по этой логике следует что если мы имеем два IP адреса на компе значит должно 
быть два физ порта на компе.

поэтому нам надо эти два порта родить на компе с одним портом.
делается это искуственным путем на уровне ОС

первый порт eth0
второй порт eth0:vasya

тогда для софта все становится понятно. первый IP = eth0
второй IP = eth0:vasya

как я понимаю label это термина аналогиный алиас просто более поздний.

не очень понимаю алиас и субинтерфейс это одно и тоже?

насоклько я понимаю label=alias=sub-interface

раньше за насттройки сети вдела пакет net-tools
в который входит ifconfig
щас вместо этого пакета пришел пакет iproute2 ( это не команда это целый пакет утилит).

An alias interface should not have "gateway" or "dns-nameservers"; dynamic IP assignment is permissible.

alias должен быть в той же сети что и основной адрес.

значит что я нашел в итоге.
новая утилита ip пакета iproute2 спокойно может присвоить несколкьо ip адресов на один сетевой 
интерфейс. 

но старая утилита ifconfig из пакета net-tools она такое непонимает. я незнаю это проблема ядра
или проблема самой утилиты ifconfig но факт есть факт.
раньше интерфейс мог иметь только один IP адрес.  а для того чтобы один интерфейс имел 
два и более IP адресов на интерфейс использовался трюк alias. он позволял создать виртуальный сетевой
интерфейс (который был програмно связан с реальным физ интерфейсом) и ему присвоить второй IP

пример

eth0 = 192.168.0.1
eth0:vasya =192.168.0.2

на уровне софта кажется что в компе две сетевые карты.
по факту она одна. а второе сетевое устройство виртуальное.

так было во времена старого ядра и ifconfig (net-tools)

новое ядро поддерживает много ip адресов на один физ интерфейс.
поэтому алиасы больше ненужны. то есть теперь ненужно заводить допонительные виртуальные 
сетевые интерфейсы чтобы на них навесить второй IP.
и утилита ip ( из пакета iproute2) она умеет понимать это. и показывать. и работаь с этим.

раньше два IP на одном физ интерфейсе конфиг выглядел так


а теперь вот так


старый ifconfig новую настройку непоймет и покажет это

а ip show все поймет и покажет


так вот, в новом ядре чтобы сохранить совместимость со старым ifconfig оставили  aliases.
только они теперь называются labels

если мы в конфиге пропишем lables то ifconfig сможет правильно понимать конфигурацию.

пример

/etc/network/interfaces
auto lo2
iface lo2 inet static
        address 127.10.0.1
        netmask 255.255.0.0
        network 127.10.0.0


iface lo2 inet static
        address 127.10.0.2
	netmask 255.255.0.0


iface lo2 inet static
        address 127.10.0.3
	netmask 255.255.0.0


я привел пример с лупбэк интерфейсом. это неважно что он нефизический. там тоже самое.

вывод ip show

~# ip -c -f inet address  show lo2
4: lo2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.10.0.1/16 brd 127.10.255.255 scope host lo2
       valid_lft forever preferred_lft forever
    inet 127.10.0.2/16 brd 127.10.255.255 scope host secondary lo2
       valid_lft forever preferred_lft forever
    inet 127.10.0.3/16 brd 127.10.255.255 scope host secondary lo2
       valid_lft forever preferred_lft forever




вывод ifconfig


lo2       Link encap:Ethernet  HWaddr 62:b2:97:7f:4c:55
          inet addr:127.10.0.1  Bcast:127.10.255.255  Mask:255.255.0.0
          inet6 addr: fe80::60b2:97ff:fe7f:4c55/64 Scope:Link
          UP BROADCAST RUNNING NOARP  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:210 (210.0 B)

видно что ifconfig ничего не знает про второй и третий адрес на lo2

добавим labels (aliases ) в конфиг так чтобы ifconfug тоже понял


cat /etc/network/interfaces

auto lo2
iface lo2 inet static
        address 127.10.0.1
        netmask 255.255.0.0
        network 127.10.0.0

auto lo2:sub1
iface lo2:sub1 inet static
        address 127.10.0.2
        netmask 255.255.0.0

auto lo2:sub2
iface lo2:sub2 inet static
        address 127.10.0.3
        netmask 255.255.0.0


посморим вывод ifconfig

# ifconfig

lo2       Link encap:Ethernet  HWaddr fa:93:1f:89:1d:0b
          inet addr:127.10.0.1  Bcast:127.10.255.255  Mask:255.255.0.0
          inet6 addr: fe80::f893:1fff:fe89:1d0b/64 Scope:Link
          UP BROADCAST RUNNING NOARP  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:70 (70.0 B)

lo2:sub1  Link encap:Ethernet  HWaddr fa:93:1f:89:1d:0b
          inet addr:127.10.0.2  Bcast:127.10.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING NOARP  MTU:1500  Metric:1

lo2:sub2  Link encap:Ethernet  HWaddr fa:93:1f:89:1d:0b
          inet addr:127.10.0.3  Bcast:127.10.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING NOARP  MTU:1500  Metric:1

видно что ifconfig вывел теперь все коректно. увидел все ip адреса

и вот до кучи какой вывод ip show

k# ip -c -f inet address show lo2
7: lo2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.10.0.1/16 brd 127.10.255.255 scope host lo2
       valid_lft forever preferred_lft forever
    inet 127.10.0.2/16 brd 127.10.255.255 scope host secondary lo2:sub1
       valid_lft forever preferred_lft forever
    inet 127.10.0.3/16 brd 127.10.255.255 scope host secondary lo2:sub2
       valid_lft forever preferred_lft forever


покажу еще способ как в interfaces прописать labels другим способом

/etc/network/interfaces

pre-up ip link add name lo2 type dummy
pre-down ip link del  lo2 type dummy

auto lo2
iface lo2 inet static
        address 127.10.0.1
        netmask 255.255.0.0
        network 127.10.0.0
        up ip address add 127.10.0.2/16 broadcast +  label lo2:sub1 scope host dev lo2
        up ip address add 127.10.0.3/16 broadcast +  label lo2:sub2 scope host dev lo2
 



еще сразу скажу , что 
просто так добавить еще один лупбэк адаптер не получится через конфиг

вот как имеем изначально
# The loopback network interface
auto lo
iface lo inet loopback


вот как хотим сделать по логике

# The loopback network interface
auto lo
iface lo inet loopback

auto lo2
iface lo2 inet loopback
        address 127.10.0.1
        netmask 255.255.0.0
        network 127.10.0.0



ан нет. так несработает.
нужно вот так.

/etc/network/interfaces

pre-up ip link add name lo2 type dummy
pre-down ip link del  lo2 type dummy

auto lo2
iface lo2 inet static
        address 127.10.0.1
        netmask 255.255.0.0
        network 127.10.0.0

обрати внимание на строки с pre-up и pre-down
про них можно прочитать в man interfaces
они нужны обязательно
они создают ( и удаляют ) device lo2
без этого конфиг несработает.



итоги  aliases = lables = sub-intefaces
для нового ядра и iproute2 они ненужны,
но если мы хотим чтобы ifconfig тоже все видел то они оставлены,

-->
когда я искал инфо по ip , iprouting2
и напоролся на продвинутый пост про продивнутые настройки карт и роутингов

https://wiki.edgarbv.com/index.php/Multiple_IP_addresses_on_one_NIC_and_Multiple_internet_connections_on_one_nic

а в нем на книгу про этоже

http://linux-ip.net/html/tools-ip-route.html

<--

нашел
когда у нас на интерфейсе несколько IP то в качестве исходящего адреса будет указан первый основной IP
The first address you added will be used as source address for outgoing traffic by default, it's referred to as primary address.

также
There is no way to swap primary and secondary addresses or explicitly set the new primary address. Try to always set the primary address first.

тут я выяснил новую интересную информацию. 
все это время мы много говорили про labels = aliases .
вспоминаем что labels=aliases имеет отношение к IP адресации.

так вот ДЛЯ ЛИНКОВ команда ip  умеет делать алиасы. и они имеют смысл текстового комментария. 
то есть алиас для линка неимеет ничего общегос алиасом для ip адреса. но это совершенно разные вещи.
alias для ip имеет смысл чтобы была обратная совместимость с командой ifconfig.
а alias для линка это текстовый комент смысла этого линка неболее того

# ip link set dev lo2 alias " second loopback "

посмотреть результа можно в ip link show

# ip -c  link show lo2
 lo2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 7a:5a:5a:17:a0:c3 brd ff:ff:ff:ff:ff:ff
    alias  second loopback



в ip address show этого показано небудет. и это логично ибо алиас имеет отношение к линку не к ip адресу

k# ip -c -f inet  address show lo2
 lo2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.10.0.1/16 brd 127.10.255.255 scope host lo2
       valid_lft forever preferred_lft forever
    inet 127.10.0.2/16 brd 127.10.255.255 scope host secondary lo2:sub1
       valid_lft forever preferred_lft forever
    inet 127.10.0.3/16 brd 127.10.255.255 scope host secondary lo2:sub2
       valid_lft forever preferred_lft forever



далее я нашел такой комент.
что type dummy это нечистый лупбэк. поэтому его и неполучается превратить в линкс с флагом loopback

еще в линуксе есть рутинговые таболицы. это когда у нас не единая таблица маршрутизации
а их несколько. по сути получается одна болшая ее можно разбить на несколько мелких а смысл от этого 
только один выхлоп он в том что при несколтких таблицах можно заимет несеолько маршрутов по дефолту.

помимо таблица создаются правила. если правило по трафику IP выполняется то происходит переход в 
соотвутсвующую таблицу.  в целом эта тема называется linux PBR = linux policy based routing.
выхлоп по мне только один что можно  спомощью такого трюка иметь несколько  марщрутов по дефолту.

vrf - virtual routing and forwarding
смысла и выигрыша от vrf я непонял. 
отличие vrf от pbr я тоже непонял.

вот статья легенькая как применятеся vrf - https://habr.com/ru/post/138698/
а так е
ще тут читал - https://baturin.org/docs/iproute2/#Network%20namespace%20management


могу тока сказать что прочитал. vrf  изоляция рабоатает на уровне L3
а namespaces это полная копия сетевого стэка то есть на уровне L2 как минимум


получается создание неймспейса = создание копии сетевого стэка


# ip netns add ns-01

создать новый namespace

# ip netns list

список неймспейсов

# ip netns exec ns-01 /bin/sh

выполнить процесс в данном неймспейсе.

прикольно то что при создании неймспейса в нем автоматом создается свой лупбэк

пример убедимся

# ip netns add ns-02

# ip netns exec ns-02 ip -c link show

1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00


# ip netns pids ns-01
узнать какие процессы в линуксе привязаны в сетевом плане к неймспейсу ns-01 (очень полезная команда)

# ip netns identify 10
узнать какой неймспейс является у процесса 10  линукс первичным
тут немного непонятно разве процесс линукса может одновременно принадлежат к нескольким неймспейсам ?

до настояшего момента мы говорили пока только про неймспейсы сами по себе как структура и о процессах линукса
которые можно подлкючить к этим неймспейсам.
щас новый аспект - сетевые интерфейс и неймспейсы

неймспеейс это софт. стэк. настала пора в него ввести интерфейсы сетевые.

создаем лупбэк lo3-ns и добавялем его в ns-01
# ip link add lo3-ns type dummy
# ip link set lo3-ns netns ns-01

проверяем что в дефолтовом неймспейсе нет lo3-ns интерфейса

#  ip -c link show  lo3-ns
Device "lo3-ns" does not exist.

все верно.

# ip netns exec ns-01 ip -c link show  lo3-ns

6: lo3-ns: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 2e:cb:07:79:39:cd brd ff:ff:ff:ff:ff:ff

все верно. интерфейс lo3-ns лежит в нейспейсе ns-01


написано что если мы переносим интерейс с неймспейса на другой неймспейс то у него обнуляется вся L3 информация - IP итакдалее и 
он перводится в состояние down. то есть полное его обнуление.


если вот такую команду заюзать

# ip link set dev ${interface name} netns ${pid}

пример

# ip link set lo3-ns netns 15

сравним эту команду с командой выше

# ip link set lo3-ns netns ns-01


как видно мы используем PId вместо имени неймспейса. что это дает.
такая команда засовывает линк в primary namespace процесса в линуксе с pid=15
то есть мы неуеказываем имя неймспейса мы его незнаем . мы говорим иди в неймспейс в котором сидит вот этот процес линукса

я бы сказал что линуксовые неймспейсы это аналог VLAN в свичах.
потому что контакт между линками лежами в разных нейспесах невозможен.

перейдем к veth интерфейсам. они всегда существуют только парами. что входит в один выходит из другого. насоклько я понимаю 
также они всегда L2 типа. их часто сранивают с патч кордом который соединяет чтото с чемто.
обычно с помощью пары veth соединяют да нейсмпейса.  я бы сказал что вефы выполняют функцию бриджа.
то есть это рутер только на уровне L2. 
если одиин неймспейс это один вилан а второй неймспейс это второй вилан.
то соединяя их с помощью вефов мы получаем в итоге один большой вилан. и получается линки которые входят в первый вилан
могут иметь связь на уровне L2 и L3 с линками во втором вилане. по мне это напоминает что на свиче есть одна группа
портов в первом вилане и есть вторая группа портов во втором вилане ну и внутри свича между ними делают перемычку уровня L2
причем виланы нетегированные. поэтому автоматом мы имеем один большой вилан. все порты одного вилана могут видеть порты
другого вилана. ну или снаружи соединяем проводом порт одного вилана с портов другого вилана. и получаем один большой вилан. 
виланы нетегированные. поэтому неймспейсы это виланы. а вефы это бридж. хотя стоп! вефы имеют уровень функционала L3
то есть на них можно вешать IP адреса. поэтому они не роль бриджа выполняют а роль рутера. они позволяют наладить 
связь между виланами на уровне IP ( L3 ). на уровне рутинга.... хммм. надо это прояснить. можно ли 
чтобы они на уровне L2 давали связь тоже.

если вефы работают как L3 только то получается такая картина.
первый неймспейс с кучей L2 портов  это свич на нем группа портов уровня L2 ( на самом свиче) в одном нетегироавнном вилане в них воткнуты кабели с компов на которых порты уровня L3 . это первый неймспейс.

второй такой же самый. только вилан другой.

добавление вефов дает то что порты на свиче витоге внутри замыкаются на внутреннем L3 порты с IP адресом. 1.1.1.1 , это еще раз скажу адрес вефа.
и получается что компы могут указать 1.1.1.1 как гейтвей и таким макаром влезть внутри свича который уже становится рутером.
при этом с фреймов будет содрана оболочка и только останется L3 пакет. и информация уже может лететь запределы первого вилана.
тоже самое со вторым вефом. 2.2.2.2

и таким образом компы через эти гейтвеи могут начать общаться друг с другом на уровне L3.

хочу еще раз заметить что хотя в мануале к ip link add есть вот такая запись

 ip link add [ link DEVICE ] [ name ] NAME ...

по факту это наглое вранье в плане link device  опции. она никогда неработает. никогда. поэтому про нее можно забыть 
при создании линка. линк создается только через использование name NAME


пример
создаем пару veth

# ip link add name veth1 type veth peer name veth2

вот так оно выглядит после создания

~# ip link show

7: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 2a:09:59:5a:ba:64 brd ff:ff:ff:ff:ff:ff
8: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 06:fe:df:dd:f9:5a brd ff:ff:ff:ff:ff:ff


root@test-kub-03:~# ip link show veth1

8: veth1@veth0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 06:fe:df:dd:f9:5a brd ff:ff:ff:ff:ff:ff

root@test-kub-03:~# ip link show veth0

7: veth0@veth1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 2a:09:59:5a:ba:64 brd ff:ff:ff:ff:ff:ff

поскольку тип веф интерфейсов peer to peer то в их имени указывается и сосед сразу.

ifconfig показывает вот так

# ifconfig -a


veth0     Link encap:Ethernet  HWaddr 2a:09:59:5a:ba:64
          BROADCAST MULTICAST  MTU:1500  Metric:1

veth1     Link encap:Ethernet  HWaddr 06:fe:df:dd:f9:5a
          BROADCAST MULTICAST  MTU:1500  Metric:1




что интересно. ключ -d показывает к какому L2 типу относится каждый интерфейс

root@test-kub-03:~# ip -c -d link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 addrgenmode eui64

тип неуказан

2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:50:56:a3:56:a7 brd ff:ff:ff:ff:ff:ff promiscuity 0 addrgenmode eui64

тип неуказан

3: dummy0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 7a:18:fe:ad:7c:40 brd ff:ff:ff:ff:ff:ff promiscuity 0
    dummy addrgenmode eui64

тип dummy

4: lo2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 7a:5a:5a:17:a0:c3 brd ff:ff:ff:ff:ff:ff promiscuity 0
    dummy addrgenmode eui64

тип dummy

5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:d4:2b:46:03 brd ff:ff:ff:ff:ff:ff promiscuity 0
    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q addrgenmode eui64

тип bridge


7: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 2a:09:59:5a:ba:64 brd ff:ff:ff:ff:ff:ff promiscuity 0
    veth addrgenmode eui64

тип veth

8: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 06:fe:df:dd:f9:5a brd ff:ff:ff:ff:ff:ff promiscuity 0
    veth addrgenmode eui64

тип veth


а вот все возможные типов линков

   TYPE := [ bridge | bond | can | dummy | hsr | ifb | ipoib | macvlan | macvtap | vcan | veth | vlan | vxlan | ip6tnl | ipip | sit | gre | gretap | ip6gre |
               ip6gretap | vti | nlmon | ipvlan | lowpan | geneve ]


самое при это странное что непонятно к какому типу относится обычный L3 физический порт.
хм...

 замечу про вот эту надпись в портах

 link/ether

она значит что на уровне L1-L2 или L2  инкапсуляция = ethernet

итак создали vet0\veth1

засунем veth1 в ns-1

# ip link set dev veth1 netns ns-01


проверяем что он туда попал

~# ip netns exec ns-01 ip -c link show  veth1
8: veth1@if7: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 06:fe:df:dd:f9:5a brd ff:ff:ff:ff:ff:ff link-netnsid 0

видно что veth1 засунут в ns-01 успешно

но что интересно. peer соседом указан какойто непонятный if7
почему так незнаю.

поднимаем интерфейс и даем IP

# ip netns exec ns-01 ip link set veth1 up

смотрим его статус

# ip netns exec ns-01 ip link show
8: veth1@if7: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN mode DEFAULT group default qlen 1000
    link/ether 06:fe:df:dd:f9:5a brd ff:ff:ff:ff:ff:ff link-netnsid 0

state=lowerdown

интересно


что интересно. вот у нас есть неймспасе. в нем несколько линков. каждый со своим IP адресом.
и подключаем этот неймспейс к процессу в линуксе ( баш пинг ) , так вот интересно то что этом процессу доступны 
все линки этого неймспейса он может пользоваться ими всеми. то есть важно то что не линки привязываются к процессу.
к процессу привязывается неймспейс.  а к неймспейсу привязываются линки. неймспейс это как сумка. в сумке навалено линков. 
а процессу отдается сумка ( с линками ). все линки неймспейса  доступны процеессу. неотдельные линки а все линки доступны 
процессу. 

итак добавялем IP адрес
# ip netns exec ns-01 ip address add 127.30.0.1/16  dev veth1

# ip netns exec ns-01 ip -c address  show

6: lo3-ns1: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 2e:cb:07:79:39:cd brd ff:ff:ff:ff:ff:ff
    inet 127.20.0.1/16 scope host lo3-ns1
       valid_lft forever preferred_lft forever
8: veth1@if7: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000
    link/ether 06:fe:df:dd:f9:5a brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 127.30.0.1/16 scope host veth1
       valid_lft forever preferred_lft forever


добавляем ip к veth0 который лежит в дефолтовом неймспейсе

#  ip address add 127.30.0.2/16  dev veth0


напоминаю

veth0 лежит в namespace=default, 127.30.0.2/16

veth1 лежит в namespace=ns-01,   127.30.0.1/16



для начала еще надо поднять veth0 

# ip link set dev veth0 up

пробуем пинговать.


не получается.

что я обнаружил. 

вот зашел  я в неймспейс ns-01. 

в нем такие линки

1: lo:
    inet 127.0.0.1/8 
      

6: lo3-ns1: 
    inet 127.20.0.1/16 

8: veth1@if7: 
    inet 127.30.0.1/16 

так вот я немогу пинговать находясь в этом неймспейсе ни lo3-ns1 ни veth1
и оказалось методом тыка что нужно было поменять состояние lo интерфейса на UP
как толко я это сделал то сразу смог пинговать и lo3-ns1 и veth1


но далее все равно проблема. я немогу пингануть veth0 который нахоится в другом неймспейсе=default


# ping 127.30.0.2
connect: Invalid argument

хм... проблема была в том что я дал veth адрес из сети 127.  ну и как я понимаю 
ping 127.30.0.2 он пытался его сделать через lo  интерфейс. поскольку на lo интерфейсе указан сеть с маской 8

lo
127.0.0.1\8

как только я сделал вот так


veth0 лежит в namespace=default, 10.10.10.1/24

veth1 лежит в namespace=ns-01,   10.10.10.2/24


то сразу пинг заработал. межу неймспейсами. то есть с дефолтового неймсейса я могу пинговать 10.10.10.2

~# ping 10.10.10.2
PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.
64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.074 ms

да интересный вопрос. если есть два интерфейса когда сеть одного входитв сеть другого

lo
127.0.0.1\8

veth1
127.10.0.1\16

и мы пинугем ping 127.10.0.2

то вопрос , какой интерфейс выбирает система для отсылкаи пинга. из этих двух.

формально я считаю что это делается через таблицу марщуртизации

вот она такая

~# ip route show

10.10.10.0/24 dev veth1  proto kernel  scope link  src 10.10.10.2
127.20.0.0/16 dev lo3-ns1  proto kernel  scope link  src 127.20.0.1
127.30.0.0/16 dev veth1  proto kernel  scope link  src 127.30.0.1

видно что в ней нет lo интерфейса. тем не менее. вот с сетями 127 вышла накладка почемуто.

ксати veth= v-eth = virtual ethernet

то что пара veth это типа как провод - это неверно. это было бы верно если бы veth были L2 портами. а они L3 порты.
пара veth это рутер с двумя дырками veth L3. поэтому передача информции идет не по проводу на уровне L1-L2 а она 
идет через рутинг и через рутер. вот что такое пара veth портов.

интересно сейчас понять вот что. два компа втыкаются в свич. компы имеют порты уровня L3 а свич имеет порты уровня L2
получается порты уровня L3 втыкаются в порты уровня L2 вот как это работает.
бридж получается это свич. если два L3 порта обьединить через бридж то это равносильно эти два L3 порта воткнуть как бы в свич.
и когда мы работает с портами надо четко понимать этот порт относится к чему - к "порту компа" или к "порту свича".
это очень важно.

отсюда например такое следствие получается. 
неважно какого уровня порт в компе L3 или выше. важно какого уровня порты в свиче куда втыкаются порты компов. 
так как в свиче порты уровня L2 то передача данных между портами двух компов возможно только на уровне L2.
через фреймы. то есть уровень связи между компами определяется не их собвстенным уровнем а уровнем свича.


-->
еще одна задача
нужно научиться пользоваться cgroups
<--

насколько я понимаю TAP это тот же самый ethernet port только виртуальный.

<--
вот это тоже пока непонятно

tap vs bridge

есть еше непонятный момент что значит userspace networking

-->

из tap интерфейса вылетаеют L2 ethernet фреймы
из tun интерфейса вылетают L3 IP пакеты

насколько я понимаю линуксовский бридж br0 это ничто иное как свич.

неймспейс я думаю еще можно представить как рутер с функцией бриджа внутри линукса. то есть там и L2 и L3 порты натыканы.


<--
задача.

вот у нас есть два veth интерфейса.
один в одном неймспейсе а другой вдругом.

надо понять как узнавать имя и нейспейс второго veth зная первый.
когда они в одном неймспейсе то это просто написано в ip link show
а когда в разных то уже непонятно

-->

неймспейсы.

они вот какие бывают

 cgroup
 ipc 
 mnt 
 net
 pid
 user 
 uts

cgroup - для того чтобы ограничивтаь исполтщование ресурсов
ipc for IPC objects and POSIX message queues
mnt for filesystem mountpoints
net for network abstraction (VRF)
pid to provide a separated, isolated process ID number space
uts to isolate two system identifiers — nodename and domainname – to be used by uname
user - как я понял чтобы иметь несколько независимых таблицы с юзерами и группами


вот еще

	      /proc/pid/ns/mnt    the mount namespace
              /proc/pid/ns/uts    the UTS namespace
              /proc/pid/ns/ipc    the IPC namespace
              /proc/pid/ns/net    the network namespace
              /proc/pid/ns/pid    the PID namespace
              /proc/pid/ns/user   the user namespace
              /proc/pid/root      the root directory
              /proc/pid/cwd       the working directory respectively




каждый процесс  в линуксе принадлежит к какому то обяазательно неймспейсу в каждом типе.

если процесс имеет pid = 1 то информация в какие конкретно номера неймспейсов он входит хранится тут

/proc/1/ns/

пример

root@test-kub-04:/# ls -1l /proc/1/ns
total 0
lrwxrwxrwx 1 root root 0 Apr 27 19:01 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 ipc -> ipc:[4026531839]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 mnt -> mnt:[4026531840]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 pid -> pid:[4026531836]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 uts -> uts:[4026531838]


насколько я понял цифра в квадратных скобках [4026531957] это не имя неймспейса нет.
это номер инода на proc. дело в том что proc это не раздел на диске
это файловая система но виртуальная. дающая связь к структурам ядра.
так вот как я понял какие то файлы на proc ихнепонятно как прочитать
внутри они пустые а ls выводы ссылку на inod (как будто это обычная фс) этого файла.
пока непонятно. 
ясно одно что у двух процессов неймспейсы одинаковые если иноды одинаковые по номеру.
но как узнать сам номер непонятно.

вот эти номера неймспейсов для процесса 1 являются дефолтовыми номерами для неймспейсов.

возникает вопрос. как вывести все неймспейсы созданные в системе.
ответ по приколу походу в том что команды такой нет.
надо просканировать все процессы в /proc узнать иноды их неймспейсов а потом 
вывести в таблице.

вот скрипт на питоне

#!/usr/bin/python
#
# List all Namespaces (works for Ubuntu 12.04 and higher)
#
# (C) Ralf Trezeciak    2013-2014
#
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
import os
import fnmatch

if os.geteuid() != 0:
    print "This script must be run as root\nBye"
    exit(1)

def getinode( pid , type):
    link = '/proc/' + pid + '/ns/' + type
    ret = ''
    try:
        ret = os.readlink( link )
    except OSError as e:
        ret = ''
        pass
    return ret

#
# get the running command
def getcmd( p ):
    try:
        cmd = open(os.path.join('/proc', p, 'cmdline'), 'rb').read()
        if cmd == '':
            cmd = open(os.path.join('/proc', p, 'comm'), 'rb').read()
        cmd = cmd.replace('\x00' , ' ')
        cmd = cmd.replace('\n' , ' ')
        return cmd
    except:
        return ''
#
# look for docker parents
def getpcmd( p ):
    try:
        f = '/proc/' + p + '/stat'
        arr = open( f, 'rb').read().split()
        cmd = getcmd( arr[3] )
        if cmd.startswith( '/usr/bin/docker' ):
            return 'docker'
    except:
        pass
    return ''
#
# get the namespaces of PID=1
# assumption: these are the namespaces supported by the system
#
nslist = os.listdir('/proc/1/ns/')
if len(nslist) == 0:
    print 'No Namespaces found for PID=1'
    exit(1)
#print nslist
#
# get the inodes used for PID=1
#
baseinode = []
for x in nslist:
    baseinode.append( getinode( '1' , x ) )
#print "Default namespaces: " , baseinode
err = 0
ns = []
ipnlist = []
#
# loop over the network namespaces created using "ip"
#
try:
    netns = os.listdir('/var/run/netns/')
    for p in netns:
        fd = os.open( '/var/run/netns/' + p, os.O_RDONLY )
        info = os.fstat(fd)
        os.close( fd)
        ns.append( '-- net:[' + str(info.st_ino) + '] created by ip netns add ' + p )
        ipnlist.append( 'net:[' + str(info.st_ino) + ']' )
except:
    # might fail if no network namespaces are existing
    pass
#
# walk through all pids and list diffs
#
pidlist = fnmatch.filter(os.listdir('/proc/'), '[0123456789]*')
#print pidlist
for p in pidlist:
    try:
        pnslist = os.listdir('/proc/' + p + '/ns/')
        for x in pnslist:
            i = getinode ( p , x )
            if i != '' and i not in baseinode:
                cmd = getcmd( p )
                pcmd = getpcmd( p )
                if pcmd != '':
                    cmd = '[' + pcmd + '] ' + cmd
                tag = ''
                if i in ipnlist:
                    tag='**'
                ns.append( p + ' ' + i + tag + ' ' + cmd)
    except:
        # might happen if a pid is destroyed during list processing
        pass
#
# print the stuff
#
print '{0:>10}  {1:20}  {2}'.format('PID','Namespace','Thread/Command')
for e in ns:
    x = e.split( ' ' , 2 )
    print '{0:>10}  {1:20}  {2}'.format(x[0],x[1],x[2][:60])
#


вот его вывод

root@test-kub-04:/var/run/netns# ~/temp/namespaces-list.py
       PID  Namespace             Thread/Command
        --  net:[4026532468]      created by ip netns add 396a8fa82a7e
        --  net:[4026531957]      created by ip netns add 1
        --  net:[4026532468]      created by ip netns add 3588
        16  mnt:[4026531857]      kdevtmpfs
       552  mnt:[4026532435]      /lib/systemd/systemd-timesyncd
      3588  net:[4026532468]**    nginx: master process nginx -g daemon off;
      3588  uts:[4026532464]      nginx: master process nginx -g daemon off;
      3588  ipc:[4026532465]      nginx: master process nginx -g daemon off;
      3588  pid:[4026532466]      nginx: master process nginx -g daemon off;
      3588  mnt:[4026532463]      nginx: master process nginx -g daemon off;
      3617  net:[4026532468]**    nginx: worker process
      3617  uts:[4026532464]      nginx: worker process
      3617  ipc:[4026532465]      nginx: worker process
      3617  pid:[4026532466]      nginx: worker process
      3617  mnt:[4026532463]      nginx: worker process


он проканировал весь /proc для всех процессов.
и вывел неймспейсы которые отличаются от тех что у процесса 1
Namepspace - это тип неймспейса и номер его инода в proc
PID - это процесс который принадлежит данному неймспейсу


первые три неймспейса неимеют PID 

	--  net:[4026532468]      created by ip netns add 396a8fa82a7e
        --  net:[4026531957]      created by ip netns add 1
        --  net:[4026532468]      created by ip netns add 3588

это значит что неймспейсы были созданы а процессы к ним никакие небыли прилеплены
это нормально. потому это независит друг от друга.мы создаем неймспейсы 
а потом  если хотим добаялвем в этот неймспейс процесс или недобавляем.

главное к сожалению другое.
если мы хотим посмотреть какие неймспейсы созданы на хосте то мы это одной удобной командой 
сделать неможем.  


вернемся конкретно к сетевым неймспейсам.

мызнаем команду

# ip netns list

она якобы показыает созданные сетевые нейспейсы на компе.
увы это брехня отчасти.

эта команда показывает только те неймспейсы которые были созданы этой же командой
или неймспейсы которые указаны в папке

/var/run/netns

когда мы этой командой создаем namespace то файл связанный с неймспейсом (дескриптор или как его там)
создается в этой папке.

верно и другое если мы туда добавим дескриптор от себя  то команда ip netns его покажет.

а все неймспейсы которые созданы и их дескрпторы не скопированы в /var/run/netns их ip netns list
непоказывает.

в этом и жопа.

получается мы  с помощью этой команды неможем узнать все неймспейсы что созданы на компе.
например докер создает неймспейсы но дескрипторы этих сетевых неймспейсов он невписывает в /var/run/netns
он их вписывает  в /var/run/docker/netns

если сделать симлинк с файлом дескриптором в папку /var/run/netns то тогда ip netns list уже покажет.

если мы говорим про информацию в каком неймспейсе лежит конкретный процесс то инфо об этом как уже говоил 
хранится в /proc/$PID/ns/@net, если сделать с этого файла симлинк в /var/run/netns 
то ip netns list покажет сетевой неймспейс в котором лежит этот неймспейс.

все это обьясняет почему мы докером контейнеры создаем а ip netns list ничего непоказывает

еще есть команда 

# nsenter 

она тоже позволяет запускать программы и присваивать им определенный неймспейс. причем в отличие от
ip netns она умеет присваивать нетолько сетевой неймспейс

ключ --net позволяет указатть сетевоей неймспейс с которым мы хотим работать.


# nsenter --net=/var/run/docker/netns/396a8fa82a7e ip -c -f  inet address show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever

root@test-kub-04:/proc/101/ns# nsenter --net=/proc/3588/ns/net ip -c -f  inet address show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever

из этих примеров видно что я в первом примере указал в качестве дескриптора неймспейса 
файл из файлов докера /var/run/docker/netns/..

а во втором случае я указал дескриптор сетевого неймспейса напрямую из параметров proc для процесса который
работает в том же сетевом неймспейсе

и получили конечно одно и тоже.

в мане к nsenter сказано что вот эти вот файлы которые  я зову дескрипторами 

:/# ls -1l /proc/1/ns
total 0
lrwxrwxrwx 1 root root 0 Apr 27 19:01 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 ipc -> ipc:[4026531839]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 mnt -> mnt:[4026531840]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 pid -> pid:[4026531836]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Apr 27 19:01 uts -> uts:[4026531838]

назвыаются contexts

замечу очень полезное замечание что в /var/run/docker/netns/ файлы создаются с такими же именами как CONTAINER-ID у контейнера


root@test-kub-04:/proc/101/ns# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
7eeebd1c32c2        nginx               "nginx -g 'daemon of…"   6 hours ago         Up 6 hours          0.0.0.0:80->80/tcp   mynginx1

поэтому зная ID контейнера сразу понятно какое имя имеет файл-дескриптор в /var/run/docker/netns/


вот как один чувак написал в интернете - It’s quite hard to explore the Linux namespace. There is a lot of documentation flowing around. I did not find any simple program to look for namespaces in the system. 


получется кстати такой вывод - ip netns list показыает толкько те сетевые неймспейсы которая она же и создала.

далее возвращаемся к исходному вопросу - если второй veth из пары лежит в другом неймспейсе то как его найти.

рассмотрим вначале случай когда второй veth из пары лежит в неймспейсе который имеет имя то есть который мы создали
через команду ip netns add и о котором есть запись в /var/run/netns

пример

создаем пару

# ip link add name veth1 type veth peer name veth2


создаем namespace

# ip netns add ns-02

пихаем второй интерфейс из пары в этот неймспейс

# ip link set veth2 netns ns-02

смотрим что стало с veth1 в нашем неймспейсе

# ip -c link show veth1
5: veth1@if4: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 4a:fd:5b:68:6c:22 brd ff:ff:ff:ff:ff:ff link-netnsid 0


смотрим nsid у veth1

видим что netnsid = 0

что это такое. это специальный индикатор в каком неймспейсе нужно искать второй интерфейс из пары.

но тут небольшая жопа. так как указано не имя неймспейса с которым мы привыкли работать ( имя это то что 
указано в /var/run/netns ) а указан некий nsid.

значит далее мы зная ns-id хотим узнать имя соотвествующего ему неймспейса

# ip netns list-id
nsid 0 (iproute2 netns name: ns-02)


и видим что для ns-id=0 соответствует имя = ns-02

теперь мы знаем неймспейс в котором лежит veth2

# ip netns exec ns-02 ip -c link show type veth
4: veth2@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 86:d8:19:ab:64:bc brd ff:ff:ff:ff:ff:ff link-netnsid 0


но !!! это еще не все. имеется подвох. откуда мы знаем имя у второго интерфейсы. откуда мы знаем что оно равно veth2
мы этого абсолютно незнаем.

приведу пример 

вот мы имеем интерфейс

p# ip -c link show type veth
9: vethedebf689@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
    link/ether 82:d4:2a:8e:87:48 brd ff:ff:ff:ff:ff:ff link-netnsid 1

с именем vethedebf689 , его создавал нея а куберентес. поэтому абсолютно неизвестно какое имя имеет его напарник.

если в том неймспейсе сто интерфейсов то непонятно.

и тут на помощь приходит доп информация!

вернемся к нашему исходному примеру 


в нашем неймспейсе для veth1  мы видим

# ip -c link show veth1
5: veth1@if4: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 4a:fd:5b:68:6c:22 brd ff:ff:ff:ff:ff:ff link-netnsid 0


а именно обращаю внимание на кусок

5: veth1@if4:

первая цифра это не порядковый номер интерфейса в списке. нет! приведу пример чтобы это было наглядно

root@test-kub-01:~/temp# ip -c link show
1: lo: 
2: ens160: 
3: docker0: 
6: flannel.1: 
7: cni0: 
9: vethedebf689@if3: 
15: veth26afb3ad@if3: 
16: dummy0: 

видно что цифры 1, 2, 3, 6, 7, 9, 15, 16 не являются порядковыми числами.

на самом деле первая цифра в строке это  ifindex из sys
возвращаемся к нашему примеру

5: veth1@if4:


для veth1  ifindex равен

# cat /sys/class/net/veth1/ifindex
5

пять

именно поэтому первая цифра в строке это 5.

теперь посмотрим на это if4
эта штука это iflink для veth1

# cat  /sys/class/net/veth1/iflink
4

iflink обозначает ifindex того интерфейсы который для veth1 является пирром.

это нам дает то что когда мы посмотрим список интерфейсов в ns-02 то  наш искомый интерфейс там должен иметь порядковый номер = 4

# ip netns exec ns-02 ip -c link show type veth
4: veth2@if5: 

что собственно мы и видим 4:


таким образом зная iflink для veth1 мы знаем ifindex для искомого второго интерфейса в том неймспейсе в котором он лежит.

я считаю что вместо veth1@if4: было бы более информативно вот так писать veth1@iflink4: но увы такого нет.


моожно заметить что если оба veth лежат в одном неймспейсе то ip -c link show показыает их не ввиде
veth1@if4:

а в более красивом виде

6: veth4@veth3: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 6e:85:da:88:2f:ff brd ff:ff:ff:ff:ff:ff
7: veth3@veth4: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 3a:17:e5:4c:ed:e6 brd ff:ff:ff:ff:ff:ff

то есть

veth4@veth3 вместо veth4@if12312321

почему так.  потому что линукс лезет смотрим iflink для veth4

~# cat /sys/class/net/veth4/iflink
7

а далее он пытается считать имя для интерфейса с ifindex=7

ну как то он сканирует /sys/class/net  думаю. и например вот это находит

# cat /sys/class/net/veth3/uevent
INTERFACE=veth3
IFINDEX=7

и понимает что имя интерфейса у которо ifindex=7  равно veth3 И ПОДСТАВЛЯЕТ ЭТУ ИНФО В ВЫВОД IP SHOW

так вот когда второй интерфейс  В ДРУГОМ НЕЙМСПЕЙСЕ ЛЕЖИТ ТО ЕГО ПРОСТО НАПРОСТО НЕТ В /SYS/CLASS/NET
поэтому линукс неможет понять какое имя у интерфейса который является пиром для нашего veth1 и пишет только то что 
он знает  а именно iflink номер.



так. ну это была еще простая задача. потому что для ns-id у нас было имя неймспейса.
а имени  может и небыть  а ns-id есть.

и нам нужно зная ns-id как докопаться до его неймспейса и там найти наш искомый интерфейс.


итак у нас нет именованных неймспейсов

# ip netns list
#

но у нас есть ns-id

~# ip netns list-id
nsid 0
nsid 1


это значит что физически сетевые нейпсейсы в системе точно есть отличные от дефолтового сетевого неймспейса
осталось их найти.

насколько я понял их напрямую ненайти.

остается только одно.
для всех процессов в системе  взять дескрипторы сетевых неймспейсов и сделать симлинки  в /var/run/netns

таким образом мы через ip netns list увидим все неймспейсы что есть на компе.

и вот что мы увидим в # ip netns list

proc77
proc76
proc75
proc7511 (id: 0)
proc74
proc7482
proc73
proc7379 (id: 0)
proc7349
proc72
proc71


и тут мы видим самое главное - что ip netns list каким то образом понимает и показывает ns-id у неймспейса.

таким образом мы узнали через непрямой метод какие $PID имеют неймспейс который имеет ns-id

ура. мы знаем $PID неймспейса с заданными ns-id. а зная $PID мы знаем неймспейс можем в него войти.


далее я публикую чужой скрипт который все и делает.



# Create netns directory if not exist
mkdir -p /var/run/netns

# Add all process namespaces to netns
for i in /proc/[0-9]*/ns/net; do
  ln -s $i /var/run/netns/proc$(echo $i | cut -d/ -f3)
done

# Remove unassociated network namespaces
for i in $(ip netns | grep ^proc | grep -v id); do
  rm -f /var/run/netns/${i}
done

# Remove children processes
for i in $(ip netns | grep ^proc | awk 'a[$3]++ {print $1}'); do
  rm -f /var/run/netns/${i}
done



что он делает.
он копирует все ns дескрипторы всех процессов в /var/run/netns

получаем вот такой каталог файлов

proc77
proc76
proc75
proc7511 (id: 0)
proc74
proc7482
proc73
proc7379 (id: 0)
proc7349
proc72
proc71

убираем в каталоге все лишние записи там где нет (id: vasya)

далее убираем еще там дубликаты 

в итоге получаем вот такой каталог

root@test-kub-01:/var/run/netns# ls -1
proc7511
proc9947

# ip netns list
proc9947 (id: 1)
proc7511 (id: 0)

все ура . мы нашли те пиды которые сидят в нужных нам неймспейсах.
таким образом мы решили проблемы - как найти неймспейс без имени но у этого неймспейса есть ns-id

смотрим какие veth есть на хосте

# ip -c -f inet link show type veth
9: vethedebf689@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
    link/ether 82:d4:2a:8e:87:48 brd ff:ff:ff:ff:ff:ff link-netnsid 1

15: veth26afb3ad@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default
    link/ether 4a:a1:e9:6a:a9:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0

ищем где сидят соседи для vethedebf689 и veth26afb3ad

рассмотрим 

9: vethedebf689@if3:
link-netnsid 1

мы имеем что сосед этого вефа сидит в неймспейсе с ns-id=1  и что этот сосед имеет ifindex=3
как видно выше такой ns-id=1 имеет

proc9947 (id: 1)

подлкючимся к этому неймспейсу 
первый вариант подключения

# nsenter --net=/proc/9947/ns/net ip -c link show type veth

3: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 8e:1e:cf:9b:07:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0

видим 36 значит ifindex=3 иммет eth0

найдем тоже самое через ip netns
торой способ это сделать симлинк  /proc/9947/ns/net в /var/run/netns
тогда мы сможем зайти в неймспейс через ip netns exec

# ip netns exec proc9947  ip -c link show type veth

3: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 8e:1e:cf:9b:07:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0

получили одно и тоже двумя способами

убедимся что для eth0 ifindex=3 через другое место
# ip netns exec proc9947 cat /sys/class/net/eth0/ifindex
3

убедились.


 а вот почему то убедиться что ifindex=3 для eth0 через nsenter не получается

# nsenter --net=/proc/9947/ns/net ls -1al /sys/class/net
total 0
drwxr-xr-x  2 root root 0 Apr 28 16:05 .
drwxr-xr-x 65 root root 0 Apr 28 16:05 ..
lrwxrwxrwx  1 root root 0 Apr 28 16:05 cni0 -> ../../devices/virtual/net/cni0
lrwxrwxrwx  1 root root 0 Apr 28 16:05 docker0 -> ../../devices/virtual/net/docker0
lrwxrwxrwx  1 root root 0 Apr 28 16:05 dummy0 -> ../../devices/virtual/net/dummy0
lrwxrwxrwx  1 root root 0 Apr 28 16:05 ens160 -> ../../devices/pci0000:00/0000:00:15.0/0000:03:00.0/net/ens160
lrwxrwxrwx  1 root root 0 Apr 28 16:05 flannel.1 -> ../../devices/virtual/net/flannel.1
lrwxrwxrwx  1 root root 0 Apr 28 16:05 lo -> ../../devices/virtual/net/lo
lrwxrwxrwx  1 root root 0 Apr 28 16:05 veth26afb3ad -> ../../devices/virtual/net/veth26afb3ad
lrwxrwxrwx  1 root root 0 Apr 28 16:05 vethedebf689 -> ../../devices/virtual/net/vethedebf689

root@test-kub-01:/var/run/netns# nsenter -t 9947  ls -1al /sys/class/net
total 0
drwxr-xr-x  2 root root 0 Apr 28 16:05 .
drwxr-xr-x 65 root root 0 Apr 28 16:05 ..
lrwxrwxrwx  1 root root 0 Apr 28 16:05 cni0 -> ../../devices/virtual/net/cni0
lrwxrwxrwx  1 root root 0 Apr 28 16:05 docker0 -> ../../devices/virtual/net/docker0
lrwxrwxrwx  1 root root 0 Apr 28 16:05 dummy0 -> ../../devices/virtual/net/dummy0
lrwxrwxrwx  1 root root 0 Apr 28 16:05 ens160 -> ../../devices/pci0000:00/0000:00:15.0/0000:03:00.0/net/ens160
lrwxrwxrwx  1 root root 0 Apr 28 16:05 flannel.1 -> ../../devices/virtual/net/flannel.1
lrwxrwxrwx  1 root root 0 Apr 28 16:05 lo -> ../../devices/virtual/net/lo
lrwxrwxrwx  1 root root 0 Apr 28 16:05 veth26afb3ad -> ../../devices/virtual/net/veth26afb3ad
lrwxrwxrwx  1 root root 0 Apr 28 16:05 vethedebf689 -> ../../devices/virtual/net/vethedebf689

как видно интерфейса eth0 нет вообще.





рассмотрим следущий veth
15: veth26afb3ad@if3
link-netnsid 0

имеем что ns-id = 0 , ifindex у пира = 3

из 

# ip netns list
proc9947 (id: 1)
proc7511 (id: 0)

видно что ns-id=0 имеет pid=7511

зайдем в этот сетевой неймспейс через nsenter 

# nsenter --net=/proc/7511/ns/net ip -c link show type veth

3: eth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 1a:98:d1:91:5e:1c brd ff:ff:ff:ff:ff:ff link-netnsid 0


видим что eth0 имеет ifindex=3

зайдем в этотже неймспейс через ip netns 

# ip netns exec proc7511  ip -c link show type veth
3: eth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether 1a:98:d1:91:5e:1c brd ff:ff:ff:ff:ff:ff link-netnsid 0

видим что обоими способами мак один и тот же. и ifindex=3 для eth0

таким образом мы решили задачу - как находить второй veth когда он лежит  вдругом неймспейсе
и нам неважно есть у того неймспейса имя или только ns-id
ns-id назначается неймспейсу автоматом системой когда мы veth переносим  в тот неймспейс.

еще раз замечу 
когда мы сами создали namepsace через ip netns add и оно получило имя,
то понять какое это имя зная ns-id легко

# ip netns list
ns-03 (id: 1)
ns-02 (id: 0)
foo (id: 4)
inet_1

видим соотвествие ns-id и имени.

далее.
если name есть то можно ему назначать ns-id

b-04:/# ip netns list
foo

root@test-kub-04:/# ip netns set foo 4
root@test-kub-04:/# ip netns list
foo (id: 4)

но какой от этого толк непонятно.


далее


есть еще важный вопрос.

мы сидим в дефолтовом неймспейсе. и видим какието неймспейсы. и даем даже им ns-id
потом создали новый неймспейс. переходим в него. увидим ли мы в нем все теже неймспейсы и будут у 
них теже ns-id

# ip netns list

# ip netns add ns-01

# ip netns list
ns-01

# ip netns set ns-01 10

# ip netns list
ns-01 (id: 10)

# ip netns list-id
nsid 10 (iproute2 netns name: ns-01)


итак мы завели сетевой неймспейс с именем ns-01
и сами назначили ему ns-id = 10

# ip netns list-id
nsid 10 (iproute2 netns name: ns-01)

# ip link add name veth1 type veth peer name veth2
# ip link set veth2 netns ns-01


мы создали veth1  и veth2

и поместили veth2 в ns-01 неймспейс

посмотрим как видны неймспейсы  из дефолтового сетевого неймспейса
создадим еще один неймспейс чтобы был чистый эксперимент
чтобы у нас был неймспейс в котором не лежит ни один из наших veth

# ip netns add netns-empty

назначим ему тоже ns-id

# ip netns set netns-empty 100

теперь самое главное посмотрим как видны неймспейсы из дефолтового неймспейса

~# ip netns list
netns-empty (id: 100)
ns-01 (id: 10)


теперь самое интересное посмотрим как видны неймспейсы из неймспейса ns-01

# ip netns exec ns-01 ip netns list
netns-empty
ns-01

мы видим что из под неймспейса недефолтового видны все неймспейсы только ns-id у них исчез.

вывод №1 = получается параметры ns-id он являются неймспейс зависимыми для тех же самых имен неймспейсов
вывод №2 = если у нас изменился только сетевой неймспейс ( а остальные виды неймспейсов остались дефолтовыми) то 
в новом неймспейсе сетевом мы видим все другие неймспейсы , имеется ввиду с точки зрения утилиты ip netns


далее
ксати удаление записи из /var/run/netns не удалеяет как я понял неймспейс а только удаляет его имя.
но потом попробуй его найти без имени.

далее
заметка про proc и его файлы

root@test-kub-04:/# stat /proc/6009/ns/pid
  File: '/proc/6009/ns/pid' -> 'pid:[4026532641]'
  Size: 0               Blocks: 0          IO Block: 1024   symbolic link
Device: 4h/4d   Inode: 74920       Links: 1
Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2020-04-28 01:54:12.748202822 +0300
Modify: 2020-04-28 01:53:37.335287839 +0300
Change: 2020-04-28 01:53:37.335287839 +0300


еще скажу вот что.
неймспейсы неймспейсами.
но когда мы переходим к процессам и их сетью то все тогда вот так:
процесс видит некоторые сетевые интерфейсы.
эти интерфейсы составляют таблицу маршрутизации
и в итоге процесс шлет пакет в сетевой интерфейс а оттуда с помощью таблицы маршрутизации он улетает дальше.
поэтому если мы хотим понять какие сети видит процесс то самое простое это взять $PID процесса и помотреть сетевые 
интерфейсы ему доступные и таблицу маршрутизации

# nsenter -net $PID ip -c link show
# nsenter -net $PID ip -c route show

так мы отходим от абстрактных сетевых неймспейсов к конкретному сетевому обмену для процессов




фуууууууууууууух! вот вроде теперь наконец все про сетевые неймспейсы и veth в них.

      
все талдычат одно и тоже что нейтворк неймсейсес они дают тебе  изолированный сетевой стек
и ты имеешь незаивисые сеетвые интерфпейсы  таблицу маршрутизации при этом никто не обьясняет 
как это сказыавется на особых каталогах линукса таки как proc dev sys. что в них меняется.
жаль . это пока непонятно.


совершенно логичная штука что интерфейс (физ или виртуальный) может принадлежать только одному сетевому
неймспейсу

насколько я понял чтобы трафик между veth интерфейсами ходил нужно также чтобы на хосте
был активирован форвардинг


хотя каждый из veth принадлжеит разным неймспейсам ( если так сделать ) но связь между ними работает.

значит все это время я работал с неймспейсами и никак не мог понять почему я немогу поставить lsns
оказыается потому что в ubuntu 16 ее нет в составе util-linux

я прописал более новый дистрибутив в apt

и наконец обновил util-linux

и теперь у меня появидся lsns

щас начнем его смотреть. что он умеет.



lsns основыввает свою работу
на информации из /proc
поэтому  lsns видит неймспейсы только те в которых запущен хотя бы один линукс процесс.

так вобщем я посмотрел на эту lsns

вот ее типичный вывод

root@test-kub-01:/# lsns -t net -o NS,TYPE,PATH,NPROCS,PID,PPID,COMMAND,UID,USER
        NS TYPE PATH              NPROCS   PID  PPID COMMAND    UID USER
4026531957 net  /proc/1/ns/net       163     1     0 /sbin/init   0 root
4026532504 net  /proc/9831/ns/net      2  9831  9796 /pause       0 root
4026532569 net  /proc/7379/ns/net      2  7379  7349 /pause       0 root
root@test-kub-01:/#


вцелом пока полезность этой утилиты под вопросом.
чтото новое она показывает но при этов неудобном виде.

пока на этом с ней все.

кстати что интересно lsns несмог увидеть неймспейсы у которых нет имент но есть ns-id


вот вывод про ns-id

root@test-kub-01:/# ip netns list-id
nsid 0 
nsid 1 

при этом я уже выяснил ранее какие процессы  лежат в этих неймспейсах

nsid 0 = proc7511)
nsid 1 = proc9947)



а вот что увидел lsns

root@test-kub-01:/# lsns -t net -o NS,TYPE,PATH,NPROCS,PID,PPID,COMMAND,UID,USER

        NS TYPE PATH              NPROCS   PID  PPID COMMAND    UID USER
4026531957 net  /proc/1/ns/net       163     1     0 /sbin/init   0 root
4026532504 net  /proc/9831/ns/net      2  9831  9796 /pause       0 root
4026532569 net  /proc/7379/ns/net      2  7379  7349 /pause       0 root

то есть видно что lsns неувидел нужные процессы.

в общем lsns я так понимаю идет в помойку

что еще интересно.

оказывается ps умеет показыват типа номер неймспейса для процесса
пример

m~# ps -eo pid,user,netns,args --sort user
  PID USER          NETNS COMMAND
  761 daemon   4026531957 /usr/sbin/atd -f
  744 message+ 4026531957 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation
 2706 mkadm    4026531957 /lib/systemd/systemd --user
 2708 mkadm    4026531957 (sd-pam)
 2766 mkadm    4026531957 sshd: mkadm@pts/0
 2767 mkadm    4026531957 -bash
    1 root     4026531957 /sbin/init
    2 root     4026531957 [kthreadd]

вот эти вот числа 4026531957 это конечно не ns-id это параметр который взят отсюда /proc/$PID/ns/net
тот который называется номером инода в файловой системе proc.

получается через ps мы можем узнать все процессы которые лежат в одном неймспейсе.
утилита ip тоже умеет такое делать НО только для неймспейсов имеющих имя
 ip netns pids NAME

а через ps мы можем увидеть все такие процессы и для неймспейсов неимющих имя.

с другой стороны если мы создаим симлинк с /proc/$PID/ns/net в /var/run/netns то тогда неймспейс станет именованным
и ip netns pids NAME сработает.


итак теперь мы умеем очень важное - мы умеем искать второй хвост у veth
а это нам очень поможет с докером теперь.

еще раз себе напомню что самое простой способ войти в неймспейс сетевой это использовать pid процесса
который сидит в этом неймспейсе

nsenter -t $PID ...

жопа в том что nsenter покажет соверщенно разное если выполним вот эти две команды

#nsenter --net=
#nsenter -t $PID

а должны одно и тоже. 
пока непонятно

<---
я закнчил на том что поставил 
докер на пустой хост.

и начал изучать какая там сеть. как работает сеть у докера.
на уровне какие интерфейсы какая ip адреса

где там бридж у докера

что в  него входит.

как получается что контейнер имеет адрем 172.17.0.х
а связь с ним работает через 127.0.0.1

как работает проброс порта из неймспейса наружу

-->

поставил докер.

докер создал интерфейс типа bridge

3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:8e:1e:18:de brd ff:ff:ff:ff:ff:ff promiscuity 0
    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q addrgenmode eui64

причем это порт уровня L3 его адрес

    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0


в докере я создал контейнер с нжинкс

root@test-kub-04:~# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
6c52734f2d22        nginx               "nginx -g 'daemon of…"   2 days ago          Up 20 hours         0.0.0.0:80->80/tcp   mynginx1

посмотрим сетевые настройки этого контейнера.

# docker inspect 6c52734f2d22  | grep -i ip
            "IPAddress": "172.17.0.2",

посмотрим теперь ip через неймспейс

# docker inspect 6c52734f2d22  | grep -i pid
            "Pid": 17056,


~# nsenter --net=/proc/17056/ns/net ip -c address show
6: eth0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0

итак мы выяснили что у контейнера ip = 172.17.0.2/16  

теперь убедимся что eth0 в контейнере имеет veth тип

~# nsenter --net=/proc/17056/ns/net ip -c -d link show
6: eth0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    veth 

итак выяснилось теперь что сетевая карта у контейнера имеет тип veth
то есть вефы это именно вот какие карточки сетевые используются в контейнерах
именно вефы только вефы использует докер в контейнерах

найдем второй veth из пары у вефа контейнера eth0@if7:, link-netnsid 0

в общем я подмонтировал ns от $PID=1 

и в неймспейсе контейнера я вижу

# ln -s /proc/1/ns/net /var/run/netns/1
root@test-kub-04:~# nsenter --net=/proc/17056/ns/net ip netns list
1 (id: 0)

таким образом мы знаем что хвост у eth0@if7: лежит в неймспейсе pid=1

ищем в нем 
причем ищем интерфейс сразу с номером 7 , поскольку if7

~# ip -c link -c -d show

7: vethad1f79f@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether da:68:27:36:52:e1 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 1
    veth
    bridge_slave state forwarding priority 32 cost 2 hairpin off guard off root_block off fastleave off learning on flood on addrgenmode eui64 

итак это 
7: vethad1f79f@if6:

посмотрим какой у него ip 

# ip -c  address show dev vethad1f79f
7: vethad1f79f@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default
    link/ether da:68:27:36:52:e1 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::d868:27ff:fe36:52e1/64 scope link
       valid_lft forever preferred_lft forever

то есть у этого интерфейса нет IP

итак мы имеем такую картину

контейнер:
ns = $pid 17056
if = eth0@if7 (veth)
ip = 172.17.0.2/16

второй хвост у вефа
ns = $pid 1
if = vethad1f79f@if6
ip = его нет

окей на уровне L2 мы узнали как устроена сетевая система связи между контейнером
и внешним миром.

еще щас посмотрим только сетевой неймспейс у контейнерного pid отличается от дефолтового или и остальные


~# lsns  -o NS,TYPE,PATH,NPROCS,PID,PPID,COMMAND,UID,USER | grep -E "PATH|/proc/1/|/proc/17056/"
        NS TYPE   PATH               NPROCS   PID  PPID COMMAND                                   UID USER
4026531835 cgroup /proc/1/ns/cgroup     134     1     0 /sbin/init                                  0 root
4026531836 pid    /proc/1/ns/pid        132     1     0 /sbin/init                                  0 root
4026531837 user   /proc/1/ns/user       134     1     0 /sbin/init                                  0 root
4026531838 uts    /proc/1/ns/uts        132     1     0 /sbin/init                                  0 root
4026531839 ipc    /proc/1/ns/ipc        132     1     0 /sbin/init                                  0 root
4026531840 mnt    /proc/1/ns/mnt        130     1     0 /sbin/init                                  0 root
4026531957 net    /proc/1/ns/net        132     1     0 /sbin/init                                  0 root
4026532575 mnt    /proc/17056/ns/mnt      2 17056 17038 nginx: master process nginx -g daemon off   0 root
4026532576 uts    /proc/17056/ns/uts      2 17056 17038 nginx: master process nginx -g daemon off   0 root
4026532577 ipc    /proc/17056/ns/ipc      2 17056 17038 nginx: master process nginx -g daemon off   0 root
4026532578 pid    /proc/17056/ns/pid      2 17056 17038 nginx: master process nginx -g daemon off   0 root
4026532580 net    /proc/17056/ns/net      2 17056 17038 nginx: master process nginx -g daemon off   0 root



видим что у контейнера свои индивидуальные неймспейсы для
 pid 
 uts
 ipc
 net


хотя можно и по другому посмотреть это же


~# ls -1al  {/proc/1/ns,/proc/17056/ns}
/proc/17056/ns:
lrwxrwxrwx 1 root root 0 Apr 30 01:36 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Apr 30 01:36 ipc -> ipc:[4026532577]
lrwxrwxrwx 1 root root 0 Apr 30 01:36 mnt -> mnt:[4026532575]
lrwxrwxrwx 1 root root 0 Apr 30 01:35 net -> net:[4026532580]
lrwxrwxrwx 1 root root 0 Apr 30 01:36 pid -> pid:[4026532578]
lrwxrwxrwx 1 root root 0 Apr 30 01:36 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Apr 30 01:36 uts -> uts:[4026532576]

/proc/1/ns:
total 0
lrwxrwxrwx 1 root root 0 Apr 29 14:48 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Apr 29 14:48 ipc -> ipc:[4026531839]
lrwxrwxrwx 1 root root 0 Apr 29 14:48 mnt -> mnt:[4026531840]
lrwxrwxrwx 1 root root 0 Apr 29 14:48 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Apr 29 14:48 pid -> pid:[4026531836]
lrwxrwxrwx 1 root root 0 Apr 29 14:48 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Apr 29 14:48 uts -> uts:[4026531838]
 
что мы видим

cgroup - у них общий
ipc - разный
mnt - разный
net - разный 
pid - разный
user - общий
uts - разный


малек прокроментирую

mnt разный это значит что у  контейнера индивидуальная файловая система
pid разный это значит что процессы контейнера не видят процессы хоста и хост невидит процессы контейнера
user общий это значит что пользователи контейнера и пользователи хоста одни и теже
uts разный это значит что hostname хоста отличается от hostname контейнера


окей. на уровне L2 интерфейсов мы разобрались как организованы интерфейсы в контейнере.
это veth в своем неймспейсе это неймспейс контейнера. и второй veth сидит в неймспейсе хоста.
и таким макаром на уровне L2 осуществляется связь между контейнером и хостом.

теперь посмотрим как устроена связь между контейнером и хостом на уровне L3


значит схема на уровне L2 такая

[ контейнер ] (eth0@if7) --- (docker0 {vethad1f79f@if6}) [ хост ] 


docker0 {vethad1f79f@if6} - значит что veth входит в состав бриджа docker0 
# brctl show docker0
bridge name     bridge id               STP enabled     interfaces
docker0         8000.02428e1e18de       no              vethad1f79f


  
таким образом мы ушли от неймспейсов которые нам теперь по барабану.
и перешли к классической сетевой структуре - интерфейсам уровня L2

фреймы между этим двумя сетевыми интерфейсами могут ходит без проблем.
еще видно что vethad1f79f входит в состав  бриджа docker0
значит фрейм влетающий в docker0 переправляется в vethad1f79f
а фрейм вылетающий из vethad1f79f вылетает в docker0





переходим к L3 связи между хостом и контейнером
ведь исходная задача которая меня заинтересовала как это так получается что мы на хосте 
даем команду curl 127.0.0.1:80   и это долетает внутрь контейнера у которого адрес 172.17.0.2

контейнер
eth0@if7
ip = 172.17.0.2/16

хост
vethad1f79f@if6 ( в составе docker0 )
ip = нет
docker0
ip =  172.17.0.1/16


кстати неудивительно что vethad1f79f@if6 не имеет ip ,  так как интерфейсы входящие в состав бриджа
немогут иметь свой ip. и это логично. так как все порты входящие в состав бриджа получают IP самого бриджа.

получается в теории 
мы имеем вот такую сеть уровня L3

интерфейсы L3
docker0 (хост) = 172.17.0.1/16
eth0 (контейнер) = 172.17.0.2/16


получается мы можем в теории с хоста "напрямую" пингануть  IP контейнера потому что 
на хосте есть интерфейс который входит в туже IP сеть что и сеть контейнера.

# ping 172.17.0.2
PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.
64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.079 ms

и curl тоже работает

# curl 172.17.0.2:80
<!DOCTYPE html>
<html>
...

\справка по iptables:
 LOCAL means ANY IP assigned on one of the interfaces of the host, including the loopback.
 LOCAL - the destinations are assigned to this host.  The packets are looped back and delivered locally.
 \
 
 

(тут такая ремарка я ниже пишу про доке-прокси docker-proxy но иногда путаю термин и пишу куб-прокси.
это неверно конечно же мы щас говорим про докер прокси. я везде вроде исправил но могло еще гдето остаться
итак верно докер прокси.)

на всякий случай посмотрим что там с таблицей iptables

root@test-kub-04:~# iptables-save

*filter
:INPUT ACCEPT [10940:1075816]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [3792:774535]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Thu Apr 30 22:45:35 2020
# Generated by iptables-save v1.6.0 on Thu Apr 30 22:45:35 2020

*nat
:PREROUTING ACCEPT [1672:305719]
:INPUT ACCEPT [1672:305719]
:OUTPUT ACCEPT [45:3375]
:POSTROUTING ACCEPT [44:3291]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER (пакеты подпадающие под это правило - пакет летит  хоста на адрес интерфейса хоста за исключением лупбэка)
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT


мы стучимся на 172.17.0.2 это "чужой" интерфейс
это пакет который летит через FORWARD

путь прохождения пакета ( согласно картинке отсюда https://aceqbaceq.blogspot.com/2014/11/linux-z0.html ) будет такой

routing decision(LOOK AT ROUTE TABLE) -> mangle (OUTPUT) -> nat (OUTPUT) -> 
-> filter (OUTPUT) -> mangle (POSTROUTING) -> nat (POSTROUTING)

в итоге когда мы 
# ping  172.17.0.2 
то сработают такие правила

mangle (OUTPUT)
пусто
nat (OUTPUT) 
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER = несработает
filter (OUTPUT)
пусто
mangle (POSTROUTING)
пусто
nat (POSTROUTING)
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE = несработает
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE =сработает

в итоге пролетев все правила iptables ip будет такое преобразование :
ip пакет поменяет scr ip на  172.17.0.1 и таким он будет выплюнут в docker0 интерфейс

и таким он прилетит в контейнер
то есть для контейнера все пакеты с хоста выглядят как прилетевшие с 172.17.0.1 ( в случае пингов )

посмотрим что будет для такой команды с точки зрения iptables преобразований

# curl 172.17.0.2:80

я посмотрел будет все ровно тоже самое что и для пинга

в итоге получаем что связь с сетью контейнера работает как связь с компом в локалке
один veth в контейнере второй veth на хосте. хостовый veth воткнут в бридж порт на хосте.
бридж порт на хосте имеет туже ip сеть что и порт в контейнере. в итоге обычная связь по локалке.
эту схему можно представить вот так. в компе карта docker0 которая воткнута в свич. в этот же свич воткнут
другой комп с картой контейнера. и они лежат в одной ip сети. вот так все просто.

так. получается со связью с контейнером с самого хоста разобрались.

теперь надо понять как работает связь с контейнером через 127.0.0.1

с точки зрения прохождения цепочек iptables порядок будет тот же самый

routing decision(LOOK AT ROUTE TABLE)mangle (OUTPUT) -> nat (OUTPUT) -> 
-> filter (OUTPUT) -> mangle (POSTROUTING) -> nat (POSTROUTING)

странно.. насколько я вижу.  когда мы пуляем с хоста на 127.0.0.1 ни одно правило ничего
неменяет в пакете. таким образом пакет будет
src ip  = 172.17.0.1  dst ip = 127.0.0.1 port=80
в итоге он неприлетит в контейнер. однакоо эксперимент показыает что пакет прилетает в контейнер

# curl 127.0.0.1:80
<!DOCTYPE html>
<html>

какже так получается...?
а получается вот как предварительно. при запуске контейнера помимо процесса с контейнером
докер запускает и процесс docker-proxy который слушает все входящие на :80 и получив перенаправляет 
в контейнер.
получается такая вещь что если мы обращаемся к контейнеру по его родному ip = 172.17.0.2
то связь происходит натуральным сетевым способом через L2-L3 механизм (ну только таблица
iptables меняет src ip ).
а если мы обращаемся на 127.0.0.1:80 то этот реквест поступает в docker-proxy а уже он его перенаправляет  в контейнер.
именно из за docker-proxy работает связь с контейнером при обращении на 127.0.0.1:80
если убить процесс с докер-прокси то связьтс с контейнером через 127.0.0.1:80 прекращает работать
( а через 172.17.0.2:80 продолжает работать ибо для этого пути docker-proxy ненужен)


Остановился на том что звездочками обозначил вопросы которые надо разобрать:

*раз уж мы напоролись на docker-proxy 
давайте его разберем

теперь надо понять что значит docker-proxy
root@test-kub-04:~# netstat -tnlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp6       0      0 :::80                   :::*                    LISTEN      17028/docker-proxy

насколько я понял docker-proxy запускается докером если мы при создании контейнера указали опцию 
проброса порта от контейнера во внешний мир -p 80:80

docker run -p

 -p, --publish ip:[hostPort]:containerPort | [hostPort:]containerPort
          Publish a container's port, or range of ports, to the host.


$ docker run --name mynginx1 -p 80:80 -d nginx

эта опция -p задаает что мы можем обратиться а любой ip порт компа и порт1 и попадем в контейнер на порт2

так вот при этой опции запускается докером докер-прокси. это было условие когда он запускается.
теперь внимательно посмотрим когда он срабатывает.

если мы с самого хоста обращаемся на ip контейнера : port то  докер прокси нам ненужен.
мы попадаем на сокет контейнера чисто из за сетевых законов

если мы с самого хоста обращемся на LOCAL_IP:port (кроме 127.0.0.1 ) то нам тоже докер прокси ненужен
мы попадем на сокет контейнера за счет правила iptables
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80

если мы с самого хоста обратимся на 127.0.0.1:port ВОТ ТУТ ПЕРВЫЙ раз когда сработает докер-прокси !

посмотрим какие таблицы\цепочки iptables срабаывают при обращении из вне на LOCAL_IP:port
согласно (https://aceqbaceq.blogspot.com/2014/11/linux-z0.html)

из вне обращаемся как

(foregn_hostB)# curl hostA:80

поскольку все что лежит за docker0 это как бы "внешняя сеть" по отношени. к хосту с классической сетевой
точки зрения то пакет пойдет по filter(FORWARD) а не filter(INPUT)


mangle(PREROUTING) - nat(PREROUTING) - routing decision(LOOK AT ROUTE TABLE)- mangle(FORWARD) -
-  filter(FORWARD) - mangle(POSTROUTING) - nat(POSTROUTING)

*filter
:INPUT ACCEPT [10940:1075816]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [3792:774535]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Thu Apr 30 22:45:35 2020
# Generated by iptables-save v1.6.0 on Thu Apr 30 22:45:35 2020

*nat
:PREROUTING ACCEPT [1672:305719]
:INPUT ACCEPT [1672:305719]
:OUTPUT ACCEPT [45:3375]
:POSTROUTING ACCEPT [44:3291]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER (пакеты подпадающие под это правило - пакет c хоста
летит  на адрес интерфейса хоста за исключением лупбэка)
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT

смотрим

важный момент в этой цепи это момент когда ядро принимает роутинговое решение то есть когда оно смотрит 
в таблицу маршутизации.  потомучто после этого момента ядро узнает какая сетевая карта будет output
(непутать с цепочкой OUTPUT) и когда ядро это поняло то мы получаем ответ для ключа -o "выходная карта"
а этот ключ широко используется в iptables таблицах.
входная сетевая карта ключ -i известна с самого начала.

в данном случае -i ens160

значит пробег по цепочкам будет такой 
 
 mangle(PREROUTING) пусто

 nat(PREROUTING)
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER = true
-A DOCKER -i docker0 -j RETURN  = false
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80

получили что dst ip теперь = 172.17.0.2

далее смотрим в рутинговую таблицу 

 routing decision(LOOK AT ROUTE TABLE)
default via 172.16.102.1 dev ens160 onlink
172.16.102.0/24 dev ens160  proto kernel  scope link  src 172.16.102.34
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1

понимаем что пакет полетит через docker0
и значит полетит через FORWARD а не INPUT

значит  -o = docker0

итого	-i = ens160
	-o = docker0
	src ip = HOSTB
	dest ip = 172.17.0.2

 mangle(FORWARD) пусто

 filter(FORWARD)
-A FORWARD -j DOCKER-USER = true 
-A DOCKER-USER -j RETURN = true
-A FORWARD -j DOCKER-ISOLATION-STAGE-1 = true
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2 = false
-A DOCKER-ISOLATION-STAGE-1 -j RETURN = true
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT = false
-A FORWARD -o docker0 -j DOCKER = true
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT = true

 mangle(POSTROUTING) пусто

 nat(POSTROUTING)
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE = false
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE = false


в итоге пакет направляется в docker0

src ip = HOSTB
dest ip = 172.17.0.2
--dport = 80

из этого я вижу что для связи из внешнего мира с контейнером docker-proxy ненужен 
связь работает и без него средставами iptables

проверим это убив процесс с докер-прокси.
я проверил. убил докер-прокси. и с внешнего хоста по локалке постучал на мой хост на порт 80
и успешно получил ответ от нжинкс то ест связь до контейнера дошла. без докер-прокси.

ПОЛУЧАЕТСЯ что докер-прокси на данный момент оказыавется только нужен если 
1) мы обращаемся изнутри самого хоста 
2) на адрес 127.0.0.1:80
и хотим при этом попасть на докер конейнер сокет.
вот только в этом единственном случае нужен докер-прокси.
все остальное успешно решает iptables 


получается также что со связью с контейнером из внешнего мира мы попутно разобрались.

* надо все таки разобрать так зачем же они ввели этот докер-прокси и можно ли его отключить 
при запуске контейнера

вот что написано в man dockerd

--userland-proxy=true|false Rely on a userland proxy implementation for inter-container and outside-to-container loopback communications. Default is true.

я отлючил userland proxy 

я вставил 

  "userland-proxy": false,
  "iptables": true,

в /etc/docker/daemon.json

# cat /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=cgroupfs"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "userland-proxy": false,
  "iptables": true,
  "storage-driver": "overlay2"
}

вот как netstat выглядит

# netstat -tnlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1112/sshd
tcp6       0      0 :::80                   :::*                    LISTEN      23756/dockerd
tcp6       0      0 :::22                   :::*                    LISTEN      1112/sshd

непонято то у нас порт конейнера слушал docker-proxy теперь его слушает dockerd
есть ли разница? непонятно
ведь мы хотели избавиться от прокси. чтоб связь с контейнером работала
через iptables only

вот как iptables выглядит теперь



# iptables-save
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*filter
:INPUT ACCEPT [279:20641]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [231:29908]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Sat May  2 21:44:14 2020
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*nat
:PREROUTING ACCEPT [1:229]
:INPUT ACCEPT [1:229]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT
# Completed on Sat May  2 21:44:14 2020

посмотрим изменилось ли чтото с точки зрения сетевой пути к контейнеру

*связь с другого компа сети в наш контейнер
*связь изнутри хоста на контейнер по его IP 172.17.0.2
*связь изнутри хоста на 127.0.0.1:80
*связь изнутри хоста на LOCAL (исключая 127.0.0.1)

*связь с другого компа сети в наш контейнер:
если пакет летит с другого компа на порт 80 то в прероутинг подменяется dest ip на 172.17.0.2
дальше в FORWARD такой пакет пропускается. итого этот путь прежний.


*связь изнутри хоста на контейнер по его IP 172.17.0.2:
routing decision - mangle (output) - nat(output) - filter(ouput) - 
- mangle(postrouting) - nat(postrouting)

routing decision
(в routing decision для локальных процессов мы определяем выходной интерфейс -o 
а также исходный интерфейс и src ip который удобнее использовать потому что
в случае когда пакет прилетает с другого компа его src ip и -i известен пофакту,
а когда мы с процесса с компа хотим куда то постучать то нам надо выбрать с какого
интерфейса компа (их же несколько) мы будем это делать и каким исходным ip будем пользоваться)

понимаем что -o=docker0
понимаем что -i=docker0
src ip = src ip (docker0) = 172.17.0.1


mangle (output)
пусто

nat(output)
DNAT --to-destination 172.17.0.2:80

filter(ouput)
accept

mangle(postrouting)
пусто

nat(postrouting)
-j MASQUERADE

в итоге цепочка выглядит

docker0 /src ip 172.17.0.1 dst ip 127.0.0.1/ -> routing decision - mangle (output) - nat(output) - filter(ouput) - 
- mangle(postrouting) - nat(postrouting) -> docker0 /src ip 172.17.0.1 dst ip 172.17.0.2/

итог: пролетели в контейнер через iptables как и раньше


*связь изнутри хоста на 127.0.0.1:80 :

# iptables-save
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*filter
:INPUT ACCEPT [279:20641]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [231:29908]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Sat May  2 21:44:14 2020
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*nat
:PREROUTING ACCEPT [1:229]
:INPUT ACCEPT [1:229]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT


routing decision - mangle (output) - nat(output) - filter(ouput) - 
- mangle(postrouting) - nat(postrouting)

--dst ip=127.0.0.1


routing decision
понимаем что -o=lo
--src ip=127.0.0.1
-i=lo

mangle (output)
пусто

nat(output)
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A DOCKER -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
в итоге происходит подмена dest-ip


кстати я хочу тут привести какие были правила на этот же счет, когда докер работал 
с опцией user-proxy

-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
-A DOCKER -i docker0 -j RETURN
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
отсюда видна разница.
что раньше если у нас dest-ip был равен 127.0.0.1 то подмена destination ip непроисходила.
именно поэтому вслучае когда мы стучали на 127.0.0.1 нужен был docker-proxy работающий

filter(ouput)
accept

mangle(postrouting)
пусто

nat(postrouting)
поскольку  dest-ip = 172.17.0.2 то
правило 
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE = true
в итоге
src ip = 172.17.0.1


в итоге
-i=docker0
src ip=172.17.0.1

-o=docker0
dst = 172.17.0.2

в итоге связь через 127.0.0.1 пройдет средствами только iptables.

*связь изнутри хоста на LOCAL (исключая 127.0.0.1)
для примера dest-ip=172.16.101.32 (ens160)


# iptables-save
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*filter
:INPUT ACCEPT [279:20641]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [231:29908]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Sat May  2 21:44:14 2020
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*nat
:PREROUTING ACCEPT [1:229]
:INPUT ACCEPT [1:229]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT


routing decision - mangle (output) - nat(output) - filter(ouput) - 
- mangle(postrouting) - nat(postrouting)

--dst ip=172.16.101.32

routing decision 
-o=ens160
-src-ip=172.16.101.32
-i=ens160

mangle (output) 
пусто

nat(output)
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A DOCKER -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80

значит -o меняется на -o=docker0

filter(ouput)
accept

mangle(postrouting)
пусто

nat(postrouting)
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
значит
--src-ip=172.17.0.1
-i=docker0

в итоге
пакет пойдет в контейнер средствами исключетльно iptables


главный итог - во всех случаях пакет доходит до контейнера 
и доходит исключиетльно средствами iptables
значит отключение user-proxy действительно его отключает.
и действительно вся связт идет средствами iptables
едиснвтенное что непонятно зачем тогда совать в netstat чтобы
dockerd типа слушал 80 порт. если это вообще ненужно.


что еще интересно заметить. поскольку для случая когда локальный процесс лезет в интернет 
в цепи обработки этого пакета нет цепочки nat(PREROUTING) которая позволяет менять dest ip
у пакета то эту роль выполняет цепочка nat(OUTPUT)
поэтому полезно запомнить что
nat(preoruting) меняет dest io
nat(output) тоже меняет dest ip
а
nat(postrouting) меняет src ip



*еще надо в целом проанализировать все правила iptables

вначале посмотрим на таблицу когда докер запущен  с опцией user-proxy

*filter
:INPUT ACCEPT [10940:1075816]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [3792:774535]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Thu Apr 30 22:45:35 2020
# Generated by iptables-save v1.6.0 on Thu Apr 30 22:45:35 2020

*nat
:PREROUTING ACCEPT [1672:305719]
:INPUT ACCEPT [1672:305719]
:OUTPUT ACCEPT [45:3375]
:POSTROUTING ACCEPT [44:3291]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER (пакеты подпадающие под это правило - пакет c хоста
летит  на адрес интерфейса хоста за исключением лупбэка)
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -i docker0 -j RETURN
-A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT

 для пакетов прилетевших с чужого компа сети на порт 80 :
dst ip подменяется на ip контейнера (172.17.0.2) 
и оно долетает до контейнера

 для пакетов исходящих от локальных процессов :
если мы стучимся на 127.0.0.1 то iptables ничего неделает. и пакет улетает на 127.0.0.1:80
просто его там перехватывает docker-proxy и только поэтому он долетает до контейнера
 если мы стучимся на остальные LOCAL адреса
например на 172.16.101.32(ens160)
происходит подмена dest ip на 172.17.0.2, src ip остается = 172.16.101.32
получается непонятно как такое пакет прилетает обратно из контейнера 
в хост. я зашел в неймспейс контейнера и посмотрел доступные ему маршруты

# docker inspect mynginx1 | grep -i pid
            "Pid": 25499,

# nsenter --net=/proc/25499/ns/net ip -c route show
default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0  proto kernel  scope link  src 172.17.0.2

получается у контейнеров дефеолтовый гейтвей это docker0
поэтому конечно пакет из контейнера долетает до docker0
попадает на хост с его хостовой таблицей маршрутизации большой
и конечно тогда уже понятно куда посылать после docker0 пакет.
то есть вся фишка в том что докер прописывает внутри контейнера
дефолтоый маршрут через docker0.



\

также хочу еще раз заметить 
как странно работает nsenter

если указываю --net то показывает один результат
если указыываю -t то показыает совершенно другой результат. что за бред

# nsenter -t 25499 ip -c route show
default via 172.16.102.1 dev ens160 onlink
172.16.102.0/24 dev ens160  proto kernel  scope link  src 172.16.102.34
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1
172.18.0.0/16 dev br-9a31314af513  proto kernel  scope link  src 172.18.0.1 linkdown

# nsenter --net=/proc/25499/ns/net ip -c route show
default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0  proto kernel  scope link  src 172.17.0.2

но прикол в том что правильно именно через --net
потому что если зайти в контейнер

# docker exec -it mynginx1 /bin/bash
и посмотреть руты из нутри то покажет 

~# nsenter --net=/proc/25499/ns/net ip -c route show
default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0  proto kernel  scope link  src 172.17.0.2


поэтому как бутто ключ -t можно забыть. он гавно. он показывает вранье

\

 если мы стучимся непосредственно на адрес контейнера (172.17.0.2) :
то src ip = 172.17.0.1 и как бы происходит прямая передача в рамках
броадкаст домена.

 таким образом для локальных процессов связь с контейнером работает так:
если мы стучимся на ip контейнера 172.17.0.2 то связь идет без проблем
поскольку хост имеет ip (docker0)= 172.17.0.1
если мы стучимся на 127.0.0.1:80 то этот пакет приходит на сокет докер прокси
и он его как то там переправляет для нас прозрачно в контейнер. если докер
прокси убить то эта связь перестает работать
если мы стучимся на любой другой локальный для хоста ip адрес то 
iptables подменяет dest ip на 172.17.0.2 а src ip остается прежний
но с этим нет проблем потому что внутрти контейнера прописан docker0
как дефолт гейтвей. поэтому контейнер может спокойно пинговать все 
сети которые есть в таблице маршрутизации самого хоста. 

таким образом докер прокси нужен только для одного случая.
когда изнутри хоста мы обращаемся на 127.0.0.1 а хотим 
попасть в контейнер.

5\3\2020 = закончил тут.!!!!!!!!!!!!!!!!

теперь посмотрим на таблицу когда докер запущен без опции user-proxy

# iptables-save
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*filter
:INPUT ACCEPT [279:20641]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [231:29908]
:DOCKER - [0:0]
:DOCKER-ISOLATION-STAGE-1 - [0:0]
:DOCKER-ISOLATION-STAGE-2 - [0:0]
:DOCKER-USER - [0:0]
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
COMMIT
# Completed on Sat May  2 21:44:14 2020
# Generated by iptables-save v1.6.0 on Sat May  2 21:44:14 2020
*nat
:PREROUTING ACCEPT [1:229]
:INPUT ACCEPT [1:229]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER - [0:0]
-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
-A OUTPUT -m addrtype --dst-type LOCAL -j DOCKER
-A POSTROUTING -o docker0 -m addrtype --src-type LOCAL -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE
-A DOCKER -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.2:80
COMMIT


* почитать хотя бы кратко про устройство сетей в докер

докер на хосте для поднимает интерфейс docker0 типа бридж L3. потом создает пары veth
первый из пары оставляет в неймспейсе сетевом хоста. и  вставляет его в состав бриджа.
второй veth из пары он сует в неймспейс контеенера. и veth который в контейнере он дает IP из той же
сети что и docker0

пример

docker0 = 172.17.0.1\16

в контейнере

eth0(тип veth) = 172.17.0.2

таким образом все ip контейнеров помещаются в сеть 172.17.0.0\16

созданный при установке докера бридж docker0 назывыается дефолтовая бридж сеть докера.

фишка в том что можно создавать дополниельный бридж сети . они называются user bridge networks
и при создании контейнера указывать что его нужно поместить в бридж сеть свою.

посмотреть список "сетей" докера

~# docker network list
NETWORK ID          NAME                DRIVER              SCOPE
f35a33c40fd7        bridge              bridge              local
e1518aab5f14        host                host                local
857eee255516        none                null                local
9a31314af513        vasya-net           bridge              local


name=bridge - дефолтовая бридж сеть докера
name= vasya-net - это бридж сеть которую создал я.


создам еще одну bridge2-net сеть

~# docker network create bridge2-net
c92fe5983a5ed768a7ff86d82ca9c32c596ab778333946221e2ad60c8a0951f2

root@test-kub-04:~# docker network list
NETWORK ID          NAME                DRIVER              SCOPE
f35a33c40fd7        bridge              bridge              local
c92fe5983a5e        bridge2-net         bridge              local
e1518aab5f14        host                host                local
857eee255516        none                null                local
9a31314af513        vasya-net           bridge              local

при создании бридж сети созждается на компе бридж интерфейс L3

root@test-kub-04:~# ip -c link show type bridge
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:8e:1e:18:de brd ff:ff:ff:ff:ff:ff
14: br-9a31314af513: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:be:63:e7:31 brd ff:ff:ff:ff:ff:ff
25: br-c92fe5983a5e: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:54:91:f8:e2 brd ff:ff:ff:ff:ff:ff


посмотрим их адреса

# ip -c -f inet address  show

3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0

14: br-9a31314af513: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-9a31314af513

25: br-c92fe5983a5e: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    inet 172.19.0.1/16 brd 172.19.255.255 scope global br-c92fe5983a5e


посморим на тоже самое средствами самого докера

~# docker inspect bridge2-net | grep -i subnet
                    "Subnet": "172.19.0.0/16",


теперь я создам новый контейнер и укажу чтобы его вставили в сеть bridge2-net

# docker run --net=bridge2-net -it ubuntu /bin/bash

посмотрим какой ip получился у этого контейнера изнутри

/# ip -c -f inet address show
28: eth0@if29: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link-netnsid 0
    inet 172.19.0.2/16 brd 172.19.255.255 scope global eth0

итак мы видим что контейнер имеет ip из сети bridge2-net

так вот в доке от докера написано что если контейнер входит в дефолтовый бридж 
то контейнеры могут общаться друг с другом только через ip 
а вот если контейнеры входят в состав юзерских бриджей то контейнеры автоматом
могу общаться друг с другом и через dns имена через имена короче.
как я понимаю это происходит через то что докер автоматом делать автоматом 
записи в /etc/hosts в контейнерах друг про друга. 
щас проверим

итак имеем контейнер 

# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
35729aeb0a08        ubuntu              "/bin/bash"              16 minutes ago      Up 3 minutes                             silly_driscoll

он имеет адрес

172.19.0.2

и имеет hosts

~# nsenter --mount=/proc/30420/ns/mnt cat /etc/hosts
127.0.0.1       localhost
172.19.0.2      35729aeb0a08


(удобно все смотрет внутри контейнера используя знание про неймспейсы)


создадим еще один контейнер в юзер сети bridge2-net

# docker run --name=b2-c1 --net=bridge2-net -it ubuntu /bin/bash
# docker start b2-c1

# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
5a3fdbce4d7b        ubuntu              "/bin/bash"              20 seconds ago      Up 2 seconds                             b2-c1
35729aeb0a08        ubuntu              "/bin/bash"              21 minutes ago      Up 8 minutes                             silly_driscoll


контейнер b2-c1
ip = 172.19.0.3


итак 

контейнер b2-c1
ip = 172.19.0.3

конейтер silly_driscoll
ip = 172.19.0.2

посмотрим их hosts
появился ли автоматом в их hosts инфо друг про друга

~# nsenter --mount=/proc/30420/ns/mnt cat /etc/hosts
127.0.0.1       localhost
172.19.0.2      35729aeb0a08

# nsenter --net=/proc/30420/ns/net ip -c -f inet address show scope global
    inet 172.19.0.2/16 brd 172.19.255.255 scope global eth0






# nsenter --mount=/proc/30913/ns/mnt cat /etc/hosts
127.0.0.1       localhost
172.19.0.3      5a3fdbce4d7b

# nsenter --net=/proc/30913/ns/net ip -c -f inet address show scope global
34: eth0@if35: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link-netnsid 0
    inet 172.19.0.3/16 brd 172.19.255.255 scope global eth0


значит из того что Я вижу. то что пишет дока от докера это полная ложь.
я создал два контейнера в юзер бридж сети. но в их hosts прописаны только  собственные ip.
поэтому это ложь что контейнеры могут  общаться пинговать друг друга по именам контейнера (dns имена)
это ложь. они в итоге могут пинговать друг друга только по ip
также как и контейнеры сидящие в дефолтовой бриджевой сети.


что я открыл 

# nsenter --net=/var/run/docker/netns/
19ddc25dea2d  76e5fdc15c50  8bcb48d07d0b

root@test-kub-04:~# docker ps
~# docker ps | awk '{print $1}'
CONTAINER
5a3fdbce4d7b
35729aeb0a08
6c52734f2d22


 то есть что в папке =/var/run/docker/netns/
 создаются файлы совершенно несовпадающие с container-id
 
это плохо.
 

вот как найти связь между container-id 
# docker ps --format "table {{.ID}}\t{{.Names}}"
CONTAINER ID        NAMES
5a3fdbce4d7b        b2-c1
35729aeb0a08        silly_driscoll
6c52734f2d22        mynginx1



и файлом описывающим сетевой неймспейс
# ls -1 /var/run/docker/netns/
19ddc25dea2d
76e5fdc15c50
8bcb48d07d0b




этого контейнера

# docker inspect 5a3fdbce4d7b  | grep -E "19ddc25dea2d|76e5fdc15c50|8bcb48d07d0b"
            "SandboxKey": "/var/run/docker/netns/76e5fdc15c50", 

это очень плохо что имя контейнера и его файл докеровский который указыает на контейнерный
сетевой неймспейс несовпадают.



короче я щас войду в контейнер и убедюсь что он неможет пинговать своего соседа по имени.


контейнер b2-c1
id  5a3fdbce4d7b
pid 30913
ip = 172.19.0.3

hosts
127.0.0.1       localhost
172.19.0.3      5a3fdbce4d7b


конейтер silly_driscoll
id 35729aeb0a08
pid 30420
ip = 172.19.0.2

hosts
127.0.0.1       localhost
172.19.0.2      35729aeb0a08



итак я вошел в контейнер
# docker exec -it b2-c1 /bin/bash


# hostname
5a3fdbce4d7b

# ip address show
    inet 172.19.0.3/16 brd 172.19.255.255 scope global eth0


и пробую пингануть своего соседа по имени


# ping 35729aeb0a08
PING 35729aeb0a08 (172.19.0.2) 56(84) bytes of data.
64 bytes from 35729aeb0a08.bridge2-net (172.19.0.2): icmp_seq=1 ttl=64 time=0.080 ms
^C
--- 35729aeb0a08 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.080/0.080/0.080/0.000 ms

работает! значит дока говорит правду!

пингуется он по container-id

но что странно как это работает?
ведь в /etc/hosts ничего нет про 172.19.0.2


# cat /etc/hosts
127.0.0.1       localhost
172.19.0.3      5a3fdbce4d7b

а вот по имени непингуется



# ping silly_driscoll
ping: silly_driscoll: Temporary failure in name resolution

такое ощущение что у современного докера появился DNS сервис
и резолвинг имен идет теперь не через hosts а через докеровский DNS

далее написано что если контейнер входит в юзеровскую сеть 
то контейнер можно налету из это сети убирать и подключать 
к другой юзерской сети. это круто щас проверим.
 а вот если контейнер входит в дефолтовую докервскую сеть 
 то налету неполучится.
 
 
 берем контейнер
 
 контейнер b2-c1
id  5a3fdbce4d7b
pid 30913
ip = 172.19.0.3

и отключаем его от сети

~# docker network disconnect bridge2-net b2-c1

посмотрим что там с сетью внутри контейнера сейчас

/# ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

ожидаемо. докер убрал eth(тип veth) из неймспейса контейнера


подключаем контейнер b2-c1 к сети vasya-net ("172.18.0.0/16")

# docker network connect vasya-net b2-c1

смотрим что внутри контейнера с сетью стало


~# docker inspect b2-c1 | grep -i ipaddress
            "SecondaryIPAddresses": null,
            "IPAddress": "",
                    "IPAddress": "172.18.0.2",
					
					
					
					
итого 

было

 
 контейнер b2-c1
id  5a3fdbce4d7b
pid 30913
ip = 172.19.0.3

стало

 контейнер b2-c1
id  5a3fdbce4d7b
pid 30913
ip = 172.18.0.2


посмотрим можно ли пинговать по IP контейнер с одной сети в другой сети

будем пинговать с 172.18.0.2 конейтер с 172.19.0.2 


/# ping 172.19.0.2
PING 172.19.0.2 (172.19.0.2) 56(84) bytes of data.
^C
--- 172.19.0.2 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2000ms

пинги не проходят.
значит между сетями  типа бридж докер неразрешает иметь связь.


еще раз укажу вот на эту делать. 
During a container’s lifetime, you can connect or disconnect it
 from user-defined networks on the fly. To remove a container from 
 the default bridge network, you need to stop the container and recreate 
 it with different network options.   


тут я вспоминаю про кубернетес. докер сети есть нетоько в докере.
они есть и в докере который работает в кубернетесе

вот список сетей на мастер -ноде

~# docker network list
NETWORK ID          NAME                DRIVER              SCOPE
be292b1499a3        bridge              bridge              local
1f0be08a419d        host                host                local
cb0b55d82ec5        none                null                local


на дата ноде к8 тоже есть докер сети

~# docker network list
NETWORK ID          NAME                DRIVER              SCOPE
df38a3a0ec7b        bridge              bridge              local
4a496e875af8        host                host                local
293243971b1a        none                null                local

поэтому изучуение как устроены сети в докере имеет прямую связь с  докером в куберентесе


docker network create  создает именно бридж сеть. у докера есть и другие 
типы сетей. но пока мы говорим о бридж сетях

 я проверил и прочитал про то что при создании контейнера нельзя
 так сделать чтобы у него было две и более сетевые карточки сразу которые принадлежат
 двум бриж сетям.
 
 то есть вот такая команда непрокатит
 
 # docker network create --name=box4 --network-bridge2-net --network=vasya-net  -it ubuntu /bin/bash

создать контейнер можно только с одной сетью.

# docker create -it --name="box6" --network=bridg2-net ubuntu /bin/bash

а уже далее мы можем добавить еще одну сеть к этому контейнеру

# docker network connect vasya-net box6

теперь контейнер подключен сразу к двум сетям
bridge2-net
vasya-net

на уровне ядра получается вот что.
в нашем сетевом неймспейсе лежит два бридж интерфейса
docker0
docker1

в контейнере лежит два veth интерфейса в сетевом неймспейсе контейнера.
а их хвосты лежат в неймпейсе хоста , там же где docker0 и docker1


кстати щас такая мода пошла что если есть команда и опция например docker create
то мануал именно для этой комбинации выглядит как man docker-create. ну хоть удобно стало

очень быстро возникает вопрос. 
как посмотреть какие контейнера лежат в той или этой сети докера.

ответ

~# docker network inspect bridge2-net | grep -i name
        "Name": "bridge2-net",
                "Name": "silly_driscoll",
                "Name": "box3",


в сети bridge2-net 
сидят два контейнера
 "Name": "silly_driscoll",
 "Name": "box3"
				

очень удобно

щас мы проверим
если на хосте поставить
net.ipv4.ip_forward = 0

то смогут ли контейнеры общаться друг с другом
могут ли они пинговать сеть хоста
могут ли они выходить за рамки хоста


# docker inspect silly_driscoll | grep -i address
          
                    "IPAddress": "172.19.0.2",
                    

# docker inspect box3 | grep -i address
            
                    "IPAddress": "172.19.0.3",
                   
				   
				   
				   



чтобы контейнеры могли общаться с 


итак ответы на вопросы
при net.ipv4.ip_forward = 0



контейнеры общаться друг с другом = да
могут ли они пинговать сеть хоста = да
могут ли они выходить за рамки хоста = нет




настройки дефолтовой бридж сети докера кастомизируются в daemon.json
если что


немного щас отойдем от кубернетеса и чуть более уйдем в сторону докера.

помимо бридж сетей в докере есть оверлей сети.
они позволяют связываться контейнерам друг с другом между хостами.


чтобы создать оверлей сеть надо  ( https://docs.docker.com/network/overlay/ )

# docker swarm init
# docker network create -d overlay --attachable overlay-net-1

существенно то что в докере сеть оверлейную нельзя создать без сворма.
то есть вначале хост должен стать мастером сворма или присоединиться к другому мастеру 
сворма. и только после этого можно создать оверлейную сеть.


как только мы создали сворм либо вошли в состав другого сворма то на хосте
возникнте две сети

ingress ( driver overlay)
docker_gwbridge ( driver bridge )

как я понял docker_gwbridge используется самим докер сервисом чтобы связываться через 
эту сеть с другими докер сервисами расположенными на других хостах

через ingress идет цитирую control and data trafic related with swarm services.
это пока неясно что значит.

вобщем эти две сети создаюбтся автоматом.

а мы при этом можем уже и свою оверлей сеть создать.

# docker network create -d overlay --attachable overlay-net-1

как я понял когда мы создали сворм то после этого у нас есть два класса 
сущностей - есть  сворм сервисы а есть standalone контейнеры.


флаг --attacahble дает то что оверлейную сеть overlay-net-1
смогую юзать нетолько оверлейные сервисы но и стэндэлоун 
контейнеры

если посмотреть в свойства ingress сети 
то увидим какой ip оно имеет

:~# docker network inspect ingress
[
    {
        "Name": "ingress",
                    "Subnet": "10.0.0.0/24",
                    "Gateway": "10.0.0.1"
                }
            ]



сразу хочу сказать что документация от докеры в 1 000 000 раз лучше написана
чем к кубернетесу.

и управляемость докер структурой в 1000 000 раз проще чем кубернетесом.
ладно проехали

значит я убрал свои сети из докера и мы иммеем вот такой конфиг

~# docker network list
NETWORK ID          NAME                DRIVER              SCOPE
f35a33c40fd7        bridge              bridge              local
727eeb050197        docker_gwbridge     bridge              local
e1518aab5f14        host                host                local
dkfgngbom561        ingress             overlay             swarm
857eee255516        none                null                local


посмотрим плотно параметры этих сетей



~# docker netowrk inspect bridge
docker: 'netowrk' is not a docker command.
See 'docker --help'
root@test-kub-04:~# docker network inspect bridge
   
        "Driver": "bridge",
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
            "com.docker.network.bridge.name": "docker0",
     
видим какая ip сеть скрывается за этой сетью
и какая сетевая карта была создана на хосте под эту сеть.


про сеть под именем host ничего интересного нет в inspect
дока пишет что контейнер можно подключить к сети хоста.
при этом контейнер будет сидеть в сетевом неймспейсе хоста.
получается как самый обычный процесс на хосте
как это было раньше по старринке.
при этом контейнер небудет иметь своего личного ip
ну это и понятно потому что процесс сидя в неймсппейсе хоста
работает со всеми сетевыми карточками которые доступны хосту.

осталось на практике понять как рабооать с конейнером в сети хоста.
и что это дает.

в доке от докера они так и пишут 
что контейнер по всем другим неймспейсам имеет их свои.
в этом смысле он изолирован от системы
но с точки зрения сетевого неймспейса он лежит в том же что и хост
(имеется ввиду неймпейс что и у $PID = 1)

кстати теперь мне непонятно что же такое runc и зачем он нужен.
если у нас есть неймспейсы + cgroup что еще надо 
чтобы иметь контейнер. вроде и так все есть.

чтобы контейнер при создании поппал в сеть host надо указать ключ

 --network host 
 
пример

# docker run --rm -d --network host --name my_nginx nginx


при этом порты ключом -p ненужно экспоузить.


в итоге имеем контейнер у которого в доступе все сетевые интерейсы хоста.
с другой стороны  его процессы и фс итп полностью изолированны от системы.

чтото типа виртуалки которая при этом имеет доступ ко всем интерфейсам хоста.

как я понял такой контейнер скорее диковинка и практическое его использование
пока непонятно. ну типа пишут что если нам надо заэкспоузить очень 
много портов то есть смысл поместить контейнер в сеть host а не в bridge.


вот что показывает нам netstat про такой контейнер

# netstat -tnlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      7312/nginx: master


7312 это pid нашего контейнера.

потому что

~# docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                NAMES
6b34bad007d1        nginx               "nginx -g 'daemon of…"   11 minutes ago      Up 11 minutes                            my_nginx

~# docker inspect my_nginx | grep -i pid
            "Pid": 7312,

посмотрим малек подробнее про 7312
 PPID   PID
    1  1131  1131  1131 ?           -1 Ssl      0   1:20 /usr/local/bin/containerd
 1131  7294  7294  1131 ?           -1 Sl       0   0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby
 7294  7312  7312  7312 ?           -1 Ss       0   0:00      \_ nginx: master process nginx -g daemon off;
 7312  7324  7312  7312 ?           -1 S      101   0:00          \_ nginx: worker process

PID - это pid процесса
PPID - это pid родительского процесса

а вот как изнутри контейнера  ps выглядит


(контейнер)# ps axjf
 PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
    0   492   492   492 pts/0      499 Ss       0   0:00 /bin/bash
  492   499   499   492 pts/0      499 R+       0   0:00  \_ ps axjf
    0     1     1     1 ?           -1 Ss       0   0:00 nginx: master process nginx -g daemon off;
    1     6     1     1 ?           -1 S      101   0:00 nginx: worker process


вопросы что за shim  я забыл 
также вопрос можно ли  с хоста видеть процессы внутри контейнера (они же вроде в другом pid неймспейсе лежат)
почему отличаются номера pid
как технически мы оказываемся внутри контейнера в команде EXEC

для этих целей 
я начал изучать вывод ps, top и все про процессы в линуксе.


в выводе ps есть столбик S
который сообщает состояние процесса. 

R - означает что процесс исполняется(RUNNING)  на ЦПУ или что он готов к исполнению но
линукс шедулер из за того что цпу занят пока что неможет его поместить на цпу и он стоит в очереди
на исполнение (RUNNABLE). тут про очереди. насколько я понял в линуксе.
интересно то что  если состояние процесса R то мы неможем понять этот процесс реально исполняется
на цпу или стоит в очереди на исполнение.


S - interruptible sleep.   в первом приближении это означает что процесс на данный момент невыполняется
на цпу и нестоит в очереди на исполнение. окей на цпу его нет и попасть на него он нежаждет.
в более глубокой расшифровке выясняется что данный процесс вызвал системный вызов system call
и ждет от него возврата ответа. узнать какой системный коллл он вызвал можно через ps колонка WCHAN

сразу надо добавить что load average учитывает нетолько R процессы за период времени но и D процессы
то есть те которые ждут ответа от дисковой системы.

кстати получается такой прикол.
cpu usage - это характеристика процессор.
а load average - это вообще нехарактеристика процессора это характеристика совершенно
другой сущности- системы. точнее группы процессов.


cpu usage это сколько % времени за промежуток процессор чтото исполнял а не отдыхал.
в мире дисков это аналог %(busy time) и это характеристика оперирует временем.
итак cpu usage это характеристика процессора+времени

а load average это характеристика процессов в штуках + время

это характеристики описывающие разные сущности абсолютно.

одно описывает цвет яблок а второе температуру в больнице.

но так случилось что между ними ( между процессором и процессами ) есть некая связь.

за 1 минуту одноядерный процессор мог работать 85% времени.
и его cpu usage = 85%

за эту 1 минуту на этом ядре могло отработать 125 процессов (они быстро возникали и быстро отрабатывали)
поэтому load average = 125

за эту же 1 минуту мог отработать всего два процесса. один занял 25% времени второй 60% времени.

я очень нелюблю load average потому что его постоянно приплетают к характеристике
загруженности цпу. а это характеристика не цпу а характеристика процессов в ОС.
а всегда хотят узнать именно загрузку процессора упорно ища ее в load average

мы ищем температуру в больнице по яркости солнца.

у меня вопрос - как так получатеся что если мы смотрим в top
то мы видим что состояние процесса = S
а его %cpu_usage неравно нулю а даже скажем 60%.
ведь S по определению означает что процесс его код ненаходяится в процессоре неисполняется 
на нем.

вот пример

# top

PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4859 root      20   0   50.1g  49.4g 144356 S  82.7 82.4  20:28.62 java
 

наверное ответ вот какой.

по первых вывод команды top относится к промежутку времени 
с начальным временем и конечным

12ч 15м 3с - старт
12ч 15м 13с - финиш

так вот top показывает состояние процесса в конечный момент времени 12ч 15м 13с.
и оно типа = S 
но это абсолютно незначит что весь промежуток времени 12ч 15м 3с - 12ч 15м 13с
этот процесс находился в том состоянии S
в этом и была загвоздка и ошибка.

за 10 секунд процесс мог миллиард раз переходитть из состояния R в состояние S
а top показывает его состояние только в конкретный момент времени. и только то!

поэтому важно понять что состояние это характеристика конкретного момента времени.

а вот %cpu_usage это характеристика неконкретного момента времени
а промежутка времени! промежутка!

почуствуй разницу момент и промежуток. точка и отрезок.

так вот за промежуток = 10 секунд (с12ч:5м:3с по 12ч:15м:13с)
цпу был занят 85% этого промежутка.

вот в чем секрет как так получаетс что процесс имеет статус S который четко 
говорит что процесс неисполняется на цпу и тем что cpu_usage <> 0 !

мы пытались совместить характеристики от разных природ.
характеристика одномоментная и характеристика промежутка.

типа как сейчас вася спит ( это состояние S ).
но до этого он 82% времени работал в течение часа.

получается что когда мы смотрим на top и мы хотим понять 
какие процессы жрут процессорное время а какие его нежрут 
то нам надо игнорировать столбец со стейтом процессов ( столбец где S )
потому что мы хотим узнать характеристку за промежуток.
мы хотим узнать кто грузил процессор последние там 10 секунд.
и тут нам в помощник cpu_usage.

мы нехотим знать какой процесс прям щас спит. нам это абсолютно похер.

это как то что мы хотим узнать кто тырил помидоры последний час с полки.
и нам похер что тот кто их тырил прямо щас их нетырит.



 


~# ps -l 30913
F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY        TIME CMD
4 S     0 30913 30896  0  80   0 -  1028 poll_s pts/0      0:00 /bin/bash


wchan=poll_s
это сокращенное имя system call 
ответ от которого ждет pid=30913

полное имя систменного колла можно посмотреть вот так:


# cat /proc/30913/stack
[<ffffffff81223304>] poll_schedule_timeout+0x44/0x70

конкретно этот system call работает с вводом выводом. 
типа когда нажал кнопку на клавиатуре то системный вызов возвращает данное событие процессу 30913
а пока кнопка ненажата то процесс 30913 находится в неисполняемом состоянии. за него 
как я понял на цпу крутится системный колл. ну или вызывается регулярно. 
процесс невыполняется это значит что его код не исполняется на процессоре.

про системные вызовы. пользовательская программа имеет очень мало прав доступа к железным ресурсам
компа. ими ведает код под названием ядро то есть доступ программы к ресурсам идет через обращение к 
коду ядра. программа говорит запиши на диск. это просьюа пердается ядру. а уже ядро неосредственно
пишет на диск. так вот системный вызов это тоже код ядра. и это посредник общения между 
программой и ядром. типа API для программы при обращении к ядру.

так R S обсудили.

но в выводе ps есть вот такие статусы - Ss S> итд

будем их обсуждать.

во первых Ss = S + s
где S - это наш обсужденный S

а s - это session leader.

поговорим о session leader.

оказывается ( man 7 credentials )

что процесс помимо PID и PPID имеет также PGID SID

PGID это PID group лидера. то есть пиды они еще входят в состав групп. 

SID - это pid лидера сессий. то есть процесс также входит в сессию.


там определенные правила когда процесс порождает ребенка ( fork или еще как там)
то какие то штуки наследуются а какие то можно изменять в ребенка.

процеессы из одной группы всегда входят и в одну сессию. 
а сессия может иметь несколько групп.

дальнейшая более глубокая и практическая фишка этого дела пока останется за кадром.

главное что мы в целом поняли теперь что такое session leader и понимаем теперь 
что такое

Ss статус.


важно еще сказать что команда ps она имеет ключи
которые показывают информацию про процессы в стиле

1) UNIX
2) BSD
3) GNU

что стоит за этими понтовыми словами мало понятно.
но на выхлопе смысл такой что при одних ключах например  статусы процессов будет выглядеть как

S, R итп - простенькие

а с другими ключами статусы будут понтовыми типа 
Ss
S>

вот.

итак мы поняли что каждый сложный статус типа

S> = S + <


рассмотрим простые статусы

 D    uninterruptible sleep (usually IO)
               R    running or runnable (on run queue)
               S    interruptible sleep (waiting for an event to complete)
               T    stopped by job control signal
               t    stopped by debugger during the tracing
               W    paging (not valid since the 2.6.xx kernel)
               X    dead (should never be seen)
               Z    defunct ("zombie") process, terminated but not reaped by
                    its parent


R, S обсудили.
Z - это статус процесса ребенка который завершил свою работу
и посла об этом сигнал родителю а тот ему подтвержлоение не шлет.


посмотрим вот эти более интересные статусы

				<    high-priority (not nice to other users)
               N    low-priority (nice to other users)
               L    has pages locked into memory (for real-time and custom IO)
               s    is a session leader
               l    is multi-threaded (using CLONE_THREAD, like NPTL pthreads
                    do)
               +    is in the foreground process group




рассмотрим 

				<    high-priority (not nice to other users)

что значит высокие приоритет.
посмотрим чему он равен

~# ps lax
F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
4     0     1     0  20   0  37844  5884 ep_pol Ss   ?          0:08 /sbin/init
1     0     2     0  20   0      0     0 kthrea S    ?          0:00 [kthreadd]
1     0     3     2  20   0      0     0 smpboo S    ?          0:00 [ksoftirqd/0]
1     0     5     2   0 -20      0     0 worker S<   ?          0:00 [kworker/0:0H]
1     0    13     2  20   0      0     0 smpboo S    ?          0:00 [ksoftirqd/1]
1     0    15     2   0 -20      0     0 worker S<   ?          0:00 [kworker/1:0H]
5     0    16     2  20   0      0     0 devtmp S    ?          0:00 [kdevtmpfs]
1     0    17     2   0 -20      0     0 rescue S<   ?          0:00 [netns]
1     0    18     2   0 -20      0     0 rescue S<   ?          0:00 [perf]
1     0    19     2  20   0      0     0 watchd S    ?          0:00 [khungtaskd]
1     0    20     2   0 -20      0     0 rescue S<   ?          0:00 [writeback]
1     0    21     2  25   5      0     0 ksm_sc SN   ?          0:00 [ksmd]
1     0    22     2  39  19      0     0 khugep SN   ?          0:02 [khugepaged]

мы видим что  

F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
1     0     5     2   0 -20      0     0 worker S<   ?          0:00 [kworker/0:0H]

мы видим что 

S< = PRI 0

то есть если у процесса приоритет = 0  то есть высший то ему покаыывают < статус.

кстати сразу видно и про этот статус

N    low-priority (nice to other users)

что N дают процессу который имеет PRI >= 25


далее посмотрим про следущий параметр

L    has pages locked into memory (for real-time and custom IO)

мне стало интересно какие процесы в линуксе имеют страницы залоченные в памяти.


# ps lax | grep -E "WCHAN|L"
F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
5     0  1134     1  10 -10   5724  3516 poll_s S<Ls ?          1:05 /sbin/iscsid

это iscsid

статус S<Ls = S + < + L + s

расшифроваываем

S - что процесс спит невыполняется на цпу.
<- что у него высший приоритет
L - что у него страницы в ОЗУ залочены некоторые
s - что он сессия лидер


смотрим следующий параметр

l    is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)
					
					
					
этот флаг показвыает то что данный процесс имеет несколько тредов.
что такое тред.
это почти что полноценный процесс. треды еще зовут lightwight processes 
					
					
посомтрим какие процессы мультитрредовые 

~# ps lax | grep -E "WCHAN|Ssl|Sl"
F   UID   PID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
4   100   550     1  20   0 100324  2624 ep_pol Ssl  ?          0:01 /lib/systemd/systemd-timesyncd
4     0   739     1  20   0 141996 18660 futex_ Ssl  ?          0:01 /usr/lib/snapd/snapd
4     0   758     1  20   0 275856  6264 poll_s Ssl  ?          0:09 /usr/lib/accountsservice/accounts-daemon
4   104   770     1  20   0 260632  5240 poll_s Ssl  ?          0:01 /usr/sbin/rsyslogd -n
4     0   885     1  20   0 277180  6028 poll_s Ssl  ?          0:00 /usr/lib/policykit-1/polkitd --no-debug
4     0  1131     1  20   0 687492 56744 futex_ Ssl  ?          1:44 /usr/local/bin/containerd
0     0  3209  1131  20   0 107700 12320 futex_ Sl   ?          0:04 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/9989cc33dd9becb27e7702d1e986310b785ce4b09ca01128c35c4c1145768ed1 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc


посмотрим для интереса а эластик он мультитредовый ?

4   111  1177     1  20   0 18392 44472 futex_ Ssl ?    412759:40 /usr/bin/java -Xms20g -Xmx20g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFract
0   111  1366  1177  20   0 135688  2260 pipe_w Sl   ?          0:00  \_ /usr/share/elasticsearch/plugins/x-pack/platform/linux-x86_64/bin/controller

видно что да


тред это почти что процесс но не процесс
группа  тредов имеют один процесс и они делят совместно общие ресурсы.
но фишка треда в том что он может выполняться на своем отдельном цпу.

получется это как бы группа процессов но у процессов у них своя память по крайней мере.
как я понял. и свой pid это точно.
а у тредов память общая. но каждый тред может выполняться на своем цпу.

вот что обозначает флаг l

команда ps может показывать нетолько процессы но и их треды

root@test-kub-04:~# ps l  -T -p 30896
F   UID   PID  SPID  PPID PRI  NI    VSZ   RSS WCHAN  STAT TTY        TIME COMMAND
0     0 30896 30896  1131  20   0 107700 11948 futex_ Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30897  1131  20   0 107700 11948 futex_ Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30898  1131  20   0 107700 11948 futex_ Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30899  1131  20   0 107700 11948 futex_ Sl   ?          0:01 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30900  1131  20   0 107700 11948 futex_ Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30901  1131  20   0 107700 11948 futex_ Sl   ?          0:01 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
5     0 30896 30902  1131  20   0 107700 11948 ep_pol Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30903  1131  20   0 107700 11948 ep_pol Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30904  1131  20   0 107700 11948 futex_ Sl   ?          0:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30959  1131  20   0 107700 11948 futex_ Sl   ?          0:01 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime
1     0 30896 30975  1131  20   0 107700 11948 futex_ Sl   ?          0:02 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime

SPID - это номер треда я так понимаю
но все треды как видно имеют единый PID


можно всегда подсмотрет сколько тредов у того или иного процесса через /proc

~# ls -1 /proc/30896/task
30896
30897
30898
30899
30900
30901
30902
30903
30904
30959
30975

если в task много - значит процесс мультитредовый.



следущий ключ


+    is in the foreground process group



посмотрим пример

~# ps ax o pid,ppid,pgid,sid,uid,stat,command | grep -E "29879|SID|29878|29825|1112" | tee

  PID  PPID  PGID   SID   UID STAT COMMAND
 1112     1  1112  1112     0 Ss   /usr/sbin/sshd -D
29825  1112 29825 29825     0 Ss   sshd: mkadm [priv]
29878 29825 29825 29825  1000 S    sshd: mkadm@pts/0
29879 29878 29879 29879  1000 Ss   -bash
29891 29879 29891 29879     0 S    sudo bash
29892 29891 29892 29879     0 S    bash
32432 29892 32432 29879     0 R+   ps ax o pid,ppid,pgid,sid,uid,stat,command
32433 29892 32432 29879     0 R+   grep --color=auto -E 29879|SID|29878|29825|1112
32434 29892 32432 29879     0 S+   tee



мне стало интересно 
вот мы вводим пароль при входе в линуксе
и зпускается bash
как при этом распределяется pgid и sid
это очень полезно чтобы отловить визуально какой процесс породил какой.



при этом вспыло  /sbin/agetty --noclear tty2 linux
как я понял когда мы жмем Alt+F2
то система автоматом запускает команду
/sbin/agetty --noclear tty2 linux
это приводит  к тому что на экране появляется приглащение на ввод логина пароля.
нажмем Alt+F5
запуститмся
/sbin/agetty --noclear tty5 linux

значит вот я выяснил как выглядит ssh подключение

# ps axf o sid,pid,ppid,pgid,uid,stat,command | grep -E "1279|1311|PID"

  SID   PID  PPID  PGID   UID STAT COMMAND
 1279  1279  1112  1279     0 Ss    \_ sshd: mkadm [priv]
 1279  1310  1279  1279  1000 S         \_ sshd: mkadm@pts/3
 1311  1311  1310  1311  1000 Ss            \_ -bash
 1311  1330  1311  1330     0 S                 \_ sudo bash
 1311  1331  1330  1331     0 S                     \_ bash
 1311  1373  1331  1373     0 R+                        \_ ps axf o sid,pid,ppid,pgid,uid,stat,command
 1311  1374  1331  1373     0 S+                        \_ grep --color=auto -E 1279|1311|PID




когда мы вошли по ssh то все процессы получают единый SID. а вот PGID с ним
уже все нетак однозначно.
поэтому лучше ориентироваться на SID если мы хотим отловить некую полную сессию что 
ктото куда то вошел и чтото назапускал.

также. если мы входим через консоль сервера то линукс юзает tty устройства /dev/tty*
а если через ssh то /dev/pts/*
это еще тоже предстоит выяснять. их разницу.


разбираемся дальше с ps и top

# man 1 top

top - 20:08:00 up 90 days,  6:28,  1 user,  load average: 0.22, 0.17, 0.09
Tasks: 137 total,   1 running, 136 sleeping,   0 stopped,   0 zombie
%Cpu(s):  2.6 us,  1.1 sy,  0.0 ni, 96.2 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 26.0/12304088 [||||||||||||||||||||||||||                                                                          ]
KiB Swap:  0.0/0        [                                                                                                    ]

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
25644 elastic+  20   0  0.098t 8.703g 6.221g S   7.4 74.2   5478:27 java
 1065 root      20   0  207760  27932   4672 S   0.0  0.2  72:00.31 vmtoolsd
 1087 root      20   0  235900   9368   2692 S   0.0  0.1   0:52.74 snapd
 7181 root      20   0   95400   6876   5936 S   0.0  0.1   0:00.01 sshd

полезные кнопки

m - меняет вверху график который показывает сколько физ памяти свободно 
M - упорядочивает строки по столбцу %MEM

RES - это сколько физ памяти занимает процесс. его резидентная часть называется.
видно что java жрет в памяти 8.703g.

%MEM это тот же самый RES только поделен на полный обьем физ памяти.
%MEM = RES / (RAM size) * 100%

так что RES и %MEM это одно и тоже только одно в гигабайтах а второе в процентрах

I - позволяет переключать как показывать %CPU . по умолчанию 100% это полностью загруженное 
одно ядро. по мне дебильно сделано.  а если тыкнуть "I" то 100% будет значить что все ядра 
процессора загружены на 100%. по мне это и есть коретный режим.

PR - приоритет процесса это мы ужезнаем.
NI - это параметр nice чему равен. (по мне PR и NI взаимосвязаныные величиы)

SHR - это размер shared памяти. важно знать что не вся shared память резидентная.
а также что это обьем памяти который потенциально может быть разделен с другими 
процессами. в итоге я считаю что на характеристику можно забить.

кстати можно выбирать какие колонки показывать.
для этого надо нажать f

и увидим выбор


Fields Management for window 1:Def, whose current sort field is %CPU
   Navigate with Up/Dn, Right selects for move then <Enter> or Left commits,
   'd' or <Space> toggles display, 's' sets sort.  Use 'q' or <Esc> to end!

* PID     = Process Id             SUPGIDS = Supp Groups IDs
* USER    = Effective User Name    SUPGRPS = Supp Groups Names
* PR      = Priority               TGID    = Thread Group Id
  NI      = Nice Value             ENVIRON = Environment vars
  VIRT    = Virtual Image (KiB)    vMj     = Major Faults delta
* RES     = Resident Size (KiB)    vMn     = Minor Faults delta
* SHR     = Shared Memory (KiB)  * USED    = Res+Swap Size (KiB)
* S       = Process Status         nsIPC   = IPC namespace Inode
* %CPU    = CPU Usage              nsMNT   = MNT namespace Inode
* %MEM    = Memory Usage (RES)     nsNET   = NET namespace Inode
* TIME+   = CPU Time, hundredths   nsPID   = PID namespace Inode
* COMMAND = Command Name/Line      nsUSER  = USER namespace Inode
  PPID    = Parent Process pid     nsUTS   = UTS namespace Inode
  UID     = Effective User Id
  RUID    = Real User Id
  RUSER   = Real User Name
  SUID    = Saved User Id
  SUSER   = Saved User Name
  GID     = Group Id
  GROUP   = Group Name
  PGRP    = Process Group Id
  TTY     = Controlling Tty
  TPGID   = Tty Process Grp Id
  SID     = Session Id
  nTH     = Number of Threads
  P       = Last Used Cpu (SMP)
  TIME    = CPU Time
  SWAP    = Swapped Size (KiB)
  CODE    = Code Size (KiB)
  DATA    = Data+Stack (KiB)
  nMaj    = Major Page Faults
  nMin    = Minor Page Faults
  nDRT    = Dirty Pages Count
  WCHAN   = Sleeping in Function
  Flags   = Task Flags <sched.h>
  CGROUPS = Control Groups



колонка USED=RES+SWAP

рассмтрим последний непонятный столбец TIME+

KiB Mem : 37075864 total,   331252 free, 22992208 used, 13752404 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 13597808 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                  TIME
 1177 elastic+  20   0 93.928g 0.030t 9.434g S 16.3 87.8 415466:44 java                     6924h
    1 root      20   0   37872   4360   2424 S  0.0  0.0   0:27.46 systemd                  0:27
    2 root      20   0       0      0      0 S  0.0  0.0   0:06.06 kthreadd                 0:06
    3 root      20   0       0      0      0 S  0.0  0.0   0:23.46 ksoftirqd/0              0:23
    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H             0:00


я помимо TIME+ вывел еще столбец TIME

TIME+ отображается в сотых секунда в общем темный лес по факту
415466:44 ознавает 415466 минут :44 секунд

TIME отображает как я понял минимум это секунды. в итоге
6924h - это часы.


важно понять вот что. и TIME и TIME+ они показыавют сколько времени суммарно процесс
обрабатывался на всех ядрах. скажем если он жрал четыре ядра на 100% два часа непрерывно.
то TIME= 8часов.  потому что два часа на четыре процессора. 
я так понял эту математику.
важно понять то что если процесс необрабатывается процессором то этот 
индикатор нерастет. то есть сервер может иметь аптайм пол года.
а TIME у процесса будет 1 минута. потому что он лежал все время в S состоянии
и на процессоре его код необрабатывался.

витге по этому параметру можно понять какие процессы больше всего крутятся процессором.

ну вот вроде мы наконец и разобрали все поля относящиейся к процессам в выводе top

также получается что если система многопрцессорная и некий процесс постоянно 
обрабатвыавается процессорами то его TIME будет намного больше аптайма.


в итоге из top можно посомтреть какие процессы сколько отжирают физ памяти.
какой процесс щас грузить процессор
какие процессы больше всего в целом с момента старта крутятся на процессорах
ну наверно и все.

W - записать настройки top.
очень полезно

E - позволяет преключать в каких единицах показывается оперативка.
в килобатах магабайтах гиагбайах.
очень удобно.

top - 21:15:54 up 108 days,  4:34,  1 user,  load average: 1.69, 1.49, 1.56
Tasks: 169 total,   1 running, 168 sleeping,   0 stopped,   0 zombie
%Cpu(s): 25.5 us,  0.4 sy,  0.0 ni, 73.9 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
GiB Mem : 63.1/35.358   [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||                                     ]
GiB Swap:  0.0/0.000    [                                                                                                    ]

 вот тут видно что память отображатется в Гиагбайтах.
 потому что отобаржать ее в килобайтах это дебилизм.
 
обращаю внимание на
 
 GiB Mem : 63.1/35.358 
 
 35.358 GB - это обьем физ памяти. это понятно.
 63.1 - это сколько занято физической памяти ВПРОЦЕНТАХ !!!!

при этом верна формула Total = free + used + buff/cache
то есть занятая память не включает память под кэш.
то есть занято чисто резидентными кусками тел процессов.

то есть в системе занято 63.1% физ памяти , а всего памяти 35.358GB

MiB Mem : 21.9/2000.254 [||||||||||||||||||||||                                                                              ]
MiB Swap:  0.0/0.000    [                                                                                                    ]


в этом примере в системе занято 21.9% памяти из 2000.254 MB


вот как при этом выглядит расрпеделние памяти в цифрах

MiB Mem : 2000.254 total,  437.559 free,  215.840 used, 1346.855 buff/cache

видно что Total = free + used + buff/cache

тыкая кнопку m 
можно менять визуализацию сколько свободно занято, сколько свободно 
и сколько под кэш использовано.

визуально это выглядит так: слева направо жирная линия - это занято.
потом рядом с ней пимпочка - это свободно.
а черное пространство справа  - это память под кэш

то есть примерно вот так

[--------------------------------------+++                                         ]

минусы это занято
плюсы это свободно
чернота это под кэш
ну а скобки это всего.

если нажимать "e" маленькую то 
RES будет меняться , цифры будут в килобайтах, мегаьайтах, гигабайтах.
тоже удобно.

 
вот так по мне выглядит правильно анстроенный top

top - 21:34:46 up 108 days,  4:53,  1 user,  load average: 1.43, 1.53, 1.48
Tasks: 170 total,   1 running, 169 sleeping,   0 stopped,   0 zombie
%Cpu(s): 27.8 us,  0.4 sy,  0.0 ni, 71.6 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
GiB Mem : 63.2/35.358   [                                                                                                    ]
GiB Swap:  0.0/0.000    [                                                                                                    ]

  PID USER        RES S  %CPU %MEM     TIME+ COMMAND
 1177 elastic+ 0.030t S 196.7 87.1 415547:33 java
  973 root      29.4m S   0.0  0.1  77:12.27 vmtoolsd
  976 root      15.4m S   0.0  0.0   4:09.42 snapd
 1056 root       5.8m S   0.0  0.0   0:34.43 lxcfs
  400 root       4.8m S   0.0  0.0   0:06.76 systemd-journal
31790 root       4.7m S   0.0  0.0   0:00.14 bash
    1 root       4.3m S   0.0  0.0   0:27.46 systemd
31769 mkadm      3.7m S   0.0  0.0   0:00.03 bash
17629 root       3.5m R   0.0  0.0   0:00.09 top
 1194 root       3.5m S   0.0  0.0   8:40.68 iscsid
 1065 syslog     3.2m S   0.0  0.0   0:01.13 rsyslogd


GiB Mem : 63.2/35.358  = отобрадается в гигабайтах
RES отображатеся в мегабайтх. (java просто очень большой поэтмоу его показывает она уже в терабайтах)
нет лишних столбцов


про load average 

load average: 2.19, 2.03, 1.69

2.19 как получили. взяли число процессов которые заходили на процессор (типа как число кошек
посравших  в подьезде) за минуту и поделили на 1. усреднение за минуту

2.03 взяли число процессов побываваших на цпу за 5 минут и поделили на 5. усреднение за 5 минут.

1.69 взяли число процессов побывавших на цпу за 15 минут и усреднили по 15.


в итоге кажое число показывает число процессов выполнявшихся или стоящих в очереди
на выполнение или стоящих в очереди иза за дисковых тормозов (D) в течение минуты.
усредение за 1 минуту более точное. за 15 минут самое неточное.
в любом случае это показатель для минуты. 
вот смешно - по этому показателю пытаются предсказать тормозит ли система.
хуйня полная. потому что как отличить это процессы которые хотели 
на цпу исполнится или которые жжали ответа от дисковой системы.
непонятно.

итоговая транскрипция - Load average это число процессов в течение минуты ( при разном усреднении)
которые 
либо выполнялись на цпу
либо хотели выполнится и стоялив очереди
либо ждали ответа от дисковой системы.

чем больше load average тем это значит что просто надо взять и смотреть
уже другие характерситики перфоманса.
прежде всего надо понять тормозит ли дисковая система.
если да. это одно.
если нет. тогда остается процессор.

в общем лоад аверадж это гавно а не характеристика.
потому что она обьединяетя в одно число и нагрузку на систему по цпу
и по дисковым траназакциям. поэтому нихера непонятно по конкретной
цифре плохо щас системе или хорошо.

только если какой то бэейслайн выработан дл сервера. вот если идет 
отклонени от него тогда можно делать выводы конкретные.

если высокий load avegare то это значит что = тормозит либо процессор либо диски либо smb
вот и гадай. больше ничего нескжаешь из этой характеристики.

а может и вообще ничего нетормозит. скажем у нас много процессов которы свербыстро 
порождаются сверх мало работают и свербыстро умирают.
тогда у нас будети выскойи load average но при этом даже cpu_usage будет маленький.
так что эта характеристика в целом нихера ничего нормально непоказывает.

чтоб чтото понять нужно  в добавов к  load average иметь данные по 
cpu_usage
и 
дисковой системе
и
сетевым шарам.

только тогда можно чтото сказать.

и тогда возникает вопрос а нахера нам лоад аверадж если мы  можем без него
обойтись всем другим.

приколькно то что изанчально ее называли cpu load average 
так вот это пиздеж полный. это никакое ни cpu load average

load average это характеристика ни цпу ни дисков ни сетевых шар
это характеристика совсем другого - характеристка ПРОЦЕССОВ.

она показщывает сколько процессов в течение минуты работали
либо с цпу (или его ждали) либо  с диском либо с сетевой шарой.

ну и что.

получается надо еще понять сколько процессов и с чем они работали.

а потом уже думать - нужно ли ускорять цпу
нужно ли ускорять диски
нужно ли ускорят сетевые шары.

ну работало столько то процессов суммарно с тем или этим ну и что.

слишком интегральная характеристика.

опять же если даже брать процессы которые только с процессором связаны.

если за минуту два процесса работали на цпу по 1 мс то la = 2
и если два процесса работали на цпу каждый по 30 секунда. то la =2 тоже.

но это ж витоге хуйня полная. в первом случае процессор неявлется узким местом
а во втором случае ну на грани.

поэтому la нихуя незначит.

хотя там вроде в плане процессора чуть поумнее они сделали.
они берут отрезки в 5 секунд. и делают по ним усреднение. 
а потом в итоге делают какую интегральную херню на минуту.
в итоге может быть якобы первый случай покажет не 2 а чтото меньшее.

но это пока только теория.

если усреднение идет по пятисекундным отрезкам. то получается в первом случае

будет число значительно меньшее чем 2.
но это пока голая теория. отличающаяся от официальной документации.

еще раз посмотрим
в чем жопа top
что часть цифр он показывает на данный момент = right now
а часть цифр это усреднение за интервал между которыми top лазиит за данными.
вообще непонятно. top скорей всего за данными лезет в proc
возникает впорос в proc какой интервал усреднения для которого 
высчитывается и показыается cpu_usage


top - 22:03:34 up 11 days,  5:27,  3 users,  load average: 0.00, 0.00, 0.00
Tasks: 159 total,   1 running, 157 sleeping,   0 stopped,   1 zombie
%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem : 21.9/2000.254 [                                                                                                    ]
MiB Swap:  0.0/0.000    [                                                                                                    ]

с этой строкой разобрались

top - 22:03:34 up 11 days,  5:27,  3 users,  load average: 0.00, 0.00, 0.00


насколько я понимаю эту строку

Tasks: 159 total,   1 running, 157 sleeping,   0 stopped,   1 zombie

она показвыает right now. так же как и списко процессов . right now.

вот эта строка

%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st

как я понимаю она выдает сумму %cpu по всем процессам.
правда с разбивкой на us,sy, id итп.


я закончил на том что хотел понять 
как top считает cpu_usage для процесса

это привело а proc

это привело к jiffy

что такое jiffy.
у линукса есть таймер который регулярно срабатывает. когда он срабатывает то запускается 
щедулер. который стопит текущие таски  на проце и запускает другие таски на проце.

интервал времени между которыми срабатывает щедулер называется jiffy.

скажем на виртуалке с ubuntu 16 он равен = 0.25 мс вроде как.

перейдем к процессу в линукс и proc
если в течение предыдущего jiffy интервала процесс выполнялся на процессоре 
(или стоял в очереди на выполнение) то линукс записывает в /proc/$PID/...
что процесс выполнялся напроцессоре +1 jiffy.

если невыполнялся то недобавляет.

система фиксирует для каждого процесса сколько времени этот процесс тусовался на цпу.
но фиксирует это она не  в секундах не в милисекундах
а в джиффях. потому что как еще раз скажу я понял каждому процессу дается на выполенение
слот времени который называется джиффи. после этого процессор прекращает работу
этого процесса и возращает управление шедулеру
который решает какой следущий процесс запустить на процессоре.


насколько я понял число джиффи задается при компиляции ядра линукса.
то есть если мы хотим его изменить то нужно преекомпилировать ядро.

посмотреть текущее значнеие можно в config файле

# cat /boot/config-4.4.0-62-generic | grep -i HZ | grep -v '#'
CONFIG_HZ=250

насклько я понимаю то это и есть чему равно джиффи. = 1/250 HZ =  0.004 с = 4 мс

как я понял в компе есть железный таймер 
который еще можно программировать чтоли.

он с одинаковой частотой генерирует сигнал на уровне хардвейр (interrupt) в проц наверное.
проц получает этот сигнал и откладвыаеттекущее выполненеие и исполняет спец код 
предназначенный при поступлении интеррапта от системного таймера.

как я понял . вот этот таймер железный и программа которая срабатвыает от его сигнала - 
эта штука работает строго через один и тот же промежуток времени.
то есть в проц прерываение поступает все время через одн промежуток.
и получается программа которая срабавыает на это прерывание 
тоже срабатывает через одинаковые промежутки.

ядро линукса - она знает частоту железного таймера. поэтому 
когда в ОС приходит очередной интеррапт от этого таймера 
то ОС четко знает что прошло +1 тик (так назвыаемый). 

минусы плюсы когда джиффи маленький.
чем чаще вызывается интеррапт хэндлер - программа которая должны выполниться при наступлении
прерывания тем больше процессор тратится на выполнение этой служебной программы
а не полезного юзерского кода.
плюс получается кэщ процессора засирается ненужными данными.

зато повышается откикаемость между задачами. также - уменьшается время бесползено тратимое 
между событиями. например джиффи = 5 мс. если прога уже заокнчила работу. как я понял.
за 2 мс в течение 5мс джифии. то 3 мс цпу будет простаивать. потому что чтобы следующая 
задача могла быть засунута в проц надо чтобы вызывался щедулер и это сделал.
а шедулера еще ждать 3мс.


в общем по мне получается что чем более много задаченее комп. тем более короткий должен быть
джиффи. чтобы мы могли переклюбчаться между задачаи часто.
но при этом все больше выполняется сам шедулер а не полезная программа и кэш процессора
засирается ненужными данными шедулера.

это прикольно что процессу выделяется интервал времени = джиффи . в течение которого
этот процесс может выполняться на цпу.
а если он выполнится успеет за меньшее время то как я понял система никак неможет 
запустить на цпу другой процесс.

это как бутто один комп и очередь пользователей. каждому дается 1 мин на пользование
компом. но если пользователь управился за 40 секунд то комп просто
стоит 20 секунд и ничего непроисходит. другой пользователь небудет запущен на комп.

система регистрирует число джиффей с момента старта системы.

например 1000 джиффей.
зная сколько длится один джиффи = скажем 4мс можно понят что аптайм системы = 1 000 х 4мс = 4 000мс = 4с

джиффи это 32 битная переменная.
но она  заполняется за 500 дней в среднем . а если джиффи = 1мс то за 50 дней
поэтому это щас 64 битная переменная. 
но софт обычно лезет в ее младшие 32 бита , если этому софту важно неасболютное
значение а только дельту считывать.

написано что вот этот вот интеррапт хэндлер он в свою очередь
вызывает линукс шедулер

всплыл попутно /proc/uptime

# cat /proc/uptime
1051140.63 2100245.85

первая цифра это аптайм в секундах
вторая цифра это время сколько все ядра суммарно отработали в холостом режиме. (поэтому
если ядер много то эта цифра легко может быть больше аптайма)

узнать сколько джиффей в штуках прошло с момента включения компа можно через

c# cat /proc/timer_list | grep -i ^jiff
jiffies: 4557732041
jiffies: 4557732041


строк несколько потому что ядер несколько.
но цифра должна совпадать у всех ядер.

итак мы имеем что с момента загрузки прошло 

4 557 732 041  джиффей

если мы знаем сколько длится один джиффи то если умножим число джиффей  на длительность
одног джиффи то это число должно совпасть с аптаймом.


вроде как выяснить время одного джиффи можно через  как я писал выше 




надо понять как достверно узнать сколько по длинне равно джиффи в текущей системе.
и переходит к вычислению cpu_usage процесса на основе proc и джиифи

# cat /boot/config-4.4.0-62-generic | grep -i HZ | grep -v '#'
CONFIG_HZ=250

1 джиффи = 1\250 = 0.004с = 4 мс

получается аптайм = 4 557 732 041 * 0.004с ~ 18 230 928 с

это вычисленная нами величина.

сверим ее с фактической

oc# cat /proc/uptime
1051627.43 

1051627.43 ~ 1 051 627

и как бы мы видим что вообще несовпадает.

поэтому вопрос остается открытым. где тут ошибка


дальше еще такой момент
вот параметр компиляции ядра HZ который определяет длинну одного джиффи по времени.
(правда тут опять же вранье так как указыается не длина по времени а величина обратная
то есть частота в герцах).

так вот. как я понял. вот эта инфорамация о том сколько прошло джиффей в штуках
она гланвым образом она нужна коду ядра. как то так. драйверам там всяким.
либо ядру либо драйверам. потому что джиффи главным образом используются для установки 
счетчиков.  например какоето событие (процесс разбудить) надо выполнить через 
четыре джиффи.  это основное примененеие этих джиффей.

так вот типа информация о джиффях непредназначена для пользовательского использования.
(бред конечно). так вот какая вылеза проблема.
очень долго длинна джиффи была постоянная 10мс. и пользовательские программы
они были так написаны что они эту величину 10мс вбили в свой код константой
. они считывали с proc число джиффей а чтобы получить секунды они умножали их 
на 10мс. так вот когда наконец в линуксе число джиффей стало меняться
то такие программы стали работать неправильно.  
поэтому завели еще одну переменную USER_HZ как я понимаю 
это тоже переменная при компилироавнии ядра. и в нее константой вбили 10мс.
и как я понимаю что когда программа запращивает у системы джиифи заивисимую 
информацию(хуй знает как конкрето это выглядит) то 
она получает информацию как бутто длинна джииффи 10мс 
таким оюбразом пользователськие прорграммы показыают время в секундах правильное
хотя реальерое джиффи в ядре другое.

но точно мне стало известно что число джиффи в 

/proc/timer_list показыаваются настоящее ядерное
потому что эта переменная timer_list она непредназанчаена для пользрвания
полтьзовательскими программами. это типа переменная для дебага.


в любом случае остается открытым вопрос почему 
чсило джииффи умноженное на длиннуу джииффи не совпдаает с аптаймом.
это дерьмо.

для справки также можно скомпилироват ядро что бы оно стало tickless
это типа значит что шедулер будет вызываться системой нерегулярно через джиффи период
а в зависимости от потребностей.
как я примерно понимаю . на текущем шаге както определяется что следущий раз 
шедулер можно вызывать через 10 джиффи. и так и происходит.
то есть на текущем шаге можно программировать через сколько джиффи будет вызван шедулер
в следущий раз.


малек разобрались.
значение длинны джиффи типа для пользоавтельских 
программ можно считать через 

c# getconf CLK_TCK
100

типа = 100 Гц => 1\100с = 10ms

лично у меня возникает вопрос. если такая мудота с джиффи
для кернел спейса и для юзер спейса то как же тогда
высчитывать чтото мне как юзеру на основе этих джиффей.

я имею ввиду например cpu_usage процесса.

переходим обратно вверх. как узнать посчитать cpu_usage 
процессом.


значит в /proc/$PID/stat
находится информация сколько данный процесс выполнялся на цпу
времени. поле 14 и поле 15
время выражается в джиффях.  как понимаю 
время выражается в USER джиффях.
поскольку как я понял - тольк один файл 
показывает кернел джиффи. 
остальные файлы proc показывают USER джиффи.


поле 14 покаывает
сколько в джиффях процесс выполнялся в юзер спейсе 
то есть исполнялся сам код процесса

поле 15 показывает  сколько в джиффях исполнялся код в кернел спейсе
то есть как я понимаю код процесса когда
вызывает через system call ядро. и вот сколько ядерный код 
исполнялся который между прочим
к самому процессу неимеет отношения прямого.

возникает вопрос. что считать иполнением процесса - только чистый 
код самого процесса или еще и вызываенные ядерный функции.

видимо надо брать сумму.

дальше я думал что нам надо узнавать сколько в секундах один 
джиффи . а оказывается и ненадо.


значит. вот тут - https://stackoverflow.com/questions/1420426/how-to-calculate-the-cpu-usage-of-a-process-by-pid-in-linux-from-c
я нашел способ который позволяет вычислить
cpu_usage процесса неузнавая сколько в секундах длинна одного джиффи.


математика такая.

j1 = поле 14 + поле 15 в /proc/$PID/stat в момент времени A1
j2 = поле 14 + 15  в момент времени A1+dt

значит за время dt процесс исполнялся на цпу (j2-j1) времени

значит cpu_usage процесса : 100% * (j2-j1)/dt  

прикол тока в том что надо получить dt в джиффях.

поскольку j1 и j2 мы узнали в юзерских джиффях. то 
dt мы будем искать не из /proc/timer_list
а из другого файла.

в /proc/stat 
находится uptime  в юзерских джиффи . об этом написано вот здесь 
(kernel.org/doc/Documentation/filesystems/proc.txt)

~# cat /proc/stat
cpu  145861 14132 71794 231683148 5981 0 518 0 0 0
cpu0 71151 5990 33124 115846673 3050 0 163 0 0 0
cpu1 74710 8141 38670 115836475 2930 0 354 0 0 0
intr 60971678 45 31 0 0 0 0 2 0 1 0 0 0 215 0 0 0 

сумма в первой строчке это аптайм в юзерских джиффи.

получается если два раза считаем файл 
то узнаем dt в юзерских джиффи.

кстати таким макаром можно узнать 
чему он равен в секундах.

значит. я посчитал. получил что на двухпроцессорной
системе за 1 секунду прошло 200 джиффи.
так как это двух процессорная система 
то значит по сто на ядро. то есть в итоге
за 1 секунда на ядре одном протикивает 100 джиффи . значит джиффи =  10мс.
в общем как и должно быть
что на i386 юзрский джиффи 100ГЦ.

ради интереса посчитаю какой кернел джиифи через 
/proc/timer_list

~# cat /proc/timer_list  | grep -i ^jif ; sleep 10 ; cat /proc/timer_list  | grep -i ^jif
jiffies: 4585001143
jiffies: 4585001143
jiffies: 4585003644
jiffies: 4585003644
root@test-kub-04:~# bc <<< "(4585003644-4585001143)/10"
250

значит за 1с на цпу протикивает 250 ядерных джиффи.
что полностью соотвествует 
~# cat /boot/config-4.4.0-62-generic | grep -i HZ
CONFIG_HZ_250=y
CONFIG_HZ=250

итак в /proc/timer_list 
мыожно узнать кернел джиффи
и сверить с теоретическим из
# cat /boot/config-4.4.0-62-generic

а в 
/proc/stat
можно посмотреть юзер джиффи.
и сверить его с каноническим значением
для i386 цпу архитектуры это 100ГЦ

все проги типа top итд
они считывают из proc только юзерские джиффи.
 и нам при расчетах надо пользоваться только ими.
 
 
получается что в proc напрямую нет данных о cpu_usage для процесса
только там есть джиффи о том сколько джиффей процесс с загрузки
системы данный процесс провел на цпу.
получается что в top 
1) в колоноке cpu_usage  
показываются данные которых нет напрямую в proc
а top сам их вычисляет калькулирует.
2) данные которые top вычисли и показал в cpu_usage
относятся промежутку времени между текущим временем
delta t назад.

где delta t это параметр # top -d 20

это все важно потому что важно понимать какой интервал 
усреднения  , без него нет смысла говорить про cpu_usage


посмотрим еще раз за top

# top
top - 01:40:11 up 14 days,  9:03,  2 users,  load average: 0.40, 0.23, 0.10
Tasks: 155 total,   1 running, 153 sleeping,   0 stopped,   1 zombie
%Cpu(s): 50.0 us,  0.1 sy,  0.0 ni, 49.8 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem : 21.8/2000.254 [                                                                                                    ]
MiB Swap:  0.0/0.000    [                                                                                                    ]

  PID USER        RES S %CPU %MEM COMMAND
18158 root     0.000t S 50.0  0.1 sysbench


 
значит когда мы видим %CPU = 50%
то мы понимаем что это вычисленное значение а не напрямую считанное откуда то из недр ОС

и мы понимаем что 50% это среднее значение за период (важно ЗАПЕРИОД!) между 
текущим моментом и делта тэ назад которая задана при запуске top в параметре -d
по умолчанию это 10 секунд если это незадано явно через ключ -d

итак еще раз
смотрим на %CPU

видим цифру ( скажем 50%)
должны понять две вещи

1) цифра полученная путем вычислений 
2) 50% это среднее за период между текущим моментом и  моментом минус дельта тэ
дельта тэ равно 10 секунд по дефолту или равно ключу -d  при запуске top


далее

top , P сортирует по %CPU


еще раз обьянсю про джиффи.

есть kernel джиффи. это та длинна в секундах 
которой ядро оперирует в своих операциях когда исполняется
ядерный код.  этот параметр задается при компилироавании ядра.

есть другой параетр user джиффи.
по факту их в природе несуществует.
это фантомная величина которая получается путем конвертации
керел джиифф умножением на коэффициент.

зачем же их придумали. затем что давным давно менять кернел 
джиффи длинну менять было нельзя.это была жесткая константа.
поэтому софт когда считывал с proc число джифффи в штуках
то чтобы их перевести джиффи из штук в секунды софт 
умножал джиффи-штуки на фиксированный коэффициент который
жестко прописан в софте.
позже когда  длинну джиффи в секундах стало можно менять
то получилась жопа.
софт считывает с proc число джиффи в штуках.
умножает штуки на коэффициаент (коэффициент это длинна одного джииффи в секундах 
и эта величина константой прописана в софт) и полуает неверные секунды.
потому что  длинна одного джииффи в секундах отличается от той константы
которая закодирована в софте.

поэтому как я понял было принято решение прорамистами ядра линукса.
что все джиффи в штуках которые можно считать с proc должны быть 
виртуальными , дожны быть такими как бутто длинна джифф в секундах 
равна 10мс (100ГЦ). тогда софт считав такое модифицированне число 
джиффи в штуках и умножив на длинну 10мс получит время джиффи в
секундах правильное.

поэтому все джииффи в proc в штуках которые можно считать из proc
они ненастящие а модифицированные.
ядро берет реалбное число джиффи и его модифицирует и только 
потом кладет его в переменные /proc
модифицируют джииффи исходя из того чему равен
коэффициент user_HZ  который можно менять при компилации
ядра. обычно оно равно 10мс (100ГЦ) . эту переменную
надо задававть равной коэффициенту старого софта
который устанавливается. исторически длинна одного джиффи
зависела от типа процессора. для i386 это 10мс.
итак цепочка

HZ - это длинна одного кернел джиффи с секундах (или герцах)
и эта величина которая реаьно исполтзуется ядерным кодом.

для пользовательского софта эти джиффи модифицируются на основе 
коэффициента user_hz

hz --> user_HZ

если ядерный джиффи = 250ГЦ = 0.004с
то за 1с накапает 250 джиффи.

если при этом user_HZ = 100ГЦ = 0.001с

то в proc будет записано что прошло не 250 джиффи а 1000 джиффи.

и софт читая переменные из /proc считает 1000 джиффи 
хотя реально прошло 250.

и софт умножим 1000 джиффи на 0.001(коэффициент захардкоженный
в софте) и получит что прошла 1 секунда.

все круто прошла  1 секунда в реале и софт узнал что прошла 1с.

вот.

едиснвтенное что одна из переменных proc

/proc/timer_list

она показыает реальные кернел джиффи.
это исключение.

там есть еще ряд параметров которые можно настраивать 
при компиляции ядра связанные с шедулером.с настройкой
его работы. (https://www.kernel.org/doc/html/latest/timers/no_hz.html)
при необходимости читать.

для ядра джиффи используются для отсчета времени.
если прошло +1 джиифи и мы знаем длтинну одного джифи
то ядро знает сколько времени в секундах прошло.

также на выполнение каждого процесса ос выделяет на цпу времени 1 джиффи.
после этого вызывает шедулер который решает какая следующая программа 
будет засунута на цпу.

CONFIG_HZ - длинна кернел джиффи в герцах ( обратная величина секундам)
CONFIG_NO_HZ - эта настройка позволяет вызывать шудулер реже в следущий
раз если четко понятно что в ближайшее время делать будет нечего.
то есть эта настрока полезна только с точки зрения
энергосбережения когда у нас много idle циклов.


!! далее.
смотрим в /proc/$pid/stat
там очень много информации служебной об процессе

в частности число minor и major 
memory faults


разбирвем что это такое.
забегая вперед скажу.
минорные фолты они нетребуют считывания с диска поэтому они быстрые
мажорные фолты требует чтения с диска поэтому они очень медленные

размер таблицы памяти 

$ cat /proc/meminfo | grep PageTables
PageTables:      24496 kB


как работает доступ цпу к физ памяти.
как я понял - цпу рабоает с виртуальной памятью. а не с физической.
и когда цпу говорит дай мне данные с такого то адреса памяти то запрос идет на спец
микросхему MMU которая транслирует адрес виртуальной памяти на адрес уже реальной 
физ памяти. у mmu есть доп помощник tlb в котором есть кэш как я понял ряда трансляций. 
если нет то mmu транслирует само но типа это медленнее. окей транлсяция прощла.
и становится понятно с какого физ адреса надо считать.

так вот как я понял верхушку айсберга .
page fault - это хуевое название . это неошибка.
это скорее микроошибка, или событие.
процессор обращается к mmu за данными из адреса виртуального.
и mmu понимает что для данного вирт адреса данных в физ памяти нет.
это и есть page fault. то есть данных в памяти нет. вот его смысл.

но есть два типа фолта.
минор и мажор.

при минор. mmu вдруг понимает что данные которые запрашиваются они 
так случилось есть в физ памяти но совсем по другому адресу чем запрашивали.
тогда mmu делает себе какито доп пометки (для себя). и выдает данные для цпу.
важно тут то что при этом событии данные с диска нечитаются.

major fault. насколько я понял главное про этот тип сообщения. это то 
что у процесса есть виртуальное адресное пространство 
это как бы формаьное количество оперативной памяти которая ему доступна.
и если процесс обращается к ячейке по адресу а память то виртуальная а не реальная
и линукс понимает что в памяти данных нет но они есть где то на диске. то возникает 
событие major page fault. хочу подчеркнуть что оно ненаступает прям во всех случаях 
когда приложение ломаится на диск. например если приложение читает данные с диска
то совсем не page fault. эта дисковая активность это совсем другое.если
 программа  в совей работе мапит файл на диске в память то возможно ( непроверял)
 это вызывает major page fault.

 число мажор и минор фолтов 
которые случились в жизни процесса можно 
посмотреть в /proc/$pid/stat
и через ps 

# ps p 3209 -o min_flt,maj_flt,cmd
 MINFL  MAJFL CMD
  4648      0 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/9989cc33dd9becb27e7702d1e986310b785ce4b09ca011


минор фолт  - нетребует обращени к диску
мажор фолт - потребовало обращению к диску

как посмотртеть для процесса в top
запустили top , далее f. далее выбираем в списке major fault. тыкаем пробел.
далее тыкаем s это даст то что top будет упорядочивать строки именно по столбцу
major fault. тыкаем esc и насладжаемся.
если у программы процесса много major fault это незначит что она просто много
обращается к диску. это значит что тело програмы которое дожно лежать полностбью
в памяти лежит в памяти неполностью и линуксек приходится периодические 
читать тело программы (ее воркинг сет) с диска. вобщем пока несовсем понятн.
нарпимер у эластика много page major fault.

еще по идея тема такая. 
когда нехватает памяти то линукс начинает ее чистить (сервис kswapd это делает).
он берет ячейку памяти. если у нее есть бекенд на диске например этио бинарник
файла на диске то он просто эту ячейку вычищает. потому что если что то 
ее мжоно прочитать обратно с диска. а если у ячейки нет бекенда на диске
то он ее как понимаю пихает в свап.  потому что просто так задискардить из памяти 
он ее неможет ибо откуда же он ее потом чиать будет.
если свапа на компе нет то линукс начинает жетско из памяти вычищать бинарники.


также увидел инфо о том что якобы якобы при свапинге якобы весь проесс целиком
засосвывается в свап файл.

также полезная команда time (только ее надо водить с полным путем иначе будет заюзан
built-in баша time а это другое).так вот команда time покажет детальное инфо
про major page faults у процесса и еще полезную инфо. например про его резидентную
часть ту часть которя реально сидела в RAM

пример
~$ sudo /usr/bin/time -v dd if=/dev/sda of=/dev/null bs=1M count=10000
^C8195+0 records in
8194+0 records out
8592031744 bytes (8.6 GB, 8.0 GiB) copied, 55.0677 s, 156 MB/s
Command terminated by signal 2
        Command being timed: "dd if=/dev/sda of=/dev/null bs=1M count=10000"
        User time (seconds): 0.00
        System time (seconds): 3.68
        Percent of CPU this job got: 6%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:55.06
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
        Maximum resident set size (kbytes): 2844
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 1
        Minor (reclaiming a frame) page faults: 607
        Voluntary context switches: 33602
        Involuntary context switches: 261
        Swaps: 0
        File system inputs: 8878096
        File system outputs: 0
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 0

также нашел инфо что якобы major faults для процесса также растут если 
его тело или чтото там он вытаскивает из файла подкачки.



также хотел про tlb, mmu, page table 
пояснить

описание как работает когда cpu хочет 
считать кусок данных из ram:

прежде всего. цпу работает в одном из колец.
в зависимости от кольца цпу позволит выполнять или 
запретит выполнять определенные инструкции.
кольцо 0 дает максимальный доступ ко всем инструкциям
и всем возможностям.
кольцо 1 и 2 неиспользуются виндовсом и линуксом.
кольцо 3 дает ограниченный набор инструкций.
кольцо 3 недает возможности самовольно переключиться в кольцо 0.

код в кольце 0 регистрирует куски кода которые будут 
выполняться при наступлении разных прерываний.

 в частности начинает работать на компе таймер 
 который регулярно формирует на цпу прерывыание на которое вызывает 
 код обрабатывающий это прерывание в кольце 0 и это шедулер.
 
 шедулер регулярно запускает уже пользовательские программы в кольце 3.
 
 
 программа в кольце 3 если ей нужно получить какойто функционал от ядра
 формирует запрос с параметрами и запускает софтовое прерывание.
 
 проц преключается в кольцо 0. передвате параметры от пользовательской проги
 к коду который обрабатвыает прерыаение. и если все оформлено правильно
 то выполняется ядерный кусок который делает то что просит 
 польщовательская программа.
 
 в кольце 3 программы работают точно не с физической памятью а с виртуальной.
 конечно пользвательская программа это незнает. программа думает 
 что работать с физической памятью.
 
 вот откуда появляется та самая виртуальная память.
 плюч вирт памяти в том что пользвательские
 проги немогут навредить друг другу. ибо они неимеют
 прямого доступа к памяти. 
 
 когда цпу работает в колльце 0 то программа работает 
 как я понял уже с физической памятью а не виртуальной. 
 обязательно или нет я незнаю. может ли программа в режиме кольца 0
 работать с виртуальной памятью незнаю.
 
 пока что ясно точно одно  - в кольце 3 исполняетя пользовательская 
 программа  и эта программа работает не с физической памятью
 а наебкой под названием виртуальная память.
 
 вот программа хочет считать данные с памяти.
 то бишь цпу надо счттать кусок данных с памяти.
 
 адрес указан виртуальный.
 
 цпу обращается к микросхеме которая впаяна в сам корпус цпу 
 под названием MMU.
 вроде бы каждое ядро имеет свое индивидуальное MMU
 mmu должен преобразовать виртуальный адрес в уже реальный физ адрес RAM
 чтобы их можно уже было считать с реальной памяти.
 mmu для этого смотрит в кэш который тоже вмонтирован тоже в корпус 
 самого цпу под названием TLB.
 в TLb есть таблица соотвествия виртальных адресов
 и физ адресов.
 если там это есть то отлично. мы узнаем физ адрес.
 далее как я понимаю именно mmu обращается уже к memory controller
 и просит его считать данные с узнанного адреса.
 (но это неточно. я ненашел точных данных кто в итоге обращается
 к memrory controller mmu или цпу.).
 далее когда физ адрес памяти мы выяснили происходит
 еще кое что интересное. цпу зная адрес ищет данные в своих кэшах.
 поскольку как я понял зная физ адрес можно искать данные в
 кэшах цпу. если в кэшах цпу нет то тогда запрос идет к 
 мемори контроллеру. 
 мемори контроллер по выданному ему адресу в физ памяти
 выдает содержимое памяти по адресу.
 отлично. конец истори.
 
 немогу сказать точно mmu ищет в кэшах и обращается к
 мемори контроллеру или сам цпу это делает а mmu только 
 отвечает ли за преобразование вирт памяти в физ память.
 это я так пока и ненашел.
 
 итак еще раз цепочка
 
 [ цпу -> mmu -> tlb -> кэш цпу ] -> memory controller -> RAM
 
 в скобочках я показал те микросхемы которые сидят внутри цпу.
 и даже мемори контроллер может сидеть внутри цпу.
 в любом случае если данные содержатся в tlb и кэше
 то цпу получает данные сверхбыстро потому что они все 
 есть внутри цпу.
 
 интереснее происходит если в tlb нет нужной информации.
 тогда mmu лезет в RAM.
 тут дальше несовсем понятно как это работает.
 но смешно. чтобы получить данные из ram
 мы лезем для этого в ram правда в другое место.
 mmu лезет в ram в page table.
 исходя из картинки что я увидел . именно mmu дает 
 запрос к мемори контроллеру.
 
 если mmu ненаходит нужное в tlb то оно генерирует 
 page fault exception и interrupt cpu
 
 странно на одной презентации я прочитал что кернел код 
 использует тоже виртуальному адресацию ...хмм.. мне кажется это брехня
 вобщем я прочитал вот что. код ядра он тоже работает не с реальным физ пространством
 а в виртуальном пространстве.
 насколько я понял все сделано вот как: все процессы которые работают ..
 
 так тут вылезло много вопросов про память в линуксе.
 
 я привык что все что в памяти можно увидеть в форме процессов через ps
 оказыает сяэто вроде не так. 
 то есть само ядро его невидно.
 
 с виртуальной памятью и ядром непонятно.
 
 ядро окащывается это не какойто бэкграунд процесс.
 а набор рутин которые можно вызвать.
 
 ...
 
 paging - это когда процесс работае с виртуальной памятью.
 и ему кажется что она реальная. но фишка в том что на самом деле
 процеесс занимает в физической памяти несттолько места как ему кажется.
 процессу можно выделять куски физ памяти тока по мере необходимости.
 типа как у дисков thin provision.
 
 paging и микросхема mmu неразлучны.
 
 как только мы активировали MMU на цпу (незнаю как) то очень важный момент -
 - как процессор там неизгоялялся - вся адресация будет виртуальная.
 процессор будет работать только с виртуальными адресами.
 это значит что все запросы на доступ к памяти будут проходить через MMU.
 mmu в свою очередь для каждого запрошенного адреса
 ищет соответствие физ адрему в своем кэше TLB.
 если в TLB записи нет то возникает exception. ( толи эксепшн то ли интеррапт.
 из того что нашел интеррапт это сигнал на ножке цпу от внешнего устройства
 типа клава мышь о событий на периферийноим устройстве которое надо обработать.
 если проц закончил выполнять команду и обнаружил на ножке своей сигнал 
 от дургого устройства интеррапт то цпу передает управление интеррапт хэндлеру
 спец программе которая обрабатыавет событие. как тока хэндлер закончит 
 свою работу цпу начнет выполнять программу дальше которую онвыполнял 
 до наступления интеррапта.
 эксепшн это типа событие произошедшее внутри самого процессора.
 его внутренние проблемки.
 поэтому я считаю что отсутствие записи в tlb это эксепшн
 но мое ощущение что люди используют оба термина 
 как попало.)
 
 так вот в TLB неоказалось записи. вознкает эксепшн.
 дальше пишут что два варианта в зависимости от производителя 
 процессора.
 один вариант что цпу обращается к ОС непонятно в какую ее часть.
 и ждет что ОС найдет ему сама необходимую трансляцию.
 как я понял к интел это неотносится.
 
 у интел работает другой вариант.
 процессор лезет в RAM по специальному адресу как я понял
 указанному в регистре CR3 (непонятно кем когда и как указанному).
 адрес там указан реальный физический.
 и идет по этому адресу и ожидает что там лежит
 специально сфорсмированная таблица page table
 которая имеет заранее оговоренный формат. и ищет в этой таблице
 ровно теже данные что он искал в TLB.
 находит их там и все отлично.
 
 отсюда важные выводы - таблицей ее наполнением и коректным содержанием
 занимается ОС. (как она это делает непонятно).
 
 также получается смешная ситуация - чтобы считать данные с ячейки RAM
 нужно еще понять откуда же по факту ее читать и для этого если в TLB записи
 нет придется полазить по той же самой RAM причем неодин раз. получается
 офигенная уменьшенеи скорости работы с памятью.
 
 если запись есть в TLB котоый находится на самом чипе цпу то 
 тут проблем нет. получим что буквально чтение из памяти будет считай
 такое же по скорости как бутто мы сразу знадли адрес физ ячейки.
 
 ОС как я понял конечно можем читать писать в TLB
 
 получается ядерный код тоже работает в режиме виртуальной адресации.
 
 из этго получается непонятная рекурсия. чтобы запрограммировать соотвествие вирт адреса
 в физ адрес нужно указать это в TLB либо в RAM в таблице. но мы же еще ..... как обьяснить.
 
 вот ядро как то оказалось в оперативке. нет еще ни данные в TLB ни в RAM нет page table.
 как же нечать с ними работат. получается что чтобы была курица надо яйцо 
 а чтобы было яйцо нужна курица.
 
 скажем сидит у нас ядро в памяти и имеет для себя записи в page table в памяти.
 как это поменять. ведь ядро то само видит только вирт адреса. и не может вырываться
 в физ адресацию. то есть ядро же заперто каки другие процессы внутри 
 невидимой тюрьмы вирт адресаци.
 
 непонятно.
 
 есть определенные трюкт точно.
 например можно сделать что вирт адреса совпадают с физ адресами. 
 тут тогда более все понятно.
 
 каждый процесс уже полтзвательский имеет свою собвственую page table.
 в которой записано какие физ блоки занимает его вирт адресаци.я
 возможно его page table является частью одной большой page table
 
 ядро у нас лежит в каком то пространстве адресов физ памяти.
 и для каждого пользтвалельского процесса прописанов его page table
 что определенный кусок памяти вирт пространства процесса ведет в физ 
 пространство где лежит код ядра.
 поэтому в вирт пространстве процесса каждого есть код ядра.
 полуается каждый процесс в своей памяти видит код ядра.
 
 вот это вот связывание адресов вирт памяти и адресов 
 физ памяти как я понял называется pagin также маппинг
 
 вирт адрес пространство это типа фронтенд
 а физ адрес пространство это типа бэкенд
 
 и говорят что код ядра замаплен в память каждого процесса.
 
 зачем так сделано.
 получается что у каждого процесса отобрали 1GB его вирт адресного пространства
 где мог бы лежать доп код юзерского процесса.
 
 сделано это для скорости. когда юзеркский код 
 делает syscall то оказыается что ядерный код уже доступерн в этом 
 адресном пронстранстве ненужно делать какието там переключения на цпу и терять
 скорость. 
 
 тот кусок куда замаплен код ядра в вирт пространстве памяти 
 процесса называю high mem 
 
 
 очень важная вешь которая пока непонятная.
 не все структуры которые выполняются на цпу видны в ps.
 ядро которое было загружено при загрузке 
 его вообще непощупать. 
 например работу шедулера линукса ее непонятно как пощупать отследить
 сколько времени она заняла на цпу и других железках 
 за какойто интервал. и это только одна из структур ядра невидимая.
 
 насколько я понимаю как все работает:
 вот загрузилось в память ядро линукса - vmlinuz.
 то есть формально кусок кода записан в какойто кусок оперативки.
 
 дальше этот код запускается. то есть начинает исполняться на цпу.
 
 это может и не особо важно.
 но когда ядро начинает выполняться на цпу тот там походу пьесы 
 с точки зрения самой же ОС нет никаких процессов как это мы привыкли.
 что если какая то программы выполняется на компе то она обязательно
 имеет вид процесса.
 такого там нет.
 код ядра просто както там выполняется.
 и вот там все варится варится варится.
 и на какомто этапе ядро порождает первый процесс 
 это init.
  у него кстати $pid = 1
  
  до этого момента как я понимаю никаки процессов во вселенной запущенного ядра нет.
  что там хуй знает. просто код который крутится на цпу.
  
  вот это конечно интересно как это там все работает с точки зрения самой же
  ОС до возникновения процессов.
  
 
 перед тем как запустить первый процесс как я понимаю 
 в системе настроена важная штука как шедулер и interrupt хэндлеры.
 
 что оно дает. каждый раз когда в системе срабатывает таймер (железка) то она 
 присылает на ножку цпу сигнал. и цпу идет в определенное место памяти 
 и там сидит кусок кода ядра под названием шедулер.
 и этот шедулер решает какой процесс будет запущен а цпу на время следущего джиффи.
 
 то есть наверно так - загрузка ядра в память.
 он так как то хуй знает работает. стой точки зрения что там все инициализируется.
 и нет процессов а просто код както болтается на цпу.
 
 потом в итоге в память записываются обработчики интерраптов.
 и вызывается шедулер (  это типа функция  ) ,
 который запускает код уже непросто как хуйзнает какойто код а уже по всей форме
 а именно в форме процесса. все первый процесс пошел на регулярное исполнение.
 
 и получается что уже никакой ядерный код невыполняется. он просто лежит в оперативке и все.
 
 а выполняются либо пользовательский код в процессах.
 либо по интерапту регулярно вызывается шедулер (кусок кода ядра) который неявляется
 процессом.это кусок кода который отрабатывается во ответ на интеррапт от таймера.
 
 получается ядро просто лежит в оперативке. 
 ну и еще процессами юзеров время от времени вызываются куски кода ядра
 чтобы выполнить какието специфические опасные задачи  - считать файл например.
 то есть код ядра просто как бы становится частью кода пользовательского процесса.
 
 а в остальном ядро никак неработает и неживет. оно мертвым грузом лежит в оперативке.
 единственный живой регулярный хартбит от ядра это регулярно срабатыывающий шедулер.
 в остальном ядро мертво.
 
 есть еще такие вещикоторые путают мозг это 
 
 kernel space
 user space
 
 это относится к характеристике RAM.
 
 а есть kernel mode и user mode - это относится к совершенно другому 
 это характеристика процессора его режима работы.
 
 
 а их мешают в кучу.
 
 плохо назвали.
 
 kernel space, userspace -  
 я бы сказал что здесь это включает вот какую сложную концепцию :
  каждый процесс пользовательский работат в виртуальном адресном пронстранстве.
  имеется в виду то что программа думает что она записывает данные в ячейку с адресом 1.
  а на самом деле запись идет в совершенно другую ячейку в физ памяти.
  плюс не все ячейки памяти которые программа думает она заняла в физ памяти 
  она реально их заняла. то есть память с которой раотает процесс это фейк.
  это некий фронтенд. виртуальнвый. а  бекенд для этого хитрый.
  
  так вот для каждого пользовательского процесса так сделано что 1GB  его виртуалтной памяти
  завязывается с тем куском физ памяти где лежит ядро.
  поэтому формально в адремном пронстранстве процесса есть и код ядра и код пользовательский
  самой программы. и в режиме работы процессора в пользовательском режиме (ring 3) код программы
  попав на цпу неможет считать и тем более записать ничего из адресов где типа лежит ядро.
  также как юзер неможет зайти на диске в папку администратора - нет прав.
  
  но все таки вернемся к коду ядра котороый лежит где то в физической памяти.
  как я понял в физ памяти ему выделяется некий оговоренный кусок.

  так вот что важно. код который относится к ядру - например новый драйвер  - он будет записан 
при считывании с диска только в ту область памяти.

ну окей. типа просто делаем некую секретарскую работу - код ядра кладем в одну область а все остальное
в другую область. 

но вот я говорю одно из первых фишек что это дает . это то что - если мы имеем какойто кусок
кода в той кернел области памяти. то этот код будет присутсвовать в КАЖДОМ пользовательском
процессе в его вирт памяти. он будет туда замаплен.
но конечно прямого доступа коду юзера в процессе небудет к памяти ядра.
он будет только через syscall.  

есть еще одно важное последствие.
процессор можно перкключить в привилигировнный режим там где доступны для кода все команды цпу.
так вот когда процессор работает в этом привилигированном режиме то он будет исполнять в этом 
режиме только команды которые лежат в кернел области RAM.
поэтому если мы хотим какойто код выполнять в привилигорованном режиме мы должны
этот код поместить именно в кернел space.

такова взаимосвязь между двумя совершенно разными железками - памятью и цпу
связь между kernel space ( кусо в памяти ) и kernel mode ( привлигированный режим работы цпу )


только код из kernel space будет выполняться на цпу  в kernel mode.


далее. есть просто процессы.
а есть kernel процессы.

просто процесс . это пользовательский код который работает в ring3. и этот код 
оформлен в виде процесса. это дает то что шедулер постоянно меняет какой из множества 
процессов выполнять на цпу на данный момент джиффи.

процесс это некая самодостаточная коробка со всеми ингридиентами для того чтобы ее
можно было в любой момент отложить . а потом обратно достать и продолжить выдавать 
заданный продукт.

в чем проблема процесса работающего в ring3. если он часто вызывает syscall
то есть функции ядра то процесс переключения цпу из ring 3 в ring 0 ( в котром выполняется
та или иная функция ядра код куска ядра ) он очень долгий в плане времени потому что
цпу надо много циклов чтобы переключиться с ring3 на ring0

и тогда возникает идея что если поместить весь код  в kernel space и чтобы он работал 
целиком в ring 0 то в плане цпу времени может существенно увелситься скорость выдаваемого
продукта.

так и делают. помещают код в kernel space.
а потом есть спец провцедура которая создает процесс который работате только в ring0 
беря кусок кода из kernel space.

в чем разница между ядром, шедулером и кернел процессом.

ядро - это просто весь код который лежит в кернел памяти мертвым грузом
как мешки с гавном. лежит в памяти и на цпу его нет.как старый текстовый файл на 
жестком диске. это ядро.

шедулер это часть кода ядра. его кусочек. который вызывается регулярно на цпу
потому что на цпу регулярно приходит сигнал от таймера и так настроено что когда приходит
сигнал то цпу исполняет шедулер.
при этом шедулер непроцесс. незнаю что это но непроцесс. у него как я понял нет 
каких то атрибутов присущих процессу. и шедулер исполняется на ring0

кернел процесс он попадает на цпу тоже регулярно как и шедулер
тоже он каки и ядро и как шедулер лежит в памяти в кернел спейс 
но в отличие от шедулера он попадает на цпу регулярно не изза сигнала таймера на цпу
а потому что его регулярно вызывает шедулер. 
мне это напоминает вот что.

на компе можно программу вызывать ну просто через командную строку. 
это типа как ну просто код выполняется на цпу.

а можно код засунуть в task scheduler и он станет джобом.
это мне напоминает процесс в ОС.

то есть процесс это правильно зарегистрированный кусок кода в шедулере.
также как любую программу можно зарегистрировтаь в виндовс шедулере и он превратиться в джоб.
также походу любой кусок кода можно зарегистрировта в линукс шедулере и он станет процессоом.


так случилось что привычнее всего кусок кода выполняющийся на процессоре в ОС
видеть оформленным в виде процесса. как бы настолько это вьелось что непредставляешь
как бы это можно было на цпу в ОС запустить программу и она бы не являлась процессом.

но по факту процесс это только один из вариантов запуска кода на цпу.
какие друние варианты я незнаю но они есть.

(надо потихньку начать закрывтаь вкладки в браузере)

я прочитал что когда сработал интеррапт и было пердано управление хэндлеру то в этот
момент линия видимо ножка процессора которая принимает интеррапты она выключена.
это непонятно. а что же если от хардвейра летит в это время сигнал он полуается пропадает?
ладно. так вот изза этого хэндлер делает минимаьную работу чтобы как можно быстрее
ножка приема интерраптов обратно включилась. поэтому интеррапт непроделывает все работу
нужную по обработке интеррапта а только минимальную а остальную часть както передает
комуто другому (типа  bottom half driver-у) а тот уже когда то потом доделывает 
работу. важно то что когда работает bottom half driver то в это время ножка на цпу
обратно включена. и это отлично. и вот поэтому хэндлер делаеют максимально быстрым и коротким.
в общем botom half это спец куски кода в ядре которые вызываются тоже особым образом
они представляют собой как бы вторую половину хэндлера. их назначение доделать всю работу 
которая осталась несделанной после работы хэндлера. главное чтобы хэндлер был мелкий
отрабатывал молниеносно чтобы сократит время когда цпу имеет дезактивированную ножку
интерраптов. когда работает bottom half то ножка уже активирована.

если код ядра выполняется  для обработки интеррапта это называется interrupt context.
то есть это когда хэндлер выполняется. в этом режиме конечно код не имеет никакого 
процесса. такой код неможет впасть в спячку чтобы потом его продолжить выполнять.
он должен быть выполнен до конца.


есть полная каша  в линуксе с такой штукой как context.
попробуем прояснить. (https://www.kernel.org/doc/htmldocs/kernel-hacking/basic-players.html)

цпу может исполнять код в следущих режимах :

* выполняется код ядра. цпу в ring 0. код не принадлежит никакому процессу.
код является обработчиком интеррапта. (это точно называется interrupt context)

* выполняется непонятно код ядра или юзеркский код. цпу ring непонятен. код не принадлежит никакому процессу. 
идет обработка tasklet или softirq (мало что понятно)

* выполняется код адра. цпу в ring 0. код ассоциирован с процессом.
данные вводные прилетели из юзерского кода (через syscall)
 идет обработка syscall (это точно называется user context)

* выполняется код юзерский. цпу в ring 3. код принадлежит процессу.
выполнятеся юзерский код в процессе в ring 3


остается вопрос что такое process context. 


один важный момент вытекает.
в ps или top указан процесс обычный какойто программы типа эластика.
которая нахродится в состоянии S.

теперь я болеее интересно трактую это состояние.

прежде всего оно означает что на цпу сейчас юзерского кода этого процесса нет. невыполняется.
но! это незначит что на цпу невыполняется ядерный код запущенный этим процессом.
процесс мог сделать syscall 
тогда все данные пользовательского кода с цпу убираются. 
процесс получает статус S. цпу переключается в ring 0. запускается ядерный код
с параметрами прилетевшими от юзерского кода. 

вот у меня два вопроса. что мы увидим в таком случае в команде top или ps
будет ли у процесса статус S, и какой будет cpu_usage для процесса.

и вот этот ядерный код который будет выполняться он в каком статусе
выполняется.приписывается ли он к процессу.  а значит можно ли выполнение этого кода
приостановить  а потом возобновить через шедулер.

в любом случае если процесс S прямо сейчас это незначит что он опосредованно
негрузит цпу прямо сейчас (через syscall). да код самого процесса невыполняется на цпу
но вместо этого на цпу выполняется код ядра. так какая разница все равно цпу занят.
это как то что лопату сейчас использует не вася а петя которому вася лопату передал.

прикольно что можно wchan напрямую для процесса читать

# cat /proc/2406/wchan

-->чуть в сторону
в top кнопка "t" позволяет покаызывать суммарную загрузку цпу

top - 20:50:21 up 18 days,  4:14,  2 users,  load average: 0.59, 0.86, 0.62
Tasks: 153 total,   2 running, 150 sleeping,   0 stopped,   1 zombie
%Cpu(s):   0.3/49.9   50[                                                                                                    ]
MiB Mem : 22.2/2000.254 [                                                                                                    ]
MiB Swap:  0.0/0.000    [              


%Cpu(s):   0.3/49.9   50

0.3 - это %us+%ni
49.9 - это $sy
50 это суммарная загрузка цпу.
<--


вот пример процесса java
который все время показывает загрузку по %cpu_usage
при этом эта загрузка вся %us и только капля %sy
и при этом он постоянно S


top - 20:54:56 up 115 days,  4:13,  2 users,  load average: 3.64, 3.38, 3.70
Tasks: 175 total,   3 running, 172 sleeping,   0 stopped,   0 zombie
%Cpu(s):  34.1/0.4    35[                                                                                                    ]

  PID USER        RES S %CPU %MEM     TIME+ COMMAND
 1177 elastic+ 0.030t S 34.7 87.8 439018:09 java

как видно
%CPU 34.7
%us 34.1
%sy 0.4

а вот другой пример
команда dd

top - 20:57:22 up 18 days,  4:21,  2 users,  load average: 0.96, 0.92, 0.75
Tasks: 153 total,   2 running, 150 sleeping,   0 stopped,   1 zombie
%Cpu(s):   0.6/49.7   50[                                                                                                    ]

  PID USER        RES S %CPU %MEM COMMAND
 5527 root     0.000t R 49.9  0.1 dd


%CPU 49.9
%us 0.6
%sy 49.7

и при этом оно постоянно R

мне непонятно если процесс постоянно запускает через syscall кернел код
то как его состояние постоянно остается R

и в тоже время предыдущий пример.
процесс все время  исполняется юзерский код. но статус постоянно остается S

а ведь S означает что процесс нырнул в syscall

мы в итоге обрато вернулись с того с чего начали
что значит S и R
и как оно связано с cpu_usage 


load average (1m) растер от 3м 40с до 4м 40 секунд с 0 до 1
и падает с 0 до 1 тоже 4м 40с

это пиздец.

вторая жопа что по времени оно растет не с одинаковой скоростб. хотя
тисходная функция константа
  
  подбирвется к макксимум очень меддленно
  падает с макмимума очень быстро
  
  падает к нулю очень мделенно а растет от нуля очень быстро
  
 
 18 минут непрервыйно нашгрузки привели к 1 во вторром la
 
 
 получается чтобы load (1) дорос до 1 нужно как минимум 3:40 минуты
 чтобы шарашил на полную
 
 чтобы load (2) дорос до 1 нужно как миниум 18 минут шарашить на полную.
 
 load растет внчале очень бодро потом средне а под конец вобще нехочетс расти
 падает она очень бодро потом средне пдаает а в конце очень нехочет уходить в ноль
 
 итолько через 50 минут load(3) дорос до 1. ну а така как  я начинал при ненулевом load(3)
 то я думаюб что он от 0 до 1 растет целый час.
 
 
 за первую минуту до 0.60)  (+0.60 за минуту)     
 за вторую минуту до 0.86)  (+0.26 за минуту)  
 за третью минуту до 0.95)  (+0.09 за минуту)
 за четвертую минуту 0.98)  (+0.03 за минуту)
 за четыре 40с    до 1.00   (+0.03 за минуту)
 
 видим что чем больше во времени работает процесс тем меньше 
 он дает вклад в среднеее. это жопа.
 
 по мне load растет как логарифм во времени
 
 
 2 треда до load()=1 дошли за 45 секунд 
 а до load()=2 дошли за 4м 40с теже.

 я запустил sysbenmch на тест диска.
 с 16 потоками чтения записи.
 
 при прочмотре top видно что процесс один.
 но ps ax -T покзаывыает что sysbench запустил lightweight треды.
 и их 16 штук. все они в состоянии D
 
 и load average показал = 15.4 за пару минут.
 таким образом все эти дисковые нагрузки вылились в load average
 и цпу тут ни причем.
 и в top этого невидно там только один процесс со статусом D
 а нужно открывать треды . и только тогда мы поймем в чем дело.
 почему такой большой load average
 
 что важного выяснилоась.
 
 вот есть у нас есть процесс.
 а у него много тредов.
 
 так вот статус головного процесса не совпадает сос статусами тредов
 
root     20265  0.0  0.0  26712   560 pts/0    Sl+  20:23   0:00 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20265  1.7  0.0  26712   560 pts/0    Dl+  20:23   0:08 sysbench --num-threads 16 --test fileio --file-total-size 3G --file-test-mode rndrw --max-ti
root     20353  0.0  0.1  35932  3064 pts/1    R+   20:32   0:00 ps auxH

если мы смотрим top то там треды непоказыываются.
а только головной процесс.
и если скажем треды жрут диск и находятся в состоянии D
то в топ этого совершенно невидно.

только # ps auxH 
это покажет

в этом плане гораздо полезнее не top
а vmstat


root@test-kub-04:~# vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0  78720  33712 1828784    0    0    20    27    3    2  0  0 100  0  0
 0  0      0  78596  33712 1828912    0    0     0     0   30   61  0  0 100  0  0
 0  0      0  78596  33712 1828912    0    0     0     0   17   31  0  0 100  0  0
 0 16      0  75720  33712 1831144    0    0 20020 21128 2258 4388  1  4 77 19  0
 0 16      0  78236  33892 1828352    0    0 83588 95032 8722 18738  2 16  9 73  0
 0 12      0  75832  33896 1830968    0    0 82880 92520 9164 18480  1 16  1 83  0
 6 15      0  77560  33944 1829160    0    0 86584 93528 8976 18708  2 14  1 84  0
 5 14      0  76432  33916 1830420    0    0 87216 93296 8544 18324  2 16 12 70  0
 0  9      0  78952  33844 1827916    0    0 83240 89764 9047 18249  0 15  1 84  0
 0 12      0  75180  33980 1830700    0    0 82112 92680 8568 18363  1 17  2 81  0
 1 14      0  74720  34060 1831832    0    0 81512 93172 7224 17593  2 15 27 57  0
 3  8      0  72144  34088 1834056    0    0 88292 95308 9693 19112  1 18  1 81  0
 0 16      0  69644  34104 1836592    0    0 83196 92740 8296 18067  1 17 11 70  0
 0 16      0  71984  34084 1834380    0    0 83180 93640 8443 18166  1 14 13 72  0


в колонке b 
он показывает число и процессов и тредов все сразу которые ждут ответа от диска.
так что это сразу видно. пплюс он показывает %wa который тоже подтвердит
что дисковая система тормозит. где тормозит.

получается vmstat полезнее чем top

при этом top в верхней графе неопказывает процессы D
это очень болшой пррокол.

top - 20:40:35 up 22 days,  4:04,  3 users,  load average: 2.77, 6.05, 7.63
Tasks: 140 total,   1 running, 139 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  4.1 sy,  0.0 ni, 50.0 id, 45.9 wa,  0.0 hi,  0.0 si,  0.0 st

видно что процессов который D uninterruptible sleep
он непоказыает.
то есть для top все шоколадно.

 у нас может быть 16 ядер на компе.
 load average  = 1 
 и типа все отлично
 а это может быть  1 процесс который грузит диск на 100%.
 
 в итоге система мертвая а load average в шоколаде.
 
 то есть load average это дебильная характеристика. ни о чем.
 
 load по мнению линукса сиюсекундная хараеткеристика данного момента.
 это сколько процессов со статусом R+D + те которые както там застряли
 на файловых шарах  на данный момент. число R+D на данный момент 
 можно посмоттреть через vmstat 
 
 root@test-kub-04:~# vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 434980  92916 1421384    0    0    23    30    5    5  0  0 100  0  0
 0  0      0 434948  92916 1421400    0    0     0     0   33   53  0  0 100  0  0
 0  0      0 434948  92916 1421400    0    0     0     0   23   48  0  0 100  0  0


видно r+b. (b это в терминах ps это статус D).
вот это и есть load на данный  момент.  итак load это исходная функция
которая их интерсует.

load average это среднее от этой функции. но непросто арифметическое среднее. неееееет.
это экспоненциальная скользящая среднее. и это уже хуй знает какой смысл имеет 
физический.


если  у нас запущено два треда по 100%. то load (первый параметр) достигает за 45 секунд.
смысл физический? 

получается что первый параметр доходит до 1 за 4м 40с
второй параметр доходит до 1 за 18 минут
третий за час - при однотредовой нагрузке 100.

думаю хватит об этой дебильной характеристике.

( про скользящую экмпоненциальную среднюю - https://ru.wikipedia.org/wiki/%D0%A1%D0%BA%D0%BE%D0%BB%D1%8C%D0%B7%D1%8F%D1%89%D0%B0%D1%8F_%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D1%8F%D1%8F )

 
 
!! возвращаемся к процессам в линуксе. к работе с памятью MMU.
к тому как линукс работает с кодом.
 
 
 
 (mmu вмонтирован в корпус цпу а не в корпус memory контроллера потому что...)
 


грузится комп. цпу работает в кольце 0.
первый код который попадет в цпу будет работать в кольце 0.





процессору нужно получить кусок данных из вирт памяти.
он обращается к своей микросхеме MMU
MMU 
наверное mmu это и есть memory controller или его часть.

назначение mmu это чтобы определить каков адрес в физической памяти
для запрошенного адреса в виртуальной памяти,то 
есть чтобы преобразовать вирт адрес в физический


освятить то что tlb сидит в цпу и поэтому доступ к физ памяти через него быстрее
чем через page table

также  я встретил что код ядра оперирует с памятью физической напрямую. без mmu .
неиспользует виртульную адресацию. а оперирует физической адресацией напрямую.
вау.




tlb - это кусок цпу.  и кэширует page-table ( которая сидит в ram )



minor page fault - это когда считывани с диск не происходит. ( и это отлично).


!! такой штук.

рассказываю про то что происходит когда в системе нет свап раздела
и мы выжираем всю физ память. что будет происходить
и почему.

буду говортиь про kswapd0, бешеное чтение с диска, oom_killer

когда линуксу физически нехваатает памяти он вызывает oom_killer
он убивает процессы с максимальными поинтами.

для кажждого процесса можно посмреть текущий оом поинт

~# cat /proc/390/oom_score
1

чем выше поинты тем он более первый в списке на убиение.
формула конечно назначени поинтов сложна но для простоты
там так - чем больше памяти занимаеим процесс тем у него поинт выше.
если процесс работает под рутом то его поинты пониже.

была у меня такая практика.
я захотел вызывать этот oom и посмотреть что будет.

в лнуксе есть папка /dev/shm
в нее смоонтирована оперативная память

root@test-kub-04:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
tmpfs          1001M     0 1001M   0% /dev/shm

для чего нужен этот shm я таки непонял но факт в том что
эта папка она ведет прямов память.
поэтому если мы в нее запишем файлы то у нас займется память.

вот как было до

~# free -m
              total        used        free      shared  buff/cache   available
Mem:           2000         144        1582           5         273        1693
Swap:             0           0           0


вот как стало после

root@test-kub-04:~# dd if=/dev/zero of=/dev/shm/vasya bs=1M count=1000
root@test-kub-04:~# free -m
              total        used        free      shared  buff/cache   available
Mem:           2000         145         579        1005        1275         691
Swap:             0           0           0

видно что shared и cache увеличился а used нет.
это наебка. ибо used очень даже увеличился.
если мы расширим размер этого /dev/shm

~# mount -o remount size=1800M /dev/shm

а потом нафигачим туда данных. то у нас получится что из 2000МБ всего
у нас занято 1800МБ

и тут начнется жопа. 
но не такая как я  ожидал.

диск начнет бешено считываться

даже SSD скорости небудет хватать. будет читаь по 8000 iops или 400МБ\с

vmstat будет показыать bi

procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1 11      0  63396    152 1825940    0    0 490812     0 9200 11787  0 59  0 41  0
 2  8      0  63716    156 1825740    0    0 463864    28 9729 11322  4 59  0 37  0
 1  8      0  63808    156 1825736    0    0 467564     0 8794 10592  1 61  0 39  0

top будет показывать мощную активность kswapd0

top - 19:58:56 up 9 min,  4 users,  load average: 9.19, 5.63, 2.56

  PID USER        RES S %CPU %MEM COMMAND
   34 root     0.000t R 50.5  0.0 kswapd0


тоесть никакого oom killer очень даже долго небудет вызываться.
те проги что были запущены будут рабоать
но например ни одна команда в bash небудет срабаываться.
будет писать что fork нет памяти.

потом минут через 5 все такие процесс какйото будет убит памяти станет
побольше и наконепц можно будет через командную строку очистить папку /dev/shm

возникает вопрос почему так долго невызвыается oom killer - не знаю.

также вопрос - откуда эта бешеная дисковая активность на чтение.
в системе нет свапа так что это не свапинг.

причем активность судя по vmware на котором сидит линукс 
идет  только  чтение.


     GID VMNAME                                    VDEVNAME NVDISK   CMDS/s  READS/s WRITES/s MBREAD/s MBWRTN/s LAT/rd LAT/wr
53720774 _test_- kub - 04                                 -      1  2884.67  2884.67     0.00   167.09     0.00   1.09   0.00

записи нет.

мне неудалось узнат какие же файлы читаются.

в общем как я понял. чтение идет потому что 
когда памяти стало мало то линукс убрал из памяти библиотеки 
и еще каието полезные файлы.

и теперь чтобы чтото там выполнить системное нужно каждый раз
это все читать с диска. 

якобы отсюда бешеная дисковое чтение.

второй вопрос что делал в это время kswapd0

kswapd0 это процесс  который заведует какие страницы из памяти
пихать в своп. этот процесс его регулярно вызывает шедулер.
если памяти достаточно ( что конкретно это значит технически я не буду щас копать)
то он обратно засыпает . если памяти по его мнению осталось 
мало то он начинает работу. он определеяет какие страницы
памяти запихнуть в своп. при этом как я понял именно kswapd0 если понимает
что памяти уже никак нехватает то именно он вызывает oom-killer процесс
который киляет процессы.

это ответ на вопрос а когда же вызывается kswapd0 а когда oom-killer
kswapd0 запускает oom-killer если считает нужным.

а сам kswapd0 занимается тем что пихает страницы в своп.

возвращшается обратно к моей ситуации - у меня своп
был вообще отсуствовал так что пихать страницы было некуда.
вопрос - что тогда делал kswapd0 если никакого свапа несуществует в
моей системе.

еще раз скажу какой процесс лучший кандидат на убиение через oom killer:
системные процессы имеют маленький приоритет что логично.
юзерские процессы имеют высокий приоритет.
процесс который имеет много детей и все они сожрали много памяти - отличный кандидат на 
убиение.

я прочитал кое что. и там написано что оом киллер вызывается непросто когда
стало мало памяти. а именно когда система понимает что свап уже непомогает
что данные постоянно гоняются из свапа и в свап.

так что логично что именно kswapd0 вызывает oom killer.

важное замечание - как я понял в свап файл могут попасть только страницы
памяти у котороых исходные данные для них взяты не из диска.
например если мы запустим бинарник хрома и он считается в память.
то страницы памяти которые заняты бинарником хрома немогут 
попасть в свап раздел. потому что зачем. если надо
их можно считать просто с диска.

в линуксе страницы памяи которые не имеют исходных данных на из файла
называются анонимными страницами памяти. полуачется они содержат 
чистые данные от программы. расчетные.

я могу подтвердить что когда у нас неактивирован свап в системе
и при этом kswapd0 начинает работать как сумашедший и у нас бешеная дисковая активность
то vmstat при этом абсолютно четко показыает что обмена данным со свап разделом нет. 
что логично и коректно.

насколько я понял из мутных обяснений в инете - 
kswapd0 это процесс который то ли регуляно вызывается по шедулеру.
то ли взывается когда срабатывает флаг что мало физ памяти свободной.
итак kswapd0 - это главный слуга задача которого освобождать память.
имено он и только он освобождает память.
поэтому конечно - если в система стало мало памяти 
то именно kswapd0 вылезет и начнет пытаться освободить память.
неважно есть у нас свап раздел или нет. свап раздел это всего лишь
следствие прислуга процесспа kswapd0. не kswapd0 прислуга раздела свапа
а наоборот раздел свапа это хвост и прислужник процесса kswapd0

линукс -> мало памяти -> вызываем освобождальщика памяти kswapd0 -> он использует свап раздел

возвращшаемся к вопросу - так че он там делает когда свапа нет.
ведь он пашет на 50% судя по цпу из опыта. а делает он вот что.
он даже без свапа пашет и освобождает память. как он это делает.
он сканирует память. и определяет какие куски памяти были считаны с диска с файлов (типа
всякие там бинарники и библиотеки). и он типа делает такую штуку что он очищает память
которую эти баинарники занимают. таким образом он свобождает память. 
то есть он во первых - память освободил ? освободил.
данные из памяи оказались на диске. только ему непришлось эти данные записывать на диск
как это было бы со свап разделом. ибо они там уже есть. но конечный резултат ровно такойже
како со свапом - част данных из памяти оказаласт на диске. в смысле конечного результата 
данные были засваплены. так что в таком вот изврашенном виде свапинг на компе даже без 
свап раздела работает.
только получается что такой свапинг в 1000 раз хуже чем это было бы со свап разделом походу.
ибо свапять самые необходимые данные - бинарники.

так вот когда потом хрому или другому бинарнику надо выполнится он считывается обратно с диска.
поскольку были "засваплены" очень горячие бинарники и библиотеки то как показал мой опыт
процесс чтения будет идти постоянно с сумашедгей скоростью - 8000 iops, 400 MB\s

kswapd0 грузит проц потому что ему постоянно нужно находить как я понял 
бинарники в памяти и освобождать память и помечать что эти куски памяти надо считать с диска.
то есть чтобы выполнить квант процесса 1 ему нужно выпилить из памяти кучу бинарников
других процессов. а потом когда квант закончится ему нужно задискардить из памяти
бинарники первого процесса а потом считать бинарники второго процесса.
поэтому идет постоянная обработки памяти этим тредом. и идет постоянное чтение с диска.
потому что выполненеие каждого процесса порождает огромное количество хард фолтов.
процесс обращается к вирт памяти а ее же в памяти нет. она на диске. генерируется хард
фолт. и идет считываение. 

так что интересно
команда free она покаыывает одной цифрой buff+cache

root@escluster-data02:~# free -m
              total        used        free      shared  buff/cache   available
Mem:          36206       22635         775          41       12795       13089
Swap:             0           0           0

а если мы их хотим посмотреть раздельно то это видно в vmstat

# vmstat 1

procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 3  0      0 805956  23204 13069176    0    0   472     0 2124 1673 19  1 81  0  0
 2  0      0 804048  23204 13071420    0    0   416     0 2682 1712 34  1 65  0  0
 1  0      0 802636  23204 13072588    0    0  1368     0 3562 2418 41  0 58  0  0

в vmstat цифры показываются в килобайтах в free,buff,cache

так я сделал такой эксперимент.
я засунул в shm много данных в итоге в  лиинуксе стало мало физ памяти.
это то что я делал раньше.
при этом я заранее запустил сканирование сколько memory major faults 
происходит на опреленном процессе который регулярно вызывается для исполнения в  моем примере
это vmtools.

так вот как только в системе стало мало памяти физ то начал активно работать kswapd0
потребляя cpu. началась сильное чтение с диска. и для vmtols начали лавинно расти
число major faults. это значит что виртуальная память для этого процесса постоянно ее небыло
в физ памяти а она была на диске и ее нужно было постоянно читать.

таким образом подовердилось что когда млао памяти и нет свапа то kswapd0 
он вычищает память путем выбрасывания из нее бинарников которые можно считать с диска.
и это бесконечый процесс.

также я выяснил что пходу пьесы kswapd0 это сугубо непараллеллящийся процесс
он сугубо одноядерный. потмоу что из двух ядер kswapd0 занимал только одно на 100% 
а второе нетрогал.

также про память я выяснил. для команды free

~# free -m
              total        used        free      shared  buff/cache   available
Mem:           2000         158          66        1705        1774          17
 
как нам понмать что память забита. потому что линукс приводит кучу путанных цифр
в которых тонешь.
так вот реаьно надо смотреть вот на что.

занятая память это "used + shared" и она недолжна быть типа больше чем 98% от total

на остальные колонки можно забить болт полный. они только путают.

used - это памят как я понял под тела процессов. сумма их резидентных частей.
shared - это типа тоже забитая память толкьо она непринадлежит какомуто 
оному процессу а всем сразу илои куче сразу. но ее также как used ее нельзя вот так
просто задискардить как это можно сделать с buff\cache.

ну  в top когда мы тыкаем кнопу 'm' чтоб в граифеском виде посмотреть 
сколько памяти занято. там он показывает проценты. в целом там можно конечно
ориентирваться. что если указано 99% занято памяти то пора бить тревогу.
но там он к used добавляет buff\cache.
сам по себе buff\cache нестрашен. но если мы забиваем shared память то она также
увеличивает buff\cache поэтому если buff\cache большой изза большой шаред памяти 
то это жопа. аесли шаред память мало збита а buuff\cache большой то это фигня.

поэтому лучше на показания в top для памяти забить. там очень долгодумать надо.

а смотреть напрямую в free -m

~# free -m
              total        used        free      shared  buff/cache   available
Mem:           2000         158          67        1705        1774          18
Swap:             0           0           0

вот тут четко понятно что память забита.

used = 158
shared = 1705

used+sahred=1863

а total = 2000

поулчается занято 93% памяти. но как я понимаю free непоказывает памят которую ядро занимает или чтото
типа того.

экспериментальным путем я выяснил что  total - ( used +shared ) >= 136 MB
если эта цифра ниже то начнется уже работа kswapd0 и он начнет выбрасывать бинарники с памяти
и процессы будут постоянно читать свои бинарники с диска.

интресно что вот скажем настала пора vmtools процессу выполняться на цпу. а его бинарников нет.
у меня был вопрос - а кто ему баинарники читает с диска. какойто спец процесс  или кто.
эксперимент показал что сам процесс и читает. 
тоесть iotop показывает дисковую активность самих процессов если нехватает памяти.
то есть с точки зрения механики - страницы засовывает на диск процесс kswapd0
а обратно их читают уже сами процессы. имеется ввиду случай когда читаются бинарники с файлов.
а вот если надо считать со свап раздела то я полагаю что чтение наверное ведет тоже
kswapd0 а не процесс у которого часть данных лежит на свап разделе.

итак получается такой алгоритм - хотим узнать как там у нас 
с памятью на хосте.

взываем free -m
и сумируем used + shared.
сравниваем с total.

total >= used+shared на 136МБ как минимум.

также можно посмотреть что думает линукс на счет сколько на данный момент осталось памяти которую
если исчерпать то он начнет свапить

это смотрим в meminfo

:~# cat /proc/meminfo
MemTotal:        2048260 kB
MemFree:          150656 kB
MemAvailable:     107304 kB

так вот смотрим на MemAvailable 
по мнению линукса это примерно сколько осталось памяти если ее исчерпать то 
он начнет свапить.

в данном случае 107304 kB = 104 MB

то есть осталось 104МБ

сравним что показывает free -m

~# free -m
              total        used        free      shared  buff/cache   available
Mem:           2000         164         144        1605        1691         103

used + shared = 1769
total = 2000

разница 2000 - 1769 = 231.

помним что разница должны бытть как минимум 136МБ.

так что 231-136 = 95МБ.

поэтмоу да 104МБ ~ 95МБ.

общий вывод такой - да MemAvailable более менее правильно показыавет сколько 
еще есть свободной в реальном смысле памяти. но это значение тоже несоклько завышено.

поэтому самый железный способ это free -m
сложить used+shared+136MB и чтобы это непревысило total


посмтрим для примера другой пример

:~# free -m
              total        used        free      shared  buff/cache   available
Mem:          36206       22522         339          41       13344       13201


used+shared+136= 22699MB
total=36206
да ситуация очень даже нормальная.
свободно типа 13507MB

посмотрим что в meminfo

:~# cat /proc/meminfo
MemAvailable:   13518076 kB = 13201MB

видим что 13507MB ~ 13201MB

цифры сходятся.

вобще ксати получается самая подробная статистика по памяти это 
/proc/meminfo

там и буферы и кэши и прочее

кстати обьем shared памяти в meminfo указывется строкой
Shmem:           1644424 kB

замапленная память насоклько я понимаю это области памяти у которых есть исходник на диске.
единственное что я незнаю если память попала в свап -будет ли она счиаться замапленной
думаю что недолжна. 

ну наконец то стал в этом хаосе цифр и инструментов понятно 
на какие цифры реаьно смотреть и каким инструментом пользвоться чтобы понять 
как там дела с оперативкой на хосте

тажке стало понятно что если в /proc/pid/stat 
растет сильно во времени число major memory faults (12 столбец) значит куски процесса
которые должны быть в памяти их там регулярно нет и их приходистя читать откуда то с диска.
(это либо свап либо бинарники я так понимаю). в общем это негуд.
все что в памяти должно лежать в памяти.
то есть мажор фолты недолжны расти во времени. вот процесс загрузидся. все там устаканилось.
и все.

еще как я понял термин свапинг применяют некоректно. 
и возникла полная мешанина.

исторически свапинг это когда ОС соавала весь процесс целиком в свап.
а типа то что щас происходит через kswapd0 это вроде как назыается paging

минорные и мажорные фолты в итоге :
минорные фолты нетребуют доступа к диску а занчит быстрые
мажорные фолты требуют чтения с диска а значит мегамедленные.

чиатть обычно нужно из какогото файла или из свапа


!!!!!


кернел процессы и юзер процессы.
по факту это все одни и теже процессы. тут просто надо начать расказыать а что такое процессы.
это код который завеернут в спец оболочку которая позволяет код останавливать 
выполнять другой процесс а потом возращаться к исходному процессу и продолжать его
выполнение. прикол в том что ОС может и выолпняет ряд кода без обертки в процессы.
как минус это то что когда код выполняетмся без процесса то его нельзя остановить
он должен выполниться целиком. это имеет минусы. об этом еще поговорим.
так вот про процессы. разница теперь между юзер процессом и керел процессом.
юзер процесс выполняется на цпу в режиме ring 3 а кернел процесс выполнятеся на 
цпу в режиме ring 0.

соотвесвтенно как следствие что чтобы процесс мог выполняться на режиме ring 0
его код должен в памяти лежать в области где кернел код весь лежит.

!!! context switching
это просто напросто переключение между процессами в линуксе по шедулеру.
так что context в термиах линукса это получается типа процесс. синоним.
в википедии так и наприсано что context switch это процесс сохранения нужных 
данных для процесса или треда чтоб потом можно было его продолжить
видим что привязка идет к процессу.


!!! вопросы
как загружается линукс - ход мысли - как убрать модуль из загрузки - что такое initrd - как его разархивировать - как ядро 
разархивировать - что такое ядро ---...- как запретить java попадать в swap -... -voluntary_ctxt_switches ? -
- linux vmstat число cs in какие числа нормально.-- /proc/interrupts что в самой правой колонке назван
ния драйверов? за что отвечают самые меняющиеся строки, почему драйвер выключил ata_piix а
имя в правой колонке ata_piix все равно есть --почему неменяются cxtx числа ? 
!!!

!!! почему на ненагруженной 2 core  цпу машине  постоянно идут высокие nonv cxtx числа для top процесса
а в тоже время на высоконоагржуенной машине для sysbench процесса вообще нет context switch 
переключений 

походу ответ такой что число переключений также сильно зависит нетолько от загруженности системы
а от самого кода.
если запустить sysbench то у него вобще никаких свичей не происходит. хотя он процессор грузит на 100%
а запустить top или pidstat или mc так они процессор негрузят при этом выдают кучу вынужденных
свичей. я так понимаю это сам код такой.
мне непонятно главное как это процесс который на 100% грузит цпу он неиммет контекст свичей никаких.
как же он может невыдавливаться с цпу шедулером. ведь сам шедулер должен же на процесоор попасть
и выполниться. пока это непонятно...

походу вот что. переключение цпу из ring 3 в rin 0 ничего неделает с его регистрами. поэтому
при переключении ring 3 -> ring 0 не происходит context switch. инфо в регистрах от процесса который
выполнялся остается сохраняется. просто процесс да перестает выполняться.

далее - шедулер это не процесс как мы уже помним. это кусок кода. 

далее я нашел что есть регистры которые доступны только в ring 0. и они сохраняются 
при переключении из  между рингами.

поэтму вот как походу работает работа шедулера:
цпу в ринге 3 выполняет процесс. 
на цпу прилетает на ножку сигнал интеррапт от таймера.
цпу останавливает выполнение процесса
цпу переключается в ринг 0 ( при этом во всех регистрах инфо неменяется).
далее цпу обращается к заранее известному адресу в памяти где лежит хэгдлер интеррапта.
далее либо этот хэгдлер это и есть уже код шедулера. либо хэндлер запускает код шедулера.
при этом как замечу инфо в регистрах цпу лежит без изменений. по крайней мере 
тех регистров которые хранят инфо о процессе.
как я понял инфо необходимо для работы шедулера лежит в регистрах которые доступны только 
в ринге 0. таким образом шедулер нетрогает и ненуждается в регистрах которые хранят инфо 
относящееся к процессу. 
далее шедулер либо сразу уже имеет всю необходимую инфо для принятия решения какой процесс 
запустить на следущий тайм слайс из регистров доступных в ринге 0 (ибо там инфо неменялось при переклою
чениях между рингами). либо шедулер подкачивает в эти свои приватные регситры инфо из памяти.
таким образом на данный момент вся инфо о процессе все еще лежит на цпу без изменений и потерь.
и если шедулер решит что на данном ядре следущим запустить ровно тот же процесс что там и и исполнялся
только что то (важно !) вся информация уже лежит в регистрах ядра. таким образом 
с одной стороны процесс был прерван и шедулер отработал.
с другой стороны вся необхоимая информация по текущему процессу осталась в цпу.
далее процесс продолжил свое исполнение и для контекст небыл переключен.
отсюда разгадка - как же так шедулер отрабатывает регулярно прервывя исполнение процесса на ядре
 при этом контекст в ядре сохраняется процесса и процесс продолжает исполенние после 
 того как шедулер отработал без пререключения контекста. без выгрузки загрузки состояния регистров в цпу.
 вот мы и получили обьяснение как процесс может на ядре работать без выкидывания его данных с ядра
 и потом загрузки их обратно то есть без переключения контекста non volatile который.
 ( и при этом шедулер там регулярно работает).
 
 
отсюда важный вывод про контекст свичи такой - если приложение делает добровольные 
контекс свичи то это пофиг. оно само возврашает управление шедулеру в течение выделенного 
таймслайса. 
а начет недобровольный контекст свичей :
 это очень плохо для грузящего цпу процесса , добровольно понятное дело что он небудет отдавать
таймслайс обратно. и значит что шедулер по каким то причинам неназначает этот же процесс
на этом же ядре на следущий таймслайс. а сует туда другой процесс. а потом когда наш процесс
обратно нужно запустить то его инфо нужно загружать в цпу (регистры tlb и прочее).
 а если это процесс негрузящий ядро типа top , mc, pidstat ну неважно главное что он цпу по своей
 приоде негрузит то для такого процесса недобровольные контекст свичи пофиг.
 
 что такое контекст свич число в /proc/$pid/status  если оно увеличилось.
 это значит что когда подошел к конце тайм слайс выделенные для данного процесса то процесс
 сам его неотдал. и хочет дальше продолжаться неуспел процесс закончить все что он хотел
 но дальше по интеррапту загрузился шедулер и покаким то причинам вычистил как грязь из дырявого
 зуба всю инфо из регистров цпу от данного процесса , загрузил в регтистры цпу ифно от другого процесса
 и запустил на этом цпу на следущий тайм слайс другой процесс.
 то есть контекст свич +1 для данного процесса значит что данный процесс был выкинут с  процессора
и вся его инфо была вычищена с процессора на следующий тайм слайс.
то есть контекст свич - значит процесс выкинули из цпу. и вещи его выкинули из цпу.
если процесс малонагруженный то нам это пофиг. это очень важно понять.
от того что наш mc, top, итд небудет работать на цпу сколько то таайм слайсов подряд 
а будет работаь скажем этот тайм слайс потом пять циклов небудет потом один цикл будет - нам 
это пофиг. это никак на наш комфорт с работой с top mc итд нескажется.

конеткст свичи надо четко отслеживать у нагруженных процессов ( java, базы , почтовики , итд).

таким образрм мы разобрались как это работает конектс свич. что он значит.
разобрались когда нужно обращать внимание на растущие числа его для конкретного 
процесса когда это неважно.


для нагруженных прцессов это число важно. и то число недобровольных свичей. добровольные нам пофиг.
для ненагруженных вообще нам любые контекст свичи неважны.

ну и получается что когда мы смотрим в vmstat число context свичей  в секунду то самое по себе 
это число неговорит ничего плохо чтото или хорошо.
 
# vmstat -w 1 -S M
procs -----------------------memory---------------------- ---swap-- -----io---- -system-- --------cpu--------
 r  b         swpd         free         buff        cache   si   so    bi    bo   in   cs  us  sy  id  wa  st
 0  0            0          596          148         1106    0    0     3     3   49  126   1   0  99   0   0
 
 в данному случае cs = 126
 ну и что.
 
 видимо это все и добровольные и недоброваольные за секунду.
 
 потому что по аналогии in = 49 это число все абсолютно интерраптов за секунду.
 !!!
 
 
 *!!! теперь всплывает следйщий вопрос
 а сколько времени за 1с у нас заняло context switch. и сколько заняла
 вся вот эта требуха -  работа шедулера, обработка интеррапта.
 
 потому что время цпу складывается из :
 работа юзер кода
 работа кернел кода вызываенного юзер кодом
 работы интераптов
 контекс свичинг
 время перкочения с ринг 3 на ринг 0
 работа шедулера
 
 и как это все увязать с покзаниями в top
 
 *!!! вопрос haproxy показывает много недобрроволльных конекст свичей.
 работет в одном треде. тред грузит цпу на 50%.
 откуда недобрловарльные выдавливания. кроме хапрокси там больше ниче неработает.
 пока вопросы открытый с haproxy
 я зауцпстил тест sysbench который запустил множество контекст свичей.
 при этом цпу загрузился на 100%.
 через vmstat я увидел что в секунде генерится ~ 3 000 000 cs
 ядра было три. значит на одно ядро генерилось 1 млн cs
 я же в хапрокси получают 50-200 cs в секунде на проде.
 да эта цифра маленькая.
 но все же непонятно как влияет число cs на скорость рабты конкретного приложения.
 также я прочитал что при работе mysql типа происходит какое то количество
 cs на каждое клиентское подключение.. пока мне непонятна связь.
  я обнаружил оченть интересуню вещь.
  
  # pidstat -tw 1
  
  оно показывает число cs нетолько для процессов но и для его тредов.
  и вуялая.  я выяснил что java который имеет 126 тредов у него куча тредов 
  каждый из которых в секунду испытывает 50 cs
  итак я выяснил  что да головной тред процесса (который походу и фигирует в proc status)
  у него нет cs но у его многчисленных тредов оно конешно есть.
  у хапрокси всего один тред. так что походу это нормально что когда работающий процесс
  имеет cs недобровольные и оно зависит от самого кода програамммы. да sysbench когда мы
  теструем цпу там код так написан что там нет недобровольных cs.
  но походу реальная программа которая работает с потоками данных  с соединениями 
  типа http итд. там походу такой код который каким то образом заставляет делать cs недорволоьные.
  
  
  вот я даю картинку для java 
  
# pidstat -tw 1
  
Average:      UID      TGID       TID   cswch/s nvcswch/s  Command
Average:      111         -      1289      4.19      0.40  |__java
Average:      111         -      1290      2.99      3.19  |__java
Average:      111         -      1291      3.59      0.80  |__java
Average:      111         -      1292      6.99      0.80  |__java
Average:      111         -      1293      7.78      1.20  |__java
Average:      111         -      1294      2.59     10.58  |__java
Average:      111         -      1385    110.38      0.00  |__java
Average:      111         -      1459     18.76     50.90  |__java
Average:      111         -      1460     21.36     46.31  |__java
Average:      111         -      1461     21.36     42.91  |__java
Average:      111         -      1462     19.36     75.05  |__java
Average:      111         -      1463     19.36     41.52  |__java
Average:      111         -      1478     20.36     44.31  |__java
Average:      111         -      1479     19.96     37.92  |__java
Average:      111         -      1480     20.76     47.31  |__java
Average:      111         -      1481     21.76     51.50  |__java
Average:      111         -      1482     21.16     27.54  |__java

итак как я уже сказал. что оказалось что java очень даже имеет недоброрвольные 
контекс свичи.


~# cat /proc/1177/status
Name:   java
Threads:        126
voluntary_ctxt_switches:        22
nonvoluntary_ctxt_switches:     4

а вот то что я якобы у процесса java всего 4 недобровльных cs
это оказалась брехня.
ибо походу это верно только для головного треда.
а вот про все его другие многочислнные треды тут хаха нет никакой информации.


вот картинка для haproxy

# pidstat -tw 1

Average:      UID      TGID       TID   cswch/s nvcswch/s  Command

Average:      111      1389         -    599.67     58.80  haproxy
Average:      111         -      1389    599.67     58.80  |__haproxy

видно что у хапрокси всего один тред. так что поэтому мы и видим в /proc/$PID/status
то что процесс хапрокси имеет активно увеличивающися количество недобровольных
вытеснений.

окей.

мы видели что 1 ядро может управится с миллионом cs.
да непонятно как при этом будет себя чуствовать приложение.

тем не менее это все наводит на мысль что все приложения работающие они имеют
недобровльные cs. просто они прячутся в тредах. и то что число cs ~ 100-200 
это абсолютно норм для цпу.


прикольно еще и то что mpstat при тесте sysbemch на cs показывал %irq = 0 
то есть понятное дело irq обработка и контекст свичи это несовсем связанные вещи.
наверно...

cs связан с процессами . а %irq связан с загрузкой цпу иза прилетевшего прерывания.


походу в итоге смотрим чрез vmstat сколько cs в сек мы имеем.
если их меньше чем миллион на ядро. ну и норм.
грубо говорят примерно пока так.


остатеся теперь тока момент о том что если карточка сетевая высокорскоростная 
то надо смотреть что там у нее с интерраптами и нагрузкой на цпу. а хотя
это совсем несвязано с context switches. это же совсем про другое.
это про interrupts.
все смешалось в гоолве cs и in


*!!! важный вопрос веб свап оказывается это норм вещь.
как тогда зареиттчть чтобы java непопдалв в свап по лююблому.

*!!! как bash запускает проги - типа форкает или что там на уровне процессов.

*!!! процессы которые имеют прикрепление к tty и не имеют. и умирают ли процессы которые 
были запущены в tty консоли а потом ее закрыли


*!!! поставить линукс на uefi


  
  
  
  
 
 
 *!!! остаетчя вопрос а что там с контекс свичами когда мы работает чреез гипервизор
 на виртуалкею
 
 
 *!!! есть также вопрос как оптиматльно для сетевых карт
 распредедять irq по цпу. учитыавая что есть карты в которых есть несоклько очередей
 для потоков
 
 
 *!!! полчается такой момент если в системе много хардвейр устройств которые генеиируют 
 много irq прерываний то под них надо отдельный цпу чтоб не томрозило. под мощнгые сетвые карты.
 получается что желеки неомгут без работать без обработки со стороны цпу
 это будет тормозить приложения или пакты терять наврено.
 
 
 
 
 
 *!!! hard irq vs soft irq
 

!!!

vmlinuz - это самораспаковывающийся архив в котором хранится сжатый образ vmlinux
vmlinux  - это уже ядро бинарное (правда без символов каких то там отладки).
vm означает virtual memory. видимо 


биос читает MBR
тот запускает grub
граб запускает кернел.

кернел это  вцелом набор вызовов аля библиотек, кучка драйверов но не всех (остальные в модулях),
управление памятью.

кернел это и все и ничего. как фундамент бетонный здания. без него никуда. но залитый фундамент
еще очень далек от здания которое будет на нем стоять.

когда загрузился заработал кернел то еще ни одного процесса как я понимаю нет запущенного.




\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
!!! как линукс работает с кодом
линукс может код выполнять обернув его в процесс,
или просто как кусок кода.

может выполнять код  в ринге 0  или ринге 3

если как выполнение идет просто как кусок кода то насколько я понимаю
это когда обрабатыается интеррапт.

может выполнять процесс в ринге 3 потом переключиться к ринг 0 по сисколлу
и выполнять код ядра но в рамках этого процесса то есть как я понимаю
выполняя кусок кода ядра но в рамках юзерского процесса то шедулер 
может остановить выполннеие и перейти к другомц процессу


важно что цпу занят либо ничем, либо цпу выполняет код процесса ( и это мы видим в top),
либо код выполняется в рамках обработки интеррапта  ( и это невидно! в top )
либо выполнять код шедулера ( и это тоже невидно в top! )

получается цпу у нас работает но мы это невидим никак в top.
поэтому не вся занятьость цпу видна в top
мы частично слепы.

а так больше ничего нет никакого кода, никакого кода ядра который бы выполнялся бы
на цпу а мы этого невидели. нет никакого кода ядра который бы выполнялся и мы это бы
невидели. код ядра выполняется либо - чере сисколл вызов процесса и это мы видим в топ,
либо через обработку прилетевшего на ножку цпу интеррапта и это мы невидим в топ,
либо шедулер всплывает и выполняется и это мы невидим в топ.

сейчас все это рассмотрим.
и будем выяснять как узнавать сколько же цпу отработал там где мы невидим.



irq cpu usage
pidstat
java почему то major faults взывыает

vmware непонятно как он с irq работает от вирт машин.
они их пробраывает как это и написано в  виртуалке ?
baclkilcist updateramfs

нужно ли заниматься irq мэнеждимгом

piix4 почему он жрет интеррапты

как понять что цпу плохо с irq српавляется

proc/interrupt что там нарисовано справа название драйвера или файла модуля.
почему он там появляется если модуля незагружен.
почему irq на эластике работает если нет устройства 

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\









*!!! нераскрытые вопросы

*!!! mmu кэши цпу
*!!! непонятно когда процесс в статусе S и выолпняется код ядра.
это время защитывается процессу в джиффях что этот
процесс типа занимался цпу . работал на нем. или нет.
*!!! также непонятен момент. есть процесс java
он все время выоленяется в юзер спейсе.(вроде как)
в тоже время он постоянно в состоянии S.
есть и наоборот. постоянно в состоянии R но выполяется в кернел спейсе.
*!!! разобраться с containerd-shim
с тем как процесс запускает процесс и это показано в ps
*!!! опять немогу понять top -d 20 он будет раз в 20 скунд показыать cpu_usage.
вопрос за какой интервал он его показывает. потому что cpu_usage относится к интервалу а не моменту.

!!! очень важное дополнение про load average.
vmstat  который показыает текущее значение R он показывает число не процессов со статусом R
а число тредов !!!! со статусом R. процесс может иметь кучу тредов. (можно посмотреьт через ps axH )
поэтому Load average он работает не  с числом процессов а с числом  тредов. 




№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№№

важно что в линуксе и к8 пока невижу красивой настройки 
чтобы пользоваться NUMA как это есть esxi



# kubectl create deployment nginx --image=nginx
и сервис 

# kubectl create service nodeport nginx --tcp=80:80

# root@test-kub-01:~# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        107m
nginx        NodePort    10.96.231.223   <none>        80:31546/TCP   13m

кхм... когда попробовал опубликовать сервис в другой раз то  у меня получилась
 другая картина

# kubectl get services
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        6d20h
nginx        NodePort    10.103.21.37   <none>        80:31104/TCP   9m21s

тут видно что cluster-ip для nginx лежит в какйото неведомой сети.

и сервис недосутпен ни по

curl localhost:31104

ни по 

curl 10.10.3.21.37:80

пока это загадка .. ?

но я продолжаю описывать как это все работало при первой публикации сервиса



проверяем что nginx заработал

root@test-kub-01:~# curl 10.96.231.223:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


замечу что сервис nginx доступен по сокету 10.96.231.223:80 только внутри нод к8.
это его внутрикорпоративный адрес. снаружи вне нод к8 он доступен 
как я понимаю по другому адресу будет.


далее про сети.

как видно из команды kubeadm init мы задали сеть под поды

--pod-network-cidr=10.252.0.0/16

как видно из команды kubeadm config view


podSubnet: 10.252.0.0/16       -- это вот наша заданная сеть под поды
serviceSubnet: 10.96.0.0/12   - это некая сервисная сеть которая отображается в kubectl get svc под
которой по факту сервис опубликованный и доступен


а если мы посмотрим на мастер  ноде какие она ip имеет мы увидим вообще третью картину

root@test-kub-01:~# ip a | grep inet
    inet 172.16.102.31/24 brd 172.16.102.255 scope global ens160
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    inet 192.168.31.64/32 brd 192.168.31.64 scope global tunl0

из них только верхний ip я сам задал в сетевых настройках.

root@test-kub-01:~# cat /etc/network/interfaces

...

        address 172.16.102.31
        netmask 255.255.255.0
        gateway 172.16.102.1


хм.... и чтото тут мысль обрывается..


====================================
=====================================
=====================================
=====================================
-------------------------------------






18/09/2019

//// Concepts


кубернетес это тимпа сложная система.
мы с ней общаемся через api. можно напрямую в api а можно через утилиту 
которая преобразует команды в api. 

кубернетес api также как sql язык он не говорит как сделать чтото
он говорит саму цель что мы хотим. а система уже сама делает 
что мы хотим.

вот мы сказали через api что мы хотим добиться компонент kubernetes control plane начинает делать работу.
то есть кубернетес это как завод какойто. мы задаем задачу а потом начинается 
ее какая сложная последовательная реализация.


походу kuebernetes control plane это как у циско control plane. 
это как ilo в hp. то есть эта хрень которая управляет системой которая уплавяет циской заводом
и кубернетесом. типа кубернетес это детский сад. а control plane это воспитательский ресурс
детского сада.


control plane это как  в дестком саду комплекс сервисов разбросанных по нодам кластера.
состоит из: 

* kubernetes master (состоит из трех процесов работающих на одной ноде которая зовется master node,
	имена процессов	kube-apiserver, kube-controller-manager, kube-scheduler )
* kubelet
* kube-proxy
(при этом эти козлы неговорят про kubeadm)


kubelet и kube-proxy работают на каждой немастер ноде.
kubelet ( непутать с kubectl) он поддерживает связь с kubernetes master.
kube-proxy реализовывает сетевые фишки k8s на конкретной ноде


19/09/2019
базовые обьекты кубернетеса

pods
volumes
service
namespace

далее. есть более высокоуровненвые обьекты - controllers они создают и контролируют базовые объекты
выды контроллеров

replicaset
deployment
stateful set
job
daemon set



master node может быть реплицирована для реданданси


//// Overview 
//// What is Kubernetes



че типа умеет кубернетес
1. он может презентовать контейнер через dns или IP
load balancing - здесь пока смутно

2. кубернетес позволяет монтировать разные стораджи

дальше там полная фигня на тему что умеет k8s ничо такого


//// Overview 
//// Kubernetes Components

поговорим более подробно о k8s master. это не три процесса как написано во введении а 4-5.

* kube-apiserver = это фронтенд мастера. он предоставляет api. его можно горизонтально масштабировать
* etcd - почему то во введении ничего не скзаано про этот компонент мастера. однако далее уже
	его причисляют к компоненту мастера. это key-value база данных в которой хранится
	вся информация кластера. то ест не в конфиг файле а в etcd
	далее они пишут что если юзаете etcd то типа помните что его надо бэкапить
* kube-scheduler - принимает решение на какую ноду помешать только что созданный под
* kube-controller-manager - походу это и есть то самое ядро тот самый мозг k8s.
* cloud-controller-manager - походу он отвечает за связь с облаками и виртуалками там.


насколько я понимаю kube-apiserver в итоге передает инфо на kube-controller-manager. он в итоге все 
и решает. мозг k8. 

далее недва как указано в предисловии а три  компонента которые работают нена мастер ноде а на каждой немастер ноде.
получается они не относятся к k8 master но они относятся к control plane. к которому также относится мастер.

итак на каждой немастер ноде работают следующие control plane процессы

* kubelet - это типа агент k8s через который мастер получает инфо как там дела у ноды.

* kube-proxy походу это вот какая штука. для обращения к поду мы обращаемся к ip этого kube-proxy
	а он уже делает проброс на под ( типа как работает haproxy ). то есть скажем под имеет ip = 2.2.2.2
	а kube-proxy имеет ip = 1.1.1.1 так вот доступ к поду идет через обращение на 1.1.1.1 а куб прокси
	уже делает проброс на 2.2.2.2


* container runtime. - хрень которая позволяет запускать контейнеры на ноде. ( docker , cri-o) то есть k8 когда понял 
что хочет запустить контейнер такойто на ноде то емучтобы его запустить нужно на этой ноде обратиться к 
container runtime компоненту который крутится на этой ноде.

(еще есть kubeadm про который эти уроды молчат)



далее. есть аддоны к k8. 
наверное они тоже формально относятся к control plane. непонятно куда они ставятся на мастер  ноду
или на ноды. или похер.

аддоны:

* dns
* вэб морда
* контейнер ресурс монитор (непонятно неужели он невходит в вэб морду)
* кластер логгинг - тоже непонятно неужели к8 не умеет логировать




//// Overview 
//// The Kubernetes API


мы общаемся с к8 и говорим что хотим от него через API

сами компоненты control plane как я понял тоже друг с другом общаются через api язык


//// Overview 
//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects

когда мы добавляем  в k8 обьект то это обьект-намерение. к8 будет стараться чтобы этот обьект
был создан и жил.

обект в базе к8 имеет spec - параметры. и status - это то что там щас с ним и как


применить yaml файл

# kubectl apply -f ./vasya.yaml

yaml в себя должен включать поля



apiVersion:  - версия api их можно подключить в к8 несколько
kind:        - тип обьекта Deployment, Pod
metadata:    - имя создаваемого обьекта
spec:	     - параметры создаваемого обьекта



отступ немного в сторону в yaml язык.

верхушка файоа нужно три ---

в yaml есть два типа чегото там maps и lists.
запись

apiVersion: v1

это и есть map. мап это когда мы присваиваем переменные.
мапы могут быть вложенными


metadata:
  name: vasya
  labels:
    app: web


здесь вложенный map. metadata имеет две переменные name и labels, а lables в свою очередь имеет app.
что интерпретатор понимал разницу между

name: vasya
labels: petya

и

name: vasya
  labels: petya

надо использовать пробелы.минимум один. а так пофиг. 
тогда интерпретоатор понимает что в первом случае мы имеем две переменные name и lables
а во втором случае мы имеем одну переменную name у которой есть подпеременные.

типа никогда не юзай TAB в yaml.

мап это присвоение переменной значения.

name: vasya
age: 34

еще есть Lists. это список равнозначных элементов. 

name:
 - vasya
 - petya
 - kolya

20/09/2019
чем применение мапов отличатется от листов. когда что выбирать. например

name: vasya
age: 34

city:
 - moscow
 - dublin
 - leningrad

так вот видно что мапы это типа набор неравнозначных параметров.  vasya и 34 это элементы про разное. а moscow, dublin
leningrad это элементы про одно и тоже.

файл в yaml можно однозначно перевести в json и наоборот.
инетрпретоартор берет наш yamд перводит его в json и отправляет в кластер.


вот примеры перевода

---
apiVersion: v1
kind: Pod
metadata:
  name: rss-site
  labels:
    app: web



{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
               "name": "rss-site",
               "labels": {
                          "app": "web"
                         }
              }
}



# kubectl apply -f vasya.yaml

# kubectl get pods




пример пода для nginx


 cat vasya.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx3
  labels:
    env: test
spec:
 containers:
 - name: nginx
   image: nginx


опять же видно минимальный набор параметров
apiVersion
kind
metadata
spec



yaml nginx pod

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd


а вот так обычно выглядит типичный код для раздела spec:

spec:
   containers:
     - name: front-end
       image: nginx
       ports:
         - containerPort: 80



а именно
name:
image:
ports:


можно делать

# kubectl apply -f vasya.yaml

а можно

# kubectl create -f vasya.yaml

поканеясно. но разница судя по интерннету в том что два похода  разных тут. в одном императивный в 
другом декларативный



что важно в yaml на что напоролся

name: vasya - правильно

name:vasya - выдаст ошибку. то есть после : надо пробел

apiversion: v1 - неправильно. оно разлизает заглавные и строчные буквы

apiVersion: v1 - правильно.


пример ошибки
$ kubectl create -f vasya.yaml
error: error validating "vasya.yaml": error validating data: ValidationError(Deployment.metadata): invalid type for io.k8s.apimachinery.pkg.apis.meta.v1.ObjectMeta: got "string", expected "map"; if you choose to ignore these errors, turn validation off with --validate=false



подсказка в кусочке  ValidationError(Deployment.metadata) значит в строке метадата под деплойментом ошибка.

у меня там было

kind: Deployment
metadata:
  name:rss

а надо

kind: Deployment
metadata:
  name: rss

заценил разницу ?

//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
///  Kubernetes Object Management

три вида управления  кубернетес кластером

1. можно сделать команду из командной строки на живой обьект работающий.
это называется imperative command ( примечание команда не использует имя файла. а чисто ссылается
только либо на уже работающий обьект итд но главное что имя файла никакого не должно 
быть использовано).


2.Imperative object configuration. это когда командная строка и в ней испольузуется как минимум 
одно имя файла.


тут встречается warning - Warning: The imperative replace command replaces the existing spec with the newly provided one, dropping all changes to the object missing from the configuration file. This approach should not be used with resource types whose specs are updated independently of the configuration file. Services of type LoadBalancer, for example, have their externalIPs field updated independently from the configuration by the cluster.

не очень понятно.

вот ранее мной вводимая команда

# kubectl apply -f vasya.yaml  = это и есть пример imperative object configuration


3. declarative obect configuraion
непонял что это. только вроде понял то что kubectl натравливается на папку с файлами.



//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
///  Names

каждый уникальный обьект в к8 имеет имя и uid по которым его точно можно дентифицировать.
а  неуникальные идентификаторы в к8 называются label и annotation

имена имеют в yaml признак такой

name:


 а uid генерирует сам кластер автоматом.


//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
///  Namespaces


физ кластер к8 поддерживает несколько внутри себя логических кластеров.
их называют namespaces

неймспейесф использую недля того чтобы разделить слегка различающиеся ресурсы типа DEV от UAT
а если у нас олчень большой кластер и дохера народа. и надо ращзделить доступ 
одной кучи народа от другой кучи народа.

а для разделение dev от test надо юзать labels

посмортреть спимсок нейсмпейсов

kubectl get namespaces
NAME              STATUS   AGE
default           Active   9s  - это нейспейс дефолтовый для новых обьектов
kube-node-lease   Active   12s
kube-public       Active   12s  - это для сервисов которые доложны быть доступны только для чтения
kube-system       Active   12s  - это нейспейс для внутренних систем к8

посмотреть какие поды в каком нейспейсе работают

kubectl get pods --namespace=default
NAME    READY   STATUS    RESTARTS   AGE
vasya   1/1     Running   0          34s
$
$
$
$ kubectl get pods --namespace=kube-system
NAME                                    READY   STATUS    RESTARTS   AGE
coredns-5c98db65d4-kjpn2                1/1     Running   0          9m40s
coredns-5c98db65d4-nxks5                1/1     Running   0          9m40s
etcd-minikube                           1/1     Running   0          8m32s
kube-addon-manager-minikube             1/1     Running   0          8m48s
kube-apiserver-minikube                 1/1     Running   0          8m38s
kube-controller-manager-minikube        1/1     Running   0          8m42s
kube-proxy-5klj5                        1/1     Running   0          9m40s
kube-scheduler-minikube                 1/1     Running   0          8m54s
kubernetes-dashboard-7b8ddcb5d6-n2jzw   1/1     Running   0          9m39s
storage-provisioner           


kubectl get pods --namespace=kube-public
No resources found.
$ kubectl get pods --namespace=kube-node-lease
No resources found.

не все ресурсы кластера находятся в нейспесе.

вот как посмотреть ресурсы вне неймспейсов

kubectl api-resources --namespaced=false



21/09/2019

//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
/// Labels and Selectors

обьектам могут быть назначены labels

это некий способ группировать обьекты в логические группы - dev test qa итп

label это key-value or map или переменная и вот как выглядит назначение labels

apiVersion: v1
kind: Pod
metadata:
  name: label-demo
  labels:
    environment: production
    app: nginx
spec:
  containers:



поиск по лейбелам 

environment in (production, qa)   - ищет компоненты у которых есть label=environment который = production или qa
tier notin (frontend, backend)   - тоже самое только мы ищем tier!=frontend or tier!=backend 
partition                         - ищет компоенты у которых есть label=partition
!partition                         - ищет компоненты у которых label=partition отсутствует

еще есть такая форма поиска

environment = production
tier != frontend

думаю смысл понятен


labels испольщуются чтобы потом быть использованным в селекторах.

важно: label может включать в себя доменное имя

label:
  abc.ru/name: vasya

если у нас лейбел имеет доменное имя то тогда поиск по этому лейбелу как я понял доступен для всех 
юзеров к8. если же доменного префикса нет то поиск по лейбелу доступен только этому юзеру. при этом 
что значит доступен тому или этому юзеру непонятно. разные юзеры которые что ? разные которые залогинились
на хост с к8 и юзают kubectl для создани обьекта или что. 



поиск по label-ам


kubectl get pods -l environment=production,tier=frontend


вот делаем под с лейбелом у которого есть доменный префикс kri.io.name

apiVersion: v1
kind: Pod
metadata:
  name: n00
  labels:
    kri.io.name: nginx-dnsname
spec:
  containers:
   - name: frontend
     image: nginx

# kubectl -f vasya.yaml

ищем по лейблу

# kubectl get pods -l kri.io.name
NAME   READY   STATUS    RESTARTS   AGE
n00    1/1     Running   0          2m19s

все таки непонятно на счет видимости и невидимости лейбелов для разных юзеров.


я для эксперимента создал другого линукс юзера vasya и попытался из под него опубликовать новый под

и получил ошибку

vasya@minikube:~$ kubectl apply -f vasya.yaml
error: unable to recognize "vasya.yaml": Get http://localhost:8080/api?timeout=32s: dial tcp [::1]:8080: connect: connection refused

оказалось что нехватает конфиг файла в верной папке

поправилось эта штука вот так 

из домашней папки vasya

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

после этого контейнер создался. но как и следовало ожидать 

kubectl get pods

показал все поды. и которые вася сосздал и которые рут создал.

поэтому пока что смысла в лейбелах с доменным префиксом я вапще невижу. чтобы оно давало такого чего нет без них.




//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
///  Annotations

аннотации делаются не для селекторов. а чтобы оставлять коменты информации об обьектах.


аннтоации как и лейбелы это map key-value переменная. называй как хочешь

"metadata": {
  "annotations": {
    "key1" : "value1",
    "key2" : "value2"
  }
}




//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
///  Field Selectors


эта штука используется для того чтобы делать выборку обьектов по признакам

по дефолту этот параметр просто пустой.

типа эти два выражения одинаковые

kubectl get pods
kubectl get pods --field-selector ""


пример

cat vasya.yaml
apiVersion: v1
kind: Pod
metadata:
  name: n00
  labels:
    kri.io.name: nginx-dnsname
spec:
  containers:
   - name: frontend
     image: nginx

$
$
$
$ kubectl get pods --field-selector metadata.name=n00
NAME   READY   STATUS    RESTARTS   AGE
n00    1/1     Running   0          3m53s




//// Working with Kubernetes Objects
//// Understanding Kubernetes Objects
///  Recommended Labels


есть рекомендованные лейбелы от создателей к8.

app.kubernetes.io/name	
app.kubernetes.io/instance		
app.kubernetes.io/version	
app.kubernetes.io/component	
app.kubernetes.io/part-of	
app.kubernetes.io/managed-by




я так и непонял. есть ли от них какая то особая фишка чем если использовать
другие лейбелы свои с доменным префиксом.
или это просто чисто предложение на бумаге.


//// Kubernetes Architecture
//// Nodes
///  



kubectl get nodes

kubectl describe nodes


в выводе мы увидим такую штуку как conditions этой ноды

22/02/2019

Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason
  Message
  ----             ------  -----------------                 ------------------                ------
  -------
  MemoryPressure   False   Sat, 21 Sep 2019 14:38:59 +0000   Sat, 21 Sep 2019 14:38:54 +0000   KubeletHasSufficientMemory
  DiskPressure     False   Sat, 21 Sep 2019 14:38:59 +0000   Sat, 21 Sep 2019 14:38:54 +0000   KubeletHasNoDiskPressure
  PIDPressure      False   Sat, 21 Sep 2019 14:38:59 +0000   Sat, 21 Sep 2019 14:38:54 +0000   KubeletHasSufficientPID
  Ready            True    Sat, 21 Sep 2019 14:38:59 +0000   Sat, 21 Sep 2019 14:38:54 +0000   KubeletReady
  Addresses:


здесь важное условие "Ready" .
это типа суммарный уровень самочуствия ноды.

между прочим что такое kubeadm непонятно.


типа ссылка на архитектуру к8 - https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node


 я смотрю свойства ноды

# kubectl describe nodes test-kub-02

...


PodCIDR:                     10.252.1.0/24
PodCIDRs:                    10.252.1.0/24


и если посмтреть на самый верх когда я инициализировал кластер. то увидим что такой же переметр я и задавал для подовской сети

kubeadm init  --pod-network-cidr=10.252.0.0/16 --apiserver-advertise-address=172.16.102.31




прикольно также что через kubectl describe nodes test-kub-2 можно увидеть инфо какие же поды крутятся на этой ноде

Non-terminated Pods:         (3 in total)
  Namespace                  Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                 ------------  ----------  ---------------  -------------  ---
  default                    vasya                0 (0%)        0 (0%)      0 (0%)           0 (0%)         22h
  kube-system                calico-node-mcwxr    250m (12%)    0 (0%)      0 (0%)           0 (0%)         21h
  kube-system                kube-proxy-l85kn     0 (0%)        0 (0%)      0 (0%)           0 (0%)         21h
Allocated resources:


пррикольно что здесь мы видим несколько неймспейсов. то есть несколько виртуальных кластеров.
и то что к8 запускает свои системные ( контрол плейн) поды в отдельном kube-system кластере неймспейсе.


дальлше по книжке. есть раздел addresses

Addresses:
  InternalIP:  172.16.102.32
  Hostname:    test-kub-02


ну тут понятно. Ip адрес через который мы подключали ноду к кластеру. и его hostname согласно /etc/hostname
книжка утверждает что internalip он роутабл только внутри кластера. что значит снаружи кластера непонятно.
так как по этому ip я могу пинговать этот хост отовсюду в моей локалке.

следущий раздел conditions

Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sat, 21 Sep 2019 21:39:51 +0300   Sat, 21 Sep 2019 21:39:51 +0300   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Sun, 22 Sep 2019 19:11:04 +0300   Sat, 21 Sep 2019 21:38:21 +0300   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Sun, 22 Sep 2019 19:11:04 +0300   Sat, 21 Sep 2019 21:38:21 +0300   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Sun, 22 Sep 2019 19:11:04 +0300   Sat, 21 Sep 2019 21:38:21 +0300   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Sun, 22 Sep 2019 19:11:04 +0300   Sat, 21 Sep 2019 21:39:33 +0300   KubeletReady                 kubelet is posting ready status. AppArmor enabled


тут все понятно. 
про последний параметр " Ready". если он = uknown это значит 
что мастер к8 неполучал отлик от ноды уже течение Unknown  node-monitor-grace-period (default is 40 seconds)

diskpressure это про то что если мало места на диске


если к8 мастер видит что нода имеет статус false ( типа чтото там невпорядке но связь  с нодо есть) или unknown больше чем указано в pod-eviction-timeout (5min)
то к8 мастер направляет шедулеру задание поудалять поды с той ноды. ха! только вопрос как он это будет делать
если с нодй например сввязь потеряна. 


прикол в том что поды должны быть удалены как с самой рабочей ноды так и должны быть удалены записи об этих подах которые как я понял 
хранятся на apiserver.
значит до к8 1.5  записи о подах с недоступной ноды все равно удалялись с аписервера. а с к8 1.5 они с аписервера записи о подах
неудаляются а переходят в состояние unknown и terminating

кстати походу когда мы вот эту команду даем

root@test-kub-01:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-86c57db685-p4wkx   1/1     Running   0          22h
vasya                    1/1     Running   0          23h


то записи об этом ищутся конкретно на аписервер.

так значит я оторубил сетевую карту на дата ноде. и где то через минуту ее статус стал not ready

root@test-kub-01:~# kubectl get nodes
NAME          STATUS     ROLES    AGE   VERSION
test-kub-01   Ready      master   23h   v1.16.0
test-kub-02   NotReady   <none>   22h   v1.16.0
test-kub-03   Ready      <none>   22h   v1.16.0


далее дейтсительно только через минут 5 статус пода который работал на ноде 2 перещел в статус terminating

root@test-kub-01:~# kubectl get pods
NAME                     READY   STATUS        RESTARTS   AGE
nginx-86c57db685-p4wkx   1/1     Running       0          22h
vasya                    1/1     Terminating   0          23h


если нода отвалилась навсегда то нужно руками удалить запись о ней из к8.
сам к8 небудет ее удалять так как он бесконечно проверяет может нода выздоровела.

в к8 мастере есть компонент node controller. он как понимаю по факту входит внутрь kube-controller-manager

кстати вспомним команду которой можно посмотреть все процесыы которые входят в контрол панель точнее даже в куб мастер плюс плагины

~# kubectl get pods --namespace=kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-564b6667d7-j5vmn   1/1     Running   0          24h
calico-node-8mjqd                          1/1     Running   0          22h
calico-node-mcwxr                          1/1     Running   0          22h
calico-node-szpgd                          1/1     Running   0          24h
coredns-5644d7b6d9-b96jg                   1/1     Running   0          24h
coredns-5644d7b6d9-g5mwh                   1/1     Running   0          24h
etcd-test-kub-01                           1/1     Running   0          24h
kube-apiserver-test-kub-01                 1/1     Running   0          24h
kube-controller-manager-test-kub-01        1/1     Running   1          24h
kube-proxy-cfnx6                           1/1     Running   0          24h
kube-proxy-l85kn                           1/1     Running   0          22h
kube-proxy-n9rtp                           1/1     Running   0          22h
kube-scheduler-test-kub-01                 1/1     Running   0          24h



так вот. невижу смысла далее специфически указывать что это или то делает конкретно node controller так как
как я понимаю пока что конкретно к нему даже обратьиться нельзя. так что будем говорить что это все делает kube-controller-manager или просто к8 мастер или 
даже контрол плейн

к8 мастер проверяет состяние ноды каждые --node-monitor-period . сколько это непонятно. и как ее посмотреть оперативно непонятно

еще раз вспомнм какие неймспейсы есть в к8 по дефолту

root@test-kub-01:~# kubectl get namespaces
NAME              STATUS   AGE
default           Active   24h
kube-node-lease   Active   24h
kube-public       Active   24h
kube-system       Active   24h


kube-system - это где поды контрол плейна крутятся.
default это куда по дефолты новые поды помещаются

kube-public непомню.( вспомнил это реаурсы которые доступны только на чтение)

далее тут упоминается о штуке taints. это атрибут который применяется к ноде. и если под удовлетворяет этому тейнту
то он может быть на этой ноде запушен развернут.  если тейнты назначаются нодам то tolerants назначаются подам.
таким образом tolerants должен быть равен taints чтобы под имел право развернутся на такой то ноде.

что я получил из эксперимента. я отключил дата ноду. потом через 10 минут включил обратно.
и под который там крутился был удален. и автоматом невосстановлен.

далее. в сравнении с esxi + vcenter мы иммеем очень даже проблему. вот какую.
при потере связи с esxi нодой виртуалки котоыре на ней работают продолжают работать.
а вот если к8 мастер теряет связь с нодой  то конечно он с ней ничего неможет сделать.
но как я понимаю он как только связщь восстановится поудаляет оттуда все поды.
ну и пока что на практике пока нет связи с нодой он неразворачивает эти поды 
на другой ноде но думаб это он тоже может делать. и так получится что 
будет несоклько версий одних и тех же подов. 
ну и из этого вытекает что связь между нодами с мастером ставноится супе мега важной
частью стабильности работы подов в кластере.

в общем щас получается как - если связь от ноды с вцентр теряется это никак невлияет на работу приложений
на ноде. в сумме система для пользвователя работает стабильно.



а вот с к8 другой коленкор. их того что вижу прямо щас из опыта. 
если связь с нодой от мастера пропала то пропавшие поды не деплоятся на новой ноде и это наверное хорошо но! по возвращении связи
с нодой все поды будут удалены.
и поручается приложение будет уничтожено.
наверное можно настроить чтобы если пропала связт с нодой то  по возврашении связи поды чтобы не уничтожались. однако получается
связь между мастером и дата нодой имеет очень большое значение.

теперь щас поговорим что делает мастер при потере связи с нодой. как я понял.
он начинает в apiserver помечать запииси отвечающие за поды как unknown или terminating но не все сразу . а со скоростью 1 под в 10 секунд.
за скорост отвечает --node-eviction-rate . как его посмотреть незнаю.

прикол в том что я только недавно понял что к8 это опен сорс замена esxi. это в целом очень красивая цель.

что я из практики увидел. я вырубил ноду с подами связь у нее. и через 6 минут  ( ане через 5) ВСЕ сразу поды мгновенно были переведны в
состоние terminating. а не последовательно каждый с шагом  1 в 10 секунд. вот так пишут а одно а на практике другое.


начиная с версии 1.4 они поняли что чтото надо делать с потерей связи удалением обьектов подов при этом.
как я понял что в к8 можно назначать ноды по availability zone. как в амазоне.

так вот при приняти решения об удалении записей о подах с какой то ноды мастер смотрит а как себя чуствуют другие ноды. 
типа возможно например тупо сам мастер имеет проблему со связь с внещним миром.
так вот.  если нода недоступна в авалибилит зоне то мастер смотрит а сколько вообще нод недоступных есть в этой зоне.
если недоступных нод 55% (--unhealthy-zone-threshold) и более то  скорость перевода записей о подах в terminating уменьшается до 
--secondary-node-eviction-rate ( 1 в 100 секунд). 
логика наверно такая что типа там все работает просто у самого мастера есть проблема связи с той зоной. в общем фигная какаая то.

далее как я понимаю опять же при проблемах в зоне если при этом число нод в целом меньше чем --large-cluster-size-threshold ( по дефолту 50)
то пометка записей о подах на удаление вообще останавливается. логика опять же непонятна. 


далее они пишут если зона вся легла целиком то скорость пометки подов на удаление становисят обычной. 
где логика? типа если их 55% и более то скорость пометки уменьшается , если при этом всего нод вообще по кластеру <50 то стопится. 
а если все в зоне легкли то скорость пометки на удаление ставновистя равна дефолтовой ( 1 в 10 сек). какойто цирки и бред.

если все зоны недостуаны то мастер считает что у него самого просто сетевая карточка отвалиалсь и прекрашается полностью 
пометки на удаление до воссстановления связи.


далее.
походу кубелет тот что агент на ноде. если его запустить с ключом -register-node то он самостоятельно лезет добавлять эту ноду 
в кластер.

для того чтобы запретить публоикацию новых подов на ноде комангда

kubectl cordon $NODENAME

но при этом те поды что там уже крутятся они будут продолжать работать

далее они типа пишут что мастер к8 следит что бы цпу и памяти хватило под поды на ноде. но ему похер на остальные процессы
на ноде. хотя тоже неясно. с чем он сравнивает с пустым сервером или с его текущей загрузкой по цпу и памяти.
тем неменее дока пишет что для системных процессов на ноде надо руками резервивроать от греха подальше ресурсы.


//// Kubernetes Architecture
//// Master-Node Communication
////


список подов на мастер ноде

  Namespace                  Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                  ----                                        ------------  ----------  ---------------  -------------  ---
  kube-system                calico-kube-controllers-564b6667d7-j5vmn    
  kube-system                calico-node-szpgd                           
  kube-system                coredns-5644d7b6d9-b96jg                    
  kube-system                coredns-5644d7b6d9-g5mwh                    
  kube-system                etcd-test-kub-01                            
  kube-system                kube-apiserver-test-kub-01                  
  kube-system                kube-controller-manager-test-kub-01         
  kube-system                kube-proxy-cfnx6                           
  kube-system                kube-scheduler-test-kub-01                 



как мы видим здесь есть поды мастера

		etcd-test-kub-01                            
                kube-apiserver-test-kub-01                  
                kube-controller-manager-test-kub-01         
                kube-proxy-cfnx6                           
                kube-scheduler-test-kub-01 

как я понимаю еще раз

etcd - расрпделенное инфо хранилище данных кластера. типа его настроек
apiserver - это фронтенд мастера к8. именно к нему обращаются другие ноды кластера.
controller-manager - это самый мозг к8
proxy - сеть к8
scheduler - отвечает за то на какую ноду положить новый под

вобщем важно отметить то что ни шедулер ни мозг к8 сами в сеть нелазают и  к ним напрямую запросы неидут. а вся связь 
с внешним миром и туда и обратно идет через аписервер.

в общем что я понял. от нод к мастеру обшение идет через аписервер по https.
все окей только нужно проконтролировать  ( как я понял) чтобы 
а) клиенты только по сертификатам могли могли стучаться к аписерверу. и еще надо чтобы сертификаты 
типа были подписаны доверенным центром. чтобы можно проерять их достоверность.

от мастера ( от аписервера) обращение к кубелету идет по https но 
по дефолту нет проверки достоверности сертификата. так что это тоже нужно допиливать.
для этого нужно юзать вот такой флаг --kubelet-certificate-authority непонятно где. тем не менее. это решит проблему.

коннекты от аписервера к подам сервисам  идет по http но можно сделат чтобы по https но нет проверки 
сертфиката подлинности. также почемуто там указано что обращение от аписервера к нодам идет по http.
непонятно что они имеют ввиду под коннектом от аписервера к нодам . так как я думал что связь от аписервера к нодам
идет через кубелет. а он как указано выше связывается по https.






//// Kubernetes Architecture
////  Concepts Underlying the Cloud Controller Manager
////


эту главу  я пропустил


//// Containers
////  Images
////


вот типа чтото важное

 Kubernetes as of now only supports the auths and HttpHeaders section of docker config. This means credential helpers
 (credHelpers or credsStore) are not supported



значит тут тема о том как кубернетес увязывать с registry от докера.  главная тема дня - где и как хранить
логины пароли от доступа к  регистри.


значит берем для тренировки регистри который предлагает dockerhub.

значит я даю напоминание про регистри  докера что там и как.

мы вытягиваем из регистри образ через команду

# docker pull akrivosheevmsk/mk-dockerhub-repo:nginx-1

akrivosheevmsk - логин на dockerhub
mk-dockerhub-repo - имя репозитория. папка в которой хранятся имаджи
nginx-1 - название имаджа. в терминах докера они имя зовут как tag

для того чтобы доступ на докерхаб работал надо чтобы на дата нодах ( на мастере ненужно) был файл с кредами от докерхаб

/root/.docker/config

его нужно скопировать на все дата ноды.

как его получить.

# docker login

и он попросит ввести логин и пароль от докерхаба и сохранит его в $HOME/.docker/config

вот берем этот файл и копируем на все дата ноды.

копируем в папку root так как кубернетес сервисы контрол панели работают от root. поэтому 
именно в папке рута ищеутся креды от докерхаба.


ну параллельно скажу что пушить в приватный регистри надо командой

# docker push akrivosheevmsk/mk-dockerhub-repo:nginx-1

возникает вопрос а как выглядит имя имаджа когда мы его закачали на локальный комп. ответ также как
и на удаленном.

так как имя имадж это tag. и он что там удаленно что локально один и тотже. а все остальное 
это уже его свойства а не имя.

поэтому на локальном сторадже скачанный имадж будет выглядеть как nginx-1

root@test-kub-03:~# docker images
REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
akrivosheevmsk/mk-dockerhub-repo   nginx-1             ab56bba91343        11 days ago         126MB


замечу что вывод команды докер пишет НЕКОРРЕКТНО. он пишет что akrivosheevmsk/mk-dockerhub-repo это = REPOSITORY
хотя это логин + репозиторий. или тогда бы написали параметры репозитория что ли.

также локально можно обращаться к имаджу по его IMAGE ID 

а что если мы создали локально имадж типа такого

root@test-kub-03:~# docker images
REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
vasya                              12                  ab56bba91343        11 days ago         126MB


имя имаджа = 12. 
логин + репозиторий =  vasya 
и неппонятно где тут логин а где имя репозитория.


вообще на докерхаб есть особые системные репозитории которые непринадлежать юзерам а принадлежат саомй компании.
и в если мы скачаем оттуда имадж то он будет выглядет так

REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
alpine                             latest              961769676411        4 weeks ago         5.58MB


и как я понимаю alpine это имя репозитория. имя папки где лежать все альпайны. 
а логин это некая дефолтная невидимая херня.

ну а для нас важно что  образ из обычного частного репозитория на докер хаб такой вид параметра REPOSITORY 
без логина в виде одного слова иметь не может. образ из частного обычного репозитория обязательно должен
иметь вид

REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
login/name


так с этим разобрались.

значит важно понять что докера на локальном компе имадж идентифицируется не по его tag 
 а по его Image id 

и можно дать одному и тому же имаджу кучу tag и кучу REPOSITORY

пример


REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
akrivosheevmsk/mk-dockerhub-repo   nginx-1             ab56bba91343        11 days ago         126MB
nginx                              latest              ab56bba91343        11 days ago         126MB
vasya                              12                  ab56bba91343        11 days ago         126MB

вот видно что у имаджа один и тот же image id но tag+repository несколько штук.

зачем нам это надо знать. а затем что обратно в частный репозиторий можно засунуть только один из них
вот этот 

REPOSITORY                         TAG                 IMAGE ID            CREATED             SIZE
akrivosheevmsk/mk-dockerhub-repo   nginx-1             ab56bba91343        11 days ago         126MB

только с ним сработает команда

# docker push akrivosheevmsk/mk-dockerhub-repo:nginx-1


то есть для тго чтобы запушить некий имадж обратно в удаленный репозиторий - его REPOSITORY
должен быть обязательно такойже как он был при скачке то есть иметь вид логин+имя репозитория.

чтобы создавать алиасы в докере есть команда tag

# docker tag kuku123 akrivosheevmsk/mk-dockerhub-repo:nginx-1

она создаст для имаджа еще один алиас.

замечу что tag может быть какой угодно. типа это же имя имаджа. нам главное чтобы REPOSITORY был такойже как при скачке.

то есть можно скачать имадж nginx

переменовать его в nginx-2 и обрратно закачать в свой репозиторий. все тоже самое как с файлами а REPOSITORY 
это как бы корректный сетевой путь на шару.


ну и возвращаюсь обратно к статье в доке.

чтобы мы могли вытягивать имаджи из удаленного частного репозитоиия нужно на всех 
дата нодах иметь конфиг с кредами от этого репозитоия

/root/.docker/config

и они там пишут что если мы виртуалки держим в облаках типа амазона то там этот файл
уже будет предустванолвен и заточен на их местный репозитори хранилище. поэтому туда свой встваить будет невозможно
он его будет перетирать. поэтому держите виртуалки не в облаках

еще один вопрос а что если мы хотим использвать несоклько частных репозиториев. ответ пока не знаю 
и парится не будуь. неактуально.




//// Containers
////  Container Environment Variables
////


тут ничего не почерпнул


перед тем как начать следущую главу я покажу вот что
вот у нас есть pod = nginx-86c57db685-p4wkx



root@test-kub-01:~# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-86c57db685-p4wkx   1/1     Running   0          2d21h
vasya3                   1/1     Running   0          46h
vasya6                   1/1     Running   0          24h


и он опубликован через service nginx


root@test-kub-01:~# kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        2d23h
nginx        NodePort    10.96.231.223   <none>        80:31546/TCP   2d21h


выясним какой ip имеет этот под.

root@test-kub-01:~# kubectl describe pod nginx | grep -i ip
Annotations:  cni.projectcalico.org/podIP: 192.168.247.65/32
IP:           192.168.247.65
IPs:
  IP:           192.168.247.65



итак что интересно под сидит в одной сети

Ip = 192.168.247.65

а сервис под которым он опубликова он сидит в другой сети

ip = 10.96.231.223

и что интересно 

1. нжинкс доступен по порту 80 через ip пода = 192.168.247.65:80
2. этот же нжинкс доступен по порту 31546 по ip сервиса = 10.96.231.223
3. ip пода пингуется а ip сервиса непингуется

далее что интересно. 
в конфиге кубернетеса нет никакой 192.168.247 сети. а есть только сеть под которой сервис работает.

root@test-kub-01:~# kubeadm config view
...
  podSubnet: 10.252.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}


как ни странно в конфиге так и указано что 10.96.0.0/12 это подсеть для сервисов.

 с другой стороны если мы наберем ip a на мастере или дата ноде то там не будет никаких 10.232 или 10.96 сетей.
зато есть сеть 192.168

root@test-kub-01:~# ip a | grep inet
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host
    inet 172.16.102.31/24 brd 172.16.102.255 scope global ens160
    inet6 fe80::250:56ff:fea3:775e/64 scope link
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
    inet 192.168.31.64/32 brd 192.168.31.64 scope global tunl0
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link




вот такая схематеихника с сетями в к8. это есть тут а то есть там а этого нет тут  а то непингуется.
каша.




//// Containers
////  Runtime Class
////


kubernetes  CRI = (Container Runtime Interface)



 у нас на хосте крутится софт который заниматся обеспечением создания и работы самих контейнров.

такой софт называется как класс -  cre = container runtime environment. 
самый известный из них это докер. но уже есть и другие.

так вот в к8 решили отвязать себя от докера чтобы можно было и другим cre пользоваться. 

и теперь к8 общается с cre через CRI . CRI Это толи плагин то ли слой. через который к к8 можно пождключить любой cre.


итак раньше была схема

kubelet -> docker -> pod

сейчас стала схема

kubelet -> CRI -> docker/rkt (или другой контейнеризатор) -> pod

в качестве CRI плагина  щас доступны

docker-shim
cri-o
containerd

то есть как я понял в к8 определили стандарт под CRI но конкретную реализацию ( как плагин) оставили для сторонних разработчиков.
и вот другие наклепали реализации CRI.


но это еще невсе. это движухи связанные с к8.
параллельно на западе решили проработать контейнерную доктрину. 
и они что сделали . они  придумали OCI ( open container initiative). 
они решили разделить функционал контейнераизации на высокоуровневый кусок и низкоуровневый. низкоуровненвый как драйвер от железки 
привязан к конкреттике ОС железа и тп. а высоуровненвый кусок отвечает за высокие абстркции. 
это например дает то что теперь якобы можно обновить докер и при этом после этого ненужно перезапускать все запущенные контейнеры.
типа потому что низкоуровенвая часть без измненеий. а обновилась только высокооурненвая.


то есть раньше было так

docker -> OC(syscalls) железо итп -> pod

если раньеш докер работал с syscall операционки. то тперь с этим завязана низкоуровневая херня от докера. 
а докер работает с высокоурвненвыми штуками.


низкоуровненвая реализация этой хрени называется runc ( возможно есть и другие альтернативные продукты). так что тперь
 это выгнляди так

docker -> runc -> oc(syscalls) железо -> pod

причем общение между docker и runc идет через OCI язык стандарт протокол.


docker -> (OCI) -> runc -> oc железо -> pod



итого теперь  полная схема  для  k8 --> .,,, --> pod


kubelet -> (CRI плагин) -> docker/rkt (или другой контейнеризатор) ->  (OCI язык) -> runc -> oc железо -> pod


вот охуенная ссылка с картинками без воды - https://events.linuxfoundation.org/wp-content/uploads/2017/11/How-Container-Runtime-Matters-in-Kubernetes_-OSS-Kunal-Kushwaha.pdf

из этого документа я узнаю что runc это OCI runtime.

альтернативы ему это : runv, gvisor

ага ! в итоге я понял что ест CRI плагины а есть OCI плагины и диаграмма такая


kubelet -> (CRI плагин) -> docker/rkt (или другой контейнеризатор) ->  (OCI плагин runc) -> oc железо -> pod


CRI плагин обычно неуниверсальынй а завязан на конкретный cre. то есть он умеет работать только  с одним рантаймом. 
то есть для docker есть свой CRI плагин через который к8 работает с докером итп.

итак

значит для докера
kubelet -> ( CRI плагин dockershim) 	---------> docker 		-> runc 			-> oc железо -> pods

для containerd
kubelet -> ( CRI плагин cri-containerd ) --------> containerd	-	-> runc либо kata konatainers 	-> oc железо -> pods


отступление значит я вижу что когда в тексте пишут kubernetes CRI то можется иметься ввиду два очень важные вещи
либо плагин через который kubelet общается с high-level cre, либо сам cre который cri compatible. к сожалению
такая неразбериха. а изначально CRI это стандарт котомрому должен удволяетововрять и плагин и cre.

когда говорят OCI runtime - то имеют ввиду low level OCI compatible runtime.

когда говорят contaner runtime то тут нужно уточнять потому что могут иметь ввиду hight level runtimne как docker containerd итп
а могут иметь ввиду Low level OCI compatible runtime такой как runc. еще раз это очень важно что когда пишут
 в тексте OCI compatible runtime
то имеют ввиду именно low-level container runtime а не high level

значит для cri-o 

kubelet	->	(CRI плагин ?)			-> cri-o		-> runc/Kata containers		-> oc железо	-> pods 



немного оступаю. на сайте cri-o нащел хорошее определение что такое pod - Pods are a kubernetes concept consisting
 of one or more containers sharing the same IPC, NET and PID namespaces and living in the same cgroup.

( чуть отойду в сторону средство визуализации работы к8 это утилита lazydoctor. работает в терминале )

плагины CRI бывают встроены в kubelet , бывают выполнены в форме отдельных линукс служб, бывают всстроены внутрь high-level runtime.

опять отхожу в сторону и привожу то что прочеk на сайте докера. то что в ядре линукс там нетакого понятия как контейнеры.
контейнеры это с точки зрения ядра это некий набор отдельных фишек. которые суммарно вместе собраны чтобы нам типа
предоставить функционал контейнеров.


еще один high level runtime - frakti  сним както непонятно какие low-level runtimes он поддерживает.

также непонятно 



полезная полезняшка - как узнать а какие runtime крутятся в нашем кластере

# kubectl describe nodes | grep "Container Runtime Version:"
 Container Runtime Version:  docker://19.3.2
 Container Runtime Version:  docker://19.3.2
 Container Runtime Version:  docker://19.3.2

далее. важное замечание.
как я понимаю docker 


тут я отмечаю очень важный момент. с какогото релиза докер разделил свой монолитный бинарник на несколько 
кусков.на несколько частей. теперь это dockerd + containerd.
далее как я понял этот проект containerd разивался уже даже более менее развивался сам и оброс доп фичами.
такими фичами что теперь containerd типа как я понял сам может контейнерами рулить. вместо докера.
что при этом интересно что докер до сих пор имеет в своем составке containerd и использует его.

то есть когда мы поставили докер то связка к8 - ... - pod выглядит так

kubelet - ( cri плагин dockershim )  - ( docker - containerd ) - runc - pods

втоже время можно постававить containerd как отдельный пакет и он будет рабоать в связке с к8 как тот же самый докер 
то есть как high-level runtime.

при этом неполучится так что ты поставил докер в составке которого уж есть containerd и настроить к8 чтобы он использвал
этот containerd  без докера. надо именно ставить containerd как отдельный пакет. возможно даже что containerd из 
отдельного пакета имеет более широкий функционал чем containerd входящий в состав докера.


если установить докер и посмотреть через ps aux то четко видно что  в процессах  есть бинарник dockerd и есть заапущенный 
бинаркник containerd.


также непонятно что если conatinerd уже умеет все что  нужно про контейнеры то что тогда делает бинарник dockerd.

или в случае отдельного использования conatinerd недостающий функционал который тянул dockerd тянет cri плагин cri-contanerd.

пока это все непонятно.

если застопить сервис dockerd  а потом его запустить из командной строки  параметры при этом никакие ненужны
то он четко выведет в консоль кучу своей служебной информации где в приницпе должно быть написано что он запускает 
containerd но у меня почему то он об этом факте непишет.

замечание. дейтсивтельно важный момент что к8 он сам контейнеры не умеет запускать формировать. к8 это только средство
управления группой контейнеров. типа как говорят по умному средство оркестрации контейнеров. вроде очевидно но нужно
это явно указать

опять же хочу сказать что неисключено что comntainerd входящий в состав докера имеет урезанный фуннкционал чем
conatinerd идущий как отедьный пакет для роли high-level runtime


далее.
команда 
docker export

берет контейнер и экспортирует файловую систему которая внутри контейнера в сжатый tar файл

вот например как можно посмотреть что там внутри контейнера на основке имаджа ubuntu

docker export $(docker create ubuntu) | tar -C rootfs -xvf -


а вот интересная картина про то как выглядят родители и дети для докера и его компонентов


10897 ?        Ssl    8:56 /usr/bin/containerd
13072 ?        Sl     0:07  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/fc9f78112ce434e06c9ef829fdc5039776a95f07
13138 ?        Ss     0:00  |   \_ /pause
13073 ?        Sl     0:07  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/c0a2c4a93de6f202bc8a80db04a43bc81245e609
13125 ?        Ss     0:00  |   \_ /pause
13262 ?        Sl     0:07  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/2f779aad7fe7c92186e42e9d0ec3cff7f296074c
13287 ?        Ssl    2:27  |   \_ /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=test-kub-02
16852 ?        Sl    17:31  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/07edab7da304dbc63c6c7ade867cfc4945289297
16870 ?        Ss     0:02  |   \_ /usr/bin/runsvdir -P /etc/service/enabled
16972 ?        Ss     0:00  |       \_ runsv confd
16977 ?        Sl     0:29  |       |   \_ calico-node -confd
16973 ?        Ss     0:00  |       \_ runsv bird6
17200 ?        S      0:29  |       |   \_ bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg
16974 ?        Ss     0:00  |       \_ runsv felix
16978 ?        Sl    30:27  |       |   \_ calico-node -felix
16975 ?        Ss     0:00  |       \_ runsv bird
17199 ?        S      0:31  |           \_ bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg
18494 ?        Sl     0:03  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/40c888985892c64fede2bd01f9893a9000eed4e1
18516 ?        Ss     0:00  |   \_ /pause
18630 ?        Sl     0:03  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/50ad07de56997b9e78f5ee7fa6274bf3c7d83cd7
18661 ?        Ss     0:00  |   \_ nginx: master process nginx -g daemon off;
18693 ?        S      0:00  |       \_ nginx: worker process
17878 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/879e5ccd9427659a86389276a918f44a7027271a
17896 ?        Ss     0:00  |   \_ nginx: master process nginx -g daemon off;
17932 ?        S      0:00  |       \_ nginx: worker process
21124 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/9b97ab0210cdb9148969cdfcd3f38eb2e026288d
21186 ?        Ssl    0:00      \_ mysqld
11008 ?        Ssl   41:17 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock


видно что сервис dockerd он как то сам по себе.

далее

containerd - 

еще раз отступлю в стороне и сравню цеопчку к8 ... pods для docker и для containerd (когда тот high level runtime)

docker
kubelet - cri plugin dockershim - docker - containerd - runc - pods

containerd 
kubelet - cru plugin cri-containerd - containerd - runc - pods

как видно в случае containerd хопов меньше.




// отступление


root@test-kub-01:~# kubectl cluster-info
Kubernetes master is running at https://172.16.102.31:6443
KubeDNS is running at https://172.16.102.31:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

// конец оступления


// отступление

наконец узнал как посмотреть какие поды на каких нода лежат

root@test-kub-01:~# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE    IP                NODE          NOMINATED NODE   READINESS GATES
nginx                    1/1     Running   0          36m    192.168.29.193    test-kub-04   <none>           <none>
nginx-86c57db685-p4wkx   1/1     Running   1          4d6h   192.168.247.68    test-kub-03   <none>           <none>
vasya3                   1/1     Running   1          3d7h   192.168.247.67    test-kub-03   <none>           <none>
vasya6                   1/1     Running   0          2d9h   192.168.246.197   test-kub-02   <none>           <none>

// конец отступления

прикольно то что контейнеры это не так как например виртуализация. поэтому там нет вот этого слоя гипервизора.
поэтмоу нет проблемы как снять мощность с процессора.

контейнеры это типа просто как обычный сервис в линуксе. и это очень хорошо с точки зрения перфоманса.

получается что Low-level runtime он просто запускает обычные линукс процессы только с доп опциями. поэтому работы рантайм
это по большей части создать процесс убить процесс. и больше там нет никакой внутренней магии которая есть при работе 
гипервизора. с контейнерами нет вот этого вот оверхеда. 

насколько я понимаю миграция контейнеров с ноды на ноду в к8 это не то что в esxi.  они банально тушатся на одной ноде и пересоздаются 
на другой ноде. максиумум что они наверное могут делать это убивать по одной реплике на старой ноде и добавлять по +1 реплике
на новой ноде.
в связи с этим всем как я понимаю совсем нет проблемы если на одной ноде один рантайм а на другой ноде другой рантайм.


********************************************************************************************************************************************

как ставить кубернетес на дата ноду.



ставим на дата ноду c docker-ом


1. надо удалить все следы старой установки предыдущего кубернетеса.

a) # kubeadm reset

b) но этого недостаточно.
надо проверить /etc/default/kubelet и удалить либо изменить там на коректный high-level runtime который будет кубелет использовать

c) /etc/systemd/system/kubelet.service.d 
проверить нет ли там лишнего и сам конфиг


д) проверить какие стоят на ноде рантаймы

# dpkg -l | grep -E "docker|container|singularity"
# ps aux | grep -E "docker|container|sycri" 
какие ненужны удалить 
какие нужно доставить

тут же скажу как узнать к какому  systemd unit относится бинарник который мы обнаружили через ps aux. берем pid бинарника и

# systemctl status  27952

все ноду зачистили от старого. рантайм если нужно доставили



1.5 на этом шаге ставим нужный нам high-level runtime
для docker это

# apt install ebtables ethtool
# apt remove docker docker-engine docker.io
# apt install apt-transport-https ca-certificates curl software-properties-common
# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
# add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
# sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable edge test"




2. ставим кубелет kubedam и kubectl 

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo 'deb http://apt.kubernetes.io/ kubernetes-xenial main' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl




3. идем на мастер ноду.  и генерируем kubeadm join

# kubeadm token generate
# kubeadm token create <generated-token> --print-join-command --ttl=0

поидее все. дата нода присоединена к кластеру и готова для приема контейнеров.




4. проверяем что дата нода test-kub-02 новая успешно присоединена

# kubectl get nodes -o wide
NAME          STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
test-kub-01   Ready    master   6d22h   v1.16.0   172.16.102.31   <none>        Ubuntu 16.04.2 LTS   4.4.0-62-generic   docker://19.3.2
test-kub-02   Ready    <none>   18s     v1.16.0   172.16.102.32   <none>        Ubuntu 16.04.2 LTS   4.4.0-62-generic   docker://19.3.2

заодно виден рантайм данной дата ноды.



**********************************************************************************************************************************************

далее.
еще раз как удалить ноду из кластера

# kubectl drain test-kub-03 --ignore-daemonsets
# kubectl delete nodes  test-kub-03


далее.
замечу. если мы ставим docker то щас сам собой ставим и containerd в форме отдельного пакета

# dpkg -l | grep -E "docker|container|singularity"
ii  containerd.io                      1.2.6-3                             amd64        An open and reliable container runtime
ii  docker-ce                          5:19.03.3~1.2.beta2-0~ubuntu-xenial amd64        Docker: the open-source application container engine
ii  docker-ce-cli                      5:19.03.2~3-0~ubuntu-xenial         amd64        Docker CLI: the open-source application container engine
#


*****************************  ставим cri-o на дата ноду




# add-apt-repository -y ppa:projectatomic/ppa
# apt-get update
# apt-get install -y cri-o-1.15
# systemctl enable crio
# systemctl start crio

# crictl info -  проверяем что cri работает нормально


# modprobe br_netfilter

правим /etc/sysctl.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1

#  sysctl -p /etc/sysctl.conf

и еще чтобы этот модуль br_netfilter грузился при перезагрузке

в /etc/modules прописываем

br_netfilter


для проверки что модуль br_netfilter грузится при перезагрузке , перезагружаем ноду. и проверяем

# lsmod | grep br_netfilter

и еще 

#  sysctl -p /etc/sysctl.conf
если ошибки невыдает то все окей.


один из пиздецов установки.
после установки cri-o она неможет скачивать имаджи если в явном виде неуказать docker.io 

то есть команда

# crictl pull busybox 

выдаст ошибку

ImageStatus "calico/cni:v3.8.2" from image service failed: rpc error: code = Unknown desc = no registries configured while trying to pull an unqualified image, add at least one in either /etc/crio/crio.conf or /etc/containers/registries.conf

а если дать команду

# crictl pull docker.io/busybox

если залезем в конфиг /etc/crio/crio.conf то там написано что типа что по дефолту крио знает про docker.io но по факту она незнает.
# List of registries to be used when pulling an unqualified image (e.g.,
# "alpine:latest"). By default, registries is set to "docker.io" for
# compatibility reasons. Depending on your workload and usecase you may add more
# registries (e.g., "quay.io", "registry.fedoraproject.org",
# "registry.opensuse.org", etc.).
#registries = [
#       "quay.io",
#       "docker.io",
#]

поэтому надо в явном виде в конфиге указать что регистри с которого качать имаджи это docker.io

# List of registries to be used when pulling an unqualified image (e.g.,
# "alpine:latest"). By default, registries is set to "docker.io" for
# compatibility reasons. Depending on your workload and usecase you may add more
# registries (e.g., "quay.io", "registry.fedoraproject.org",
# "registry.opensuse.org", etc.).
#registries = [
#       "quay.io",
#       "docker.io",
#]


registries = [
        "docker.io",
]


замечу что это все можно проверить до установки kubelet на хост.  
и это НАДО исправить до установки kubelet иначе он нормально не встанет на хосте.
и эта нода в кластере будет в состоянии NotReady.

это был первый пиздец установки cri-o




далее проверяем что контейнеры можно успешно создать через cri-o

для этого используем универсальный cli от кубернетеса для cri совместимых high-level runtimes. = crictl  ( которая входит в cri-tools)

правда для этого помимо имаджей которые мы качаем из регистри нужно еще иметь под рукой json файл.
пример 


# cat pod-config.json
{
    "metadata": {
        "name": "nginx-sandbox",
        "namespace": "default",
        "attempt": 1,
        "uid": "hdishd83djaidwnduwk28bcsber2"
    },
    "log_directory": "/tmp",
    "linux": {
    }
}




 
# crictl runp pod-config.json

проверяем что pod создался 

#  crictl pods

# crictl inspectp ee002afffc6b6
{
  "status": {
    "id": "4fab35b5cdc2019aba49e634d4087fe52497075de218f59cb620c1b9cd1c390e",
    "metadata": {
      "attempt": 1,
      "name": "nginx-sandbox",
      "namespace": "default",
      "uid": "hdishd83djaidwnduwk28bcsber2"
    },
    "state": "SANDBOX_READY",
    "createdAt": "2019-09-28T21:49:31.679226027+03:00",
    "network": {
      "ip": "10.88.0.5"
    },
    "linux": {
      "namespaces": {
        "options": {
          "ipc": "POD",
          "network": "POD",
          "pid": "POD"
        }
      }
    },
    "labels": {},
    "annotations": {}
  }
}


замечу что pod это только оболочка конверт для контейнеров внутри. который мы еще внутрь невпихнули.





crictl это cli к high-level runtim который отвечает k8 cri стандарту. это возможность проверить high-level runtime без установленного kubelet.
это как cli к докеру. только cli к докеру оно работает только  с докером. а crictl со всеми k8 cri слвместимыми high level runtimes.

поскольку мы обращаемя к high-level runtime то можно указать какой low-level runtime мы хотим использовать. по дефолту это runc

# crictl runp --runtime=runc pod-config.json


еще замечу. как я понимаю контейнеры которые созданы через runv они имеют прямой доступ к кернелу. 
и гугл считает что это небезопасно. что контейнеры имеют прямой доступ к кернелу.и гугл придумали gVisor  который тоже low -level runtime
но он как то так создает контейнеры  которые не имеют прямой доступ к кернелу. а имеют доступ к gvisor который уже имеет доступ к кернел.


так вот crictl runp создает только пустой pod внутри которого нет никаких контейнеров.

на следующем шаге можно создать контейнер и всунуть его в под.


# crictl create 4fab35b5cdc20 container-config.json pod-config.json

где 4fab35b5cdc20 - id уже ранее созданного пода. и еще надо создать pod-config.json. в любом случае эта команда почему то неработает. и 
хватит на этом.

будем считать что мы проверили что cri-o работает.


далее продолжает установку этой дата ноды в кластер схеме описанной выше начиная с пункта 2. 



важно!! теперь прописываем настройки для kubelet
в /etc/default/kubelet  

Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/crio/crio.sock


вот еще полезная команда

# systemctl show kubelet
чтоб посмотреть файлы и настройки которые жрет сервис kubelet при запуске


очень полезная команда для траблшутинга

root@test-kub-04:/etc/crio# curl -v --unix-socket /var/run/crio/crio.sock http://localhost/info
*   Trying /var/run/crio/crio.sock...
* Connected to localhost (/var/run/crio/crio.sock) port 80 (#0)
> GET /info HTTP/1.1
> Host: localhost
> User-Agent: curl/7.47.0
> Accept: */*
>
< HTTP/1.1 200 OK
< Content-Type: application/json
< Date: Sat, 28 Sep 2019 20:42:18 GMT
< Content-Length: 239
<
* Connection #0 to host localhost left intact
{"storage_driver":"overlay","storage_root":"/var/lib/containers/storage","cgroup_driver":"systemd","default_id_mappings":{"uids":[{"container_id":0,"host_id":0,"size":4294967295}],"gids":[{"container_id":0,"host_id":0,"size":4294967295}]}}root@test-kub-04:/etc/crio#




************ конец установки cri-o 

очень полезная команда
как удалять системные поды.
ибо простой командй ты ее нифига неудалишь

kubectl delete pods --namespace=kube-system calico-node-4sq8z













перед тем как пробовать ставить kubeadm join  на хост который мы полчили путем клонирования из другой дата ноды
надо сделать

# kubeadm reset   <--- !!!!!!!!!!!!!




установка containerd
помни об особом файле /etc/systemd/system/kubelet.service.d/0-containerd.conf




установка cri-o





было предположение что kubelet не поднимается изза того что cri-O + kubelet имея cgroup driver = systemd
менял и у cri-o и у kubelet   cgroup driver на  = cgroupfs

делается это так

для cri-o это 

/etc/crio/crio.conf

~# cat /etc/crio/crio.conf | grep cgroup
cgroup_manager = "cgroupfs"

для кубелет это 

Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_CGROUP_ARGS

но оказалось что это не было причиной незапуска kubelet. но в целом получается попутнонаучился 
менять cgroup driver для этих двух компонентов. но как сказал это было ненужно. причина незапуска kubelet была не в этом.


про cgroup. это штука которая позволяет нарезать лимиты ресурсов ( цпу память итп) для процессов линукса
а cgroup driver это штука которая дает доступ функционалу cgroup. что такое это более точно пока неясно.

паралельно скажу что такое namespace. это такая штука которая позволяет друг от друга изолировать процессы 
итп. эта штука для изоляций чего то от чего то в линуксе. например мы можем гарантировать что процесс
не сможет увидеть другие процессы. или процесс будет невидеть какието папки на фс.

итак namespace это инстурмент для изоляций. а cgroup это инсттрумент для лимитов.

далее.

очень интеересно можно посмотреть свойства сервиса с каким параметрами он был запущен и в каких файлах он также ищет свой конфиг

#  systemctl show  kubelet



ExecStart={ path=/usr/bin/kubelet ; argv[]=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_CGROUP_ARGS

Environment=KUBELET_EXTRA_ARGS=--container-runtime=remote\x20--container-runtime-endpoint=unix:///var/run/crio/crio.sock KUBELET_KUBECONFIG_ARGS=--bootstrap-
EnvironmentFile=/var/lib/kubelet/kubeadm-flags.env (ignore_errors=yes)
EnvironmentFile=/etc/default/kubelet (ignore_errors=yes)

FragmentPath=/lib/systemd/system/kubelet.service
DropInPaths=/etc/systemd/system/kubelet.service.d/0-crio.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf





==============================================================================================


перед установкой kubelet  нужно на машину поставить runtime. ( docker, conatimerd, cri-o)
проверить что оно может успешно создать контейнер само.

после этого поставить кубелет.

после этого надо прописать в настройках кубелета какой рантайм ему юзать.

  и вот обратная задача . у нас есть дата нода. надо понять какой рантайм юзает кубелет.

( есть и еще одна задача понять через какой cgroup driver работает кубелет но это отдельная задача)

посмотрим через ps aux


root@test-kub-02:~# ps aux | grep kubelet


kubelet на мастере

/usr/bin/kubelet 
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 
--kubeconfig=/etc/kubernetes/kubelet.conf 
--config=/var/lib/kubelet/config.yaml 
--cgroup-driver=cgroupfs 
--network-plugin=cni 
--pod-infra-container-image=k8s.gcr.io/pause:3.1



kubelet на дата ноде на runtime docker



/usr/bin/kubelet 
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 
--kubeconfig=/etc/kubernetes/kubelet.conf 
--config=/var/lib/kubelet/config.yaml 
--cgroup-driver=cgroupfs 
--network-plugin=cni 
--pod-infra-container-image=k8s.gcr.io/pause:3.1


kubelet на дата ноде на runtime containerd

 /usr/bin/kubelet 
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 
--kubeconfig=/etc/kubernetes/kubelet.conf 
--config=/var/lib/kubelet/config.yaml 
--container-runtime=remote 
--runtime-request-timeout=15m 
--container-runtime-endpoint=unix:///run/containerd/containerd.sock


как видно только  в ряде случаев можно увидеть какой рантайм прописан в кубелете.

наша исходная задача понять к какому рантайму обращется кубелет. дело в том что кубелет может не мочь успешно запуститься.
поэтмоу способ что выше в целом бесполезен. так как сервис просто может не иметь возможност стартануть изза ошибок. 
например недоступности
рантайма. поэтому надо искать в  настройках systemctl для этого сервиса kubelet

(если же кубелет стартанут и нода успешно присоеденилась к кластеру то узнать рантайм для ноды нет проблем
 kubectl describe noda_name )

главный смысл такой - видно что при старте кубелета ему можно указать сокет через который работает runtime.

# kubectl ... --container-runtime-endpoint=unix:///run/containerd/containerd.sock

экспреимантальным путем было установлено что прописать этот параметр можно в нескольких местах.
эти места обусловлены в основном свойствами самой системы systemd

1) # cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS


сюда можно вставить как строковый параметр.
но  это плохо так как  как только мы удалим пакет kubelet тот этот файл будет удален и если  мы поставим kubelet заново
то нужно будет настройку прописывать заново.

2) вставить настройку в файл var/lib/kubelet/kubeadm-flags.env

но тут тоже самое как пункт 1. как тока удалим поставим пакет то настройка будет перетерта.

3) /etc/default/kubelet

сюда можно вставить настройку и она не будет перетерта при удалении и установке пакета заново.


вот как это будет выглядеть для рантайма containerd

KUBELET_EXTRA_ARGS= --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock


для cri-o это будет  выглядеть как 

KUBELET_EXTRA_ARGS= --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/crio/crio.sock


4) также  можно вместо пункта 3 создать файл /etc/systemd/system/kubelet.service.d/0-containerd.conf

# cat 0-containerd.conf

[Service]
Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"

видно что настройка такая же самая что и в /etc/default/kubelet 
только добавили секцию [Service]

это потому что  настройка через 0-containerd.conf обусловлена свойствами архитектуры самой systemd

а настройка через пункт 3 обусловлена тем что этот файл указан в конфиге /etc/systemd/system/kubelet.service.d/10-kubeadm.conf


итак если мы собираемся использовать недефолтный docker как рантайм то у нас есть два места где мы обязаны хотя бы в одном из них 
прописать сокет нашего рантайма.


таким образом теперь мы знаем где прписывать наш рантайм в кубелете после того как мы поставили кубелет из пакета. но перед присоедининением
ноды к кластеру через kubeadm join



далее.
посмотреть какие конфиг файлы испольщует кубелет сервис можно через 

# systemct show kubelet

там откроется простынь . я из нее выбрал самый смак

[Service]
Environment=

"KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"


Environment=

KUBELET_EXTRA_ARGS=--container-runtime=remote\x20--runtime-request-timeout=15m\x20--container-runtime-endpoint=unix:///run/containerd/containerd.sock 

KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf\x20--kubeconfig=/etc/kubernetes/kubelet.conf 

KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml


еще была тема что старая версия cri-o не работает коректно в связке с кубелетом когда онииспользуют дефолтовый драйвер systemd для cgroup.
и надо было и кубелет и cri-o переключать на cgroupfs драйвер для cgroup. 

это для кубелета делается через опцию  --cgroup-driver=cgroupfs

а для обратно systemd это --cgroup-driver=systemd

опцию задаем тоже через пункт 3 или пункт 4.

это для кубелета.


для cri-o задаем через конфиг

/etc/crio/crio.conf

но как оказалось  новая версия cri-o рабоатет нормально в дефолтовой поставке.

более того создатели к8 пишут что сам линукс работает с cgroup через systemd поэтому нежелаььно использовать одновременно два драйвера и sytemd и cgroupfs
это может вызывать конфликт в распределении ресурсов. 


ставим cri singularity

в общем есть старый singularity он как докер но у него нет k8 cri support. зато он есть в пакетах. но он нам неподходит.
singularity with k8 cri support надо ставить только из исходников



выводы осле ебалы с cri-o

1. cri-o гавно. на нем нужно ставить правильные cgroup драйверы иначе неработает. и в целом его полноценно завести
удалось только как то вначале а потом никак.

2. сеть cni это вторая по геморою комппонент. calico у него там какие то заебы с BGP . в итоге нахуй этот компонент.
а вот flannel гораздо менее капризен и более надежен. итак выбор CNI за flannel

3. единственно нормально работающими рантаймами являются docker и containerd.

4. хорошо прописано с подробностями установка того или иного рантайма в доке к8 , глава про CRI.

5. проработать в обратном порядке все вкладки открытыве в хроме

***************************************************************************************************


возвращаемся к CRI runtime class

это тема о том что подам при запуске можно указывать на каком hight lebel runtime запускаться

типа эти поды на докере эти на containerd эти на говно cri-o

чтоб эта херная работала надо чтобы runtimeclass фича была активирована на apiserver и кубелетах.
по дефолту она актвивроана. как проверить непонятно ну ихер с ним.

что стало дальше понятно. что эта фича cri runtime class она про выбор Low-level runtime а не high 
level runetime. эти пидары об этом непишут.

зачем это надо например. докер например это обычные контейнеры. софт конейнеры. они дают доступ  контейнерам напрямую к 
ядру. а например gvisor low level runtime он недает контейнерам доступ я кернелу линукса . 
это реально надо нпример тем кто раздает контейнеры как публичный сервис.  и нельзя чтобы прорвыв гнилого софта дал доступ
к другим контейнерам.

в общем немного смешно. вначале все жалововались что виртуализация это больллльшой и платный оверхед.
потом перешли на более легкую контейнеризацию и бесплатную. а щас уже обратно начали накручивать фишки чтобы дать 
доп изоляцию которую дают гипервизоры.



RuntimeClass это формально  unit  в базе данных кубернетес


пример как юзать runtimeclass

https://asciinema.org/a/215564



есть еще такая хрень как shim. с ней пока мало понятно. 
там есть картинки что над каждым контейнером висит родителбьский процесс shim. на этом пока все.


перед тем как начать понимать и юзать runtimeclass надо настроить несколько low-level runtimes 
чтобы их можно было уже выбирать с помощью runtimceclass

итак настроим containerd чтобы мон мог работать с   runc и с kata
 

***** 30/09/2019 установка kata на containerd*****************************************

ctr - containerd cli

containerd умеет работать нетолько с runv но и kata.



походу kata эта штука которая создает контейнер используя реально гипервизом quemu.  получсется kata это типа как JVM. 
и мы там прописываем и сколько памяти отдаем контейнеру и сколько процессоров.

они пишучт что классические конейнеры - software контейнеры мы должны использовать для доверенного софта.
а для разного непонятно какого говнософта надо использовать sandbox контейнеризаторы такие как kata или gvisor 

имеем containerd уже установленный.


$ ARCH=$(arch)
$ BRANCH="${BRANCH:-master}"
$ sudo sh -c "echo 'deb http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/xUbuntu_$(lsb_release -rs)/ /' > /etc/apt/sources.list.d/kata-containers.list"
$ curl -sL  http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add -
$ sudo -E apt-get update
$ sudo -E apt-get -y install kata-runtime kata-proxy kata-shim


# mkdir -p /etc/kata-containers/
# cp /usr/share/defaults/kata-containers/configuration.toml /etc/kata-containers


прописываем kata в containerd, берем cat /etc/containerd/config.toml
ищем место

 [plugins.cri.containerd.default_runtime]
        runtime_type = "io.containerd.runtime.v1.linux"
        runtime_engine = ""
        runtime_root = ""
      [plugins.cri.containerd.untrusted_workload_runtime]
        runtime_type = ""
        runtime_engine = ""
        runtime_root = ""


и вставляем туда кусок для ката чтобы было так как у меня ниже.


 [plugins.cri.containerd.default_runtime]
        runtime_type = "io.containerd.runtime.v1.linux"
        runtime_engine = ""
        runtime_root = ""
      [plugins.cri.containerd.runtimes.kata]
         runtime_type = "io.containerd.kata.v2"
     [plugins.cri.containerd.runtimes.kata.options]
       ConfigPath = "/etc/kata-containers/config.toml"
      [plugins.cri.containerd.untrusted_workload_runtime]
        runtime_type = ""
        runtime_engine = ""
        runtime_root = ""


по идее все. но если наша дата нода является виртуальной машиной esxi. то kata container незаработает. а точнее незарабоает 
qemu на котором базируется kata.
а qemu незаработает так как ему для запуска нужны доступными спец опции процессора который отвечают за виртуализацию.
поэтому надо активировать для виртуальной машины опцию "Expose hardware-assisted virtualization to the guest operating system". 
для этого надо чтобы VM была версии 9 и выше. и опция "Expose hardware-assisted virtualization to the guest operating system"
 доступна для активации только через вэб морду. 
итак повысили VM ver до 9. открыли веб морду вцентра - правая кнопка на VM - edit settings - VM hardware - CPU - ищем глазами 
опцию "Expose hardware-assisted virtualization to the guest operating system" и ставим рядом с ней галочку. 

итак эта опция позволяет софту внутри виртуалки юзать опции виртуализации процессора. 
теперь qemu может успешно стартовать. теперь kata может успешно создавать контейнеры.


через ctr - это cli от containerd  тестируем - создаем контейнер с kata


$ sudo ctr image pull docker.io/library/busybox:latest
$ sudo ctr run --runtime io.containerd.run.kata.v2 -t --rm docker.io/library/busybox:latest hello sh

если все верно. то контейнер создатся. и ошибок не будет.

готово.

можно еще запустить проверку что у ката все хорошо. 
это

# kata-runtime kata-check
WARN[0000] modprobe insert module failed: modprobe: FATAL: Module vhost_vsock not found in directory /lib/modules/4.4.0-62-generic
  arch=amd64 error="exit status 1" module=vhost_vsock name=kata-runtime pid=7280 source=runtime
ERRO[0000] kernel property not found                     arch=amd64 description="Host Support for Linux VM Sockets" name=vhost_vsock pid=7280 source=runtime type=module
System is capable of running Kata Containers
System can currently create Kata Containers


видно что ката хочет модуль vhost_vsock которого нет я данном кернеле. однако ката пишет что тем не менее оно может работать на 
такой системе. что делает модуль vhost_vsock я не нашел.


что еще хочу заметить 

берем конфиг containerd
cat /etc/containerd/config.toml

его кусок

[plugins.cri.containerd.default_runtime]
        runtime_type = "io.containerd.runtime.v1.linux"
        runtime_engine = ""
        runtime_root = ""

      [plugins.cri.containerd.runtimes.kata]
         runtime_type = "io.containerd.kata.v2"
         [plugins.cri.containerd.runtimes.kata.options]
           ConfigPath = "/etc/kata-containers/config.toml"

      [plugins.cri.containerd.untrusted_workload_runtime]
        runtime_type = ""
        runtime_engine = ""
        runtime_root = ""


в нем видно что в контейнерд прописывается  default low-level runtime
таковым типа является io.containerd.runtime.v1.linux

потом мы указали свой дополнительный рантайм кастомный io.containerd.kata.v2
в нем мы указали что мол все подробности ищи в конфиге etc/kata-containers/config.toml

а еще есть возможность указать рантайм для подов который помечены как untrusted - plugins.cri.containerd.untrusted_workload_runtime 
в нашем случае он пустой.



далее как конкретно теперь указывать для containerd какой low-level runtime использовать при запуске контейнера.

для запуска дефолтного рантайма

# ctr images pull docker.io/library/redis:latest
# ctr container create  docker.io/library/redis:latest redis3 
либо так
# ctr container create --runtime io.containerd.runtime.v1.linux  docker.io/library/redis:latest redis4


для запуска ката рантайма

# ctr container create --runtime io.containerd.run.kata.v2  docker.io/library/redis:latest redis5


смотрим какие рантаймы в итоге получились

# ctr containers list

CONTAINER      IMAGE                               RUNTIME
redis3         docker.io/library/redis:latest      io.containerd.runtime.v1.linux
redis4         docker.io/library/redis:latest      io.containerd.runtime.v1.linux
redis5         docker.io/library/redis:latest      io.containerd.run.kata.v2


как видно redis3 и redis4 имеют дефолтный рантайм.
а redis5 имеет ката рантайм

документация по установке kata - https://github.com/kata-containers/documentation/blob/master/how-to/containerd-kata.md
на удивление там все очень точно описано а не хуй как как в к8 доках

еще замечу что деволтный low-level runtime 
это runc ( все время путаю с runv)

***** 30/09/2019 установка kata на containerd - готово *****************************************

ctr

как создать контейнер с использованием default low-level runtime

# ctr images pull docker.io/library/redis:latest
# ctr container create docker.io/library/redis:latest redis
# ctr containers list


далее.


возврвщаемся к runtimeclass. 
итак мы имеем runc и kata.

теперт будем учиться их выбирать при создании подов.


теперь нужно активировать подддежку runtimeclass в к8.

это нужно активировать в кубелет и в apiserver

для кубелет нужно вставить параметр 

# kubelet .... --feature-gates RuntimeClass=true


чтобы переустановка кубелета непотерла наши кастомные настройки мы их пропишем в /etc/default/kubelet

# cat /etc/default/kubelet
KUBELET_EXTRA_ARGS="--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --feature-gates RuntimeClass=true"


теперь надо активировать эту фичу на apiserver.

для начала посмотрим какой конфиг имеет к8 кластер вообще

root@test-kub-01:~# kubeadm config view
apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  dnsDomain: cluster.local
  podSubnet: 10.252.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}


хотя дальше дают команду которая якобы дает более полный конфиг кластера


root@test-kub-01:~# kubeadm config print init-defaults
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: test-kub-01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.16.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}


прикольно  что по дефолту кубелет на мастере запускается с 
KUBELET_KUBEADM_ARGS="--cgroup-driver=cgroupfs ..."

а не с systemd

паарметры запуска apiserver прописаны в файле 

/etc/kubernetes/manifests/kube-apiserver.yaml

но это нам непоможет для активации runtimeclass

так. совершенно непонятно как через api к8 поменять параметры настройки api сервера.
но к счастью конкретно опцию RuntimeClass можно активировать через командную строку при запуске apiserver 
ровно также как это мы делали для кубелет.

вот страница на которрй прописано что можно активировать у apiserver при запуске

https://v1-12.docs.kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/

но подьебка что так как указано в доке  а именно

# kube-apiserver ... --feature-gates RuntimeClass=true

так несработает


а сработает только вот так

# kube-apiserver ... --feature-gates=RuntimeClass=true



чуть в сторону. что интересно
на к8 мастере работают такие компоненты составляющие контроль панель как 

*кубелет
*controller-manager
*scheduler
*kube-apiserver

что охиренно непонятно. вот  у нас в кластере есть только мастер нода. 
 то есть все хозяйство кластера крутится только на мастер ноде.

так вот компоненты контроль панели с одной стороны заявлено что они развернуты как контейнеры

root@test-kub-01:~/.kube# kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-s6w6d              0/1     Pending   0          7m33s   <none>          <none>        <none>           <none>
kube-system   coredns-5644d7b6d9-x6bvz              0/1     Pending   0          7m33s   <none>          <none>        <none>           <none>
kube-system   etcd-test-kub-01                      1/1     Running   0          6m46s   172.16.102.31   test-kub-01   <none>           <none>
kube-system   kube-apiserver-test-kub-01            1/1     Running   0          6m52s   172.16.102.31   test-kub-01   <none>           <none>
kube-system   kube-controller-manager-test-kub-01   1/1     Running   0          6m49s   172.16.102.31   test-kub-01   <none>           <none>
kube-system   kube-proxy-mzr9b                      1/1     Running   0          7m33s   172.16.102.31   test-kub-01   <none>           <none>
kube-system   kube-scheduler-test-kub-01            1/1     Running   0          6m35s   172.16.102.31   test-kub-01   <none>           <none>


а с другой стороны они же запущены как простые линкс процессы вообще ниразу неконтейнерного типа 
а как класиические процесы линкс на мастере.

вот только один пример.  о том что в классических процессах запщуен тоже apiserver

root@test-kub-01:~/.kube# ps aux | grep api

root      8947 16.1 19.7 551644 404368 ?       Ssl  01:00   1:37 kube-apiserver --advertise-address=172.16.102.31 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key

что за хуйня ? так в итоге то контроль панель она что и как контейнер работает и как неконтейнер ???????

когда я гашу кубелет на мастере то это никак не гасит ни kube-apiserver ни controller-manager процессы.

 я выяснил как запускается apiserver

# ps xf

  1089 ?        Ssl    0:01 /usr/bin/containerd
 8612 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/768733cac0fb79b0e84437c81d9523b97e6a4cc5
 8651 ?        Ss     0:00  |   \_ /pause
 8666 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/36eca73d007455802b99077353cce667708660a5
 8704 ?        Ss     0:00  |   \_ /pause
 8729 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/7996630f1e9b69d95676808f7ecc84edd4a6eb99
 8790 ?        Ss     0:00  |   \_ /pause
 8739 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/00f769ba84cc575c5a336f127a588144072bd237
 8819 ?        Ss     0:00  |   \_ /pause
 8877 ?        Sl     0:01  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/bb5cf487eb63c9e542ebcd9ed2043302533cc979
 8947 ?        Ssl    2:54  |   \_ kube-apiserver --advertise-address=172.16.102.31 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kuberne
 8898 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/cf649a35a4c27416c353b3830b3e3222f5b5a0e0
 9005 ?        Ssl    0:06  |   \_ kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorizatio
 8974 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/b30b8bfb027c82690879ad797385e16d4355d071
 9040 ?        Ssl    0:03  |   \_ kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf -
 9026 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/4450015229f3035cca7916e657bbe59d309c18c4
 9122 ?        Ssl    0:08  |   \_ etcd --advertise-client-urls=https://172.16.102.31:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --dat
 9545 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/8aca5b886539be2c1c69bb33e08f968e500ff9dc
 9576 ?        Ss     0:00  |   \_ /pause
 9604 ?        Sl     0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/85560a90a15ca92d30693b835944a225a5257c1d
 9634 ?        Ssl    0:00      \_ /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=test-kub-01


containerd запускает conatinerd-shim а тот уже apiserver


возникает вопрос - как перезапустить контейнер который относится к контрол панели. 
ответ - надо перезапустить кубелет. а он уже перезапустит все контейнеры контрол панели


вовзрашаюсь обратно.

вносим изменения в конфиг kube-apiserver.yaml 
а именно добавляем строку
    - --feature-gates=RuntimeClass=true



root@test-kub-01:/etc/kubernetes/manifests# cat kube-apiserver.yaml
...
...
  containers:
  - command:
    - kube-apiserver
   ...
   ...
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --feature-gates=RuntimeClass=true
    

перезапускаем кубелет

проверяем что оно перезапустило apiserver и что фича runtimeclass активирована на apiserver


s# ps aux | grep api
root     20507 16.4 18.5 551024 380536 ?       Ssl  01:45   1:00 kube-apiserver --advertise-address=172.16.102.31 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --feature-gates=RuntimeClass=true


все окей.


минус только такой реконфигурации в том что при переустанвоке к8 пакетов эта настройка будет перетерта.

итак наконец то на дата ноде в службе кубелет активирован runtimeclass

root@test-kub-05:~# ps aux | grep kubelet
root     23359  0.6  4.8 683008 98636 ?        Ssl  01:55   0:04 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --feature-gates RuntimeClass=true

и на мастер ноде runtime class активиован для apiserver


root@test-kub-01:/etc/kubernetes/manifests# ps aux | grep apiserver | grep -i runtime
root     20507 13.7 19.4 551388 399344 ?       Ssl  01:45   2:45 kube-apiserver --advertise-address=172.16.102.31 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --feature-gates=RuntimeClass=true
 
можем двигать дальше. 
и начинать создавать контейнеры для которых будем укаывать на какой low-level runtime им выполняться.


чтото я непонял. 
на хосте с containerd запущены контейнеры но почему то ни 
# runc list
# ctr containers list

 нифига непоказывают. как бутто на хосте ниодного контейнера незапущено

к8 дока пишет что dockershim ( cri плагин между к8 и high-level runtime dockerd ) не умеет работать с runtimeclass
 другими словами что dockerd не поддерживает 
разнообразные low-level runrtime а только один. ну это мы и сами знаем.


далее. конкретно про containerd сказано  что в его конфиге мы прописываем low-level runtumes через строку

[plugins.cri.containerd.runtimes.${HANDLER_NAME}]

мы выше как раз уже конфигурировали его секцией

 [plugins.cri.containerd.runtimes.kata]
         runtime_type = "io.containerd.kata.v2"
         [plugins.cri.containerd.runtimes.kata.options]
           ConfigPath = "/etc/kata-containers/config.toml"


схема походу такая. в конфиге high-level runtime в данном случае containerd мы прописываем путь к бинарнику 
который является low-level runtime 
и для этого пути мы указываем некое имя от фонаря которое и называется handler.  схема работает так - к8 через cri
 плугин обращается к containerd
и говорит что создай контейнер в которого Low-level runtime = handler.  containerd принимает и вызывает бинарник
 который указан в секции handler в его конфиге.

то есть к8 говорит создай контейнер с рантаймомо kata. containerd лезет в конфиг . ищет секцию где прописан ката.
 и вызывает бинарник 
который указан для kata. само имя в конфиге continerd kata не имеет занчения. можно написать вася. например

 

 [plugins.cri.containerd.runtimes.вася]
         runtime_type = "io.containerd.kata.v2"
         [plugins.cri.containerd.runtimes.вася.options]
           ConfigPath = "/etc/kata-containers/config.toml"

и надо будет тогда в конфиге пода указать вася как runtimclass
главное это то что в конфиге containerd указан бинарник к low-level runtime .


[plugins.cri.containerd.runtimes.kata]
         runtime_type = "io.containerd.kata.v2"
         [plugins.cri.containerd.runtimes.kata.options]
           ConfigPath = "/etc/kata-containers/config.toml"



вот в нашем случае указан не просто путь к бинарнику а путь к огромному конфигу /etc/kata-containers/config.toml 
в котоооором указан нетолько бинарник
а и вообще все другое .в этом конфиге куча всего указана.


кубелет передает containerd информацию о желаемом для Pod low-level runtime через поле runtimeclass:

то есть кубелет передает к кcontainerd в поле runtimeclass имя функции (handler) которая отвечает за Low-level runtime.

containerd получив runtimeclass= vasya ищем в своем конфигу путь к бинарнику который отвечает за обработку vasya



вот как это выглядит в конфиге пода указание runtimeclass

POD
apiVersion: v1
kind: Pod
...
spec:
 ...
 runtimeClassName: gvisor



а вот как еще раз выглядит в containerd строчка отвечающая за путь к бинарнику для этого рантаймкласса
CONTAINERD
[plugins.cri.containerd.runtimes.gvisor]




еще пример
POD
apiVersion: v1
kind: Pod
...
spec:
 ...
 runtimeClassName: kata


CONTAINERD
[plugins.cri.containerd.runtimes.kata]


runtimeclass это типа синоним слова Low-level runtime запускальщик


итак мы прописываем runtime class в конфиге пода. также мы прописываем это в конфиге containerd.
но чтобы это заработало нужно также прежде всего прописать саму эту сущность runtime class в базу данных кластера самого.

 

создаем файл


по шаблону из доки

apiVersion: node.k8s.io/v1beta1 
kind: RuntimeClass
metadata:
  name: myclass  # The name the RuntimeClass will be referenced by
handler: myconfiguration  # The name of the correspond

# cat ./runtime-kata.yaml

apiVersion: node.k8s.io/v1beta1 
kind: RuntimeClass
metadata:
  name: kata 
handler: kata

# kubectl apply -f ./runtime-kata.yaml


и вот уже пример моего pod-a

root@test-kub-01:~# cat vasya.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: vasya10
  labels:
     stage: prod
spec:
  containers:
    - name: frontend
      image: akrivosheevmsk/mk-dockerhub-repo:nginx-1
  runtimeClassName: kata


ксати можно заметить что юзается мой личный акаунт для имаджей на dockerhub

что очень странно.  что через cli от containerd невидны все запущенные на хосте поды контейнеры. 
а видны только те которые были запущены через этот ctr. 

а вот через обобщенный cli - crictl видно все на хосте


root@test-kub-05:~# crictl pods
POD ID              CREATED             STATE               NAME                          NAMESPACE           ATTEMPT
e72c292bbf7df       5 minutes ago       Ready               vasya10                       default             0
b39582046be46       2 hours ago         Ready               nginx9                        default             0
e971d00565ff0       23 hours ago        Ready               kube-flannel-ds-amd64-pvdl5   kube-system         0
81920c62ddbc9       24 hours ago        Ready               kube-proxy-zn5fd              kube-system         0
root@test-kub-05:~#
root@test-kub-05:~#
root@test-kub-05:~#
root@test-kub-05:~#
root@test-kub-05:~#
root@test-kub-05:~# crictl ps
CONTAINER ID        IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID
b4486d6dc4d1c       ab56bba91343a       5 minutes ago       Running             frontend            0                   e72c292bbf7df
a00bba6e42ba5       f949e7d76d63b       2 hours ago         Running             nginx               0                   b39582046be46
ea8eadff39b48       8a9c4ced3ff92       23 hours ago        Running             kube-flannel        0                   e971d00565ff0
cafff485ea2f5       c21b0c7400f98       24 hours ago        Running             kube-proxy          0                   81920c62ddbc9
root@test-kub-05:~#
root@test-kub-05:~#
root@test-kub-05:~# ctr containers list
CONTAINER    IMAGE                             RUNTIME
redis3       docker.io/library/redis:latest    io.containerd.runtime.v1.linux
root@test-kub-05:~#




что я вмжу что почему то 

# runc list

нихера не показывает контейнеры которые работают на этом рантайме.

а вот 

# kata-runtime list

показывает все запущенные под ней контейнеры исправно.


непонятно как через kubectl узнать runtime того или иного пода.


что еще сущенственно. что runtimeclass указывается для всего пода целиком. то ест для всех конейтнеров внутри пода.

поэтому вот эту опцию runtimeclass удобно указывать наверху а не внизу вот так


root@test-kub-01:~# cat vasya.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: vasya10
  labels:
     stage: prod
spec:
  runtimeClassName: kata
  containers:
    - name: frontend
      image: akrivosheevmsk/mk-dockerhub-repo:nginx-1



итак чтобы runtimeclass(выбор Low-level runtime на дата нодах) заработал надо его прописать в следущих местах



1. добавить саму эту сущность в базу данных кластера

2. сконифигирировать его на high-level runtime на дата нодах.

3. собственно установить этот Low-level runtime на дата нодах.

4. прописать в поде.




супер толковое видео по практическому подлкючению runtimeclass https://asciinema.org/a/215564
 




ффуууухххх.. это была очень длинная и муторно описанная глава в говно доке от к8

****************************************************************************************************************************8

//// Containers
////  Container Lifecycle Hooks
////


перед тем как контейнер будет удален возможно выполнение определенных команд. это называется  у к8 PostStart

также при старте контейнера тоже возможно выоплненеие определенных команд. единсвтенное что нет гарантии когда это будет выоплнено до entrypoint
или после entrypoint и эту штука называется у к8 как PreStop


можер запуситить каккой то бинарник. а можно запустить вызов по HTTP протоколу.


контейнеру на остановку дается время terminationGracePeriodSeconds и если наш хоок работает дольше к8 все равно уничтожает контейнер.

хуки запускаются один раз и если они немогут отработат к8 непарится чтобы их перезапустить. то есть к8 непарится
о том чтобы хуки отработали или неотработали. запускает их а там как получится

как я понял запуск хуков неотображается в kubectl descirbe pod

но вот ошибки и проблемы неудачи при отрабатывании хуков уже фиксиурюися в kubectl describe pod


Events:
  FirstSeen  LastSeen  Count  From                                                   SubobjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
 ping: failed to "StartContainer" for "main" with RunContainerError: "PostStart handler: Error executing in Docker Container: 1"
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook




//// Workloads
////  Pods
////  Pod overview


под это минимльная executable структура которая может быть задеплоена на к8. 
задеплоена значит что эта сущность появится на дата нодах.

можно прописвыать сущности в к8 но это недеплой.

pod внутри себя заключает один или несколько контейнеров

дока пишет что если в поде несколькьо контейнеров то типа надо чтобы это были конетйнеры разного назначения.
а если мы хотим деплоить контейнеры одного и того же назначения то надо обязательно использовать несколько подов.а не  в один их 
лепить. если контейнеров несколько то это наример веб + сторадж + база данных. а если нам надо несколько вебов  задеплоить
то надо для каждого веба свой отеднлдедьный под. так они пищшут.

если в поде несколкьо контейнеров то один из них может запускатся прежде других и использоваться как иницииализирующий 
контейнер.

контейнеры в поде делят общую сеть и общий сторадж

если нам надо опублоиковать кучку однообразных подов. то есть получается как бы поды зареплицированы. то
это делается обычно через такую структуру как Controller

они говорят что контролллеры они заново создадут под если он где то упал.
интересно а если просто одинокий под упал то к8 не следит за ним вообще?

некоторые виды контроллеров

DaemonSet
StatefulSet
Deployment


могу сразу скзать что DaemonSet используется к8 для публикации служебных сервисов. это как службы 
у виндовса


пример пода

~# cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: bb1
    image: busybox
    command: ['sh', '-c', 'echo hello im bb1 && sleep 3600']



как посмотреть вот этот вывод пода

~# kubectl logs  myapp
hello im bb1



//// Workloads
////  Pods
////  Pods

контейны внутри пода могут друг с другом взаимодействоать в том числе через
 inter-process communications like SystemV semaphores or POSIX shared memory

под рассматривается как ненадежная структура в к8.
если он умирает то обычно он не пересоздается если под был опубликован сам по себе как под.
если под будет пересоздан то его UID будет другой

контиейныр в поде шарят один сетевой неймспейс. а именно ip и порты. 
то есть они все их могут юзать

если на апи сервере под помечен на удаление то эта инфо поступает на кубелет
и если кубелет рестартанул а под быд неудален с хоста
то после рестарта кубелета попытака удаления пода будет повторена.
ну логично что рестарт кубелет никак не должен влять на удаление пода.



далее они в деталях рсписывают как происходит процеесс удалени пода

посылаем команду на удаление пода.

на апи сервере обьект пода помечание как terminating. и ему дается 30с
(grace period0 на это. когда это время кончится то под будет считаться dead если он за это время не удалился

кубелет видит что под на апи сервере получил статус terminating он запускает на всех контейнерах внутри пода prestophook 
( эта фича контейнера а не пода).
далее если истекает grace period то он запускает на всех контейнерах sigterm и дает на его выполнение 2 сенкуды.

как я понимаю потом если контейнеры еще не удалились то они убиваются через sigkill

сотвтесвтенно если все эти процессы удаления отрабатывают раньше то они закрываются раньше чем 30секунд 

после этого кубелет посылает на аписервер сигнал что запись о поде на аписервер можно удалить и запись о нем удаляется.

далее.
kubectl delete имеет опцию --grace-period=<seconds>  которая позволяет указать другой grace period удаления пода 

далее.
можно указать даже --grace-period=0  --force ( используются вместе)

это вызовет force deletion пода

что это такое. это значит что из аписервер запись удаляется мгновенно и из etcd удаляется мгновенно. 
и неждется подтверждение удаления от кубелета на дата ноде.
при этом на дата ноде конечно же тоже запускается процесс удаления пода. но еще раз важно подчернуть что аписервер
 вообще неждет сигнала 
от кубелета от том что тот фактически удалил под а сразу мгнвенно удаляет у себя о нем запись. 
на дата ноде кубелет дает некоторое короткое время. непишут скока. как я думаю 2 секунды.
 после чего контейнеры килятся через sigkill

далее.
в темплейте от пода можно указать для контейнера флаг privileged.
это дает этому контейнру несолкько бОльшие возможности с фичами кернела линукса чем обычные контейнеры имеют.

 
//// Workloads
////  Pods
////  Pod Lifecycle




~# kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
myapp     1/1     Running   150        6d6h
nginx9    1/1     Running   0          8d
vasya10   1/1     Running   0          8d


рассмотрим строку STATUS

в данном случае видим Running


прикольная команда

 kubectl get pods -o json | jq


вот ее вывод про отдельный под



   {
      "apiVersion": "v1",
      "kind": "Pod",
      "metadata": {
        "annotations": {
          "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"env\":\"test\"},\"name\":\"nginx9\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"nginx\"}]}}\n"
        },
        "creationTimestamp": "2019-10-01T20:23:05Z",
        "labels": {
          "env": "test"
        },
        "name": "nginx9",
        "namespace": "default",
        "resourceVersion": "104732",
        "selfLink": "/api/v1/namespaces/default/pods/nginx9",
        "uid": "04e21c7a-ab62-4cf6-b1c7-01b64c24c04c"
      },
      "spec": {
        "containers": [
          {
            "image": "nginx",
            "imagePullPolicy": "IfNotPresent",
            "name": "nginx",
            "resources": {},
            "terminationMessagePath": "/dev/termination-log",
            "terminationMessagePolicy": "File",
            "volumeMounts": [
              {
                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                "name": "default-token-79944",
                "readOnly": true
              }
            ]
          }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "test-kub-05",
        "priority": 0,
        "restartPolicy": "Always",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
          {
            "effect": "NoExecute",
            "key": "node.kubernetes.io/not-ready",
            "operator": "Exists",
            "tolerationSeconds": 300
          },
          {
            "effect": "NoExecute",
            "key": "node.kubernetes.io/unreachable",
            "operator": "Exists",
            "tolerationSeconds": 300
          }
        ],
        "volumes": [
          {
            "name": "default-token-79944",
            "secret": {
              "defaultMode": 420,
              "secretName": "default-token-79944"
            }
          }
        ]
      },
      "status": {
        "conditions": [
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2019-10-01T20:23:06Z",
            "status": "True",
            "type": "Initialized"
          },
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2019-10-01T20:23:08Z",
            "status": "True",
            "type": "Ready"
          },
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2019-10-01T20:23:08Z",
            "status": "True",
            "type": "ContainersReady"
          },
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2019-10-01T20:23:06Z",
            "status": "True",
            "type": "PodScheduled"
          }
        ],
        "containerStatuses": [
          {
            "containerID": "containerd://a00bba6e42ba52b4ea35db8c6f1ec3de374df93fe0d30ee81e7fb8b3a5a87017",
            "image": "docker.io/library/nginx:latest",
            "imageID": "docker.io/library/nginx@sha256:aeded0f2a861747f43a01cf1018cf9efe2bdd02afd57d2b11fcc7fcadc16ccd1",
            "lastState": {},
            "name": "nginx",
            "ready": true,
            "restartCount": 0,
            "started": true,
            "state": {
              "running": {
                "startedAt": "2019-10-01T20:23:07Z"
              }
            }
          }
        ],
        "hostIP": "172.16.102.35",
        "phase": "Running",
        "podIP": "10.252.1.2",
        "podIPs": [
          {
            "ip": "10.252.1.2"
          }
        ],
        "qosClass": "BestEffort",
        "startTime": "2019-10-01T20:23:06Z"
      }
    },




то есть вот такой способ получить полную детальую информацию о поде


а теперь другой вариант получить инфо о томже самом поде

root@test-kub-01:~# kubectl describe pods nginx9
Name:         nginx9
Namespace:    default
Priority:     0
Node:         test-kub-05/172.16.102.35
Start Time:   Tue, 01 Oct 2019 23:23:06 +0300
Labels:       env=test
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"env":"test"},"name":"nginx9","namespace":"default"},"spec":{"conta...
Status:       Running
IP:           10.252.1.2
IPs:
  IP:  10.252.1.2
Containers:
  nginx:
    Container ID:   containerd://a00bba6e42ba52b4ea35db8c6f1ec3de374df93fe0d30ee81e7fb8b3a5a87017
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:aeded0f2a861747f43a01cf1018cf9efe2bdd02afd57d2b11fcc7fcadc16ccd1
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 01 Oct 2019 23:23:07 +0300
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79944 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-79944:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-79944
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>

видно что этот способ выдает невсе из того что первый способ




итак вопзрващаемся к статусу пода.

он может быть

pending
running
succeded
failed
unknown


сттранно что скажем terminating у них неуказан



penfing означает что для одного из контейнеров внутри пода нет имаджа в системе.
далее система пытается его скачать

running - все контейнеры в поде былти созданы и хотя бы один из них работает

succedede - система дала команду контейнерам в поде остановится и они все за отведенное
		время успешно остановились и код выхода 0.

failed - по крайней мере один контейнер в поде закончил работу с ненулевым кодом выхода
		или неуспел сам себя удалить и его уже удалила система

unknonw - система неможет получить с хоста от кубелета статус этого пода. обычно изза проблем
			со связью межлду мастер нодой и дата нодой




мы тока что говорили про фазы пода ( который в kubectl get pods выглядит как status 

но это типа все же фаза). потому что у пода есть и статус.










 



----


conrollers разные виды

repicaset
она главным образом предназначена чтобы гарантировать что какое то заданное число подов работает.

реплика сет задается через шаблон в котором указано сколько реплик подов должно работать и шаблон по которому
replicaset может пересоздать под вслучае если какойто экземпляр упал

далее они пишут что в практике нам непридется пользлваться этим контроллером потому что 
есть контролллер более высокого уровня Deployments котоырй умеет рулит replicaset

как посмотреть список работающих replicaset

# kubectl get rs

более детально посмотореть инфо о конкретной реплике например реплике frontend


# kubectl get rs/frontend


также можно посмотреть список работающих подов

# kubectl get pods

и можно увидеть что поды принадлежать на самом деле репликесету.

для этого 

# kubectl get pods frontend-88bvr -o yaml

и мы увидим искомую инфо в поле ownerReferences


ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend


далее. если мы создаем какието отдельные поды которые сами по себе. то нужно четко в их шаблоне неуказать label такой же как прописан 
в репликесет. потому что реплика сет следит за всеми подами которые имеют такойже label как прописан в шаблоне репликасет.

например. если мы укажем  в репликсет что label подов которые она создает и обслуживает = vasya
потом запустим данную репликусет
потом запустим какойто отдельный под с label=vasya
то репликасет сразуже обратит внимание на данные под. и если число подов указанных в репликесет уже равно максимум то новосоздынный под
будет уничтожен. хотя новый под не имеет никакого отношения  к подам которые создные через репликасет.

если мы удалим репликусет то автоматом будут удалены и ее поды. 
хотя можно указать флаг и будет удалена только репликсет  а поды останутся

также есть такая супер штука Horizontal Pod Autoscaler
она позволяет автоматом менять число развернутых подов в завтимиости от загруженности цпу на подах.
скажем развернуто 3 пода. и их загрузка подскочисла до 70% их вирт процов. тогда к8 автомтом доразвернет еще несоклько подов.
чтоб в целом снять нагрузку с подов.

есть еще контроллер daemonset. его нужно использовать для тех подов которые завязаны на обслуживание самого сервера 
на котором крутятся
поды. скажем поды по  мониторингу.

repplicaset это эволюция от replicationcontroller. последний не поддерживает labels.

далее речь идет об контролллеере replication controller


apiVersion: v1
kind: ReplicationController


публикуем ли мы под или контроллер - это фигурирует в строке kind:

получить список replicationcontrollers

# kubectl describe replicationcontrollers/nginx

посмотреь подробности про конкретный repicatoncontroller


# kubectl describe replicationcontrollers/nginx

поды могут быть отвязаны он relicaset или reolicationcontroller если им сменить label

странно но они предлагат такой подход подов для репликейшн контроллера. - создаем новый контроллер с 1 подом.
потом на старом контроллере делаем -1 реплик. потом на новом +1 реплика. на старом -1.
и когда на старом останется 0 реплик то старый удаляем.

репликйшнконтроллер отвествененн пишет книжка всего навсего за 
1)  число работающих подов  равно заданному
2) работающие поды отвечают некоторому селектору

есть контроллеры типа Jobs их применяют когда под подразумевается что он сам в какое то время закончит работу.

переходим к Deployments

значит опубликовать под контролллер и прочее мы можем командной

# kubectl apply -f vasya.yaml

посмтртеть список опубликованных deployments

# kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           3m46s

когда мы обновили деплоймент он делает обновление подов вот так : создает новую 
репликусет и в ней создает один под. 
а в старой репликесет
он уменьшает на один число подов, потом в новой репликесет он добавляет +1 под, в старой репликесет уменьшает число подов еще на 1.
 и так 
до тех пор пока в старой репдикесет число подов упадет до нуля а в новой до максимума.
типа умное обновление.

дальше книжка пишет вот о каком случае. скажем деплоймент в данный момент обновляет поды до версии 2 ( путем уменьшения числа подов
на старой репликсет и увеличения числа подов на новой репликсет). и тут мы еще больше обновили деплоймент до версии 3.
так вот к8  мгновенно останавливает поды версии 2 и убивает их. и репликусет убивает. и начинает процесс обновления подов 
до версии 3. то есть запуск обновления во время идушего обновления мгновенно останавливает и убивает текущее обновление
 и запускает новое обновление.

каждый раз когда мы меняем описание template в .yaml деплоймента то система сохраняет историю изменеия yaml деплоймента 
и поэтому можно откатить деплоймент на версии обратно

rollback - это откат изменений.
rollout  - это наоборот "выкатка" новой версии


kubectl rollout - выкатывает новую версию
kubectl rollout undo - откатывается к прошлой версии

как изменить число реплик для деплойментса с именем nginx-deployment

# kubectl scale deployment.v1.apps/nginx-deployment --replicas=10

где deployment.v1.apps/nginx-deployment это то как описывать имя деплоймента в командной строке

а вот как делать автоматическое расширение числа подов 

# kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80

active ReplicaSets - это репликасет в которой есть хотя бы один под.

Proportional scaling - это вот что. запускаем процесс обновления деплоймента. он же как работает. он создает новый реплика сет.
дальше он создает несколько новых подов в новой реплике. как только они стартанули. он гасит столько же подов в старой реплике.
далее представим что мы в середине этого процесса. (есть две реплики сет .часть новых подов создана. и часть старых удалена.) 
и в этот 
момент мы даем команду на увеличение числа реплик в деплойменте.  так вот  Proportional scaling значит
 то что поды будут добавлены и 
в старой репликсет и в новой репликасет. без этого механизма ( Proportional scaling ) поды добавились бы либо только 
в старую реплику 
либо только в новую.


деплоймент можно запаузить 

kubectl rollout pause deployment.v1.apps/nginx-deployment

далее они чтото пишут про  особую выгоду для обновлений после паузы
в чем фишка этого непонятно
видно только одно. пока деплоймент запаузен новая репликасет несоздается.
она создается только после того как деплоймент будет распаузен

kubectl rollout resume deployment.v1.apps/nginx-deployment

можно мониторить статус выкатки 

kubectl rollout status deployment.v1.apps/nginx-deployment

статус может быть - в прогрессе. окей или fail.


по дефолту к8 хранит 10 послених версих yaml для деплоймента. так что можно смело откатываться на 10 версий обратно лехко

по дефолту при обновлении деплоймента это работает так - создается новая репликасет там создается +1 под. после этого
на старой реплике удаляется -1 под.
можно выставить в деполойменте опцию что будет работать по другому. вначале на старой реплике будет удаляться -1 под.
а только потом на новой реплике будет создававться +1 под.

следущий контроллер - statefullset
описание как обычно мутное.
но вроде так. скажем три реплики задано. втоарая нестартанет пока незаработала первая,
третья нестартанет пока незаработала вторая.
обновление вроде как проходит тоже оригинально.
чтобы обновить первую реплику к8 уничтожает третью и вторую. а потом обновляет первую.
вот такое хуевое описание.

интернет пишет что деплоймент используется для stateless приложений, а statefullset испольуется
для statefull приложений. также инет пишет что реплики деплоймента получают один общий
volume а реплики statefullset каждая получает свой индивидульный volume

инет пишет что деплоймент юзают там где реплики должны быть одинаковыми а stetefullset исполь
зую там где реплики должны быть уникальными

создатели к8 тоже пишут что деплойментс юзается для стэйтлесс прог  а statefullset юзается 
для стейтфулл прог.

еще что я нашел в инете. когда в деплойменте удаляют под то новый под вместо него
имеет новый хвост.  скажем был под

вася-kjekwfwe0

станет

вася-fkwejflwekj

а под из statefullset если его удалить будет воссоздан и стаким же именем как и был.

еще инет пишет что если удалить под из деплоймента то новый под будет создан 
на другой ноде. если удалить под из stetefullset то под будет создан обратно 
на тоже самой ноде. а если нода легла то под небудет создан нигде и его статус будет 
помечен как unknown \terminated.  k8 считает что поды из стейтфуллсета должны сидеть
ровно там где они были созданы ибо они требуют "устойчивой" сети и стораджа.


следущий тип контроллера это daemonset.
он гарантирует что п крайней мере на двух нодах крутятся реплики подов. 
а в идеале на всех нодах.
этот контролллер используется для того чтобы через него собирать инфо о нодах
например логи или  ceph

в yaml файлах вот за что отвечают ряд строк
 
kind: DaemonSet - тип хрени что разворачивается

metadata:
  name: fluentd-elasticsearch  - имя этой хрени




поды в демонсете либо сами отсылают инфо кудато либо они могут иметь сокет
через который к ним можно обратиться



контроллер деплойментс обеспечивает то что число реплик должно оставаться таким
каким оно задано

у обьектов в к8 есть нередко владельцы. например у подов которые были созданые репликойсет
если создать репликусет и она создаст поды. и посмотреть потом инфо  подах


# kubectl get pods --output=yaml

то мы увидим что владелец пода это my-repset

ownerReferences:
  - apiVersion: apps/v1
    controller: true
    blockOwnerDeletion: true
    kind: ReplicaSet
    name: my-repset


следущий вид контроллера это ttl controller.
он работает только с контроллерами типа jobs.
и служит для того чтобы автоматически зачищать джобы которые закончили свою работу или находятся
в состоянии terminated. если в свойствах джоба строчку определенную прописать то ttl controller 
смоожет делать свою работу. по этой строчке ttl controller ловит что ему надо бы
поработать с таким то джобом.  эту строчку можно менять даже после того как джоб уже 
создан

# kubectl get pods --selector=job-name=pi
позволяет показать список подов типа Jobs с именем pi

а вот как посмотреть  логи пода
$ kubectl logs $pods

следущий тип контроллера это Jobs
они бвыают трех типов.

1)непаралельный Job - запускается один под. контроллер считает что все закончено когда
под успешно закачивает свою работу

2) параалельный Job with fixed completion count - хз что это по описанию. насколько я понял
там запускается уже несколько подов. и там в джобе мы указываем сколько подов из числа
запущеных должно успешно закончится и тогда джоб будет считать что он закончен .

3) паралельный job with work queue - тоже хуйня в описании. там написано что поды вместе
делают некую общую работу. ( скажем каждый под обсчитвает кусочек экрана).
но дальше там хуйня какая то написана про то когда Джоб считается выполненым



по крайней мере для джобов можно указать сколько раз можно пробовать рестартовать изза ошибок 
под прежде чем этого больше неделать


также можно указать дедлайн по времени сколько под может работаьт.

по дефолту после того как джоб отработал ничего недулаяется.
так как можно логи посмотреть итп

когда под в джобе валится то джоб стирает старый и создает новый

следущий тип контроллера это CronJob, он создает Jobs по расписанию




Далее рассматриваем таку сущность как Services.
они пишут что под(группа контейнеров) получает свой IP.
и при пересоздании пода его IP изменится и это проблема
так как с этим подом может взаимодействовать другой под

чтоб другие поды знали что другой под изменил свой IP используют Service


при создании сервиса указывается IP порт и label от подов который он контролирует.

тогда к этиим подам можно получать доступ через IP этого сервиса

поды могут менять свои IP но IP сервиса не меняется.

возникает вопрос а как же там деплоймент или репликасет. как они отличаются
от сервиса. ответ - хуй знает. книжка это хуево затрагивает.

при создании сервиса можно неуказать лэйбел подов которыми он заведует.
тогда придется вручную это делать потом. в общем нахуй это щас изучать

Every node in a Kubernetes cluster runs a kube-proxy.
эта херня отвественна за работу с сетью внутри ноды и наружу с внешним
миром.

эта херня может работать в нескольких режимах

вот как нашел в инете 
kube-proxy has an option called --proxy-mode，and according to the help message, this option can be userspace or iptables.(See below)

kube proxy отвечает за выдачу IP для services

в к8 есть какой то ingress api ( хуй знает что это ), 
также был добавлен iptables proxy

этот proxy,  так понимаю что как ни крути образован ничем иным как kube-proxy

так вот он может работать в user-space ( имеется ввиду ядро неядро).
это получается медленнее чем в kernel space однако я продолжу.
как только создаеьтся service то Iptables создает IP:port1 на этой ноде
и также создается на этой ноде я так понимаю на лупбэк интерфейсе port2
который прослушивает kube-proxy сервис. и цепочка работает так.

клиент обращается на ip:port1
iptables делает переобращение этого запроса на loopback:port1 который
слушает сервис kube-proxy который перенаправляет его на под от сервиса.
по умолчанию выбор пода идет по round-robin
фишка такого мегасложности наверное в том что для клиента неизменный
ip:port1 для доступа к сервису.
на kube-proxy тоже порт для сервиса остается 
постоянным. а сам куб-прокси сервис уже сам следит за подами у сервиса
как они там создаются умирают. интерсно что за протокол какая связь
между сервисом куб-прокси и подами.


а вот описание user-space режиме kube-proxy из стэковерфлоу

In the userspace mode, the iptables rule forwards to a local port where a go binary (kube-proxy) 
is listening for connections. The binary (running in userspace) terminates the connection, establishes
 a new connection to a backend for the service, and then forwards requests to the backend and responses 
back to the local process. An advantage of the userspace mode is that because the connections are created 
from an application, if the connection is refused, the application can retry to a different backend.

еще раз тут запрос от клиента идет в бинарник kube-proxy 
который работает в userspace ядра и из него уже в 
бэкенд сервиса ( под). 


следущий режим работы kube-proxy это режим iptables ( назвали 
дебильно однозначно)

в этом режиме kube-proxy поток через себя не пропускает
а только конфигурирует соотвествуюдие правила
iptables и после этого поток идет от клиента
сразу в бэкенд ( в под).
плюс этого подохода в том что несипользуется куб прокси
живующий в юзер спейсе поэтому типа скорость и латенция круче. но есть и минусы. о них ниже.

  а вот описание со стек оверфлоу про этот режим

In iptables mode, the iptables rules are installed to directly forward packets that are destined 
for a service to a backend for the service. This is more efficient than moving the packets from the kernel
 to kube-proxy and then back to the kernel so it results in higher throughput and better tail latency.
 The main downside is that it is more difficult to debug, because instead of a local binary that writes a log 
to /var/log/kube-proxy you have to inspect logs from the kernel processing iptables rules.

In both cases there will be a kube-proxy binary running on your machine.
 In userspace mode it inserts itself as the proxy; in iptables mode it will configure iptables rather
 than to proxy connections itself. The same binary works in both modes, and the behavior is switched 
via a flag or by setting an annotation in the apiserver for the node.


как я понял из говнодоки кубернетеса и чтения инета.
еще одна разница есть в этих режимах.

если в режиме юзер спейс куб-прокси звонит на под и он 
не отвечает то куб прокси это понимает и сам ( приложение
клиент даже этого незнает) пробует  получить ответ
уже от другого пода.

а вот в режиме iptables так как в процессе передачи 
инфо куб прокси неучаствует а участвует клиент
и под напрямую то если под неотвечает 
то это должно оббрабатывать самое приложение.
для того чтобы заноов попорбовать retry получить ответ
от сервиса. тогда куб прокси вроде как перепишет правило
и запрос пойдет к другому поду. хуй знает это плохо описано. либо самое правило в iptables изначально
так прописыватся что каждое новое соединение обращается 
к рандомному поду. считай round robin технология. в любом
случае в случае неответа от пода приложению 
нужно самому делать retry к сервису чтобы получить 
попробовать ответ.

в этом минус режима iptabes

следущий режим ipvs.
тут нужно знать что такое ipvs изначально.
в целом в этом режиме kube-proxy бинарник трафик
через себя тоже не пропускает.
а только делает конфигурационные действия на начальной
стадии
фишка этого режима в том что есть разные алгоритмы
балансировки (как я понимаю) к подам на бэкенде.

далее.
есть возможность чтобы для сервиса заэкспоузить не один порт. а несколько.

при создании сервиса можно указать какой IP у него будет.

они пишут что dns round robin не надо использовать
как что и зачем и почему вообще непонятно.

далее.
при создании сервиса можно (непонятно зачем) указать 
опцию чтобы кластерный IP не создавался при создании 
сервиса.  если при этом в сервисе указан селектор
то в DNS все равно создается А запись  ( то есть по факту
все равно ссылка на IP будет). и типа этот IP ведет напрямую на поды. что это за хрень и чем она отличается
от случая когда кластерный IP для сервиса создается
непонятно.

если селектор не указан то в DNS создается CNAME запись.
то есть ссылка не на конкретный IP а на литерную штуку

далее.
при создании сервиса создается clusterIP
прикол втом что он доступен только внутри кластера
. вместо clusterIP который снаружи недоступен ( непонятно почемучто и как) можно
 при создании сервиса создать NodePort. 
это значит что на каждой ноде будет забронирован порт. и тогда если обратиться на

NodeIP:port 

то будем попадать на сервис. и уже типа снаружи.

есть еще вариант.
при создании сервиса можно указать для доступа к нему 
типа IP от внешнего клаудного балансировщика.
типа при обращении к клаудному балансировшику будет
идти перенаправление на поды сервиса ( что чего непонятно)

необязательно но стронгли рекоменд - это иметь DNS сервис в к8.

к8 при работе с aws балансиоровшиком имеет 
несколько доп фишек
можно использовать ssl терминацию на aws балансировшщике.

чтоб от балансировщика в подам уже шел расшифрованный поток


значит как уже было вверху сказано
что вместо кластерного IP можно сервис опубликовать через
CNAME в DNS

также как уже было выше сказано
еще можно опубликовать сервис через external IP (не путать с кластерным IP).
при этом непонятно почему это будет идти связь
при обрашении на external IP внутрь подов этого сервиса

Iptables operations slow down dramatically in large scale cluster
 e.g 10,000 Services поэтому в этом случае вместо 
режима iptables используют ipvs

дальше книжка пишет что любой сервис в к8 получает запись 
в DNS

каждый сервис в DNS получает запись 

my-svc.my-namespace.svc.cluster.local
 
которая ведет в cluster IP

а если сервис headless то он получает в DNS такую же запись но на ведет
 на множество IP адресов подов.

всопмним что при публикации сервиса можно публиковать
неодин порт а несколько.  и в этом случае их именуют.
так вот для этих именованных портов создается srv запись.
детали суть и смысл покрыты мраком.

22/01/2020

24/01/2020

короче нода  скубернетесом оказалась полностью забита нахер по диску.
прчичину  я  не понял.

в итоге я удалил то что занимало больше всего
а именно папку

 rm -rf /var/lib/docker/containers/4f549e82f6f60a3f30353cf80e8c6120cad976547f236e0d0b8f491de6b57038

надо бы научиться выяснять к чему она относилась. что было удалено.

я перегрузил. но почемуто куб так и не запустился. на порту 8080 никто ничего не слушал
и я начал его ставить с нуля заново.
--------

------
///отступ
как удалить ноду из кластера

# kubectl drain test-kub-05 --ignore-daemonsets --delete-local-data
#  kubectl delete nodes test-kub-05



\\\


helm изучить.


&&&& 2/4/2020




установка elasticsearch в кубернетес через helm

helm это такая скриптовая фигня. чтобы запуская ее скрипты 
мы в итоге формировали скрипты которые есть кубернетес.

официально они зовут что helm это пакетный менеджер кубернетеса.
смешно.

в итоге что он дает.
мы в итоге можем редактировать конфиг который очень похож на классический
конфиг от оригинальной программы. например конфиг для эластиксерч будет очень 
похож его синтаксис на синтаксис конфига эластика как он есть из deb пакета вместо 
вот этих киллометровых yaml хуевин. а хельм потом берет и это все уже конвертирует в километры
yaml конфигов и скармливает кубернетесу.

в итоге мы работаем с человеческим классическим конфигом к проге. натравливаем хельм и он 
это конвертирует в километры yaml которые есть кубернетес.

это очень удобно.

в этом и есть смысл хельма.

ставим хельм.

root@test-kub-01:~# apt install snapd

теперь нужно поставить helm chart repository

#  helm repo add stable https://kubernetes-charts.storage.googleapis.com/

# helm repo add elastic https://helm.elastic.co

пробуем поставить elasticsearch 

# helm install  elasticsearch elastic/elasticsearch   --set service.type=LoadBalancer


посмотреть что мы поставили 
-# helm list
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
elasticsearch2  default         1               2020-06-09 04:47:23.191718082 +0300 MSK deployed        elasticsearch-7.7.1     7.7.1

удалить

# helm delete elasticsearch2
release "elasticsearch2" uninstalled

сразу выяснилось что хвосты за собой хельм невычищает доконца.

pvc он за собой зависшие неудаляет.

# kubectl get pvc
NAME                                          STATUS    VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS    AGE
elasticsearch-master-elasticsearch-master-0   Pending                                                               11m

их приходится удалять за ним руками

# kubectl delete pvc elasticsearch-master-elasticsearch-master-0


создаем PV - путь к папке. указываем как вести если pvc удалили.
создаем сторадж-класс и привязываем к PV

в поде указываем сторадж-класс.

как под в итоге получает pv 


схема

pod -> pvc (storage class) -> pv


POD создает PVC в котором указан STORAGE CLASS 
PVC на основе сторадж класса обращается к его PV

если PV уже занят другим PVC то PV посылает нахер наш PVC.

кто первый сел того и стул. 

только один PV может владеть PVC.

насколько я понял неможет такого быть чтобы у несколких подов был один pvc.
потому что хотя в конфиге пода имя pvc задается как слово. но финальное имя pvc = слово в конфиге пода + имя пода.

дальше еще интереснее.

если мы убьем под. то его pvc остается висеть.
дальше еще интереснее
если мы убьем pvc то PV которым он владел будет иметь статус Retain . это значит 
что он всебя невпустит никакой новый pvc.

только если мы сменим политику на PVC с retain на recycle только тогда мы
сможем подключить этот pv снова к pvc .

я незнаю что будет если тотже самый pvc будет ломиться. может его pv пустит.
я незнад удаляются ли данные в pv при наступлении recycle.

миллион вопросов изза тупой архитектуры.

еще прикол в том что PV монтируется напрямую как папка внутрь конейтнера.

еще прткол в том что конфиг хельма отличается от конфига кубернетеса.

пример

кусок из stetless sset кубернетеса

volumeClaimTemplates:
   - metadata:
      name: example-local-claim
     spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-storage
      resources:
        requests:
          storage: 2Gi


кусок про тоже саоме из хельма

root@test-kub-01:~/helm/01# cat values.yaml
---
# Permit co-located instances for solitary minikube virtual machines.
antiAffinity: "soft"

# Shrink default JVM heap.
esJavaOpts: "-Xmx128m -Xms128m"

# Allocate smaller chunks of memory per pod.
resources:
  requests:
    cpu: "100m"
    memory: "512M"
  limits:
    cpu: "1000m"
    memory: "512M"

# Request smaller persistent volumes.
volumeClaimTemplate:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "local-storage"
  resources:
    requests:
      storage: 2Gi

самое дебильное что они даже слова меняют

у кубера
templates

у хельма
template

зачем эти дебилы так делают ?


супер полезная вещь.
вот мы устанавливаем прогу через хельм. там скрипты на скриптах 
но в итоге он формиурет yaml и его скрмилвает куберу.
так вот в кубере мжоно посмотрет так какой же на выходе был yamд файл.
напрмиер

# kubectl get vasya -o yaml

это очень важно когда дебажишь почему же хельм неставить прогу.
можно посмореть в yaml файл и увидет где там коряво .

но это был еше неконец проблем.

вот мы говорим что дескат дай нам папку на хосте.
но процесс внутри контейнера имеет свой uid:guid а папака на хосте имеет своим пермишны.

два варианта. один это в конфиге пода указать какой uid:guid он имеет типа при контакте 
с внешним миром колорче  я не  разюирался пока опция такая

securityContext:
        fsGroup: 1000
		


а есть второй вариант.
можно папке назначить gid тогда тот под который примаунчен к этой папке 
когда он будет к не йобращаться то процесс пода будет как бы иметь другой gid

вот эта опция

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
  annotations:
    pv.beta.kubernetes.io/gid: "1234"
	

и тоже надо разбирсять.

я вообще сделал в итге просто. я папке дал 777 пренмишнс.
=================================

как оупубликовать StatelessState + PersistentVolume

storage class

# cat sc.yaml

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer


PersistentVolume

# cat pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume2
  annotations:
    pv.beta.kubernetes.io/gid: "0"
  labels:
    type: local
spec:
  storageClassName: local-storage
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/disks/vdb1"


StatelessService

# cat ss.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web2
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: example-local-claim
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
   - metadata:
      name: example-local-claim
     spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-storage
      resources:
        requests:
          storage: 2Gi

установка 

# kubectl apply -f *.yaml

готово.

поговорим что чо значит.

что такое persistent volume.

это на дата ноде папка которая монтируется внутрь пода.


вначале нам нужно сооздать storageClass

в нем главные строки


  name: local-storage
provisioner: kubernetes.io/no-provisioner

первая это его имя.
второе это тип стораджа который за ним стоит. no-provisioner означает тип : папка на локальном 
для дата ноды диске.  еще быавют всякие там амазоны , iscsi и прочее.


потом определяем PersistentVolume

в нем главные строки

  storageClassName: local-storage
    storage: 10Gi
    path: "/mnt/disks/vdb1"
	

path - путь к папке
10Gi - мы говорим сколько максимум места можно отожрать из этой папки
local-storage - это мы указали имя вышестоящего сторадж класса к которому эта папка относится.

Persistentvolume это типа бэкенд обеспечение для сторадж класса.


далее уже создаем StatelessService

что в нем важно:

        volumeMounts:
        - name: example-local-claim
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
   - metadata:
      name: example-local-claim
     spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-storage
      resources:
        requests:
          storage: 2Gi
		  


в разделах volumeMounts и volumeClaimTemplates должны совпадать строки


        volumeMounts:
        - name: example-local-claim
  volumeClaimTemplates:
      name: example-local-claim

example-local-claim - сам по себе это имя неважно. важно чтобы было написано одно 
и тоже в обоих секциях.

и важны эти строки

storageClassName: local-storage
          storage: 2Gi
		  
		  
		  в первой мы указыаем имя сторадж класса который создали выше.
		  во второй указываем какой обьем из него хотим взять. конечно надо чтобы 2Gi <= 10Gi (который указан 
		  гдето там выше).
		  
		  
		  вот это важные моменты которые нужно , должны быть прописаны , и в таком виде , в нужных местах.
		  
		  тогда stateleessservice создаст под который внутрь себя получит примонтированную папку
		  укзанную в персистент вольюме. ( подозреваю что с технической точки зрения mount namespace пода
		  получит папку из персистент вольюма. )
		  

далее.
в линуксе каждый юзер имеет uid и gid.
uid - это число. типа логин но не в форме букв а в форме числа.
gid - это тоже число означает группу к которой данный uid входит в состав.

в зависимости в какую группу входит uid то ему линукс дает те или иные права делать чтото.

далее. gid обозначает primary группу. она одна.
а есть еще до 15 secondary групп куда может входит uid.

пример

$ id
uid=1000(mkadm) gid=1000(mkadm) groups=1000(mkadm),
4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),110(lxd),115(lpadmin),116(sambashare)

видно что юзер mkadm имеет uid= 1000 и gid = 1000
но еще mkadm входит в тонну других групп. они и есть секондари.

так вот разница между примари и секондари в том что  - если юзер создает файл то 
владельцем файла прописывается = юзер с uid
но это еще невсе.
в линуксе в отличие от виндовса владеьльцем файла является нетолько какойто юзер
но дополнительно владельцем является группа !
то есть у файла формально сразу два владельца  - некоторый юзер и одноврееннно с этим
некоторая группа ! ( вотличие от виндовс где владельцем является только юзер 
и никаких групп).
получается что когда мы создаем файл то ему нужно прописать владельца юзера 
и владельца группу.

так вот. когда файл создает юзер с uid и с gid то владельцем юзером для файла
прописыается uid а владельцем группой прописыватся gid.

то есть его праймари группа.  вот где у нас участвует gid в первую очередь - 
при создании файла gid владелец файла.

юзер uid=Вася gid=Honda создал файл. автоматом линукс указыает что 
воадельцем юзером файла является uid а владельцем группой файла является gid

при этом линукс назначает права на этот файл такие что и юзер имеет полные 
права на файл (что логично) и груупа имеет полные права на файл !

получается с файлом с одной стороны может делать что хочешь юзер
и одновременно с этим с файлом может делать все что хочет любой член
который также входит в туже группу !

далее вспоминает про секондари группы наконец. они то с какого бока

$ id
uid=1000(mkadm) gid=1000(mkadm) groups=1000(mkadm),
4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),110(lxd),115(lpadmin),116(sambashare)

а с того что наш юзер может иметь доступ к файлам у которых владельцем прописана
секондари группа.

если есть какойто файл у которого указано что его владельцем является группа sambashare
то наш юзер mkadm сразу имеет к нему доступ.

таким образом примари группа выстреливает при создании обьекта.
а секондари группы дают право доступа к обьектам уже созданным.

естесвтенно это неотменяет что gid также дает право доступа к обьектам у которых
указана группа владелец gid.


тперь мы знаем разницу между примари и секондари группами в линуксе для юзера.
конечно понятно что в целом группы неделятся на примари и секондари. 
это верно для конкретного юзера.
для одного юзера группа А будет примари  а группа Б секондари
а для друрго юзера наоборот группа Б будет примари а группа А будет секондари


----------------
fsGroup
далее. 
у пода может быть такой параметр fsGroup
его смысл - Allocating an FSGroup that owns the pod's volumes	fsGroup
то есть владельцем волюма будет группа с id=fsGroup
вольюм это папка на диске. также как и файл она имеет владельца юзера
и владельца группу.

тут возникает очень важные вопросы.
во первых есть user namespace который позволяет сделать так что 
в контейнере будут свои юзеры никак несвязанные с юзерами в системе.
скажем в контейнере запущен процесс и у этого процесса породителем указан uid,gid которых нет
в родительской системе. более того они могут иметь такие же uid gid как родительская система
но аутентифицироваться с помощью них на родителской системе нельзя. 
то есть root в контейнере изолирован от root на хосте.
(это надо прояснять как это на практике)

в общем когда мы говорим про юзером и права на папку непонятно это с какой стороны
со стороны внутри контейнера или снаружи для хоста.
в любом случае а как это выглядит со стороны хоста ?


разобраться с этой штукой !|

интересно то что вот мы хотим посмотреть а какие контейнеры у нас есть в системе.
но ведь контейнер это процесс. так вот. процесс который обеспечивает контейнер он с точки
зрения процесс неймспейса лежит в другом неймспейсе. и вообще то в из под дефолтового 
неймспейса недолжен быть виден. но так случилось что информацию и процессах сидит в папке /proc
а папка это такая штука которая уже неимеет отношения к неймпейсу процессов
она имеет отншение к неймспейсу папок. получется пародокосальная ситуация.
когда мы пишем команду ps aux мы как бы говорим - покажи нам процессы которые лежат в
НАШЕМ НЕЭМСПЕЙСЕ ПРОЦЕССОВ. утилита лезет в /proc а там ей видна инфо обо всех процессах
нетолько которые крутятся внашем неймспйсе но и в других процессных неймспйспах.
в итге утилита показывает все процессы и их чужих нам процессных неймспейсов.
и таким мкаром мы видим в  нашем процессном неймспйсе инфо о процессах на которых крутстя
котейнеры в других процессныъ неймспейсах.
это первый бред.
ну и отсюда ошибка что видя эти процессы интуитивно возникает вывод что эти процессы
крутятся в нашем процессном нейсмейсе. нет. это нетак. они крутятся на нашем процессоре.
но неймпейсе процессный у них другой.

далее. но сейчас мы всеже говорим немного о другом.
я хочу посмотреть какого юзера владельца имеет процесс который с точки зрения нашего неймпейса юзеров
(не путать с неймспейсом процессов) и какого юзера владельца имеет этот же процесс
но в неймпейсе юзеров контейнера.

root@test-kub-02:~# docker ps
CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES
9bfd57e885ae        4e9f801d2217           "/opt/bin/flanneld -…"   6 days ago          Up 6 days                               k8s_kube-flannel_kube-flannel-ds-amd64-tht2n_kube-system_6908c56e-5b6a-40d4-9cb4-ed0cf5d1eb1e_3

:~# docker inspect 9bfd57e885ae
[
    {
      
        "State": {
            
            "Pid": 2295,


итак узнали номер процесса контейнера.
кстаи возникает вопрос вот этот номер процесса он с чьей точки зрения такой.
с точки зрения нашего процессного неймспейса или его процессного неймспейса.


смотрим номера неймспейсов процесса 2295

~# ls -1al /proc/2295/ns/
total 0
dr-x--x--x 2 root root 0 Jun 15 19:53 .
dr-xr-xr-x 9 root root 0 Jun  9 02:41 ..
lrwxrwxrwx 1 root root 0 Jun 15 19:53 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Jun 15 19:53 ipc -> ipc:[4026532476]
lrwxrwxrwx 1 root root 0 Jun 15 19:53 mnt -> mnt:[4026532486]
lrwxrwxrwx 1 root root 0 Jun 15 19:53 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Jun 15 19:53 pid -> pid:[4026532487]
lrwxrwxrwx 1 root root 0 Jun 15 19:53 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Jun 15 19:53 uts -> uts:[4026531838]

тут я недоделал.



значит если мы запустили контейнер прямо из докера

# docker run -it ubuntu -c /bin/bash

то user namespace ровно такойже самый как и на хосте


root@test-kub-02:~# ls -1al /proc/24250/ns
total 0
dr-x--x--x 2 root root 0 Jun 16 20:33 .
dr-xr-xr-x 9 root root 0 Jun 16 20:33 ..
lrwxrwxrwx 1 root root 0 Jun 16 20:34 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Jun 16 20:34 ipc -> ipc:[4026532637]
lrwxrwxrwx 1 root root 0 Jun 16 20:34 mnt -> mnt:[4026532635]
lrwxrwxrwx 1 root root 0 Jun 16 20:33 net -> net:[4026532640]
lrwxrwxrwx 1 root root 0 Jun 16 20:34 pid -> pid:[4026532638]
lrwxrwxrwx 1 root root 0 Jun 16 20:34 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Jun 16 20:34 uts -> uts:[4026532636]
root@test-kub-02:~#
root@test-kub-02:~#
root@test-kub-02:~# ls -1al /proc/1/ns
total 0
dr-x--x--x 2 root root 0 Jun  9 02:41 .
dr-xr-xr-x 9 root root 0 Jun  9 02:41 ..
lrwxrwxrwx 1 root root 0 Jun 16 20:29 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Jun 16 20:29 ipc -> ipc:[4026531839]
lrwxrwxrwx 1 root root 0 Jun 16 20:29 mnt -> mnt:[4026531840]
lrwxrwxrwx 1 root root 0 Jun 16 20:29 net -> net:[4026531957]
lrwxrwxrwx 1 root root 0 Jun  9 02:41 pid -> pid:[4026531836]
lrwxrwxrwx 1 root root 0 Jun 16 20:29 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Jun 16 20:29 uts -> uts:[4026531838]
root@test-kub-02:~#

мы видим что у контейнероа созданного руками в докере : 
cgroup -тоже что и  на хосте
ipc,mnt,net,pid - разные с хостом
user - одинаковое с хостом
uts - разный

это интересно.





и так получается что информация о чужих нам процессах она отображена в 

--------- 



 




:~# user add test2
No command 'user' found, did you mean:
 Command 'fuser' from package 'psmisc' (main)
 Command 'userv' from package 'userv' (universe)
 Command 'kuser' from package 'kuser' (universe)
 Command 'users' from package 'coreutils' (main)
user: command not found
root@test-kub-01:~# useradd test2
root@test-kub-01:~# su - test2
No directory, logging in with HOME=/
$
$
$
$ id
uid=1001(test2) gid=1001(test2) groups=1001(test2)

==

DNS система куба:

	DNS имя пода (как выглядит)

если под является часть сервиса а сервис опубликован в неймспейсе,
то под будет иметь DNS имя

имя-пода.имя-сервиса.имя-неймспейса.svc.cluster.local

	DNS имя сервиса (как выглядит)
выглядит как

имя-сервиса.имя-неймспейса.svc.cluster.local

есть такой момент особенности.
если сервис обычный то этот dns имя резолвится в IP адрес сервиса.

а если сервис headless то его имя резволится в ip адреса подов которые 
под ним ходят.

	SRV записи
вначале поговорим что это вообще такое а потом как это сделано у куба.
значит DNS сервер держмт в своем чреве разные типы записей. что 
это значит на практике. это значит что разные типы записей хранят
разные типы полей. например. запись типа A она хранит IP адрес. пример

vasya.ru A 192.168.1.1

тоесть если мы говорим днс серверу дай нам запись типа А с именем vasya.ru
то мы ожидаем что в ответ получим IP адрес в ответ.
так вот dns сервер может отдавать нетолько ip адрес в ответ.
например есть запись типа SRV. если мы ее запросим то в ответ
мы получим сразу аж четыре параметра в ответ

 priority       = 0
 weight         = 100
 port           = 3268
 svr hostname   = dc-01.mk.local
 
 то есть смотри в чем фишка здесь что мы в ответ получили не ip адрес
 а совсем другие поля. то есть dns сервер умеет хранить нетольок ip адреса
 в своих ячейках но и другую информацию. все зависит от типа ячейки.
 
 чем хороша ячейка SRV. она позволяет нам узнавать на каком порту и на 
 каком сервере  у нас в домене работает токойто  сервис. в целом сервис имеется ввиду в абстрактном смысле ( то есть ни tcp\udp ни сервисы куба нипричем). сервис ровно 
 втом смысле в котором нам самим нравится. единсвтенное как я понял что 
 данный сервис должен базироваться на некотором транспортном протоколе
 потому что в ответе будет дан порт этого транспортного протокола.
как я понял в кроме tcp\udp есть другие транспортые протоколы такие как
QUIC\RSVP к примеру. 

имя SRV записи имеет вид

_service._proto.domain
 
 знаки подчеркивания обязательны (такой дебилизм). 
 service - это символьное обозначение нашего сервиса например vasya
 proto - это символьное обозначене транспотного протокола например tcp\udp
 domain - имя домена
 
 примеры
 
 _sip._tcp.mk.local = сервис sip на основе tcp в домене mk.local
 в ответ будет дан
 
 priority       = 0
 weight         = 100
 port           = 3268
 svr hostname   = dc-01.mk.local
 
значит мы знаем что в домене нашем сервис sip доступен на сервере
dc-01.mk.local на tcp порту 3268 с таким то приоритетом и весом

приоритет это на какой сервер лезть первее. а вес указан как относительный
приоритет если несколько серверров с одинаковым приоритетом (такой дебилизм) что если и приориттет и вес для нескольких сервров один и тот же незнаю.
 
 
 еще примеру мы хотим узнать 
 на каком порту и на каком сервере у нас в домене доступен сервис хапрокси. тогда мы можем запросить у домена эту инфо
 
 тип записи = SRV
 имя записи = _haproxy._tcp.cluster.local
 
получим

 priority       = 0
 weight         = 100
 port           = 3268
 svr hostname   = dc-01.mk.local
 
 сервис http
 
 тип записи = SRV
 имя записи = _http._tcp.cluster.local
 
 сервис куку
 
 тип записи = SRV
 имя записи = _kuku._tcp.cluster.local
 
 в целом удобная штука
 
 и насколько я понял также должна быть A запись в dns системе 
 для имени сервера который был возвращен в SRV записи, например выше
 был вовзращен dc-01.mk.local значит для него еще должна быть A запись 
 на DNS сервере. чтобы мы в итоге знали и числовой порт и IP адрес
 сервера куда обращаться.
 
 
 
 так вот возвращаемся к DNS серверам куба 
 у него тоже есть SRV записи.
 
 об этом написано здесь
	https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
	https://github.com/kubernetes/dns/blob/master/docs/specification.md

значит когда мы создаем сервис и в нем создаем именованные порты то 
для них создаются SRV записи. пример сервиса

	apiVersion: v1
	kind: Service
	metadata:
	  name: ss-6-expose
	  namespace: es-office
	  labels:
		app: es-6
	spec:
	  type: NodePort
	  ports:
	  - port: 9300
		targetPort: 9300
		protocol: TCP
		name: tcp-elastic-transport
		nodePort: 30893
	  selector:
		app: es-6


сервис имеет 
	имя ss-6-expose
	неймспейс es-office
	порт 9300
	протокол tcp
	имя порта tcp-elastic-transport
	
значит полное доменное имя для сервиса будет
	ss-6-expose.es-office.svc.cluster.local
	
тогда полное доменнное имя к SRV записи для порта 9300 будет

	_tcp-elastic-transport._tcp.ss-6-expose.es-office.svc.cluster.local

запросим эту запись с dns

использование nslookup

$ nslookup -type=тип-записи имя-записи имя-DNS-сервера


$ nslookup -type=SRV \ _tcp-elastic-transport._tcp.ss-6-expose.es-office.svc.cluster.local \ 10.252.3.65 \

_tcp-elastic-transport._tcp.ss-6-expose.es-office.svc.cluster.local     service = 0 100 9300 ss-6-expose.es-office.svc.cluster.local.

вспоминаем что нам должен DNS сервер вернуть :
 priority       = 0
 weight         = 100
 port           = 3268
 svr hostname   = dc-01.mk.local
 
 значит тогда 0 это приоритет
 100 это вес
 9300 это порт
 протокол tcp это мы сами знаем на основе _tcp  в имени
 dns имя сервера = ss-6-expose.es-office.svc.cluster.local
 
 тоесть все параметры мы получили
 
 я бы назвал записи типа SRV никаким незаписями сервисов. я бы их назвал
 записями именованных портов.или именнованных сокетов.
 
 также я бы сказал что реализация SRV записпей в кубе буквально бесполезная. в классическом домене (типа виндовс AD домен) мы реально 
 можем с помощью SRV записей чтото искать чегото незная. как я уже приводил выше пример. мы знаем только что именованный порт имеет имя sip
 и знаем имя домена. делая SRV запрос мы узнаем и сервер и порт. еще 
 раз пример. 
 делаем запрос к DNS серверу для имени = _sip._tcp.cluster.local
 
получим

 priority       = 0
 weight         = 100
 port           = 3268
 svr hostname   = dc-01.mk.local
 
 и узнаем что именованный порт sip находится на сервере dc-01.mk.local на порту 3268
 
 ничего подобного мы неможем сделать в кубе.
 если мы знаем имя именованного порта например tcp-elastic-transport
 (ну и протокол tcp) то мы  абсолютно неможем найти через запрос к dns куба ни номер порта ни сервер. почему? потому что мы неможем делать
 запрос в виде _имя-порта._tcp.домен так несработает. как видно выше
 надо делать запрос в виде _имя-порта._tcp.полный-путь-к-сервису
 
 _tcp-elastic-transport._tcp.ss-6-expose.es-office.svc.cluster.local
 
так мы и незнаем ни имя сервера ни имя сервиса. мы этого ничего и незнаем.
так что мы несможем составить имя SRV записи чтобы его запросить.

а если мы знаем полное домеенное имя сервиса но незнаем имя порта
то как я понимаю мы тоже неможем узнать имя порта и тоже неможем составить
полное имя для SRV записи и неможем его запросить. вобщем то херня какаято . непонятно какойто тоогда от этого SRV в кубе смыал на практике

для сервиса куба у которого есть cluster-ip SRV запись в качестве
сервера будет возвращать просто dns имя сервиса. пример как выше

сервис ss-6-expose в неймспейсе es-office в домене cluster.local
имеет порт elastic-transport

тогда SRV запись для elastic-transport вернет в поле имя сервера dns имя самого сервиса
 ss-6-expose.es-office.svc.cluster.local
 
ну оно и понятно. ибо сам сервис и облуживает этот порт.то есть сокет 
для данного порта выглядит как 
	IP сервиса:порт
	
тоесть еще раз сам сервис и является типа сервером обслуживающим порт.

а вот если сервис без cluster-ip типа headless
то понятно что данный сервис резволится в ip подов . они являются бекендом
для сервиса а значит и для портов этого сервиса

пример для headless сервиса.

$ kubectl get svc ss-6-internal    --namespace=project-2
NAME            TYPE        CLUSTER-IP      PORT(S)   
ss-6-internal   ClusterIP   None            9200/TCP 


$ kubectl get svc ss-6-internal    --namespace=project-2 -o yaml
...
 - name: http-elastic-client
    port: 9200
    protocol: TCP

проверяем что SRV запись для http-elastic-client есть и куда она ведет

делаем запрос
$ nslookup -type=SRV  _http-elastic-client._tcp.ss-6-internal.project-2.svc.cluster.local 

ответ
     0 50 9200 10-252-1-51.ss-6-internal.project-2.svc.cluster.local.
     0 50 9200 10-252-3-70.ss-6-internal.project-2.svc.cluster.local.

мы узнаем что порт = 9200
и мы узнаем что он крутится на двух подах. едиснвтенное что мне 
удивительно это то что в именах подах стоят IP адреса а не символьные
имена.вот это странно. 

узнаем какой адрес у первого пода

$ nslookup -type=A 10-252-1-51.ss-6-internal.project-2.svc.cluster.local

Address: 10.252.1.51



пример SRV записей для именованных портов дляобычного сервиса с cluster-ip

$ kubectl get svc ss-6-expose  --namespace=es-office
NAME          TYPE       CLUSTER-IP       PORT(S)          
ss-6-expose   NodePort   10.99.248.42      9300:30893/TCP  

видно что сервис имеет cluster-ip

$kubectl get svc ss-6-expose  --namespace=es-office -o yaml
...
 - name: tcp-elastic-transport
    nodePort: 30893
    port: 9300
    protocol: TCP

видно что сервис имеет именовыенные порты


проверяем что SRV запись для tcp-elastic-transport есть и куда она ведет

$ nslookup -type=SRV  _tcp-elastic-transport._tcp.ss-6-expose.es-office.svc.cluster.local

ответ

0 100 9300 ss-6-expose.es-office.svc.cluster.local.

убеждаемся что для класического сервиса для его портов бкендом является
сам этот сервис с его cluster-ip

также хочу сказать что SRV записи создаются кубом нетолько для именованных портов сервиса но и для самого сервиса.

https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
https://github.com/kubernetes/dns/blob/master/docs/specification.md

об этом в первой ссылке ненаписано а во второй написано.
так что если есть сервис то для него на dns куба создается SRV запись

вот примеры

$ nslookup -type=SRV  ss-6-expose.es-office.svc.cluster.local 
0 100 9300 ss-6-expose.es-office.svc.cluster.local.

$ nslookup -type=SRV  ss-6-internal.project-2.svc.cluster.local  

0 50 9200 10-252-1-51.ss-6-internal.project-2.svc.cluster.local.
0 50 9200 10-252-3-70.ss-6-internal.project-2.svc.cluster.local.

ха! отсда сразу видно какие порты имеет данный куб сервис
и куда они ведут!

елинственное что непонятно абсолютно это то что судя по википедии
https://ru.wikipedia.org/wiki/SRV-%D0%B7%D0%B0%D0%BF%D0%B8%D1%81%D1%8C
на мой взгляд такие имена в SRV записях просто недопустимы какие использует куб. вот что непонятно.

о! я нашел там же оказвыется я был неправ. типичное dns имя пода выглядит как 

pod-ip-address.my-namespace.pod.cluster-domain.example

типа такого

172-17-0-3.default.pod.cluster.local

под который создан стейтфулл сетом имеет такое dns имя

IP адрес + имя сервиса

типа такого


10-252-1-51.ss-6-internal.project-2.svc.cluster.local


я взял под

elasticsearch-data-0 
	у него ip = 10.252.2.107
	он лежит в default неймспейсе
	
я проверил запрсом к DNS что этот под имеет внатуре вот такой
dns name

$ nslookup  10-252-2-107.default.pod.cluster.local      10.252.3.65
ответ
Address: 10.252.2.107

  я вот чего удивлен. было бы логично если бы под имел имя в DNS такое
  же как его hostname. но этого нет. он именуется по своему IP
  


 вообще в плане имени пода либо сервиса и его записи в dns куба
 стоят вот такие вопросы
		- какое имя у записи
		- какого она типа 
		- на что она ведет указывает

для практических целей я выяснил вот что:

для пода:
	если мы знаем его IP + его неймспейс  то тогда в DNS есть запись 
		- ее имя IP.namespace.cluster.local
			например 10-252-1-44.default.pod.cluster.local 
		- ее тип  А 
		- ведет она понятно на 10-252-1-44 ip пода 

	
для сервиса у которого есть cluster-IP:
	если имя сервиса = vasya и его неймспейс kuku 
	то в dns есть запись 
	- ее тип А
	- ее имя vasya.kuku.svc.cluster.local
	- ведет она на cluster-ip
	
	пример
	$ nslookup  -type=A  elasticsearch-data.default.svc.cluster.local
	Name:   elasticsearch-data.default.svc.cluster.local
	Address: 10.106.20.86

	также для этого сервиса есть запись вида SRV.
	с тем же именем. то есть
	- ее тип SRV
	- ее имя vasya.kuku.svc.cluster.local
	- ведет она на порты которые обслуживает этот сервис а сервервами 
		явлется сам этот сервис
	
	пример
	~$ nslookup  -type=SRV  elasticsearch-data.default.svc.cluster.local      

	0 50 9200 elasticsearch-data.default.svc.cluster.local.
	0 50 9300 elasticsearch-data.default.svc.cluster.local.

	в итоге обращение полезно узнать какие порты открывает данный сервис
	и понятно что они обслуживаются cluster-ip сервиса.
	пример 
	
	сервис vasya. его ip=10.10.10.10 его порты 9200,9300
	вот этот сокет 10.10.10.10:9200 10.10.10.10:9300 и обслуживает
	этот сервис
	
если сервис headlees то у него нет cluster-ip
вместо этого его А запись выдаст несколько ip адресов под которыми 
крутятся поды этого сервиса
таким образом если мы сделаем запрос типа А к этому сервису
то мы узнаем все IP подов которые под ним круттся

пример

$ nslookup  -type=A  elasticsearch-data-headless.default.svc.cluster.local      

Address: 10.252.2.107
Address: 10.252.1.44
Address: 10.252.2.110

вот. под этим сервисом крутится три пода.

как видно 
		- имя записи имя-сервиса.нейимпейс.svc.cluster.local
		- тип А
		- ведет на IP адреса подов
		
если мы обратимся к той же DNS записи сервиса но тип SRV
то 

$ nslookup  -type=SRV  elasticsearch-data-headless.default.svc.cluster.local      

0 16 9200 elasticsearch-data-2.elasticsearch-data-headless.default.svc.cluster.local

0 16 9300 elasticsearch-data-2.elasticsearch-data-headless.default.svc.cluster.local

0 16 9200 elasticsearch-data-0.elasticsearch-data-headless.default.svc.cluster.local

0 16 9300 elasticsearch-data-0.elasticsearch-data-headless.default.svc.cluster.local

0 16 9200 elasticsearch-data-1.elasticsearch-data-headless.default.svc.cluster.local

0 16 9300 elasticsearch-data-1.elasticsearch-data-headless.default.svc.cluster.local

мы видим что данный  сервис имеет порты 9200 и 9300
а серверы которые их обслуживают это поды. указаны их dns имена
что примечательно что  получается в случае когда поды работают 
под управлением именно headless сервиса то поды имеют нзвание не в 
форме ip а уже в форме 

хостнейм.имя-сервиса.неймспейс.svc.cluster.local


elasticsearch-data-0,1,2 это именно имя хостнейма

далее я проверил на другом headless сервисе его SRV записи и там поды неимеют имени в форме хостнейм. там поды имеют классическое имя в виде IP.

из чего я делаю вывод что это завиисит не от сервиса а от доп полей 
в шаблоне пода. если там присутствует доп поле какоето  то 
под будет иметь имя в форме хостнейм а не в форме  IP

хотя вот здесь https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
прямо пишут что якобы у headless set поды всегда имеют имя
в виде имя-сервиса-цифра.имя-сервиса.неймспейс.svc.cluster.local

но как я уже сказал у меня почемуто в случае одного из heqdless сервиса
поды неимеди такого dns имени. они имели ip в имени выглядело это вот так

pod dns имя = 10-252-1-51.ss-6-internal.project-2.svc.cluster.local
почемунезнаю.

если при старте аписервера активировать фичу SetHostnameAsFQDN
то тогда при создании пода будет браться его dns имя до первой точки и прописываться
внутрь пода в его hostname. 
скжаем для только что указанного примера внутрть хоста будет прописан hostname=10-252-1-51

ну это так просто для справки

тут мне иентересно стало а какой dns сервер указан в resolv.conf
в подах

оказалось

$ sudo kubectl exec -it es6-cluster-0 --namespace=project-2  -- cat /etc/resolv.conf                            
nameserver 10.96.0.10
search project-2.svc.cluster.local svc.cluster.local cluster.local mk.local
options ndots:5

что это за 10.96.0.10

оказывается это сервис для coredns серверов

$ kubectl get svc --namespace=kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)
kube-dns   ClusterIP   10.96.0.10   <none>       53/UDP,53/TCP,9153/TCP

так что это удобно. вместо того чтобы каждый раз искать новый IP 
от пода coredns мы всегда обращаемся к неизменному адресу сервиса

--
далее я в файле local provisioner.txt 
описал как установит и юзать pv(local) + local provisioner
и что это такое
--

в итоге вот что стаовится понятно. 
вот у нас есть куб и  в нем куча опубликованных yaml файлов. 
каждое приложение это куча разных yaml сущностей.
надо както это все поддерживать (уметь удалять нарпимер).

нужно либо место где будут лежать все yaml файлы по папочкам. и ты 
такой смотришь - ага вот эта прога занимает эти yaml файлы
чтобы ее удалить надо пройтись по этим файлам. то есть нужен гитлаб
для храннеия этих yanlk файлов упорядоченных

или например нужно публиковать yaml файлы в кубе через helm
тогда у нас каждая программа как единоце целая как публикация видна в хельме. и мы по крайней мере можем ее удалить целиком. 
значит надо научиться для любого yaml файла превращать его в helm chart
потому что helm он непубликует просто yaml файлы. он публикует чарты.
но это надо учить как создавать чарты

если этого неделать то очень скоро все сущности в кубе станут просто месивом непонятным что является частью чего и если удалить это на что 
это повлияет.

есть еще вариант исполтьзловать gitpos проги.

можно конечно для начала пойти еще более простым путем.
поставить приватный git сервер. 
там публиковать yaml. и руками их оттуда забирать и накатывать.
или удалять.

значит я выбираю такую схему:
	код публикуется в git ( пакет git )
	код визиуализируется через gitweb+nginx
	код оттуда пихается в куб через fluxcd
	
установка git+gitweb+nginx расписана в файле git+gitweb+nginx.txt

 пока все таки остановимся на варианте без fluxcd
 просто все изменения публикаций в куб идут через yaml которые публикуются в git
 ====
 
 как в кубе посмотреть спмсок подов на ноде
 
  kubectl get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name --all-namespaces -n elastic-prod | grep kub2-06
  
 ===========
 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ 
ЗАНОВО НАЧАЛ ВСПОМИНАТЬ АРХИТЕКТУРУ  КУБА (2022 ГОД):

прежде чем начать чтот делать с этой парашей к8 надо 
поставить minikube. 
УСТАНОВКА MINIKUBE:
кстати его установка это тоже своего рода ебала.
значит я сделал так. я создал lxd виртуалку. и внутри нее 
поставил миникуб. туда же я поставил kubectl. поехали:

создаем lxd виртуалку
 	 lxc image list ubuntu: | grep ubuntu | grep 86_64 | grep 22.04|    grep 2023 | grep Apr | grep VIRT

 	 lxc image copy ubuntu:f1dd0e76efe8   local:
	 lxc launch  local:f1dd0e76efe8  minikube3

далее нам нужно виртуалку подправить так как миникуб требует
2 цпу и 2000МБ памяти иначе шлет нахер
в итоге надо добавить чуть больше так как часть сьест гипервизор
 	 lxc stop minikube3
 	 lxc config set     minikube2  limits.memory 2400MB
   lxc config set     minikube3 limits.cpu 2
   lxc start minikube3
заходим внутрь виртуалки
   lxc exec minikube3   -- bash

миникуб можно ставить поверх докера или подмана или как бы 
на голую ос. задается это через 
	$ minikube start --driver=docker/podman/none

однако я вот что скажу. если ставит на голую виртуалку (на ос) без
использования докера или подмана то нужно дохера доустанавливать
будет руками поэтому нахер такой вариант. установка поверх подмана
тоже несмогла успешно тработать сука. поэтому ставим поверх докера.
это единственна установка которая работает. 
значит нужно для начала поставить докер

    apt-get update
    sudo apt-get update
    sudo apt-get -y install ca-certificates curl
    sudo install -m 0755 -d /etc/apt/keyrings
    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc

    sudo chmod a+r /etc/apt/keyrings/docker.asc

    echo   "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
       $(. /etc/os-release && echo "$VERSION_CODENAME") stable" |   sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

    sudo apt-get update
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
    docker run hello-world

 все докер поставили . теперь можно ставить миникуб
    curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb

    sudo dpkg -i minikube_latest_amd64.deb
    minikube start  --force --memory=1900mb --driver=docker

ключ --force нужен чтобы ставится под рутом. 

теперь надо поставить kubectl 
    snap install kubectl --classic

наконец ебала закончена. можно проверить что куб 
отвечает сука
   $  kubetl get nodes
КОНЕЦ УСТАНОВКИ MINIKUBE


далее вот что скажу. вот у нас есть виртуалка. вот мы в нее 
докер поставили. вот мы на это поставили minikube. это вызывает
то что создается контейнер докера внутри которого крутится к8.
так вот если я остановлю виртуалку то и докер контейнео тоже 
остановится. так вот - очень важная мысль - если я хочу обратно
запустить миникуб то мало запустить докер контейнер. так несработает!
куб не запуститься. а правильный метод такой - после запуска 
виртуалки нужно запустить команду
	minikube start
и только так у нас снова запустится к8.
прикол в том что внутри контейнера докера у нас создаются субдокер
контейнеры. это реально прикол.

остановить миникуб
	minikub stop

запустить миникуб заново
	minikub start

ТОЕСТЬ еще раз важно понять вот что - если я перезапустил 
виртулку то нужно при этом еще запустить руками повторно миникуб
руками. сам миникую нестартует автоматом. для этого
внутри виртуалки запускаем 
	minikub start
а точнее (потому что мы работаем под рутом)
	minikub start --force





есть такой стандарт OCI. что он определяет: 
	- the container runtime spec
	- image specifications 
	
утилита skopeo. как ее поставить. ставим podman. а далее 
	$ podman run docker://quay.io/skopeo/stable:latest copy --help
skopeo это утилита от редхат вроде бы. ее плюсы - она позволяет
опериповать имаджами. не требует прав рута. не требуем демона.
позволяет посмотреть свойства имаджа удаалеенно не загружая на комп.
в целом неясно нахрен она нужна. (размножение утилит и программ теперь
имеет форму эпидемии)


  config map
это такая хрень которая физически хранится в etcd и она используется для того чтобы там хранить конфиги в виде ключ значение
которые можно прикреплять к подам и другим сущностям, тоесть под при старте может использовать конфиг мап как конфиг файл
например для нжинкса который стартует в этом поде. тоесть можно  к поду прикрепить persistent volume  в котором будет конфиг для нжинкса а
можно создать конфигмап и прикрепить его к поду вместо persistent вольюма. в чем разница persistent volume хранится на сторадже тот который общего 
назначения а конфиг мап хранится в etcd
единственное что я не понимаю как именно config map лежаший в etcd 
впихиывается внутрь контейнера. через какой механизм? ведь внутри
контейнера в итоге должна видим оказаться какая то папка подмонтррованная
или как?




pod
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-1
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2   <-=== имаджж. это понятно
    ports:
    - containerPort: 80   <-=== это хрен знает пока что значит

оформляем это в файл и запускаем
	$ kubectl apply -f 1.yml

посмотреть статус
	$ kubectl get pods
	$ kubectl describe pods POD_NAME

прикол в том что мы увидим IP пода (скажем 10.244.0.3). но мы не сможем до него запинговаться изнутри нашей виртуалки. также несработает curl 
потому что ситуация такая- у нас есть виртуалка. в ней вращается
контейнер в докере. внутри короторого работате куб. и внутри
той же виртуалки работает наш под с жинксом. поэтому если мы 
хотим достучаться до жинкса надо заходить внутрт контейнера
с кубом. тоесть

	$ docker ps
	$ docker exec -it 1bdff4b43d6f bash
	$ curl 10.244.0.3

где 1bdff4b43d6f это номер докер контейнера внутри кторого 
работает куб

также скажу что (по крайней мере в случае разворота на миникубе)
этот под пингуется и доступен по порту 80 изнутри миникуба (изнутри
виртуалки)







про статусы подов.
вот я взял и перезагрузил виртуальки на которых работает куб.
и вот я проверяю статус подов.
$ kubectl get pods
NAME                               READY   STATUS         RESTARTS        AGE
counter-f679bbcf7-5s4tw            1/1     Running        0               3m52s
counter-f679bbcf7-flzxq            1/1     Terminated     0               130m
counter-f679bbcf7-mfbdg            0/1     NodeShutdown   0               3m54s
my-replicaset-1-fm8x8              1/1     Running        2 (7m3s ago)    30h
my-replicaset-1-xzfvg              1/1     Running        2 (5m51s ago)   30h
nginx-deployment-9456bbbf9-5xhts   1/1     Running        2 (5m51s ago)   30h
nginx-deployment-9456bbbf9-6l7jb   1/1     Running        0               2m47s
nginx-deployment-9456bbbf9-ddgjt   1/1     Terminated     1 (133m ago)    30h
nginx-deployment-9456bbbf9-qn2wm   0/1     Completed      0               6m34s
nginx-deployment-9456bbbf9-qwx6z   1/1     Running        2 (5m51s ago)   30h

видны поды со стремным статусом: Terminated, NodeShutdown, Completed.
значит что выяснилось по этому поводу - если мы виртуалки где куб возьмем и перезагрузим
то для куба такой расклад это типа полуаварийная ситуация потому что перед тем как тушить 
ноду надо делат drin node и прочее чтобы куб "мигриовал"(на самом деле никакой там нет миграции
он тушит поды на одном хосте и запускает на другом что это за миграция такая). и тогда это 
штатная ситуация по пезеграузке ноды. а так получается то что аварийная пеерзагрузка ноды и 
тогда вот что получается - если поды был запущен просто как под то после перезагрузки виртуалок
куб этот под незапустит нет. а тот экземпляр который был запущен до рестарт виртуалок
он будет висеть в статусе terninated, nodeshutdown и прочие ужасы. если под входит в состав 
реплики сет то после рестарта виртулок куб запустит новые экземпляры подов а старые будут 
висеть со стремными статусами. это какойто тупой пиздец. логика 
охуенная.
вот пример

NAME                               READY   STATUS         RESTARTS        AGE
counter-f679bbcf7-5s4tw            1/1     Running        0               3m52s
counter-f679bbcf7-flzxq            1/1     Terminated     0               130m
counter-f679bbcf7-mfbdg            0/1     NodeShutdown   0               3m54s

эти поды входят в состав реплики сет в которой указано что нам нужно 1 под
	$ kubectl get rs
	NAME                         DESIRED   CURRENT   READY   AGE
	counter-f679bbcf7            1         1         1       8d

сотвественно после перезагрузки вииртулок у нас есть один под running как и заказно в реплике сет.
а два других пода которые остались с момент до перезагрузки они висят со стремными статусами
и их нужно вычищать руками.


а вот когда под был запущен внутри миникуба то почемуто при рестарте
миникуба  под , ( отдельный под ) тоже почемуто успешно запущен.
странно.



?? надо научитья находить поды конетйнеры которые terminaed и прочие стремные
на хостах есть ли они там или они только на api сервере висят как обьекты в его базе.
далее показать как их вычищать руками.
?? из каких процессов состоит под на хосте?
&& deployment и pv. он один у всех подов?
&& crictl ?



если убить под на хосте через kill -9 $PID то куб его перезапустит.
тогда вопрос разница между подом и реплика сет ?



deployment относится к  контроллерам куба. 

depployment создает replica set. она уже в свою очередь создает поды. ( что на счет pvc он крепится один ко всем подам или на каждый под свой
pv?)

нельзя так сделать чтобы был создан деплоймент без реплики сета. нельзя так сделать чтобы был создан деплоймент который напрямую создает и управляет
подами.

как я понял если нужно поменять характеристики подов то деплоймент создает новую реплику сет с новыми подами а старую реплику сет он уничтожает.
таким образом деплоймент неоперирует подами напрямую. он оперирует репликами сетами.

формально на уровне манифестов  деплой и реплика сет очень похожи
так в чем же разница. просто дело в том что внутри манифеста деплоймента описываются параметры реплики сета которой он будет рулить.


\\\ DEPLOYMENT

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
  labels:
    my-label: my-value
spec:
  replicas: 3
  selector:
    matchLabels:
      my-label: my-value
  template:
    metadata:
      labels:
        my-label: my-value
    spec:
      containers:
        - name: app-container
          image: my-image:latest
configuration of replica set

\\\

значит что  я проверил из практик - дейтсивельно если поменять
манифест у деплоя. (скажем версию имаджа) и накатить этот манифест
то деплой сделает вот что - он создает +1 новый rs и  в нем 
прибавляет число подов. а в старом rs удаляет 1 под. и так до тех 
пор пока в старом rs не станет число подов равно 0. причем при этом
старый rs  в итоге не удаляется. он продолжает висеть. если еще 
раз модифицировать deploy и накатить манифест то будет создн 
еще один rs и два старых в каждом из которых число подов равно 0 
будут прододжать висеть. 
значит еще раз - мы написали манифест деплоя. накатили в куб.
он создает rs. а этот rs создает поды. далее если мы меняем деплой
так что никакиз измеенений особых это не несет например мы в 
манифесте поменяли число реплик и накатываем манифест снова в куб
либо мы вручную поменяли число реплик через командную строку 

	kubectl scale deployment --replicas=2 depl1

то куб тогда все делает  в рамках этой же rs.
а если изменеия серьезные (например мы поменяли версию имаджа)
то деплой создает новый rs и в нем начинает прибавлять число подов
на 1. а  в старом rs он начинает убавлять число подов на 1. 
и так до тех пор пока в старом rs число подов не станет равен 0.
далее что интересно старый rs неудаляется. он продолжает висеть и 
сущестовать в кубе. насколько я понял далее куб сохраняет штук 8
или 10 старых rs.

далее вот что. у пода неважно он порожден в рамках стандалон 
или в рамках rs. так вот. у пода может быть в спецификации такое 
поле как labels. так вот если у нас есть rs, и оно породило поды.
так вот если мы создадим теперь руками стандалон под у которого
будут все теже самые labels то система автоматом это замечает и 
система (точнее rs) считает что появился под который принадлежит
этому rs, далее происходит вот что rs проверяет сколько подов 
сейчас есть в системе и сравнивает с тем числом которое заявлено
в манифесте rs. в данном случае rs оббнаруживает что число подов
превышает это число. и тогда rs УНИЧТОЖАЕТ этот последний созданный
нами руками под. таким образом если в системе появился под созданный
руками и он имеет теже самые labels что и поды которые принадлежать 
rs то этот rs замечает это и считает что данный под тоже принадлежит
ему. и если число подов превышает заданное то данный под будет уничтожен.   и тут я хочу добавить то что к8 при создании подов
через rs то к8 автоматом добавляет свой кастомный label к таким 
подам. важно понять что такой лейбл отсутствует в манифесте rs,
этот кастомный лейбел автоматом добавляет сам к8. 
щас покажу на примере. покажу на примере деплоя. да это не rs.
но деплой порождает rs.

# cat dep1.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: depl1
  labels:
    my-label: depl1-lbl
spec:
  replicas: 3

как видим в манифесте есть только лейбел 
	my-label: depl1-lbl
а теперь посмотрим какие лейбел имеют поды которые породит этот 
манифест

# kubectl   describe  pods  depl1-787b5d7547-52955
Labels:           my-label=depl1-lbl
                  pod-template-hash=787b5d7547


как видно помимо лейбла указанного в манифесте   my-label=depl1-lbl
у нас под имеет и еще один лейбел которго нет в манифесте
	pod-template-hash=787b5d7547

это важно понять. поэтому если мы создадим руками вот такой под

# cat pod.yml 

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-2
  labels:
     my-label: depl1-lbl

то никаких проблем с этим подом не будет. он будет успешно создан
и никаких проблем с этим подом не будет. rs на этот под никакого 
внимание не обратит. потому что данный под не имеет полного набора
лейбл которые имеют поды созданные rs. а если мы создадим вот такой
под 

# cat pod.yml 

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-2
  labels:
     my-label: depl1-lbl
     pod-template-hash: 787b5d7547
spec:

то вот такой под уже будет замечен rs. и rs грохнет этот под.
ибо если на тот момент у нас уже будет полное число подов созданных rs
то этот под получается лишний. и rs его грохнет. то есть такой под
будет причислен к подам которые контролиуруются rs. которые принадлежат
к rs. и rs заинтересуется этим подом. 




\\\ REPLICA SET


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replicaset
  labels:
    my-label: my-value
spec:
  replicas: 3
  selector:
    matchLabels:
      my-label: my-value
  template:
    metadata:
      labels:
        my-label: my-value
    spec:
      containers:
        - name: app-container
          image: my-image:latest 

\\\


далее пишут вот что . что нужно проверить 
Lets say you use ReplicaSet-A for controlling your pods, then You wish to update your pods to a newer version, now you should create Replicaset-B, scale down ReplicaSet-A and scale up ReplicaSet-B by one step repeatedly (This process is known as rolling update). Although this does the job, but it's not a good practice and it's better to let K8S do the job. A Deployment resource does this automatically without any human interaction and increases the abstraction by one level.

тут  я напишу одну полезуную вещь. вот у нас есть манифест
rs. мы в нем ничего поменять не можем кроме как последней секции
где указан параметр самого пода. например номер версии его имаджа.
все остальное менять бесолезно. при попытка накатить такой манифест
к8 пошлет нахер и напишет что поля которые мы модифицированы они
не подлежат измнению. окей с этим разобрались. хорошо мы хотим
поменять версию имаджа в подах. мы меняем ее версию в манифесте
и накатываем его. и тут важный момент ЭТО НИКАК НЕ изменит
те поды которые уже работают под управлением этого rs. да. нихуя
это их не изменит. новая версия имаджа будет ТОЛЬКО в тех подах
которые мы развернем новые. тоесть если мы через scale увеличим
число подов на 1. то у этого нового пода будет новая версия имаджа.
тоесть мало того что нам надо поменять манифест нам надо после этого убить все старые поды тоесть сделать scale=0. а потом обратно прибавить
scale скажем до 10. тогда у нас все старые поды будут удалены и 
развернуты новые поды с новой версией имаджа. вот такая бабкина схема.
если у нас поды отвечают за некоторую степень отказоустойчивости
и\или поток распараллеливается на них. то нам соверешенно не подход
ит схема эта. кстати я щас еще кое что проверю. итак я проверил вот что
положим мы развернули два пода с версией имаджа 1. потом мы меняем
манифест на имадж 2. потом мы прибавляем число подов на 1. 
проверяем версию. у нас два пода с версией имаджа 1 и один под с
версией имаджа 2. потом я уменьшаю числоп подов до двух. вопрос
какой под буде уничтожен - последний с более новой версией имаджа
или один из старых? сколко бы я ни пробовал убивается всегда тот 
под который был создан последним. это значит что как бы  я не изьебы
вался у меня не получится путем манипулиций с числом подов сделать
так чтобы у меня как то потихоньку удалились старые поды. тоесть
если бы при уменьшении числа подов к8 удалял самый старый под 
то можно было бы дергая число подов то на +1 то на -1 можно было бы
вытравить из этого множества поды со старым имаджем при этом бы
у меня отказоусточивость менялас лишь чуть чуть. на 1 под. нет
так не получится. если я хочу удалить все старые поды это можно 
сдеалать толко одним споосбом - надо довести число подов до 0.
тоесть мое приложение будет 100% не доступно. будет даунтайм.
и только доведя число реплик до 0. уничтожатся все поды старые. 
и потом обратно поднимая число реплик появятся новые поды а старых
уже не будет. это полный пиздец с точки зрения доступности приложения.
фактически это аналог того что я полностью останаливваю свое
приложение. полностью его гашу. и потом запускаю обнолвенный бинарник
и тогда оно имеет обновленную версию.
поэтому вверху по английски и описано что если я хочу обновить прилож
ение которое сидит на групе подов (котоыре управляются репликой сет)
то едиснвенный нормальный вариант (чтобы отказоустоцчиовоасть и парале
льность страдала но лишь чуть чуть) состоит в том что мы создаем
+1 новую rs. в ней пропсиываем новую версию имаджа. далее мы на новой
rs прибавляем число подов на +1. а на старой rs убавляем число подов
на -1. только так можно более менее аккуратно обновить приложение.
можно конечно тогда и по другому сделать. можно на новой реплике
выставить такое же число подов как на старой реплике. а потом
каким то макаром перенарпавтиь трафик со старых подов на новые поды.
ну или сделаь так чтобы старые сессии заканчивали свою работу на старых
а весь новый трафик тек только на новые поды.  в любом случае
без созданя новой реплики не обойтись. вот такая суть.
тогда это получается что по сути реплика по своей природе предна
значена за управлением группы подов с фиксированной версией имаджа.
они наметртво приварены друг к другу. если у нас версия имаджа
изменилась то старые поды и реплика которая за них отвечала они 
ВМЕСТЕ идут в помойку. тоесть поды это колеса, двигатель, кресла, руль
а реплика это корпус автомобиля. если внутри все сгнило то на свалку
идет ВСЕ ВМЕСТЕ и то что внутри и обертка. да.  реплика сет это как 
бы обертка вокруг конкретной версии подов. если поды идут на свалку
то и ихняя реплика тоже идет на свалку. они составляют одно целое.
охуеть. как только мы хотим поменяь поды мы должные четко сразу
понимать что и реплика тоже идет на свалку. вместе с этими подами.
едиснвтенный случай когда реплика может продолджать работу это если 
мы можем себе позволить довести текущее число подов до нуля. тогда
можно изменить параметр имдажа в текущей реплике. и использовать
эту текушую реплику для новых подов. но обычно так делать нельзя.
поэтому надо четко еще раз скажу предсталвят - кажой новой версии
подов должна выдываться своя новая реплика. и они вместе рожддаются
и умирают тоже вместе. поды это внутрение органы  а реплика это
тело в которое эти органы напихиваются. они рождаются вместе 
и умирают вместе. этого эти суки не пишут. доходи до этого сам.
получается что надо всегда думать не так что поды это расходник.
а так что поды это расходник и реплика сет это тже расходник. 
и что 
		 поды + репллика сет = одно целое
конкретная версия подов имеет реплику сет. это единое целое.
оно одновременно создается. и умирают они идут в помойку тоже
исключиетельно вместе. это как бахилы для больного. каждому 
новому больному свои бахилы. и кода больной выходит из больницы
то и бахилы эти сразу идут в помойку. 





далее интересные штуки я обнаружил.
первая,  rs недобавляет к подам кастомные лейблы как это делает
деплой. тоесть ровно те лейблы которые мы в манифесте прописали у rs
ровно только эти лейблы и будут добавлены в поды. возникает 
тогда вопрос. а что будет если я создам два rs у которых в манифесте
будет указаны одинаковый лейбл. как тогда будет ? у нас первый rs
создаст поды. они заработают. потом создам второй rs он создаст поды
с точно таким же лейбл. и что будет? по идее первый rs увидит что 
появились поды с таким же лейблом как у него и наверное будет конфликт?
оказывается нихуя не будет конфликта. новые поды успешно создадутся. 
но как это может быть ? оказываетя как я понял - если мы создали голый  
под стендэлон, то  у него в манифесте отсутствует строчка 
		Controlled By:  ReplicaSet/rs-2
в это строке указано какая выщестоящая хрень контролирует этот под.
как я понимаю в этом случае куб натраливает на этот под rs те что есть
в надежде что у этого пода лейблы совпадут с лейблами в rs и если это 
так то rs начнет принимать решение что с этим подом надо делать. например удалить его. потому что да. самое частое что будет с подом
который мы запустили руками и который был захвачен репликой сет это 
то что реплика сет видит что нужное число подов и так уже есть
тогда этот новый под нахер ненужен и она его грохает. вот и вся история
который обычно при этом бывает.
Если же под создается через rs то у него 
как я понимаю уже при создании это поле 
		Controlled By:  ReplicaSet/rs-2
уже есть. и поэтому к8 абсолютно похеру что поды с таким же лейбл
сушествуют в другом rs. это становится нихуя неважно. важно то что 
к8 видит что данный под не просто сам по себе болтается а он уже 
находится под управлением вышестоящей структуры в данном случае 
под управлением rs. поэтому другим rs абсолюино нет никакого дела
до данного пода внезависиомсти от того что его лейблы совпадают с
ихними лейблами. это пиздец. потому что когда то я читал такую инфо 
что если под имеет лейбл который есть у какого то rs то этот rs 
автоматом считает этот под своим (захватывает его)  и отсьюда 
идут последствия. в частности этот rs видит что подов стало больше
чем у него указано и он этотт под грохает. 
оказывается все работает по другому. если у пода нет строчки
	Controlled By:
то тогда действительно на этот под натравливаются все rs котоыре
есть в системе. и какойто из них захвыатывает этот под под своюю 
власть и решает дальге что с этим подом делать. обычно он его 
просто гроахает. а если такая строчка есть то к8 понимает что 
данный под был создан в рамках манифеста какогото rs тоесть у пода
есть уже хозяин и значит друггого хозяина ему искать ненадо.
поэтому в этом случае у нас аболсютно спойокно в системе может
болтаться поды которые имеют одинаковые лейблы но при этом
они будут управляться принадлежать разным rs. и в этом нет никкой
проблемы. офигеть.
также еще раз напомню что если rs при создании пода ему назначает 
только те лейблы которые мы указали в магнифесте rs то деплой
создаст rs в который запихнет не только лейблы из манифеста но 
и добавит свой некоторый автоматический лейбл. а этот rs в свою
очередь создаст поды, получается нетолко с лейблом который мы указали
в манифесте но и с автоматичеким лейблом котоырй соформировал придумал
к8 при создании деплоя. деплой как уже понятно сам напрямую за подами 
не следит. он имим не занимается он их НЕ создает. деплой создает
и удалеяет только rs. а уже rs заниматеся подами. 

	depploy ---> rs ---- > поды

деплой это генерал. rs это сержант. поды это лохи рядовые.

вторая особенность о которой впрочем я уже сказал состоит в том
что таким макааром у нас в к8 могут крутится поды с совершенно
одинаковыми лейблсами но при этом они будут принадлежать совершенно
разным rs. и никакого конфликта не будет.

значит я щас что сделаю. я создам деплой. в манифесте укажу лейбл.
покажу что он создаст rs в котором будет дополниельный лейбл 
которого у нас в манифесте нет. потом я покажу что в поде будет
оба лейбла. 
потом я создам rs который будет иметь тот же лейбл что и в манифесте
деплоя. и что в системе будет создан еще под у который будет этот лейбл
и не будети никакого доп лейбла.  и то что никкакого конфликта
между этим подами не будет.
потом я создам еще один rs в нем укажу такой же лейбл. и еще
в манифесте укажу тот доп лейбл который есть в деплое. и покажу
что под который будет создан не вызовет никакого конфликта 
с деплоем. что деплой а точнее егоный rs непопытаетя захватить
власть над этим подом что он останется во власти этого третьего rs.
поехали
apiVersion: apps/v1
kind: Deployment
metadata:
  name: depl1
  labels:
    my-label: depl1-lbl
spec:
  replicas: 1
  selector:
    matchLabels:
      my-label: depl1-lbl
  template:
    metadata:
      labels:
        my-label: depl1-lbl
    spec:
      containers:
        - name: depl1-nginx
          image: nginx:1.14.0

при этом в свойствах деплоя (смотрим через describe) кастомного лейбла нет. есть
только лейбл указанный в манифесте

Labels:                 my-label=depl1-lbl

деплой создал rs: depl1-787b5d7547
а вот в его свойствах уже имеем ДОП лейбл

Labels:         my-label=depl1-lbl
                pod-template-hash=787b5d7547  <==== вот он 

мы видим что rs имеет лейбл  pod-template-hash=787b5d7547 
которого нет в манифесте

ксатти в этом rs написано что он находится по контролем деплоя
		Controlled By:  Deployment/depl1
что странно в describe у rs это то что в нем не указано как 
назвыаются поды которыми щас этот rs руководит. 
тоесть в describe деплоя указано какие rs он руководит
было бы логично чтобы в describe для rs была бы информациякакие 
имена подов он руководит. это тупая странная хрень.
вот у нас есть rs. как нам узнать какие поды сидят под этим  rs.
и вот  я узнал в инете как нам найти поды которые принадлежат
rs или деплою - решение такое искать поды используя поиск 
используя label поля в свойствах подов. тоесть вот я выше
в свойствах rs я имею лейблы

Labels:         my-label=depl1-lbl
                pod-template-hash=787b5d7547


мы знаем что если у rs в его свойствах стоят такие лейблы значит 
и поды порожденные этим rs будут иметь ровно такие же лейблы. значит
мы будем искать поды которые имеют такие же лейблы и это можно 
сделать вот так

# kubectl get pods --selector  my-label=depl1-lbl,pod-template-hash=787b5d7547

NAME                     READY   STATUS    RESTARTS   AGE
depl1-787b5d7547-vgqqr   1/1     Running   0          10h

хочу заметить что флаг --selector не имеет отношения к полю selector
в свойствах пода он имеет оотношение к полю labels , об этом
можно прочитать тут  $ kubectl get pods --help

и вот в свойствах подя через describe я вижу

Labels:           my-label=depl1-lbl
                  pod-template-hash=787b5d7547

и написано кто этим подом управляет
		Controlled By:  ReplicaSet/depl1-787b5d7547


итак наш под имеет два лейбла. один (my-label=depl1-lbl) тот который указан в манифесте
деплоя. а второй (pod-template-hash=787b5d7547) автоматический созданный самим к8.

тоесть важная мысль в том что под созданный в итоге в результатте
публикации деплоя у нас имеет нетолько лейбл который прописан
в манифесте деплоя но и лейбл которго нет в манифесте , его сам создает
и назначает сам к8 автоматом

теперь создаю rs в котором будет лейбл (my-label=depl1-lbl) из
манифеста деплоя

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-3
  labels:
    my-label: depl1-lbl
spec:
  replicas: 1
  selector:
    matchLabels:
      my-label: depl1-lbl
  template:
    metadata:
      labels:
        my-label: depl1-lbl
    spec:
      containers:
        - name: nginx
          image: nginx:1.14.0


и тут выяснилась интересная вещь!
данный rs создается но pod она не создает. точнее он создатеся
и тут же убивается! смотрим влогах rs

----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  10m   replicaset-controller  Created pod: rs-3-7jqqq
  Normal  SuccessfulDelete  10m   replicaset-controller  Deleted pod: rs-3-7jqqq
  Normal  SuccessfulCreate  4m6s  replicaset-controller  Created pod: rs-3-sgnp8
  Normal  SuccessfulDelete  4m6s  replicaset-controller  Deleted pod: rs-3-sgnp8

типа что за хрень?

напоминаю что у этого rs лейбл my-label=depl1-lbl такой же как прописан в деплойменте. смотрим на всякий случай логи describe в деплое

Events:
  Type    Reason             Age                From                   Message
  ----    ------             ----               ----                   -------
  Normal  ScalingReplicaSet  19m (x2 over 26m)  deployment-controller  Scaled down replica set rs-3 to 0 from 1


тоесть деплой увидел что реплика сет rs-3 имеет такой же лейбл
какой лейбл указан в свойствах деплоя. и деплой данный rs захватил
этот rs под свой контроль! и это видно в свойтвах этого rs!

	# kubectl describe  rs  rs-3
	Controlled By:  Deployment/depl1

тоесть изначально я опуликовал просто независмый rs но 
его заметил деплой и он ЗАХВАТИЛ этот rs под свой контроль!

а вот интересное что есть в свойствах деплоя

OldReplicaSets:  rs-3 (0/0 replicas created)
NewReplicaSet:   depl1-787b5d7547 (1/1 replicas created)

тоесть деплой заметил rs-3 с его лейбл my-label=depl1-lbl 
и деплой принял решение что данный rs должен принадлежать ему, 
перейти к нему под его контроль. но (видимо) так как деплой
текущий имеет два лейбла , точнее сам деплой имеет всего
один лейбл my-label=depl1-lbl но внутри в своих кишках он 
имеет вторйо лейбл который он назначает тому rs который на данный
момент является текущим для этого деплоя. поэтому как только 
деплой захыватывает власть над rs этим то он в этой реплике сет
число подов переводит в 0. поэтому мы выставили в манифесте rs
число подов в 1 а деплой переводит сам это число в 0!
далее я подумал ну окей rs-3 имел только один лейбл. а если я 
опубликую rs-4 котоырй будет иметь два лейбла - оба лейбла
которые есть в текущем реплике сет порожденной деплоем. что тогда будет делать деплой? ведь у него есть текущий активный rs а мы создаем еще один rs
у котрого есть такие же самые лейблы.  так вот практика
показала что  "активный" реплика сет останется прежним!
он останется неизменным. а захваченная реплика сет созданная
нами руками она будет просто напросто переведена в статус "OldReplicaSets" что на практике значит что поды созданные 
этой репликой будут срочно переведены по числу в количество 0 штук.

вот так это хитро работает.

возникает вопрос - при каком условии деплой  считает что rs 
который появился в его поле зрения считает что этот rs можно и 
нужно захватить? что если в деплое изначально скажем прописано
два лейбла. а в rs созданном рукаме у нас один лейбл. будет ли 
такой rs захыватывтаь наш деплой ?

вот я создаю деплой вот такой
  name: depl2
  labels:
    label1: lbl1
    label2: lbl2

в свойствах деплоя этого тогда я вижу вот такое
		OldReplicaSets:  <none>
		NewReplicaSet:   depl2-b6dcc5c8d (1/1 replicas created)


теперь я создаю rs вот такой
	name: rs-5
  	labels:
    	label1: lbl1


так вот практика показала что наш rs созданный руками который содержит
только часть лейблов которые указаны в деплое, то такой rs наш деплой
НЕТРОГАЕТ. и такой rs не будет захвачен 
нашим деплоем.
если я модифицирую rs добавив второй лейбл который у нас есть 
в деплое 

	name: rs-6
  	labels:
    	label1: lbl1
    	label2: lbl2

то такой rs уже попадет в сферу интересов деплоя и он будет 
захвачен в сферу управления деплоем
и вот так будет записано в деплое после захвата rs-6

OldReplicaSets:  rs-6 (0/0 replicas created)
NewReplicaSet:   depl2-b6dcc5c8d (1/1 replicas created)

тоесть rs-6 будет захвачен но он сразу будет переведен в 
разряд "старый rs" и его число подов будет переведено в 0.
а текущий "активный rs" останется неизменным.

итак более менее все стало понятно как работает поведение 
между развернутым деплоем и разворачиваемый руками rs.
как деплой ловит этр rs. или не ловит. при каком условии. и что 
делает деплой если он отловил rs. 

далее
что происходит если мы развернем под в ручную а  у нас есть rs
у которых ровно такой же лейбл тоже теперь понятно. такой под
будет отловлен rs и скорей всего rs этот под грохнет. потому
что сам rs обычно уже сам насоздавал нужное число подов а туут 
вылез какойто доп под который просто ненужен . поэтому его 
rs грохнет.

теперь что произодует если мы создадим два rs которые никак не связаны
с деплоями (они их не вылавливают) и оба этих rs имеют в своих 
свойствах имеют один и тот же лейбл. то что у нас будет с подами
которые создадут оба этих rs. ответ говорю заранее - поды от обоих 
rs будут успещно созданы и не будут мешать друг другу. да у
них будут одинаковые лейблы. но у них будет указано что часть подов
под уравлением одного rs а часть от другого rs. и никаких проблем.
а что будет если при этом создать стендэалон под у которого такой
же лейбл?- будет то что я написал в предыдущем абзаце. данный под
будет "захвачен" одним из rs (кто первее того и тапки) и далее такой
под скорее всего будет просто убит, потому что rs уже насоздавал
до этого нужное число подов и этот новый ему нахер ненужен.
показываю
создаю два rs

# cat rs-7.yml 
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-7
  labels:
    lbl: lbl7
spec:
  replicas: 1
...


# cat rs-8.yml 
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-8
  labels:
    lbl: lbl7
spec:
  replicas: 1
...


в итоге будет  создано два пода. 
которые имеют одинаковый лейбел

# kubectl describe  pods   rs-7-998c2 | grep "Label"
		Labels:           lbl=lbl7
# kubectl describe  pods   rs-8-7c8bb | grep "Label"
		Labels:           lbl=lbl7

и без всяких проблем.
и при этом один под принаддлежит одному одному rs 
а второй под принадлежит второму rs


# kubectl describe  pods   rs-7-998c2 | grep "Controlled By"
		Controlled By:  ReplicaSet/rs-7

# kubectl describe  pods   rs-8-7c8bb | grep "Controlled By"
		Controlled By:  ReplicaSet/rs-8


вот эту информацию которую я накидал выше надо останвится
и осознать

также получается интересная вещь - за подами охотятся rs.
если мы публикуем сендэлон под у которого строка
		Controlled By:  
пуста то это под беспризорник и за ним сразу начинают охотиться
rs. и если лейбл совпадет то контроль за подом будет захвачен
этим rs. 

Более того, совершенно по такой же аналогии за rs тоже охотятся. охотится за ними деплои. как тольлко
мы опубликовали rs  и у него также остуствует строчка
		Controlled By:  
это значит что этот rs это бсерипризорник и это сразу ощеривает
деплои. они все набрасываются на этот rs проверяя совпадение лейблов
и если лейблы совпадут то этот деплой захыватывает этот rs. 
а далее деплой в этом rs изменяет число подов на  ноль. и в свойствах 
деплоя этот rs будет переведен в статус "OldReplicaSet"
такие дела..


удаление деплоя автоатом удаляет и rs которые он 
контролировал. это хорошо

насколкьо я понимаю то rs может неуправлять ни одним подом
потому что scale может быть равен 0

что касается деплой то как я понимаю то неможет такого быть
что есть деплой и нет ни одного rs которым он управляет. такого
быть неможет. если есть деплой то обязательно есть егоный rs.

если под был порожден самой rs то имя пода будет начинаться
с имени rs

далее я выяснил вот что. если мы создали стендэлон под. 
потом создали rs такой что у него лейбел такой же как у стенд
элон пода то при запуске rs он этот под захватит и сделает
частью себя. скажем у rs прописано scale=2 тогда будет создан 
один новый под а  в качестве второго просто будет захвачен 
этот уже существующий под. 
а с деплоем все  несколько интереснее. положим у нас есть запущенный
rs. у него есть лейбел. далее мы запускаем деплой у которого 
такой же лейбел. тогда деплой при запуске захватит этот rs 
и сделает его своим "старым" rs.  далее он уменьшит в старом rs 
число подов на один. далее деплой создаст сам свой новый "активный"
rs и увелчичит в нем число подов с 0 до 1. далее он будет последователь
но уменьшать на один число подов в старом rs и увеличивать на один в 
новом пока в старом число подов не станет 0. а в новом число подов
не станет равным тому числу что прописано в деплой манифесте.
вот лог такого процесса
  Scaled down replica set rs-1 to 1 from 2
  Scaled up replica set depl1-586c56c8d8 to 1 from 0
  Scaled down replica set rs-1 to 0 from 1

что при этом забавно это то что поды уже существующего rs 
будут уничтожены неиспользованы. а буудут использованы только новые
поды которые породит сам деплой. это несколько отличается от того 
от того что описал несколько выше от случая когда у нас стенд элон
под запущен и мы разворачиваем rs. этот rs захыватывает наш под
но в отличие от деплоя наш rs неуничтожает уже развернутый под
а интергирует его в себя и использует. это забавно.

что интересно в describe например пода если при
глядеть то в колонке From указано кто какой компонент
оставил сообщение. как видно ниже это 
шедулер  и кубелет. это забавно

 Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  70s   default-scheduler  Successfully assigned default/depl1-586c56c8d8-6v99v to minikube
  Normal  Pulled     67s   kubelet            Container image "nginx:1.14.0" already present on machine
  Normal  Created    67s   kubelet            Created container depl1-nginx
  Normal  Started    67s   kubelet            Started container depl1-nginx


когда деплой запускает rs а тот в свою очередь запускает поды
то в совйствах пода будет написано что под управляется rs
а свойствах этого rs указано что он управляется деплоем

в итоге получается примерно следущая картина - если  мы разворачи
ваем новый стендэлон под. когда он запустился то посколку в его
свойствах неуказано чтобы он (под) был под управлением rs 
поэтому на под набрасываются все rs пррверить несовпадает ли лейбл
у пода с леблами у rs. если это окаыватеся так то под оказывается
захвачен rs. и тот rs как правило просто уничтожает этот под.
если же ниодин rs непризнает интереса к этому поду он выжиывает 
и начинает жить в системе. такова судьба нового пода.

если мы развернули новый rs то у него в его свойствах нет того
чтобы он был под управлением когото. поэтому на этот rs сразу 
набрасываются все деплои которые работают в системе. они начинают
яростно сравивать свои лейблы с лейблами этого rs.  и если 
лейблы совпадают то какойто деплой захватывает этот rs. и в тот 
же момент деплой начинает уменьшать до нуля все то число подов 
которое указано в rs. в итоге данный rs не будет управлять ни 
одним подом. такова судьба нового развернутого rs.
если же ни один деплой не признает в этом rs своего то rs 
останется stand-alone rs. и тогда этот rs успешно развернет все те
поды которые записаны в его манифесте. такова судьба ново разверну
того rs.

если мы развернули новый деплой. то его судьба такова - деплой 
стартует. он начинает искать есть ли уже в системе rs который бы
обладал бы всем лейблс как у в манифесте деплоя. если деплой находит
такой rs то он его "захыватывает" и причисляет к своим "старым" rs
далее деплой создает новый rs и причисляет его к своему "активному" rs
(дада у деплоя может быть куча старых rs но толлько один активный)
далее деплой уменьшает число подов на 1 на старом rs и прибавляет число
подов на активном rs. и так пока на старом rs число подов не уйдет 
в ноль а на активном не станет равным числу поодов указанному в 
манифесте деплоя. если же деплой ненаходит в системе rs. то он 
создает новый rs и херачит запускает через него то число подов 
которое указано в манифесте деплоя. такова судьба нового запущенного
деплоя....
у меня только один вопрос - если у нас системе есть несколько rs
одновреенно каждый из которых имеет лейблы как у деплоя. такое ведь
возможно. он что их все захваит, сделает старыми и уменьшит их 
поды до нуля? щас проверю - проверил, оказалось что да - при 
запуске деплой захватит ВСЕ rs которые совпадают с ним по лейблам
и все их он сделает старыми. ихние все поды он сведет к нулю.
он создаст абсолютно новый rs и в его рамках запустит новые поды
которы в итоге и будут крутиться в системе. 
отсюда полезный вывод - каждый стенд элоун rs долэен иметь свои 
лейблы чтобы при развороте какогото нового деплоя он случайно
не захватил неожиданно назапланированно их в свою власть , 
по крайней мере он сможет захватить только один rs максимум. 

получается поды эта та самая конечная хрень на нижнем уровне 
где круится приложение. rs эта хрень которая создана чтобы следить
за кучкой клонов одной группы подов чтобы следить что число подов
не больше и не меньше чем это заказано. если под упал то rs это
увидит и заново поднимет новый экземпляр. итак rs следит чтобы
число подов было равно заданному. также через rs можно поменять
число подов. вот и вся функция у этого rs. проблема с rs 
в том что если мы хотим обновить поды запустить там более новую 
версию приложения то rs плохо это умеет делать. ( яэто описывал выше)
единственный способ убрать все старые поды и заменить их на новые 
поды это вначале обязательно уменьшить число старых подов до нуля.
и только после этого вернуть обратно число подов до заданного.
у нас не получится через rs развернуть новые версии подов а потом
убит старые версии подов. этого rs делать не умеет. если мы увеличим
число подов то новые поды будут новыми. но как нам избавитьмя от ста
рых? поды rs удаляет только если мы уменьшает число эксземпляров но
проблема в том что rs удаляет поды в обратном порядке. вначале он 
удаляет самые свежие и последними самые старые. поэтому чтобы обнов
ить поды нам прихоиися идти друим путем. создавать новый rs там 
разворачивыать новые поды. а потом в старом rs  уменьшать число
подов до нуля. а потом и вовсе удалить старый rs. это ручная работа
поэтмоу ее решили автоматичзироваь и придумали деплой. он это 
делает автоматом. тоесть он делает ровно это но без ручного участия. 

уже более менее с подами, rs, деплоями проясняется, что зачем и 
почему с ними.
пока на это все.. пауза.

двигаем  дальше. получается что в целом что можно выжать из 
того что я описал выше про поды, rs, deploy => публткацией подов
и rs не стоит заниматься вручную. это типа внутренняя кухня к8,
руками надо пбуликовать только деплои. ниже опускаться ненадо.
если хотим обновить версию программы (которая в конечно итоге крутится
в подах) то нужно поменять манифест деплоя и переапплаить его.
далее деплой автоматом погасит старые поды и поднимет новые поды.
вот это и есть мой главный вывод из того что выше описано.




двигаем дальше





создать одиночный под и убиить его. посмотреть воссоздаст ли 
автоматом его сситема.
я проверил - если убить руками процесс пода то к8 поднимет этот под.



реплика сет .надо понять что значит запускаются несколько копий 
того же пода. что это значит на уровне сеетевых настроек на уровне 
подов - какие там ip адреса, какие порты. нужно понять что 
одинакооового и что разного с точки зрения сетевых настроек, 
дискоовой файовой системы у подов которые в реплика сет
запущены как изнутри их неймсппейсов так и снаружи их неймспейсов тоесть с точки зрения неймспейсов хоста где они крутятся. что на счет PV если его прикручивать
к реплика сет. они что все к одному PV присасываются ? 
это надо все хрошо поонять чтобы потом понять разницу между 
отдельным подом и подами порожденными репликой сет и потом
между подами в statefull set. также нужно понять разицу между 
запуском отдельных подов и подами в реплике сет. в чем разцница.  
а потом разницу между деплойментом
и репликой сет. ==>
 как я понимаю разницу между подом и репликой сет. 
можно создать 5 подов отдельных у которых внутри нжинкс и
 конфиг внутри пода одинаковый на всех подах. а с точки зрения 
 хоста нжинкс с этих подов будет доступен 
 в виде IP1:port1, IP2:port2, IP3:port3 
 например 10.100.100.10:80, 10.100.100.11:80, 10.100.100.12:80,
10.100.100.13:80 итд.  
то есть зайдя на хост можно достучаться до любого нжикса 
по этим сокетам. между хостами сокеты доступны из за ухищрений 
в iptables и оверлейной сети. в рамках одного хоста эти ip доступны 
из за виртуального свича который крутится на хосте. далее мы хотим 
поиметь на кластере куба не 5 таких подов
а 100 таких подов. тут на помощь приходит реплика сет. 
вместо того чтобы в манифесте прописывать 100 конфигов для каждого пода. 
либо нужно 100 манифестов.
( при том что вручную прописывать ip адреса каждого пода 
ненужно, куб сам назначает ip адреса под которыми хост 
может достучаться до пода. по портам тоже самое ручного 
менеджмента ненужно. более того 
посольку каджый под доступен через свой индивидуальный ip то нет проблем
 с пересечением портов. более того если нескько нжинксов в подах 
 крутится на одном хосте
то тоже проблем с портами нет так как каждый под доступен по своему
 индивдуальному ip поэтому каждый нжинкс без проблем доступен 
 по порту 80. более того ненужна какая то спецаильная опция 
 чтобы пробросить порт изнутри пода наружу в нейсмпейс хоста. 
 если внутри пода сервис слушает какойто порт то автомтом 
 этот порт проброшен в сетевой неймспейс хоста, если в манифесте 
 пода указан паарметр port то этот параметр имеет чисто 
 информационное значение уведомительное для человека и он никак 
 не влияет на факт проброса или непроброса порта изнутри пода 
 в стевевоей неймспейс хоста. вот пример манифеста пода

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-6
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
---

в этом манифесте параметр ports имеет чисто увеомиельный характер никак не влияющий на факт проброса порта изнутри пода на хост.
) 

так вот получатеся что если мы хотим запутсить 100 подов 
то нам ненужно парится ни об пробросе портов изнутри пода на хосты 
ни об пеересчении портов на хоста
ни об ip. за назначние ip отвечает куб. пробросо идет вобще автоматом.
а пересечения портов нет потому что каждому поду свой индивильный ip.
таким образом проблема запуска 100 подов с нжикнсом с точки зрения 
манифеста упирвется лишь только в то что нам надо скопировать 
100 раз манифет пода и в нем менять имя пода. пример
для трех подов.

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-1
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
---

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-2
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
---

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-3
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
---

так вот реплика сет по сравннеию с ручным запуском кучи подов
 недает ничего особого неделает ничего особого кроме того что 
позволяет сэкономить места в манифесте. 
реплика сет это своего рода цикл for..next и вместо портянки 
на 100 подов мы будем иметь короткий текст манифеста


apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replicaset
  labels:
    my-label: my-value
spec:
  replicas: 100
  selector:
    matchLabels:
      my-label: my-value
  template:
    metadata:
      labels:
        my-label: my-value
    spec:
      containers:
        - name: app-container
          image: nginx:1.14.2


так что реплика сет неделает нихера кроме как того что сокращает
 размер манифеста и все. никаких доп функций по сравнению 
 с манифестом пода она неделает за исключением еще одного момента
 что она позволяет динамически менять число 
 реплик kubectl scale --replicas=2 rs/my-replicaset
тоесть еще раз реплика сет это всего наввсего цикл for для 
манифеста pod и больше ничего. никакого нового функционала реплика сет
над фукнционалом пода она неделает в плане сети и всего такого. 
получается запуск нескольких подов через реплику сет похож на старт 
нескольких классических systemd сервисов нжинкс на хосте которые читают
из одного конфига но биндятся не на 0.0.0.0  а каждый на свой 
индивидуаьльный ip адрес. вот и все если прводить аналогию с 
классческимим сервисами.

^^
||
\\
 \\
   == то что   я выше написал это я писал несколько раньше
чем то что я писал еще выше. поэтому оно малек не совсем продвинутое.
итак с точки зрения манифеста я согласен что манифест реплики сет
он позволяет компактно описать поднятие миллиона подов чем через
манифест подов. итак реплика сет дает более компактный по размеру
манифест чем манифест подов. второе что реплика сет дает возможность
динамически менять число подов в очень удобной форме. это очень
удобно. итак реплика сет дает вомзосжност легко менять число подов
как туда так и обратно. реплика сет является тем инстурментом к 
которому обращается деплой. возитться с отдельными подами это не 
комильфо. более точно - не просто с разными подами а  одинаковыми подами
с ихними экземплярами, однотипными подами. реплика сет не подходит
для упроавления разными подами, а только для управления одинаковыми
подами. реплика сет позволяет туда сюда (больше меньше) менять число
подов (экземпляров). делать это легко. изменив манифест реплики сет
можно в итоге удалить старые поды и поднять новые поды. но как я уже
писал выше на практике это неудобно потому что выпилить старые поды
все до одного можно только уменьшив число подов до нуля. поэтому
чисто техически можно c помощью модификации реплики сет обновить
версии подов (а точнее убить поды со старой версией имаджа и поднять
поды с новой версией имаджа) но из за того что в какойто момент
чило подов станет равно нулю ( а подругому поды со старой версией 
имаджа не выпилить ) то реплика сет намертво как единое целое 
связана с текущей версией имаджа с той с которой она и была изна
чально поднята. поэтому упрощенно можно сказать что на продакшене
обновить поды в текущей реплике сет нельзя. обновленный имадж надо
разворачивать через новую реплику сет. а старую гасить. именно
так и делаает деплой. делает он это сам освобождая нас от этого 
ручного труда. итак реплика сет позволяет нам легко менять число
клонов подов с одинаковым имаджем. версия имаджа константа.
с какой развернули с такой и работаем. также реплика сет следить
чтобы число клонов было константа. не больше не меньше. если 
поды падают она их строго запускает новые экземпляры. если появляется
лишний под то она его грохает. таковы функции реплики сет. 
подами вручную никто не работает. это малопродуктивно. да если под
упал то к8 его заново поднимет ( я проверил) но типа это слишком
убогий функционал. нам надо менять их число. то больше то  меньше.
нам надо чтобы число подов (клонов однотимных) было не больше 
и не меньше заданного числа. вручную (через манифесты подов) это 
было бы трудоемко. если у нас в системе появился (почемуто) лишний
экземпляр то это нужно было бы както самостояетельно отдельно 
выслеживать через систему мониторинга. поэтому реплика сет это автома
тизатор некоторых типовых но важных задач при lifecycle работы с 
подами (подчеркну с одинаеовыми подами клонами).

если мы разврачиваем приложение то нам автоматом нужна сразу куча
одинаковых подов потому что: нужно обеспречить отказоустойчивость,
нужно распараллелить нагрузку. реплика сет позволяет это все сделать
чтобы это не делать руками с манифестами одиночных подов.


ЗАДАНИЕ: надо поменять конфиг жинкса через конфиг мап. (при 
условии что поды созданы через деплой) заодно
станет понятно а можно ли к части подов подключить один
конфиг мап а к другой части подов другой. заранее думаю что так
нельзя

для начала как попасть внутрь контейнера  в поде

$ kubectl exec --stdin --tty shell-demo -- /bin/bash

тут я хочу обарратить внимание что у kubectl есть очень
интереснй ключ -- он ест по крайней мере в связке

$ kubectl exec 

и он обозначает для кубцтл что левее -- идут аргументы запуска 
самого кубцтл а все что правее -- это идет команда и ее параметры
которые будут запущены внутри контейнера!!!
тоесть еще раз -- оьясняет кубцтлю что левее -- идут опции запуска
самого кубцтл а правее -- идут параметры того что нужно запустить
внутри контйенера. чтобы было понятно чтобы было однозначно и четко.
жалко только то что описано это все кратко что чтобы понять все это 
нужно поебаться внаале с поиском и осознанеием.
более подробно откуда есть пошла хуйня -- я описал в bash.txt 
искать там нужно по 
                      | --
                      | double dash 


а вот еще я нашел в инете про -- в kubectl:
The double dash (--) in the command signals the end of command options for kubectl. Everything after the double dash is the command that should be executed inside the pod . Using the double dash isn’t necessary if the command has no arguments that start with a dash
(нашел тут https://stackoverflow.com/questions/69684693/why-does-kubectl-exec-need-a)
это тоже самое что я сказал просто другой это сказал
кстати вот еще пример про это же. программа lxc как зайти
в контейнер\виртуалку и выполнить там команду. а вот так

$ lxc exec minikube4 -- bash

как видим -- слева определяет аргументы которые относятся к запуску
команды lxc а аргументы справа от -- определяют параметры того что 
и как будет запущено внутри контейнера\виртуалки

переходим ближе к конфигмапу
значит что такое ConfigMap (пока так поверхностно), это как бы мы
можем в etcd засунуть хрень вида ключ:значение а потом ее совать 
внутрь пода в разном виде. один вариант можно выбирать из всего конфиг
мапа отдельные ключ:значение и подсовывать в ENV внутри пода. такое
я щас рассматривать не буду. второй вариант можно конфигмап смонтиро
вать в папку внутри пода. вот это я щас буду рассматривать.

пример конфиг мапа
замечу что конфигмап имеет имя special-config
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  SPECIAL_LEVEL: very
  SPECIAL_TYPE: charm
  VASYA: kuku

тогда мы 
$ kubectl apply -f file.yml
и можем убедиться то он имеет ровно такой же вид
$ kubectl get configmap 
$ kubectl get configmap -o yaml special-config


а вот как этот конфиг мап примонтировать в папку внутри пода
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod2
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "sleep 3600" ]
      volumeMounts:
      - name: config-volume
        mountPath: /var/run/01
  volumes:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never

получается конфигмап special-config "превращается" в 
вольюм "config-volume"
который монтируется в папку /var/run/01

спрашивается что мы увидим в этой папке? заходим внутрь пода(через ко
манду которая указана выше kubectl exec ...) и мы увидим три файла

# ls -1 /var/run/01
SPECIAL_LEVEL
SPECIAL_TYPE
VASYA

содержимое каждого файла равно значению для ключа как это прописано
в конфигмапе. получается для каждого ключа к8 создает в папке файл.
а внутрь файла сует значение.

кстати я проверил. если изменить манифест конфимапа наример добавить
новый ключ:значение то без проблем этот маифест можно накатить в к8
и новые ключ:значение (то бишь новый файл) мгновенно появится в поде
без всяких там перезагрузок итд. 

что еще я проверил - хотя свойствах пода указано что конфигмап
примонтирован внутрь пода в режиме RW но пофакту изнутри пода записать
чтото в эту папку не получится. пошлет нахер. напишет что доступ
только RO

поскольку у нас в деплое один шаблон пода для всех подов то отсюда
следует вывод что если мы примонтировали конфигмап в манифесте деплоя
то эта папка будет примонтирована во все поды. она для всех подов
будет иметь единый вид. нельза так сделать чтобы часть подов деплоя
имела примонтированный один конфигмап а другая другой конфимап.
так как каждый ключ в конфимапе превращается в файл то если мы таким
макаром зададим конфиг какойто проги то этот конфиг будет един для всех
подов. у нас получаетя поды деплоя имеют разные IP но на этом из индиви
дуальность заканчивается. они все во всех остальных смыслах одинаковые
безличные неиндивидуаьные. клоны. получается на какой под наш входной
запрос ни попадет он будет обработан одинаково. 

если вернуться к виду конфигмап манифеста
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  SPECIAL_LEVEL: very
  SPECIAL_TYPE: charm
  VASYA: kuku

то больше всего интерсеса предтсвляет секция data
здесь каждый ключ будет трансформирован потом внутри пода в файл
а значение ключа будет трансфорировано в содержимое файла.
теперь становиться понятно как с помощью конфиг мапа задавать конфиги 
в поде для программы. условно говоря если мы хотим в папку /etc/nginx/conf
засунуть конфиг nginx.conf через конфиг мап то это делается так
создаем конфиг мап

apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  nginx.conf: |
       blablabla
       blablala
       blablalbla


потом в манифесте деплоя мы пишем
apiVersion: apps/v1
kind: Deployment
...
...
    spec:
      containers:
        - name: nginx
          image: nginx:1.14.2
          volumeMounts:
          - name: config-volume
            mountPath: /etc/nginx/conf
      volumes:
          - name: config-volume
            configMap:
              name: special-config

и все будет в щоколаде

вот пример конфимапа который к8 по дефолту сует в каджый под
# kubectl get configmap  -o yaml kube-root-ca.crt
apiVersion: v1
data:
  ca.crt: |
    -----BEGIN CERTIFICATE-----
    MIIDBjCCAe6gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p
    ...
    ...
    yV6yD9OofFgYdA==
    -----END CERTIFICATE-----
kind: ConfigMap
...
...

соотсветвенно я смотрю в этот манифест и понимаю что в секции data
прописан ключ ca.crt со значением MII...
это значит что при монтровании этого конфиг мапа в папку в поде
будет создан файл ca.crt с содержимым ключа.
гениально.

так на первом этапе что такое конфигмап и что он нам дает если
его примаунтить в папку внутри поды мы поняли

кстати еще раз вопрос - что будет если запустить два rs у которых
один и тот же лейбл. что будет с подами? ответ с подами все будет
окей. поды каждой rs успешно запустятся. и не  будут друг с другом
конфликтовать. да у них (подов) одинаковый лейбл но у них в строке
	Controlled by
указано что они находятся под управлением rs значит к8 похеру на
все остальное. если бы под был стенэлон и не был бы управляем
никаким rs тогда да. тогда бы его захватил rs у убил бы. 
созвучный вопрос что если у нас два деплоя поднято с одинаковым
лейбл. будет ли проблема с поднятием подов между ними? ответ тоже
что проблем не будет. деплои породят rs а те породят поды которые
будут иметь от рождения строку controlled by где будет указан rs
поэтому эти поды не будут захватываться другими rs и они успешно
запустятся. никаких проблем.




вопросы: 
  - какой физ смысл статуса COMPLETED у подов ?
дело вот в чем. вот мы запустили под. неважно стэндэлон или 
порожденный rs или породжденный деплоем (а точнее опять же поро-
жденный егоным rs) и если процесс в поде закончил работу - например 
мы в поде запускали команду ls, то работа пода закончена потому что
процесс в поде закончил свою работу причем неважно он ее закончил
добровольно неизза ошибки тоесть штатно или процесс был убит ядром
из за ошибки внутри процесса - в любом случае процесс умер и под
подучается тоже умер. так вот тогда в пооле STATUS в kubectl get pods
будет указана Completed. вот что значит этот статус. дальше поведение
к8 зависит от настройки "restartPolicy" внутри пода. если она явно
не указана то по дефолту она равна "Always" это значит что если 
процесс внутри пода умер то к8 в этом поде заново запускает новый
контейнер с этим процессом (помним что под это не контейнер это 
группа контейнеров. ). под остатется тем же самым в нем лишь умирает
один из контейнеров. его то к8 буде заново запускать естественно
в новом контейнере ( а может к8 будет заново стартовать упавший тот 
же самый контейнер хер его знает это надо проверять) -значит
я проверил на пракктике - к8 непытается запустить прежний контейнер
нет. к8 создает новый контйеенер. вот что значит статус пода
Completed. откуда он берется и все такое. Если в манифесте пода
либо манифесте реплика сет или деплоя укзано 
	restartPolicy: Never
то тогда при остановке процесса внутри пода к8 не будет пытаться 
его перезапустить контейнер  с этим процессом. и этот под так и 
будет висет в системе (kubectl get pods) в этом статусе. 
получается согласно статусу под у нас будет сущесовтвать внутри него
только не будет контейнера с полезным процессом. что значит под
будет сущестовать - это значит что про него будут записи внури к8.
в частности логи. и наверное pause контейнер тоже будет существовать
жить в системе. как я читал что так сделано чтобы человек мог 
ознакомиться с логами пода. 
Если же в своствах манифесте пода\репики\деплоя указано 
	restartPolicy: Always
то к8 будет вместо упавшего контейнера создавать новый с полезным
процессом. Если он снова падает то снова создавать новый контейнер
с плезным прцоессом и так до беконечности. едиснвтенно что каждый
следущий пересоздание этого контейнера будет проходить все через
большую паузу. и это логично если видно что контейнер на стартует
нахрен его 100 раз в секунду пересоздавать. так вот в этом случае 
в свойствах такого пода будет статус CrashLoop что то в этом роде.
и тогда вия этот сттаттус мы понимаем что у нас в настройках 
пода стоит restartPolicy: Always , что полезный контейнер умер, 
система его создала и запустила снова а он снова упал. и это было 
неоднократно.


далее. у меня был вопрос в голове чем отличается под стендэлон
от пода порджаенного rs. с точки зрения сетевых свойств. так вот 
как я понимаю никаких особых отличий нет. внутри виртуалки на которой
поднимаются поды там стоит вирт свич. каждый под\контейнер создается
в своем сетевом неймспейсе потом пробрасыввыается в неймспейс 
хоста через veth мостик и как бы получается что внутри хоста 
поднята куча сетевых карточек L3 и на каждой карточке сидит свой
под. и кстати как я понял все карточки сидят в одной IP сети. 
скажем сеть 10.244.0.0/16  и когда под создается то его группа
контенйнеров получает IP из этой сети. поэтому похеру как бы создан
очередной под то ли как стендэлон то ли через rs то ли через деплой.
rs и деплой это лишь типа батч скрипт ( по аналогии с башем) создания
пода. хрен которая позвояет меньше работать руками не более того.
поэтому неважно каким оразом создан под. они все с точки зрения сети
одинаковые. они все просто напросто сидят  в одной плоской L3 IP 
сети 10.244.0.0/16 . вот так все просто. никактго магического 
отличия стенд элон пода от пода порожденного rs нет с точик зрения
его сетевых свойств. теперь это прояснено. теперь это стало понятно.
поды сидяший на рахных хостах всего навсего сидят в разных ip сетях
и прозрачно соединяются друг с другом через маршрутизацию но подам
об этом знать ненадо. а моет быть даже поды на разных хостах 
сидят в одной ip сети. щас я это проуерить не могу так как у меня
миникуб тоеть один хост.

<====== отановися тут




- обноление подов у реплики сет если мы обновили манифест реплики сет. 
как она делает. вначале убивает все старые пода а потом создает новые 
или как? ответ - нет. при накатке обновленного манифеста
во первых к8 проверить непоменяли ли мы такие поля в манифесте 
реплики сет которые менять нельзя. тоест в реплике сет после ее 
создания не все поля можно менять. если мы попытается так сделать
то к8 пошлет нахер и напишет что мы пытаемся модифицировать такие
поля которые модифицировать он недаст. а далее если это не так то
не произойдет нихрена. дело в том что можно подумать что если мы
поменяли манифет реплики сет и накатили ее то реплика сет убьет
старые поды и создаст ровно такое же число обновленных подов. так
подсказывает интуиция. однако это не так. это полная хуня. реплика 
сет несделает нихрена. она не тронет старые поды! если мы увеличим
число подов то новые поды (например у нас было до обновления 
манифеста указано в манифесте 6 подов, потом обновиили манифест,
и потом мы помняли число подов на 8. так вот дополниельные два пода
уже будут созаны на основе нового манифеста.) будут созданы на основе
нового манифеста. старые поды при этом останутся не тронутыми. 
если же мы хотим уничтожить старые поды то это можно сделать только 
одним способом нужно указать в реплике сет обновленной что число
подов ноль. тогда она уничтожит все поды. потом надо указать новое
число подов и она их создаст на основе обновленного манифеста.
вот так эта шарага раотает.

в чем разница как это делает деплоймент? ответ такой: заметим 
что если у нас было 6 подов. потом мы обновили манифест то нам 
чтобы убить старые поды и создать 6 подов на основе обновленного
манифеста нужно поменять число реплик до нуля. дождаться пока
все старые поды убдут уничтожены. потом поменять число реплик
до 6 и тогда будут созданые новые поды. и это все придется делать
вручную.  при этом можно заметить что если  наша коммерческая
программа крутилась на этих подах и мы захотели нашу прогу обгновить
то бишь обновить поды то у нас число подов будет доведено до 
нуля а потом уже до 6. тоеть наше приложение на какоето время
будет неработать будет даун тайм. в реальной жизни мы такое
себе позволить нихрена не можем. так вот как работает обновление
подов через деплой. во первых когда мы развернули деплой то он 
автоматом создает реплику сет. если мы обновили манифест деплоя то 
деплой создает новую реплику сет. далее деплой сам автоматом в старой
реплике сет уменьшает число подов на 1 а в новой реплике сет он
увеличивает число подов на 1. таким макаром в итоге старой реплике
сет число подов станет ноль а в новой оно станет равно заданному числу.
и все это будет сделано автоматом. нам тока нужно обновить манифест
деплоя и накатить. еще я напомню еще такой момент - если мы только
что развернули новый деплой то он сразу ищет по всем репликам
сет нет ли такой у которой будет тот же набор лейблов что указан
в манифесте деплоя. и если он такую или такие реплики сет находит
и эти реплики сет ненаходсят под управлением какого ниубудь другого
деплоя то наш деплой захватывает эти реплики сет. и присваивает
им внутри себя статус "старые" реплики. далее он создает новую реплику
сет. и далее он начинает в старые репликах сет гасить в них поды
а в созданной им реплике сет начинает поднимать новые поды. таким
образом во всех захваченных репликах сет число подов в итоге станет
равно нулю. и только в реплике созданной им самим число подов станет
равно заданному числу. еще раз подчеркну при каком условии реплика 
сет может попасть под власть деплоя. нужно чтобы выпонислось обяза
тельно два условия. первое- реплика доожна быть бесхозная стенд
элон. тоест она не должна находистя по упоавлением какого ниубудь
дргого деплоя. в своствах реплики сет это выгляди ттак что остуствует
строка "Controlled by". второе условие - реплика должна иметь все
те же лейблы что манифест деплоя. 

тут я хочу сделат ремарку что вот мы обновили манифест деплоя,
накатили, у нас деплой обновил поды. так вот (я проверил) при 
этом поды новые по сравнению с подами старыми получают новые
другие IP ! скажем было два старых пода запущенных деплоем 
и они имели IP 
	10.244.0.92
	10.244.0.93
а новые поды будут иметь IP
	10.244.0.94
	10.244.0.95
то это значит что запуск подов через деплой еще нам не дает воз-
можность работы с подами с точки зрения обращения к ним от других
каких то программ. потому что у нас меняются IP. значит нужно что
то еще добавлять в эту шарманку. например хотя бы в DNS регистри
ровать эти IP и обращаться к этим подам по DNS. но как ясно реше
ние через DNS это все равно гавно. потому что dns кэшируется на компе
и значит обращение от внешнего приложения к этим подам может не полу
чаться потому что нужно на внешней проге обновить будет кэш dns.
короче решение через dns это гавно. нужно чтото другое и понятно
что этот компонент котоырй нужно добавит это Service. об этом
будем далее разговаривать. 

также нужно заметить что во время обновления когда у нас  встарой
релпике сет деплой уменьшает число подов (гасит их) а в новой 
реплике сет он поднимает новые поды - так во время этого процесса
у нас получается существуют и старые поды и новые поды и строго
говоря у нас внешнему приложению доступны как старые поды так
и новые поды. тоесть у нас на какоето время у нас доступна
сборная солянка из старой и новой версии подов.


далее я еще раз подчеркиваю про то как работает реплика сет.
а именно то что - положим у нас есть реплика сет и она развернула
поды. потом мы обновили   манифест и накатили его.
так вот с этим подами ничего не произойдет. нихуя не произойдет.
тоесть скажем у нас 
			уже запущено две пода 
			с нжиксом 1.14.1 , если мы поменяем манифест реплики сет на
			 нжикнс 1.14.2 и применим его то неизменится ничего на этих 
			 подах. ничего. они так и останутся нетронытыми и 
			 раотающими на 1.14.1 . если же мы после этого скажем реплике 
			 сет что число реплик уже не две а три то новый +1 под будет
			  запущен уже на 
			нжинксе 1.14.2 , таким образом мы будем иметь два нжинкса 
			на 1.14.1 и третий на 1.14.2. вот это жопа. таким оббразом
			 получается что единственный способ
			внатуре обновить поды котоыре упралвяются рпликой сет это
			 поменять манифест. накатить его на куб. далее сделать 
			 scale=0 тоесть убить все поды до единого.
			и потом сделать scale=исходное_число. только таким макаром мы 
			будем уверены что поды под управлением реплиики сет имеют 
			ровно тот конфиг которые задан 
			в обновленном манифесте. более того я проверил такой способ.
			я пробовал дергать число подов в большую сторону и в 
			меньшую сторону. много раз. и четко видно что к8 если он 
			удаляет под то строго тот который был запущен последним по 
			времени. таким образом дергая число реплик туда сюда невозможно
			убить старые поды. тоесть мы взяли изменили число реплик +1
			потом на -1, потом опять +1, потом опять -1. таким макаром
			мы старые поды не вытравим нихуя. единственный способ это 
			уменьшить число реплик в ноль. и потом меняем число реплик
			уже на заднанное скажем 10. и все наши 10 подов будут
			иметь уже ту версию имаджа как в манифесте.


<===== закончил тут



- так как у нас под крепитя к реплике сет через label что будет если в реплике сет уже созано сколько надо подов но мы создадим еще руками отдельно подов
		через манифесты подов с таким же label. будет ли както их трогат рпика сет или нет. почему это важно потому что деплоймент он как обновляет поды.
		он создает новуб реплику и начинает там поднимать новые поды а на старой реплике начинает гасить поды. так вот как я пнимаю поды  в старой реплике 
		и в новой релике имею одинаовые label как же они тогда неперепутыватся ?



надо понять как заставит нжинкс выводить кастомную страницу. чтобп онимать что за под ее обслуживает. 
надо понять на счет порта в поде. если внутри пода слушается 80 порт то его нужно както по особенному пробрасыать на хост ?
надо научться искать контейнер\под на хосте зная название пода в кубе.

\\\ 09.09.2022
запуск cowsay
$ sudo docker run docker/whalesay cowsay boo-boo

openhci runc о чем оно на практике ?

визуальизатор json = jq

snap как он раобтает?
	- как я понял скачивается файл в котором через squashfs находится отдельная автномная
		файловая система  на которой есть все файл и бибиотеки нужные этой проге. то есть программа
		скачиваетс со всеми зависимостями. далее эта фс монтируется через /dev/loopX в папку
		непонятна в чем разница с докер контейнеарми. и пока что вопрос как работает snap оставляем.
		посмотреть установленные проги чеерз snap:
			- $ snap list
		увидеть подмонтированные снапы папки:
			- # df -h | grep -E "snap|Mounted"
						Filesystem      Size  Used Avail Use% Mounted on
						/dev/loop0      2,5M  2,5M     0 100% /snap/gnome-system-monitor/163
						/dev/loop2      768K  768K     0 100% /snap/gnome-characters/761
						/dev/loop3      640K  640K     0 100% /snap/gnome-logs/106
						/dev/loop4      2,7M  2,7M     0 100% /snap/gnome-system-monitor/174
						/dev/loop5       66M   66M     0 100% /snap/gtk-common-themes/1515
						/dev/loop6      249M  249M     0 100% /snap/gnome-3-38-2004/99

		бинарники которые ставятся через snap ставятся в папку /snap/...
		например 
			- # which jq
					/snap/bin/jq

		я пока примерно понимаю система снапов работает так. качается файл снап-пакет он распакоавыается в /snap/... там кладется бинарник программы и библиотеки которые ему нужны.

далее малек переходим к podman. в чем разница его с докером. вначале про докер. 
 у докера есть клиент cli это /usr/bin/docker , когда мы его юзаем типа $ docker run -it busybox
 то он пишет в unix сокет /var/run/docker.sock который является сокетом процесса /usr/bin/dockerd
данный процесс запущен по root. таким образом docker то клиент cli который обращется к серверу
dockerd который сервер и он запущен в системе под рутом и именно он создает контейнеры точнее
он наверное тоже там обращается еще к чемуто (например к containerd) что создает контейнеры.
тут тонкие моменты. запускаем мы docker под vasya но этот vasya должен иметь права записи 
в сокет /var/run/docker.sock. а у этого сокета права на запись есть у пользователя root
и у группы docker. таким образом vasya по дефолту неможет никак пользоваться докером его 
нужно включать в группу docker. далее в инете постоянно пишут про опасность докера в том что 
его демон dockerd работает под рутом. я не понимаю в чем тут опасность. самим dockerd вася пользо
ваться напрямую неможет. может только писать в его сокет. получаеься что максимум может быть
только можно через сокет завалить сервис dockerd и только то. насколько я понимаю то что вася 
будет в группе docker недает ему каких то особых прав в системе. далее пишут что контейнеры
когда создаются то имеют предка dockerd и его он сдох то контейнеры тоже сдохнут.
из того что я вижу что контейнеры имеют предка pid=1 но никак не dockerd. так что здесь
тоже непонятно. хотя практика показал что если убить процесс dockerd то да все контейнеры
тут же сдыхают. и вот вобщем исходя типа из того что иметь процесс под правами рут и еще 
то что он родитель у контейнеров что это плохо придумали другую прогу podman.  
podman - ее плюс в том что она умеет создавать контейнеры без клиент-серверной архитектуры.
нет никакого демона к которому бы podman обращался и ненужно иметь рут права при создании 
контейнера.

после того как добавили юзера в группу docker нужно сделать логофф логон. чтобы этого неделать
надо 
	- $ su - $USER

--
Проблема docker контейнер и pid 1.
наткнулся на статью про проблему с докер контейнерами и pid 1.
\\написано что - zombie defunct sigterm . oh my god!\\
написано что есть проблема вот какая: вот у нас процесс породил другой процесс,
в свойствах процесса всегда указано кто у него родительский процесс кто его породил. какое это 
имеет значение. когда дочерний процесс закончил свою работу (точнее процесс направляет в ядро запрос чтобы ядро убило процесс) то после этого ядро неполностью убивает процесс а почти и процесс висит в списке процессов и ядро ждет когда родителский процесс примет некую информацию статус код возврата от дочеррнего процесса и только когда 
родительский процесс примет статус окочаний тогда дочерний процесс будет полностью стерт ядром 
из системы. иначе дочерний процесс висит в системе в статусе zombie.поэтому для кааждого дочернего процесса важно иметь здоровый работающий родительский
процесс. причем непросто работающий а такой который еще будет принимать статусы об коде возврата
дочерних процессов. потому что для того чтобы процесс мог принимать инфо о кодах возврата от дочерних процессов это надо чтобы в парент процессе был некий кусок кода которым за это ответчает. так вот статья пишет о том что например вот мы запустили контейнер. в нем запустили
нашу самописну программу. она запустила пару дочек. далее скажем дочка закончила работу 
и ядро ждет когда родительский процесс (наша самописная программа) примет коды возврата от дочек.
а наша самописная программа неимеет кода который это делает. и тогда в системе будут висесть 
зомби процессы. ???


\\ оффтоа неочень понятно как может новый линукс в контейнерер работать на старом ядре
тоесть в новом линуксе идет обращение к какомуто новому сисколуу который есть толко в новом
линуксе а у нас на хосте старое ядро. как тогда это может работтать? либо идет расчет
на то что по использует на 99% старые сисколлы ? ведь процесс на контейнере как работает
он юзает библиотеки которые находятся на фс контейнерной но вызовы к сисколлами идут к
сисколлам ядра хоста тоесть скажем у нас в контейнере федора новая а на хосте ядро убунту
старого. \\


\\оффтоп. где докер хранит свои имаджи ? \\

теперь в деталях как это работает.


как я понимаю. есть процесс. он порождает другой процесс (через fork() ) получаем дочерний процесс
далее дочерний процесс хочет закончить свою жизнь он делает системный вызов return\exit (как то так) ядро получает управление. ядро убивает все структуры дочернего процесса но оставляет немножко
структур в частности запись в таблице процессов в ядре. далее ядро посылает сигнал SIGCHLD к parent процессу. далее ядро ожидает что в родиетльском процессе в коде прописан хэндлер для 
сигнала SIGCHLD который должен вызывать сисколл wait() который в свою очередь считает код
возврата от дочернего процесса и далее (я незнаю) либо сам wait() вычищает остатки процесса
из системы либо ядро само потом отедьно это вычищает. здесь важно то что если в процессе нет
кода который обрабаывает SIGCHLD  с вызоываом wait() то ошметки дочернего процесса так и будут
висеть в системе. и статус процесса будет Z (zombie). важно понимать что система посылывает 
к паренту сигнал SIGCHLD (тоесть как она узнает когда это делать) потому что дочка делает
сисколл exit\return который и сообщает ядру что процесс хочет закончить свою жизнь. также
еще раз важно понять что обработает парент процесс сигнал SIGCHLD или нет зависит от того 
есть ли в его коде кусок кода который хэндлер суть которого в том чтобы если к паренту
пришел от ядра сигнал SIGCHLD значит надо его особым обраом обрабоатвать в частности вызывать 
сисколл wait(). если этого куска кода в паренте нет то дочерний  процесс небудет вычищен из
системы и будет иметь статус Z. как я понимаю при этом статусе  у процесса вычищены все его струкртуры кроме как записи в таблице процессов в ядре. поэтому если в системе появляются
процессы зомби значит их родителский процесс написан дебильно. и сделать тут ничего нельзя
кроме как переписыать родительский процесс. в чем минус зомби процессов втом что они могут 
заняь все pid в табице процессов а их число ограничено. также получается чтолюбой процесс
когда они заканчивает свою жизнь кратковременно находится в состоянии Z. руками вычистить
убить процесс Z никак нельзя кроме как : попробвать послать парент процессу руками сигнал
SIGCHD авось он просто его незаметил (хотя это бред) или убить роительткий процесс. тогда 
автомтом родителем детей станет процесс pid=1 у котрого точно есть хендлер sigCHLD процессов.
и либо он их автомто убьет сам либо надо к pid=1 послать руками сигнал SIGCHLD.
С другой стороны есть такой еще момент насколько я понимаю что для ряда сигналов
если в коде программы хендлер не прописан то используется некий дефолтовый для данного сигнала
хендлер поствляемый самим ядром. но наверно не для всех сигналов есть дефолтовый хендлер.
вопрос есть ли он для сигнала SIGCHLD.

пример кода который создает хэндлер для сигнала SIGINT (тот который формирутеся когда
мы жмем Ctrl+C)


#include<stdio.h>
#include<unistd.h>
#include<signal.h>

void sig_handler(int signum){
  printf("\nInside handler function\n");
  signal(SIGINT,SIG_DFL);   // Re Register signal handler for default action
}

int main(){
  signal(SIGINT,sig_handler); // Register signal handler
  for(int i=1;;i++){    //Infinite loop
    printf("%d : Inside main function\n",i);
    sleep(1);  // Delay for 1 second
  }
  return 0;
}

причем кастомный хэндлер отработает один раз а потом то что он будет делать будет сброшено
до дефолтового действия. тоесть Ctrl+C на первый раз будет проигнорировано а на второй раз
уже оно остановит процесс.

вывод работы

$ gcc -o 19.exe 19.c
$ ./19.exe
1 : Inside main function
2 : Inside main function
3 : Inside main function
4 : Inside main function
5 : Inside main function
6 : Inside main function
7 : Inside main function
8 : Inside main function
^C
Inside handler function
9 : Inside main function
10 : Inside main function
11 : Inside main function

далее важный момент а какая связь между сигналами в линуксе и interrupts на цпу ?
может это одно и тоже? нет. что такое сигнал линукса - это когда мы через программу 
запускаем сисколл который выполняет кусок кода ядра и этот кусок кода ядра в таблице
процессов в своствах процесса добавляет некое свойство в поле сигнал. и в этом поле
ядро прописывает скажем SIGINT. тоесть сигнал это чисто некое поле в свойствах процесса
в таблице процессов в ядре. далее когда шедулер доходит до этого процесса и видит что
у процесса указано свойство в поле сигнал то шедулер (который тоже кусок кода ядра) вместо
того чтобы совать на цпу код процесса запускает обработчик этого сигнала. откуда берется этот
обработчик. когда процесс стартует то в его теле может быть указан такой обработчик называется
хендлер. вот этот хендлер и будет использован для обработки данного события сигнал. если 
в коде программы такого хендлера небыло то в ос часто есть свой дефолтный обработчик
сигнала. как мы видим сигнал это свойство процесса в таблице процессов которая лежит в ядре. 
интеррапт это совсем другое. это сигнал который прилетает на ножку цпу  и условно говоря 
ни о каких процессах тут вобще речи неидет. это событие на уровне цпу на уровне его свойств.
никаких ОС линуксов ядер процессов вообще может небыть работающих на компе.
итак еще раз сигнал это хрен является особенностью линукса его ядра. это его сущность. 
сигнал это одно из свойств которым может обладать процесс. некое поле ячейка в которую 
можно записать некое значение которое обозначает "сигнал". сам код процесса это свойство
никогда невидит не чуствует и необрабатывает. сигнал обрабатывается шедулером. когда шедулер
доходит до процесса и видит что  в его свойствах появилось поле сигнал то шедулер вместо
того чтобы пихать код процесса на цпу делает совсем другое шедулер на цпу пихает обработчик
данного сигнала. откуда берется обработчик сигнала. по дефолту в ядре есть обработчики всех
или почти всех сиогналов , скорее всего всех сигналов. так что получается сигнал это сообщение
одного куска ядра другому куску ядра о том что с этим процессом надо чтото сделать особое.
но также в коде процесса может быть написан свой обработчик того или иного сигнала и тогда
при старте процесса ядро регистрирует для этого процесса этот кастомный обработчик сигнала
и тогда будет выполняться этот кастомнйы обработчик.
интерраапты это совсем другое. они не связаны ни с процессами ни с наличием ос на компе.
на ножку цпу специальную поступает сигнал от контрорллера интерраптов который в свою очередь 
получает сигнал от какойто железки . железка сигнализирует что у нее произошло событие
которое цпу должен обработать. на компе есть область памяти куда некий код(который может быть крошечным и необязательно что это полноценная ОС) заранее  пихает обработчики интерраптов.
от контроллера интерраптов в цпу прилетает номер интеррапта. исходя из этого цпу знает 
в по какому адресу нужно обратиться чтобы считать обработчик данного интеррапта.
таким образом интеррапт это сигнал о событии на какойто железке который прилетает на цпу
и далее цпу выполняет небольшой код обработчик который позволяет обработать событие произошедшее
на железке ( в железку прилетел пакет). обычно если на компе есть ОС то она при старте
регистрирует в той спеиальной области памяти свои кастомные обработчики интерраптов.
сигналы же в линуксе они сигнализируют не об событиях на железках для цпу они сигнализируют
шедулеру (ядру) о событии наступившем для процесса , что некое событие произошло с процессом
и ядро должно это событие обработать. условно говоря процесс это ребенок а сигнал идет
мамаше что ребенок обосрался и надо за ним убрать. таким образом сигналы и интеррапты 
они по своей природе совсем разное. другое дело что из практики выходит так что порой
обработчик интеррапта ( а это ведь тоже код ядра) как один из итогов может сформировать и приклеить некий сигнал для некоего процесса. пример - мы тыкнули на клаве Ctrl+C . клава это железка она послал на контроллер интерраптов сигнал. контроллер кинул интеррапт на цпу.
цпу выполняет обработчик интеррапта (который код ядра линукса потому что на компе стоит линукс)
и этот обработчик приклеивает к процессу в терминале сигнал SIGINT. далее шедулер когда доходит
до процесса видит что у процесса появилися сигнал SIGINT и он вызывает обработчик данного сигнала 
в ядре. таким образом интеррапт это сигнал прям для цпу ( по его ноге) о событии на железке 
тоесть одна железка жалуется другой железке. а сигнал это  когда один код ядра сообщает шедулеру (другому куску кода ядра) что с процессом нужно выполнить некое спец действие. например надо 
его грохнуть. интеррапт это событие уровня цпу-железка когда железка сообщает цпу что  у нее некая беда которую надо обработать,  а сигнал это событие уровня  некий кусок 
кода ядра сообщает другому куску кода ядра(шедулеру) что надо чтото сделать с процессом.

самый частый случай сигнала. мы в командной строке взывает kill -9 $PID
это приводит к тому что мы вызывает сисколл который кусок ядра который в свойства процесса $PID 
записывает сигнал SIGTERM. и тогда шедулер запускает процедуру убиения процесса.
также сам процесс когда хочет себя грохнуть то вызывает сисколл exit\return (незнаю как точно
он пишется) и этот сисколл опять же записывет в свойства процесса свойство SIGTERM.
вообще получается сигнал это наша просьба к шедулеру чтото сделать с процессом.
типа как ребенок заказывает у матери мол покорми меня или жопу подотри. 
и шедулер (мамаша)  начинает обрабатывать ребенка(процесс).
итак с сигналами интерраптами разобрались. вот еще по теме нашел инфо -  When the parent process receives SIGCHLD, the subprocess has already finished, but it is still in the OS’s process table. It is dead but still hanging around: it is a zombie process.
To completely eliminate the subprocess, the parent process needs to either read out its exit status, or tell the OS that it is not interested in the exit status. The parent process can also pre-emptively tell the OS that it is not interested in the exit status of its subprocesses, and in this case no SIGCHLD signals are delivered to it at all. 
а вот еще интересно с того же источника - SIGCHLD signal canbe blocked by the process. Blocking a signal means that the process tells the OS, that it is currently not ready to process it, and it should be delivered later, when the process has unblocked the signal. If SIGCHLD was blocked in the callr subprocess, that would be a good explanation for the OS not sending it.
получается якобы можно программировать процесс так что типа ему нельзя будет присвоить сигнал.
странно. ведь прилелением сигнала занимается ядро у которого все права.
еще важная инфо ядро шлет паренту sigCHLD нетолько когда он закончил работу но и когда
например дочка была suspended. так что обработчик этого сигнала у парента должен считывать 
и анализировать код возврата , причину. прикольно что часто пишут что сигнал это сигнал именно
процессу о том что с ним чтото происходит. а на самом деле это сигнал шедулеру ядра.
что еще прикольно - согласно man 7 signals по дефолту если к процессу приходит сигнал SIGCHLD
то он по дефолту ИГНОРИРУЕТСЯ! тоесть если у нас нет своего кастом хендлера для данного 
сигнала в коде нашей проги то сигнал игнорируется. а значит мы будем получать зомби процессы
от наших дочек. еще раз скажу что логика удержания зомби процессов со стороны ядра состоит в том
что если парент запустил дочку значит ему должно быть интересно с каким статусом дочка
закогнчила работу поэтому пока парент процесс незаберет этот код возврата кернел удерживает
в памяти зомби процесс.

программа которая создает дочку и потом коректно считывает код возврата

#include<stdio.h>
#include<sys/wait.h>
#include<unistd.h>
 
int main()
{

    pid_t child_pid = fork();
    if (child_pid == 0){
        printf("child: i am inside the child, waiting for 10 sec in the child\n"); 
        sleep(10);}
    else
    {
        printf("parent: i am inside the parent. i am waiting and doing nothing until the child  exits to continue\n");
        wait(NULL);
        printf("parent: i am continuing inside the parent because child has terminated\n");
    }


    if (child_pid == 0){
        printf("child:child is saying GoodBye!\n");}
    else
      {
         printf("parent:parent is saying GoodBye!\n");
      }
    return 0;

}


вывод на экран:
parent: i am inside the parent. i am waiting and doing nothing until the child  exits to continue
child: i am inside the child, waiting for 10 sec in the child
child:child is saying GoodBye!
parent: i am continuing inside the parent because child has terminated
parent:parent is saying GoodBye!


в предыдущем примере мы юзали wait так что он блокировал исполнение пэрента. тоесть
пока чайлд неумирал парент сидел и ждал в команде wait.
а следущий пример мы юзаем waitpid+signal это дает то что у нас неблокируется исполненеие
пэрента пока раотает дочка

$ cat 21.c
#include <stdio.h>
#include <signal.h>
#include <stdlib.h>
#include<sys/wait.h>
#include<unistd.h>
#include<time.h>

pid_t pid;
int finished=0;

void zombie_hunter(int sig)
    {
    int status;
    waitpid(pid, &status, 0);
    printf("parent: Got status %d from child\n",status);
    finished=1;
    }

int main(void)
    {
    signal(SIGCHLD,zombie_hunter);

    pid = fork();

    if (pid == -1)
        {
        exit(1);
        } 
    else if (pid == 0)
        {
           time_t starttime = time(NULL);
           time_t seconds = 3; 
           time_t endtime = starttime + seconds;
 
           while (starttime < endtime) {
            sleep(1);
            printf("child: i am running...\n");
            starttime = time(NULL);
            }

          exit(0);
        }

    while(!finished)
        {
        printf("parent:i am running...\n");
        sleep(1);
        }
    }

вывод на экран
$ gcc -o 21.exe 21.c
$ ./21.exe
parent:i am running...
parent:i am running...
child: i am running...
parent:i am running...
child: i am running...
parent:i am running...
child: i am running...
parent: Got status 0 from child


видно что и чайлд и пэрент работают одновременно. и в тоже время мы ловим в пэренте момент
когда чайлд сдох. тоесть мы имеем то что у нас работает парент. потом он запускает чайлда (через форк) и потом он продолжает работать недожидаясь пока чайлд отработает, они работают параллельно,
и что важно когда чайлд заканчивает работать то паренту летит сигнал и парент этот сигнал 
ловит и отрабатывает и также важно что чайлд полностью прекращает работу неоставаясь висеть
в списке процессов как зомби. делается это через установку хэндлера для сигнала SIGCHLD в котором 
мы юзаем waitpid(pid, &status, 0). waitpid считывает код возврата с которым отработал child.
а ядро узнав об этом уже само полностью вычищает все остатки чайлда из системы. тоесть я хочу 
сказать что чайл через exit(0) заказывает  у ядра что чайлд закончил свою работу и просит ядро
уничтожить его. ядро этого неделает оно ждет когда парент считет код возврата. парент через waitpid считывает код возврата. а ядро увидев это уже полностью вычищает чайлда из системы.

далее. в man 7 signal написано что по дефолту если у пограммы нет своего кода который 
описывает хэгдлер для сигнала SIGCHLD то тогда используется дефолтовый хэндлер от ядра. 
и этот хэндлер он игнорирует данный сигнал. это значит что если такой парент(без своего хэндлера сигнала SIGCHLD) породит чайлда и потом чайлд закончит свою работу то получается что он будет 
висеть в системе в состоянии зомби. 

так чтобы проверить эту информацию - что если в программе нет кастомного хэндлера для сигнала SIGCHLD то деолфтовый хэндлер в ядре игнорирует данный сигнал для парент процесса 
и тогда если мы парентом зафоркаем чайлда и чайлд закрнчит свою деятельность через exit(0)
то у нас в при работающем паренте в системе будет висеть его чайлд со статусом зомби.
 а потом когда мы убьем парент процесс то у чайлда поменяется парент процесс им станет pid=1
 а он точно имеет хэндлер SIGCHLD и он уже считает код возврата с этого зомби процесса
 и ядро тогда удалит наконец этот процесс из списка процессов.
 вот немного модифицировання программа что была выше. в ней модифицировано в сущности 
 только то что мы убрали из нее хэндлер signal.

 $ cat 22.c
#include <stdio.h>
#include <signal.h>
#include <stdlib.h>
#include<sys/wait.h>
#include<unistd.h>
#include<time.h>
#include <sys/types.h>


pid_t pid;
int finished=0;

int main(void)
    {

       pid = fork();

        if (pid == -1)
        {
           exit(1);
           } 

        else if (pid == 0)
        {

           time_t starttime = time(NULL);
           time_t seconds = 60; 
           time_t endtime = starttime + seconds;
 
              while (starttime < endtime) {
               sleep(1);
               printf("child: i am running...\n");
               starttime = time(NULL);
               }
           exit(0);
            }

        else if (pid > 0)
        {
          pid_t pid_p = getpid();
          printf("parent: parent PID=%d...\n", pid_p);
          printf("parent: child PID=%d...\n", pid);
             }




    while(!finished)
        {
        printf("parent:i am running...\n");
        sleep(1);
        }
    }


как она работает. запускется парент. он форкает себя. далее на экране пишется pid парента
и пид чайлда и далее чайлд работает 60 секунд после чего делает exit. а парент продолжает
работать бесконечно. чайлд пока работает раз в секуну выводит на экран 

child: i am running...


как только эти сообщения прекратились значит чайлд сделал выход. 
и значит можно смотреть какой статус у парента и чайлда.

итак вывод на экран:

$ gcc -o 22.exe 22.c

$ ./22.exe 
parent: parent PID=24427...
parent: child PID=24428...
parent:i am running...
parent:i am running...
child: i am running...
parent:i am running...
child: i am running...
parent:i am running...
...
parent:i am running...
parent:i am running...
parent:i am running...
parent:i am running...
parent:i am running...

открываем еще один терминал и смотрим статус процессов
$ ps -o user,pid,ppid,tty,stat,command -p 24427,24428
USER       PID  PPID TT       STAT COMMAND
vasya    24427 16569 pts/15   S+   ./22.exe
vasya    24428 24427 pts/15   Z+   [22.exe] <defunct>

видим что статус pid=24428 является Z(зомби). 
в строке параметров указано [22.exe] <defunct>
если посмотреть на cat /proc/24428/cmdline то там будет вообще пусто.

итак получается: главный вывод - если в программе нет кода хэндлера для сигнала SIGCHLD
то порлжденный чайлд будет зомби когда он закончит свою работу. потому что дефолтовый 
хендлер от ядра он игнорирует сигнал SIGCHLD который прилетает к паренту. вот это вот 
и есть тот механизм та причина кто виноват в появлении зомби процессов. причем соверенно
ясно что при окончании работы чайлд процесса он всегда будет впадать в зомби состояние,
единственное что если есть кастомный хэндлер для SIGCHLD в парент программе то чайлд 
будет висеть в этом состоянии сверх короткое время, тоесть это все нормально. а вот если
он висит долгое время это уже ненормально. итак сам статус зомби это нлрмальная штатная
ситуация а вот если он долго висит в этом сосотоянии это ненормально. еще важно понять что
SIGCHLD информирует нас не толтко о том что чайлд был затерминейчен но и о том что вообще состояние чайлда изменилось - чайлд закончил работу, закрашился, остановился продолжился, то есть сигнал означает состояние чайлда изменилось. также еще важно знать что SIGCHLD можно не только игнорировать но например заблокировать на какоето время. 
а вот о том же  man waitpid (есть кое какая новая информация) -  A child that terminates, but has not been waited for becomes a "zombie".  The kernel maintains a minimal set of information about the zombie  process  (PID,  termination  status, resource  usage information) in order to allow the parent to later perform a wait to obtain information about the child.  As long as a zombie is not removed from the system via a wait, it will consume a slot in the kernel process table, and if this table fills, it will not be possible to create further processes.  If a parent process terminates, then  its "zombie"  children  (if  any) are adopted by init(1), (or by the nearest "subreaper" process as defined through the use of the prctl(2) PR_SET_CHILD_SUBREAPER operation); init(1) automatically performs a wait to remove the zombies.
например видно что при умирании парента новым парентом необязательно становится pid=1
что очень хорошо видно в убунту 18. там новым парентмо становится процесс "systemd --user" 
которы запущен под правами юзера.





\\естьт какаято еще тема что если в пэрент прилетит сразу несоклько сигналов то хэндлер
будет выполнен только один раз. а чтобы он для всех сигналов выполнился то надо както 
изголятся. я думаю это очень важно тоже на эксперимнте найти и разобрать. именно из за 
эттго (херово написанного хендлера) когда много форков то часть из них так и убдет оставаться
зомби потому что толко у части форков парент будет считывать коды возврата\\




а что будет если грохнуть родительский процесс кто принимает тогда информацию
об коде возврата от дочернего процесса. ответ в случае смерти родителя родителем становится
pid=1. кхм... и что для процессов в контейнере тоже это имеет место ? тоесть создаем
контейнер, там запускаем несколко башей. грохаем головной баш и кто тогда будет родителем
у дочерних процессов? pid=1 который на хосте? проверяем.


процесс с systemd --user почему то написано что он для одного а на самомо деле
именно он наследует детей убитых процессов а не pid=1 что за херня. если его убить
то графическй сеанс мгновенно закончится.


зомби процесс получается кода ребенок завершил работу но его родитель не считал 
код возврата работы ребенка через сисколл wait(). когда чайлl закончил работу то 
ктото(ядро?) шлет парент процессу сигнал sigCHLD о том что чайлд закончил свою жизнь.
далее обязанность парента состоит в том что в нем должен быть код (более точно там должен
быть хэндлер(хрен знает что это ) который должен обработать сигнал ) который должен 
считать код возврата от чайлда. только тогда ядро удалит остатки от чайлда из таблицы
процессов.как я понял код возврата от чайлда можно считать вызвав сисколл wait()




--
файл 
kubeadm-flags.env 

насколько я понял этот файл использует kubeadm при начальной инициализации ноды
в кластере. а далее его использовать для изменения конфига ноды бесползено

---

 kubelet  invalid capacity 0 on image filesystem

 врое как это можно игнорить. пищут что повялется только при заауске к8с.

---
@@@@@@@@@@@@@@@@@@@@
возобновил изучение 20 июня 2023

тема: понять что такое replica set 
                    vs deployment 
                    vs replication controller 
                    vs statefull set 

pod

начнем с подов

создадим его в yaml

$ cat pod3.yml 
apiVersion: v1

kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: webserver1
    image: nginx:1.14.2


metadata - это информация для к8. так сказать служебная информация о поде 
для к8. его обвязка. это как упаковка с надписями на коробке. как конверт.
на конверте не скзаано о чем содержимое письма. там указано откуа и куда оно летит.
это как железный контейнер.


spec - это информация уже о том что внутри коробки хранится. о его начинке.

метадата - это информация для почтальона ( к8) 
а spec это информация уже для конечного потребителя.

куб когда оперирует подами он в spec не смотрит. он смотрит в метадату.


поды конечно же внутри состоят из контейнеров. в данном случае у нас один контенер в поде.

$ kubectl apply -f po3.yml

$ kubectl get pods  pod-1
NAME    READY   STATUS    RESTARTS   AGE
pod-1   1/1     Running   0          63s


посмтреть инфо о поде можно двумя споосбами
$ kubectl describe pods  pod-1

и более подробный
$ kubectl get pods  pod-1 -o json

...
   "metadata": {
        "annotations": {...},
        "creationTimestamp": "2023-06-17T16:54:33Z",
        "name": "pod-1",
        "namespace": "default",
        "resourceVersion": "91087",
        "uid": "e18de211-4fb3-4c59-a1ee-629218d512d4"
    },
...


теперт чуть добавим в метадату хреней. а именно добавим label


$ cat pod4.yml 
apiVersion: v1

kind: Pod
metadata:
  name: mywebapp4
  labels:
    label1: web4
spec:
  containers:
  - name: webserver1
    image: nginx:1.14.2


замечу что label это dictionary а не list (доббоебы)
смотрим как изменилась метадата

$ kubectl get pods mywebapp4 -o json
    "metadata": {
        ...
        "labels": {
            "label1": "web4"
        },
        ...


 лейбл добавился в метадату.
 зачем нужен label. дело в том что поды в к8 они не болтаются обычно сами по себе. за ними обычно ктото 
 присматривает. например replica set. так вот мы в реплике сет прорисываем какие ей лейблы искать
 и тогда реплика сет знает за какими подами ей надо следить.  итак лейблы нужно подами для того чтобы
 вышестоящие струкруты могли найти нужные им поды и потом за ними следить. 


 перейдем к вышестоящей хрени = repliace set
replica set = ее задача это найти поды которые к ней относятся  , далее следить
за этими подами  поддерживать заданное число подов.

тут возникают важные вопросы:
- в чем отличие реплики сет (rs) vs replication controller (rc)
- поды крутятся в виде контейнеров на хостах. и их можно найти на этих хостах. 
а на каком хосте и в каком виде живет и существует реплика сет? на каком хосте она сидит? 
в какой форме она заключена. как ее найти. 
- когда мы публикуем реплику сет то она сама создает поды или они уже должны сущестовать?
- как она понимает какие поды ееные (за которыми ей надо следить) а какие к ней неотносятся
- что если мы опбуликовали реплику сет а поды которые к ней относятся уже есть на хосте?
- что если реплик сет создала поды а мы еще ей насунули руками подов

поехали отвечать по порядку


- в чем главная задача цель назнчаение функция replica set (сокращенно rs)
ответ - в том что когда ее создали она ищет среди уже крутящихся развернутых до нее подов те за 
которыми ей надо следить живы здоровы они или сдохли. (тут кстати надо уточнять за каким состоянием пода следит
rs). в самом постом случае rs следит не сдох ли под за которым она следит. если да то rs сразу пересоздает 
под вместо сдохщего так чтобы общее число подов здоровых и живых стало равно тому числу (scale) что прописан в 
ее манифесте. парметры вновь создаваемого пода также прописаны в манифесте rs. получется rs это чтот вроде мамаши
или systemd которая следит за подами и в случае их умирания сразу пересоздает под. документация
пишет что даже если мы хотим заупстиь только один под то надо это делать не в виде stan alone пода а именно 
через rs чтоб если что под всегда был жив. поучается что если в ос за процессами слежит systemd то в к8 
за подами следит rs. возникает вопрос что из себя представляет rs это процесс какйото в ос или что. на каком хосте
она сидит. и как узнает что под жив или умер. пока это неясно. 

- как rs узнает за каким подами ей следить. ответ такой - rs проверяет список подов работающих на всех 
нодах (возникает вопрос а как часто ?). я думаю что rs сидит где то на control plane и либо она сама регулядрно 
опрашивает все поды либо отслеживает статус подов который kubelet докалдывает в control plane. вобщем это 
тоже пока неясно. в любом случае с какойто частотоой rs проверяет список раобтающих подов. и rs проверяет 
есть ли у пода  в его свойствах поле 
		"ownerReferences"
если оно есть то это означает что данный под нахорится под контролем другой rs или какйото еще хрени к8.
и тогда такой под rs неинтересен. она сразу на него забивает болт.
а вот если у пода не такой сеекции то наступает вторая фаза. rs берет список label у этого пода и проверяет
совпадает ли это  с тем что у нее есть в селекторах. точнее так - у пода может быть дохера лейбелов гораздо больше
чем в селекторах rs. но если все селекторы найдены (тоесть скажем в rs указано в селекторе два лейбела а в поде их двадцать два.
но если среди этих двадцать два есть два таких что они совпдаают с лейбелами в секлекторе rs ) то rs считвает что она 
нашла очредной под который имеет к ней отношение. такая двуступечатая схема дает то что у нас две rs могут иметь одинаковые 
лебелы в селекторах однако при этом никакого конфликат за поды между ним небудет! точнее он возможен но это исключение. щас обьясню. вот у нас пустая система. подов нет. мы создали rs-1  у него лейбел указан a:"1" . реплика сет стартанула проверила 
есть ли поды. подов нет. тогда она  создает из своего шалона новый под и пропиывает в нем лейбел a:"1" более 
того она в нем пропиывает "ownerReferences" в котоом она укзывает что она владаелец этого пода.
далее мы создаем +1 rs-2 у нее указан тоже такойж е лейбел  a:"1" .  это реплика сет стартует и ищет в системе поды кооыре
уже работают. находит наш под. первым делом проверяет наличиеи "ownerReferences" находит его у пода. сразу от него отваливает.
и раз stand alone подов она ненашла то она созадет +1 новый под  с лейбелом a:"1" и полем  "ownerReferences" котором указано
что она владдеец этого пода. наща первая реплика сет rs-1 провет этот созданный под видит в нем "ownerReferences" и сразу
от него отваливает. таким образом у нас в системе будет два пода с одинаковым лейбел. но между репликами сет не будет никакого
конфлтикат какая из них за каким подом присматривает.
некий кофилкт возмоэен вот когда. - мы берем и рукками сздаем +1 (треий ) под. с лейбелом a:"1"
поскольку мы содали руками stand alone под то у него не будет секции "ownerReferences" и поэтому этим подом заинтерсуется
обе реплики сет. как они его будут делить межу собой я незнаю. но последствие будет одно. какаято его себе захватит
и нарисцет на нем свой "ownerReferences" после этого другая rs должна потреять к нему интерес. а далее еще интереснее
так как захвативашая этот под rs уже имеет достаточное количество подов на присмотре то она этот  под созаднный руками 
уничтожит.
вот так вот моного теории . но очень важной теории ниже будет много практики.



- в чем отличие реплики сет (rs) vs replication controller (rc)
отвечая на этот вопрос я задегаю вперед но тем не менее.
rs являестся как сказать следующей эволбционной модификацией от rc. тоесть вначале был rc. 
а потом его заапгрейдили и придумали rs. итак rs это эволюциооное развитие от rc.
значит поговоиим о плюсах и минуса между ними. раньше у rc были свои плюсы 
а у rs были свои. но сейчас rc это тухляк. его не ркоеменудется использовать.
на данный момент rc неимеет никаких плюсов по сравннеию с rs. на данный момент про rc можно забыть
у него только одни минусы по сравеню с rs. но все же погооим про их разницу.
rs умеет работаь с более продвинутыми селекторами ( что такое селекторы и что такое леблы будем напримерах
разобрано ниже). а rc неумеет работать с продвинутыми селекторами. поговрим еще об одгом аспекте.
о том как обновляются поды за которым присматривают rs либо rc. я буду об этом подробно говорить ниже 
а пока вывод- rc и rs они пригляюдывают за подами. если под за которым приглядыает rc либо rs 
погибает то они тут же создают новый согласно тому шаблону пода который указан в ямле rc\rs.
обновление ямля rc\rs никак не влияет на поды. об этом я тоже будут говорить ниже. 
единственная воможность обновить поды это уменьшить число scale до нуля. и потом обратно 
довести scale до нужного числа. 
так вот такой путь он норм но у нас уменьшается редунданси у приложоения которое крутится на этих подах.
так вот в итоге путь такой - создаем новый +1 rc\rs в нем указвыем scale=1  а потом в старом rc\rs
мы уменьшаем scalt на -1. и таким макаром мы в новом rc\rs увеличиваем число подов а в стаом уменьшаем.
когда мы убьем все старые поды то старуйы rc\rs мы удаляем. так вот этот обновления он как бы получается ручной - 
там убавить тут прибавить. так вот раньше этот процесс можжно было автоматизировать а именно у куба
была команда которая делала то что я описал автоматом. команда такая

$ kubectl rolling-update имя_текущего_rc   -f  имя_файла_с_новым_rc

так вот для replication set такой команды нет. так вот в этом была фишка rc по сравннеию с rs.
так вот на данный момент эту команду выпилили из kubectl так что rc болше неимеет такого премузетва
по сравнению с rs. 
поэтому я и говорю что rc больше неимеет преимуществ перед rs. об этом и доке к8 они пишут. мол больше не юзайте
rc.  в целом поэтому про rc можно далее вобще забыть. но  я всеже ниже с ним еще несколько повожусь.
но все же еще раз для ясности скажу что rs vs rc => вывод такой rc это старая версия rs.  rc делает тоже самое что 
rs но на данный момент имеет более узкий функцонал. а именно rc не поддерживает такие продивинутые селекторы
как это умеет rs. это первое отличие которое я нашел rc от rs. есть и еще одно - значит поды rc больше нельзя автоматом
обновить через kubectl rolling-update а вот поды от rs можно. правда не поды от rs который был создан руками назовем
его stand alone rs а  поды того rs который был создан через deployment. в этом второй плюс rs перед rc.
вот такое большое теоретическое вступление но очень важное. ниже много практики.





- поды крутятся в виде контейнеров на хостах. и их можно найти на этих хостах. 
а на каком хосте и в каком виде живет и существует реплика сет? на каком хосте она сидит? 
в какой форме она заключена. как ее найти. 

ответа я ненашел. ясно то что этой хрени нужно следить за подом жив он или нет. под сидит на хосте.
каким макаром реплика сет знает живо здоров под или нет. непонятно. инфо нет. 
реплика сет переживает перезагрузку всех хостов значит инфо о нет сохраняется в etcd.
на каком хосте она сидит - там где под или где непонятно. возможно это часть api сервера.
кусок его памяти. возможно инфо о здоровье подов прилетает в реплику сет через запрос к кубелетам
на хостах. пока так.


- когда мы публикуем реплику сет то она сама создает поды или они уже должны сущестовать?
и так и так можно. если подов нет то реплика сет начинает их сама создавать. 
если поды есть то она их находит и сама ничего несоздает. если поды есть но их число
меньше чем закаазано в ямле реплики сет то она досоздает то число которое нехватает


- как она понимает какие поды ееные (за которыми ей надо следить) а какие к ней неотносятся
тут самая хитрость. документация уебанская. они говорят что репдика сет смотрит список подов
и смотрит какие labels есть вэтих подах. потом она сравнивает эти леблы с теми selector
которые прописаны в ямле реплики и если есть совпдаение то реплика считает что она нашла 
"свои" поды. это частично правда. и в этом огромное наебалово. если бы реплика смотрела 
только в лейблы подов то был бы полный пиздец потому что если у нас опубликовано две реплики
и в кажой прописан одинаковый селектор то каждая реплика дралась бы за тот под в котором 
есть лейбл совпадающий с селектором. однако этого не происходит. почему. потмоу что релпика на
самом деле когда ищет свои поды то она смтоит не только в список селекторов в поде но и кое куда еще.
а именно реплика сет смотрит есть ли у пода в разделе metadata хрень ownerReferences если она есть то 
это знчит что под уже рулится какотой другой хренью например другим реплика сетом. и тодга нашашеплика 
сет плевать хотела на этот под

$ kubectl get pods pod-5   -o json


    "metadata": {
        "ownerReferences": [
            {
                "apiVersion": "apps/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "ReplicaSet",
                "name": "nginx-replicaset",
                "uid": "acbdff8a-4c5a-4de2-976b-85b3644f3b2b"
            }


а вот если у под нет "ownerReferences" то тогда наша реплика начинает проверять список лейблов на этом поде.
поэтому можно иметь миллион реплик сетов у которых прописаны одинаковые селекторы и они небудут воевать друг 
с другом за поды. ебанаты из к8 это все вообще не обьянсяют в своих доках. уродды.


итак еще раз когда мы опубликовали 


$ cat pod-rp3.yml 
apiVersion: v1

kind: Pod
metadata:
  name: pod-rp3
  labels:
    app: nginx2
    custombuild: "1"

spec:
  containers:
  - name: webserver1
    image: nginx:1.14.2


$ kubectl get pods pod-rp3 -o json
{
    "metadata": {
        "labels": {
            "app": "nginx2",
            "custombuild": "1"
        },


как видим у него нет никакого ownerReferences

теперь запускам репликусет

$ cat rp3.yml 
apiVersion: apps/v1
kind: ReplicaSet

metadata:
  name: rp3

spec:
  selector:
    matchLabels:
      app: nginx2

  replicas: 2 

  template:
    metadata:
      labels:
        app: nginx2
    spec:
      containers:
      - name: nginx
        image: nginx:1.13.2
        ports:
        - containerPort: 80


проверим что наш под опубликованый выще был найден этой репликой ,и захвачен в зону своего влияния

$ kubectl get pods pod-rp3 -o json
    "metadata": {
        "labels": {
            "app": "nginx2",
            "custombuild": "1"
        },
        "ownerReferences": [
            {
                "controller": true,
                "kind": "ReplicaSet",
                "name": "rp3",
           }


вот! у нашего пода появилась доп секция "ownerReferences" 
и в ней ксати указано что данным подом теперь владеет реплика сет  "rp3"


получается созданный руками под был обнаружен и захвачем этой репликой в сферу своего влияния как бутто 
он был создан ею.

но также реплика создала и еще +1 под сама. так как у нее в ямле указано что должно быть два пода
один она нашла а второго небыло. и она недостающий создала сама на основе паарметров пода который былуказан в
вв ее ямле 

   spec:
      containers:
      - name: nginx
        image: nginx:1.13.2
        ports:
        - containerPort: 80


итак смотрим под который создала сама реплика
$ kubectl get pods  rp3-vlbwp -o json
{
    "metadata": {
        "labels": {
            "app": "nginx2"
        },
        "name": "rp3-vlbwp",
        "ownerReferences": [
            {
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "ReplicaSet",
                "name": "rp3",
            }


под в своем имени содержит имя реплики плюс к имени дбавлена некая хрено типа хэша.
теперь я докажу что наш первый под который мы создвали руками - что его реплика неизменила а только 
присоединили в сферу своего влияния.
если мы посмим в ямль нашего ручного пода то заметим версию жинкса
$ cat pod-rp3.yml 
    image: nginx:1.14.2

а в ямле реплики указана другая версия жинкса


$ cat rp3.yml 
        image: nginx:1.13.2


теперь проерим какая версия жинкса сейчас у нас у нашего ручного контейнера

$ kubectl get pods pod-rp3 -o json
{
    "metadata": {
        "labels": {
            "app": "nginx2",
            "custombuild": "1"
        },
        "name": "pod-rp3",
        "ownerReferences": [
            {
                "controller": true,
                "kind": "ReplicaSet",
                "name": "rp3",
            }
        ],
    },
    "spec": {
        "containers": [
            {
                "image": "nginx:1.14.2",
        "containerStatuses": [
            {
                "image": "nginx:1.14.2",


итак 1.14.2 тоесть ровно то что было указано в ямле когда мы создавали под руками.
ничего неизмелиось.  хотя в ямле реплики сета указана другая версия 1.13.2

плюс видно что в нашем ручном поде есть лейбл 
  "custombuild": "1"
  которого нет в а ямле от реплики сет. я спецлиаьно включил этот лейбл в ямль пода и не включил в ямль 
  реплики чтобы четко было видно то что под  который был создан руками что реплика сет его не тронула.
  потому что так как  лейбла "custombuild": "1" нет в ямле релики то поды которые создаст именно реплика
  в них не будет этого лейбла . таким образом четко видно то что если у нас был stand alone (созданный руками )
под и мы создали реплику сет и в поде ест лейбл который ест в селекторе реплики то реплика захыватывает этот 
под в свое влияние тоесть релпика будет следить за этим подом. далее реплика добавляет в метадату этого 
пода секцию "ownerReferences" котораясообает всем другим репликам отом что этот под теперь под ее юрисдиукицией.
при этом реплика болше ничего нетрогает  в поде - ни леблы ни содержимое поды. под остается в остальном неизменным.
осоебенно его начинка. это очено важно для помниания. специификация пода которая указана в ямле реплики используется
только для тех подов которые создает сама реплика в случае если на момент ее создания этих подов меньше чем
указано в ее настройке replicas: 2.


далее мы имеем еще более поразительну вещь. вот наш под попал под власть реплики. мы можем имзенить конфиурацию
этого пода и реплика на это никак не прореагирует.
берем ямль пода и меняем в нем версию жинкса

$ cat pod-rp3.yml  | grep image
    image: nginx:1.15.2

публукуем под
$ 
kubectl apply -f pod-rp3.yml 
pod/pod-rp3 configured


смотрим свойства пода
$ kubectl get pods pod-rp3 -o json
    "metadata": {

        "labels": {
            "app": "nginx2",
            "custombuild": "1"
        },

        "name": "pod-rp3",

        "ownerReferences": [
            {
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "ReplicaSet",
                "name": "rp3",
            }

    "spec": {
        "containers": [
            {
                "image": "nginx:1.15.2",


тоесть под по прежнему находится по управлением реплики и версия жинкса у него
та что указана в магифесте пода а не в магифесте реплики


что поразиельно то что в свойтвах реплики соверещнно нет инфо о подах за котоырми она следит

$ kubectl get rs rp3 -o json
{
    "metadata": {
        "name": "rp3",
    },
    "spec": {
        "replicas": 2,
        "selector": {
            "matchLabels": {
                "app": "nginx2"
            }
        },
        "template": {
                     ...
    "status": {
        "availableReplicas": 2,
        "fullyLabeledReplicas": 2,
        "observedGeneration": 1,
        "readyReplicas": 2,
        "replicas": 2
    }
}


тоесть указано что поды есть. но что это за поды в ней не указано. 
разве это не дебилизм.
если я имею рплику как мне иузнать за каким подами она следит.
мне что парсить все поды блядь


из того что я описа выше вытекает важный вывод - те параметр пода котоыре указаны в ямле реплики
если их поменять то это совершенно не повлечет того чтобы реплика начала уничтожать поды и заменять их на 
тот что укзаан в ее ямле. нихуя такого нет. параетры пода указанные в реплике касаются только тех подов которые
реплика будет создавать сама. а создавать она их будет только если один из подов за которым она следит сдох.
если все поды живы а мы обновили ямль реплики то непроизодет НИХУЯ. чтобы у нас обновились поды надо 
либо убить их руками, либо уменьшить в ямле реплики число подов а потом его увеличтить. вот такое 
важный вывод. эти суки которые в к8. они об этом ничего непишут. уебки.
а ведь интутиивно ожидаешь того что если обновил ямвль реплики то все поды должны быть обновлены. но этого небудет.

итак еще раз как работает релика сет. вот мы ее создали\опубликовали.
в ее ямле указан паарметр пода котоыре она может создавать . но она их небросается создавать нет.
она первым делом ищет поды которые уже созданы и есть в системе. далее она ищет среди этих подов те 
которые stand alone которые созданы руками у которых нет секции "ownerReferences" . ей это гвоорит что даные
поды неаходятся под управлением другой релплики сет.  если таких подов нет то реплика сет с текущими подами 
ничео неделает. это важно понять.  а если она все же находит поды у которых нет секции "ownerReferences"  ттолько 
тогда она обращает внимание на такие поды и начинает обсматривать их лейблы. если леблы таких подов совпадают с 
селекторами реплики сета только тогда эта реплика сет "захыватывает" эти поды. тоесть она добавляет в них 
секцию "ownerReferences"  в коорой прописано чтотеперь они находиятся под ее пристальным вниманием и также 
реплика считает что  на один под ей нужно создавать меньше. если реплика среди уже существующих подов найдет 
достаточное количество того что ей надо то она создавать вообще ничего не будет. а вот если она нашла 
мало подов то лько тогда она создаст ровно столько сколько нехватает уже сама на оснвое того шаблона пода
котоыр у нее пропсиан в ямле. это пиздец потмо учто эти суки из к8 обо всем это м молчат.
итак если будет найдено достатноче количество пдоходящих подов то реплика сама создавтьа небудет ничего.
это значит что начинка этих подов будет совершенно неткакая как указано в ее ямле. 
а вот если реплика ненайдет ниодного подходяешего пода тогда она создать все сама и начина будет ровно така
как указано в ее ямле. если рпелика захывает уже существующих под то она совешенно нетрогает его начинку.
когда под находистя под властью реплики мы можем его спокойно менять через манифест пода и это никак не 
заставит реплику делаеть ничего.  интересно чтообновление контейнера в поде неприводит к пересозданию пода.
это поразиетельно. а раз под небыл терминирован то с точки зрения реплики все без измеенений. 

отсьюа важные выводы - еси мы хоим быть уверены что все поды в реплике такие как у нее прописаны в манфесте
надо после создания рпелики уменьшить число подов до нуля а потом уже до нужного числа.
еще вывод- изменеие ямля реплики невлечет пересоздание подов. они не будут пересозданы. нихуя.

реплика пересоздает под (тоесть создает новый) только если наблюдаемый под был уничтожен.

берем под который находисят под управлением реплики сет
$ kubectl get pods rp3-vlbwp  -o json

 "ownerReferences": 
            {
                "controller": true,
                "kind": "ReplicaSet",
                "name": "rp3",


"spec": {
        "containers": [
            {
                "image": "nginx:1.13.2",


        "containerStatuses": [
            {
                "image": "nginx:1.13.2",
                "state": {
                    "running": {
                        "startedAt": "2023-06-17T17:47:28Z"   # время старт контйенера
        "podIP": "10.244.0.20",
        "startTime": "2023-06-17T17:47:27Z"  # время старта пода




$ kubectl get pods rp3-vlbwp 
NAME        READY   STATUS    RESTARTS   AGE
rp3-vlbwp   1/1     Running   0          57m


значит подчеркнут что этот под  я несоздава руками. это под который создал сам реплика сет.
значит я создал толко что руками ямль для пода который должен этот под поменять

$ cat pod-rp3-vlbwp.yaml 
apiVersion: v1

kind: Pod
metadata:
  name: rp3-vlbwp
  labels:
    app: nginx2

spec:
  containers:
  - name: nginx
    image: nginx:1.15.2



прмиеняем наш манифест

$ kubectl apply -f pod-rp3-vlbwp.yaml
Warning: resource pods/rp3-vlbwp is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
pod/rp3-vlbwp configured


3$ kubectl get pods rp3-vlbwp
NAME        READY   STATUS    RESTARTS       AGE
rp3-vlbwp   1/1     Running   1 (106s ago)   74m



видно что под был изменен. (изменен но без уничтожения и пересоздания)

$ kubectl get pods rp3-vlbwp -o json
{
    "metadata": {
        "labels": {
            "app": "nginx2"
        },
        "name": "rp3-vlbwp",
        "ownerReferences": [
            {
                "kind": "ReplicaSet",
                "name": "rp3",
            }
        ],
    },
    "spec": {
        "containers": [
            {
                "image": "nginx:1.15.2",
        "containerStatuses": [
            {
                "image": "nginx:1.15.2",
                    "running": {
                        "startedAt": "2023-06-17T19:01:12Z"
                    }
                }
            }
        ],
        "podIPs": [
            {
                "ip": "10.244.0.20"
            }
        ],
        "startTime": "2023-06-17T17:47:27Z"


видим что под по прежнму под упрвленеим репили сет. мы споокойно смогли его поменят его внутрненности
его контенера. версия жинкса 1.15 как указао в ямле пода а не как в ямле реплики сет.
реплик сет получается реально тупая.

еще один момент. вот у нас запущена релика сет. у нее есть все нужные ей поды. и тут мы создаем руками 
еще один под который конечно же неимеет "ownerReferences и имеет лейблы такие как в селекторе у реплики сет.
она получается его тоже захватит но так как у нее уже есть все нужные ей поды то она удалит именно этот под. 
щас проверим что она удалить именно наш новый под а не один из тех что у нее есть.

создаю реплику  где 0 подов

$ cat rp3.yml 
apiVersion: apps/v1
kind: ReplicaSet

metadata:
  name: rp3

spec:
  selector:
    matchLabels:
      name: vasya

  replicas: 1

  template:
    metadata:
      labels:
        name: vasya

    spec:
      containers:
      - name: vasya
        image: nginx:1.13.2
        ports:
        - containerPort: 80



применяю.  что интеересно походу это то что имя подов котоыре создаст реплика никак нельзя
прописать в ямле рпелики. только имя контейнеров можно прописать в ямле. но это нето.


значит у нас создлся под с 1.13.2 имаджем

$ kubectl get pods rp3-jb24t -o json | grep "\"image\""
                "image": "nginx:1.13.2",
                "image": "nginx:1.13.2",


щас я руками создам под с имаджем 1.15.2 и посмтрим какой под удалит рпелика сет  - тот который
она создала или мой ручной


$ cat pod.yml 
apiVersion: v1

kind: Pod
metadata:
  name: rp3-c1
  labels:
    name: vasya
    custombuild: "1"

spec:
  containers:
  - name: webserver1
    image: nginx:1.14.2


$ kubectl apply -f pod.yml 
pod/rp3-c1 created

под был создан

$ kubectl get pods | grep rp3-c1

но в спике созданных его нет

а в совтйсхвах реплики сет есть событие 

$ kubectl get rs rp3
Events:
  Type    Reason            Age    From                   Message
  ----    ------            ----   ----                   -------
  Normal  SuccessfulCreate  6m54s  replicaset-controller  Created pod: rp3-jb24t
  Normal  SuccessfulDelete  89s    replicaset-controller  Deleted pod: rp3-c1

тоесть бы удален именно новый ручной под. а старый был оставлен.

что интересно вот эти две команды выдают разное. 
тоесть у них не одни и теже поля

#  kubectl get  rs rp3  -o json
#  kubectl describe rs rp3

пуолчается что можно узнаиь имена подов которым заведует реплика сет через 

$ kubectl describe rs rp3


следущие вопросы
- что если мы опбуликовали реплику сет а поды которые к ней относятся уже есть на хосте?
уже ответил
- что если реплик сет создала поды а мы еще ей насунули руками подов
уже ответил



- пооходу в реплике  кода ее создали можно менять толко спек пода
а вот спек реплики пошел нхуй менять неьльзя. только уалять ее и пересоздавать. пример
я в ямле реплики добавил label дополниельный и накатил но куб послел нахер
$ kubectl apply -f rp3.yml 
The ReplicaSet "rp3" is invalid: 
... field is immutable



- selctor и labels должен сопваватьвать 
если у нас есть ямль реплики то у нас в нет прописывается свойства рпелики и а именно какой у нее селектор
по которому она ищет поды и там же прописывется шаблон пода котоырй будет создан если готовых подов не окажется. в шаблоне 
подо просывается лейбл по которому реплика сет будет его искать. так вот набор лейблов и набор сеелекторов должны собвпадать.
пример


apiVersion: apps/v1
kind: ReplicaSet

metadata:
  name: rp3

spec:
  selector:
    matchLabels:
      name: vasya
      label: "1"

  replicas: 1

  template:
    metadata:
      labels:
        name: vasya

    spec:
      containers:
      - name: vasya
        image: nginx:1.13.2
        ports:
        - containerPort: 80


вот селектор реплики
  selector:
    matchLabels:
      name: vasya
      label: "1"


вот лейбл щаблона пода
    metadata:
      labels:
        name: vasya

здесь втдно что сеелекто имеет один набор лейблов 
а набор лейблы пода имеют другой набор поэтмоу
при попытке создать такой реплик сет к8 пошлет нахер

$ kubectl apply -f rp3.yml 
The ReplicaSet "rp3" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"name":"vasya"}: `selector` does not match template `labels`



- ReplicaSet is the next-generation ReplicationController that supports the new set-based label selector. It's mainly used by Deployment as a mechanism to orchestrate pod creation, deletion and updates. Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.

ага это выдержка из доков к8. полезно для понимания.

а вот обьяснение что такое set-based lable selector 
Set-based label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: in,notin and exists (only the key identifier). 
тоесть мы можем требовать чтобы селектор не просто был равен чемуто а  входил в какоето множество.
пример set-based label селекторов

environment in (production, qa)
эта штука требует чтобы environment был равен любому из production или qa


tier notin (frontend, backend)
это селектор требует чтобы tier небыл равен frontend или backend

partition
этому селектору пофиг чему равен partition главное чтобы в селекторе присуствовал partition.
говоря другими словами selector = partition.* (мы поним что лейбейлы в у к8 это не списки  а dictionary у коиорго есть
ключ и значение. ) так вот в данном случае мы говорим что ключ должен бть равен partition а какое у него значение пофиг

!partition
в этом примере селектор труебует чтобы лейбл имел ключ нерваный партишн.  а валью вобще пофин какое



- repl contoler понимает команду rolling-update а реплика сет нет
когда то можно было заюзать команду kubectl rolling-update для  автоматчиеского обновления подов за которыми наблдает 
rc (replication controller) но на данный момент эту функцию выпилили. оэтому на данный момент команды
kubectl roling-update нет вообще.





под и реплик сет саомтоятльные хрени но реплика сет содврежит всебе одписание пода на случай если его 
надо создать
реплика доблавядет в опиание пода графу "ownerReferences"
а деплоймент давбляет в имя реплики хэщ и еще доавляет в под графу pod-hash...
как я пояле если под создать руками для релики сет можно то создать руками реплику сет длядепломента уже нельзя
(опмсать две ошибки)

- kubectl apply -f pod-rp3.yml 
error: unable to decode "pod-rp3.yml": json: cannot unmarshal number into Go struct field ObjectMeta.metadata.labels of type string
krivosheev@test:~/test/rp3$ 



так щас будует кусок который возможно повтоярет част того что написано выше:
мы щас говорим об rs.

если под ломается rs создает новый. в чем хитриый приздец реплики сета это то что если 
мы обновим манифест rs то ей это вощем то похуй в том плане что за этим не последует обновление 
подов за которыми она присматрвиет. она не тронет поды. чтобы обовить поды 
их надо убить. вот новые поды будут согласно нвоому манифесту. или надо уменьшить число 
scale  а потом прибавить обратно. суть одна - реплика сет нетрогает поды если обновился манифест rs.
она создает новые поды тольк заместо сдохших. долбеомы из к8 об этом совершенно не делают 
акцент в своем ебанутом доке.
еть еще тонкий вопрос - как реплика сет отличает поды за которм он следит от всех остальных.
на этот эти суки тоже неделают акцент. например  у нас есть две разных rs 
у них  в манифестах прописаны  одинаковые label. как они делят межлу собой поды?
на вивжненйий момент - как реплика опредяет поды котоыре ей принаджлжат.
эти суки приушут что на основе лейблс. это полуправда. иначе бы две реплики с одинаковыми лейблс 
дрались бы за тот же самый под.на смом деле  рплика сосмотрит если у пода сецкия ownership.
если есть то ее такой под неколышет а вот если жтой секции нет то тода она смотрит на его лейбл.
если лейбл совпдаает  селектором rs то rs считает этот под своим. если при этом у rs уже находится под
присмотром то число подов которое равно ее scale то этот под будет уничтожен. 

насолкько я понимаю когда под создает сама реплика она сразу ем пропсывет секцию ownership 
это дает то что на такой под другие реплики тоогда не обраащат внимание.


далее еще более сложный момент - что если у нас реплика соддана не руками а деплойментом.
тогда вот что. делпоймент доаблавяем в своства реплики не толкьо тот лейбл который мы прописали в манифесте
дплоймента а еще доп лейбл 
    "pod-template-hash": "5547ff7cd6"
подчеркну этот лейбел отсутсвует в манифесте деплоймента. мы его туда не помещали. это сам деплоймент
добавит допонилельный лейбел к поду. 
прада непонятно нахуй нужен этот доп лейбл. походу это персестарахоквка.
у нас как происходит обноавлние подов за которым набллюдает деплйомент - деплоймент создает +1 rs имя которой
строится из названия деплоймента. если у нас есть два деплоймента то у нас они имеют разные имена. значит и rs которые 
создают эти деплойменты(сокарщенно deploy) будут гарантрованно разные. имена подов генеирируется на основе имен rs 
раз имена rs разыне то и иимена подов будут разные. в кадой rs которая создана пропиывается какой deploy ею руководит 
поэтому опят же никкой путаницы. те поды которые будутсозданы этими rs в них каждая rs пропыивает в ownerReferences
что она руководит этим подом поэтмоу путаницы о том что под был создан одной rs скажем от первого deploy 
а в итоге под попал под захват другой rs от другого deploy на мой взгляди исчлючен. поэтому нахуй нужен 
это доп лейбл "pod-template-hash" не понимаю. как бутто это перестраховка.
если  у нас в свойтвах пода при его создании в "ownerReferences" прописано какая релика является его владельцем
а в свойствах реплики в свою очередь пропиано какой deploy является ее владельцем то непонятно нахуй было еще 
доп лейблы городить.

вот пример

$ kubectl describe  deploy  rss-site
...
NewReplicaSet:   rss-site-7cdf5b859 (2/2 replicas created)


$ kubectl describe rs rss-site-7cdf5b859
Labels:         app=web
                pod-template-hash=7cdf5b859
Selector:       app=web,pod-template-hash=7cdf5b859
Controlled By:  Deployment/rss-site
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed

здесь в описании реплики видно какой deploy ей заведует.
а также виден доп лейбл 
	pod-template-hash=7cdf5b859
который деплой прилепил к реплике сет.

ха! но ведь эта секция Labels в реплике сет она не относится к поиску подов. за это ответчает секкция Seelector.
сеция Label отвечает за то что она метит текущий обьект (rs в данном сулчае) что ее (rs) мог найти какйото вышестоящий
api обьект среди других !! тоесть pod-template-hash=7cdf5b859 эта лейбл по которому depliy который рулит этой rs 
ищет ее среди других rs. этот лейбл неимеет отошение к поииску подов! ибо деплой незанимается урпавлением подов.
деплой занимается только уравлением rs. а уже они управляют подами! но все авно непонятно зачем деплою лепить на rs доп лейбл 
если в свойствах этой rs итак по руски указано какой деплой ей управляет

$ kubectl get  rs rss-site-7cdf5b859 -o json
{
    "metadata": {
        "ownerReferences": [
            {
                "controller": true,
                "kind": "Deployment",
                "name": "rss-site",
            }
        ],

вот! нашей rs записано что ей управляет deploy по имени  "rss-site"
поэтому найти эту rs для деплоя нет проблем. 
единсвтенное что  я могу предположить на счет нахера деплою леить на rs доп метку 
		label:pod-template-hash=7cdf5b859
состоит в том что деплой он в процессе обновления подов через rs он получается постоянно создает 
новые rs и удаляет прежние rs и возможно он запиывает себе эти метки куда то чтобы знать какая rs была создана 
щас а какая в более старые времена (например для процесса отката обновлени ??)
но опять же нахер ему метка для этого - он же может для этого запоминать у себя названия rs они ведь 
уникальные. историю rs можно вести по имнам этих rs без импользвования доп меток... не понимаю..

ксати! я же помню из практики что в манифесте rs должны совпдаать поля в селекторе и в спеке пода.
поэтому раз в свойствах rs указано в селекторе 
	Selector:       app=web,pod-template-hash=7cdf5b859
значит следует конено же ожидаь что в поде будут точно такие же лейбелы. и точно1

$ kubectl describe pods rss-site-7cdf5b859-7mg9r
Labels:           app=web
                  pod-template-hash=7cdf5b859
Controlled By:  ReplicaSet/rss-site-7cdf5b859

вот видно что этот под контроливется нашей rs. и то что у него в лейблах пристувует 
	pod-template-hash=7cdf5b859
так что этот лейбл долаяяется нетолько деплоем в rs но и rs  в конечном итоге егодобалвяет в под!

покаызаваю манифест деплоя

$ cat d4.yaml 
kind: Deployment
apiVersion: apps/v1

metadata:
  name: rss-site
  labels:
    app: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web



  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: front-end
          image: nginx
          ports:
            - containerPort: 80


как видим в маниесте делоя мы не пропиывади лейбл
	pod-template-hash=7cdf5b859
это все отсебятина деплоя


 с разбором деплоя я конечно малек забежал  вперед ведь щас мы 
 гвооиим об rs


 - дебилый выввод статуса rs

 $ kubectl get rs
NAME                  DESIRED   CURRENT   READY   AGE
rp3                   10        10        10      25h


вот эти цифры они показывают несколько rs в штуках развернуто. нет. 
они покзывают сколько подов находится под управлением\присмотром данной ОДНОЙ rs.
тоесть rs одна а подов под присмотром 10.
это по мне очень дебильная система вывода инфо. интуитивно кажется что развенут 10 штук rs.

а вот еще дебильизм вывода  с другой стророны


$ kubectl describe rs rp3
Name:         rp3
Replicas:     10 current / 10 desired
Pods Status:  10 Running / 0 Waiting / 0 Succeeded / 0 Failed

Replicas:     10  = это тоже не про то что у нас 10 штук rs разврнуто. это прото что у нас 10 подов находится
под присмотром. причем развернуть их могли и до создания rs. тоесть нефакт что их создала наща rs.
так вот если replicas уже отображает сколько подов нахртся под присмотром нахуй еще раз эту инфо дублировать 
в следудщей строке. Pods Status:  10 Running = говорит ровно о том же самом. 
единсвтенное что я могу педположить что Replicas:     10   возможно значит сколько подов для наблюдения указано
в манифесте rs, а  Pods Status:  10 показывает сколко по факту подов нахтся шас под наблюдением. но опятьже 
по моему все это уже отображено в строке "Replicas:     10 current / 10 desired" тоесть у нас указано что
в манифетсте указано 10 подов и 10 пдов у нас и наблдеается. нахуй тодга дублировать эту инфо в строке  Pods Status:  10 Running
непонятно



| replication controller
расссмотрим его.

как я писал выше его юзать уже ненадо.
это устаревшая версия rs

эта секцтя про 
replication controller



apiVersion: v1

kind: ReplicationController
metadata:
  name: repl-contr-vasya
spec:
  selector:
    property: nginx-vasya

  replicas: 2


  template:
    metadata:
      name:  pod-vasya
      labels:
        property: nginx-vasya
    spec:
      containers:
        - name: cont-vasya
          image: nginx:1.15.2
          ports:
            - containerPort: 80



$ kubectl apply -f repl-contr-vasya.yml 
replicationcontroller/repl-contr-vasya created

$ kubectl get rc
NAME               DESIRED   CURRENT   READY   AGE
repl-contr-vasya   2         2         2       7m17s

rc это репликейшн контроллер
rs это реплика сет 
смотреть спмсок хреновин куба можно через 
$ kubectl api-resources
там можно подсмотреть не только список всех хреновик которые может созавать куб
но и их укороченные названия


при этом получим поды с именами
repl-contr-vasya-8wvhd 
repl-contr-vasya-ddxxx 

приклоно что строка   name:  pod-vasya неимеет никаткго занчения
иимена подов строятся на основе имени репл контроллера + хэш


вот что покзывает оисание репл контроллера


$ kubectl describe rc repl-contr-vasya
Name:         repl-contr-vasya
Namespace:    default
Selector:     property=nginx-vasya
Labels:       property=nginx-vasya
Annotations:  <none>
Replicas:     2 current / 2 desired
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  property=nginx-vasya
  Containers:
   cont-vasya:
    Image:        nginx:1.15.2
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  14m   replication-controller  Created pod: repl-contr-vasya-ddxxx
  Normal  SuccessfulCreate  14m   replication-controller  Created pod: repl-contr-vasya-8wvhd


в целом все похоже как и у репл сет.


теперь я хочу сравнить ямль для реплики сета и для репл контроллера.
в целом разница только в том как опиываетс селектор
у реплики сет вот так

spec:
  selector:
    matchLabels:
      name: vasya


у репл контроллера вот так

spec:
  selector:
    property: nginx-vasya



но далее чувак пишет что у рпелики сет более мощный аппарат по распиыванию селектора.
тоесть реплик сет например может иметь вот такую дуру в селекторе


spec:
   selector:
     matchExpressions:
      - {key: app, operator: In, values: [soaktestrs, soaktestrs, soaktest]}
      - {key: teir, operator: NotIn, values: [production]}

а видимо реклика контроллер нет

расшифровка этой дуры такая что  app может быть либо soaktestrs, либо soaktestrs, либо soaktest
а teir не может быть production


еще раз упомяню что если мы создали ручной rs или rc то обновлять поды за которыми они следят 
надо по схеме :
 As explained in #1353, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.

а вот фраза от том что ненадо публиковат просто standalone поды. ибо если с ним чтот случится то никто под перезапусткать
небудет.ужно запускать под только под присмотром rs или rc. он за ним будет следить и перезапустит его в случае
если он terminated или deleted : 
Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.


далее полезнякшки о том когда ненадо юзеать rc\rs а юзать другой тип контроллера:
Use a Job instead of a ReplicationController for pods that are expected to terminate on their own (that is, batch jobs).
тоесть если мы ожидаем что под будет рабтать невечно тоесть типа как cron то надо юзать контроллер Job.
видимо дальше с ним  я будут знакомиться

далее. ои пишут что если работа пода связана с конкретным хостом на котором он крутится то надо юзать DaemonSet:
Use a DaemonSet instead of a ReplicationController for pods that provide a machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied to a machine lifetime: the pod needs to be running on the machine before other pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.


еще раз про то какова история перехода от rc к rs  и как обновляли поды котоыре под их присмотром
раньше:
вначале был replication controller и ручной процесс обноавления подов 
через : the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.


потом они чуть облегчили ручной труд и добавили

$ kubect rolling-update названия_rc -f имя_нового_rc


потом они придумали replicaset + deployment , выпилили kubectl rolling-update
и сказали забыть про rc . так что его в целом можно неизучать!


получается раньше репликейшн контроллер как плюс имел то что можно было его поды обновить 
легко через rollinh-update. щас это выпилили. также рпликшеншн контролер неимеет подержки крутых селектов
как это есть у реплики сета. так что наданный момент репликешйн контролллр неимеет нкиаких плюсов.


я думаю  что rc также как rs необновляет поды которые уже крутятся если в ямле rc заменить спеку у 
подов.



полчаетс что rc это старье и атавизм о котором можно забыть.
подами управялет реплика сет . реплкой упралявет деплоймент. 




схема обновления подов через rc\rs руками - создаем +1  rc\rs. у него 
обычно все тоже самое кроме скажем верссии докер имаджа. началное чиcло реплик = 1.
далее на старом контроллере уменьшаем число реплик на -1 а на новом увеличиваем на +1.
возникает вопрос почему новый под созданный на новом контроллере не подпадает под юрисдикцию 
старого контроллера . ответ потому что когда контроллер создает новый под он сразу же в нем прописывает
поле "ownerReferences" поэтому когда под создан старый контроллер это видит и мгновенно отваливает от этого пода

возикет идея.  а почему бы нам не создавать новый контроллер а обновить магифест старого и потом делать так - уменьшаем 
число подов на 1 потом обратно прибавляем. потом опять уменьшаем число подов на 1 потом прибваляем. 
может быть тогда он так удалит все старые поды и оставит толко новые? проверим

имеем 

s$ cat repl-contr-vasya.yml 
apiVersion: v1

kind: ReplicationController
metadata:
  name: repl-contr-vasya
spec:
  replicas: 2
  selector:
    property: nginx-vasya


  template:
    metadata:
      name:  pod-vasya
      labels:
        property: nginx-vasya
    spec:
      containers:
        - name: cont-vasya
          image: nginx:1.15.2
          ports:
            - containerPort: 80



его поды

$ kubectl describe pods repl-contr-vasya-8wvhd   | grep -i image:
    Image:          nginx:1.15.2

$ kubectl describe pods repl-contr-vasya-ddxxx   | grep -i image:
    Image:          nginx:1.15.2


обнволяем манифест контроллера

$ cat repl-contr-vasya.yml | grep -i image
          image: nginx:1.13.2

применям обновлванный манифест контроллера

$ kubectl apply -f repl-contr-vasya.yml 
replicationcontroller/repl-contr-vasya configured

проверяем что сейчас контроллер содержит новую версию имаджа


$ kubectl  describe rc repl-contr-vasya | grep -i image
    Image:        nginx:1.13.2


теперь меняем число реплик на -1 и +1

$ kubectl scale rc   repl-contr-vasya    --replicas=1
replicationcontroller/repl-contr-vasya scaled

$ kubectl scale rc   repl-contr-vasya    --replicas=2
replicationcontroller/repl-contr-vasya scaled

проверяем поды
$ kubectl describe pods  repl-contr-vasya-8wvhd   | grep -i image:
    Image:          nginx:1.15.2
 @test:~/test/repl-controllers$ kubectl describe pods  repl-contr-vasya-cchkj  | grep -i image:
    Image:          nginx:1.13.2

один под обновился. 

теперь еще раз -1 и +1


$ kubectl scale rc   repl-contr-vasya    --replicas=1
replicationcontroller/repl-contr-vasya scaled

 @test:~/test/repl-controllers$ kubectl scale rc   repl-contr-vasya    --replicas=2
replicationcontroller/repl-contr-vasya scaled


s$ kubectl describe pods  repl-contr-vasya-8nrgj  | grep -i image:
    Image:          nginx:1.13.2
 @test:~/test/repl-controllers$ kubectl describe pods  repl-contr-vasya-8wvhd  | grep -i image:
    Image:          nginx:1.15.2

ага. видно вот что. походу с точки зрения rc его поды имеют некий id. 
и если мы уменьшаем scale то он удаляет не тот под который самый стартый а тот у которого id самый меньший. и это 
всегда будет последний запущенный под. поэтому  условно говоря дераргая scale туда сюда на единицу мы будем толкьо
пересоздавать последний под. а первые будут отсавться безизменныйми.
таким макаром дергая скейл мы поды не обновим. также полчается чтоу нас скейл фактор при этом уменшается на один а если у нас
изчанально один под то вобще неможем такой тактиктой полтзватся.
и еще вот мы обновили ямл rc в нем заявлены уже новые версии подов. а по факту на проде у нас будет сборная солянка
из старых и новх версий . полуается состояние на проде отлвитчает от того что написано  в ямле rc. вобщем у
ткого подхода обровления подов одни минусы. поэтому понятнно что они говорят что надо обновлятьь поды под дургоуму.
создем +1 новый rc и потом в старом удаляем -1 scale а в новом прибавляем +1 scale тогда у нас все хоороо на кажом этапе.
пока на новом rc непоявится +1 здоровый под мы на старом ничего не удаляем. все надежно. и хорошо.


а вот такой пост про сравнение rc vs rs vs deployment 
написал на стековерфлоу

Replication Controller VS Deployment in Kubernetes

replication controller is the old version of replica set.

so replication controller(rc) vs replica set(rs) vs deployment(deploy).

rs can work with set-based selectors. rc can not.

some time before we could automatically update pods of rc via

kubectl rolling-update old_rc_manifest -f new_rc_manifest

now this feature is removed from kubectl.

if we create rs manually we can not automatically update its pods - that is there is no any command that delete old pods one by one and create new pods one by one. we should do it manually. However if we create rs via deployment then we can update pods automatically using deploy features.

so rc is the old stuff with no pros. instead use rs. if we want to update pods of rs in a usefull manner do not create rs manually , instead use deploy. deploy automatically create rs, rs create pods.


| minikube
чтобы не ставить к8 руками то побыстрому можно поставить minikube

стартует \ стопим его череез

$ minukube start
$ minikube stop

при этом kubectl надо ставить отдльно руками
потому что встроенный  minikube kubectl  это гавно

| как узнать версию k8

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}

Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.3", GitCommit:"9e644106593f3f4aa98f8a84b23db5fa378900bd", 
GitTreeState:"clean", BuildDate:"2023-03-15T13:33:12Z", GoVersion:"go1.19.7", Compiler:"gc", Platform:"linux/amd64"}

версия куба это "server version". в даннном случае это "v1.26.3"


| api

запускаем мы разворот манифеста а нам на экране ошибка

no matches for kind "Deployment" in version "extensions/v1beta1"

прикол в том что для разворота разных контроллеров нужны разные версии api.
скажем мы хотим деплоймент развернуть тогда теперь легко узнать какую версию апи мы хотим заюзать (по ходу у дебилов
из к8 присуттувует сразу несолкьо версий апи)

так вот

$ kubectl api-resources | grep -E "APIVERSION|Deployment"
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
deployments                       deploy       apps/v1                                true         Deployment


получается для разворота магифеста деплоймента надо юзать apps/v1
тоесть

# cat d4.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: rss-site
...



| labels
| selector

я уже выше их обсуждал.
щас хочу сказать то что если у обьекта есть селектор это значит что на основе 
него он ищет некие дочерние обьекты. например на основе селектора rs ищет свои поды

а если у обьекта есть labels это значит что на основе этих labels уже этот обьект будет 
искать какйто более выскопоставленыый родителсткий обьект. например лейблы в rs говорят о том 
что на основе этих лейблов этот rs может быть обьектом поиска для deployment


итак label характирирузует сам обьект. а selector характириузует какойто другой обьект который этот обьект
будет искать.

пример rs


$ kubectl describe rs rss-site-7cdf5b859
Name:           rss-site-7cdf5b859
Namespace:      default
Selector:       app=web,pod-template-hash=7cdf5b859
Labels:         app=web
                pod-template-hash=7cdf5b859
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/rss-site
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=web
           pod-template-hash=7cdf5b859
  Containers:
   front-end:
    Image:        nginx
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>


его поле Selector:       app=web,pod-template-hash=7cdf5b859
показывает те лейблы на основе которых он ищет свои поды

а поле 
Labels:         app=web
                pod-template-hash=7cdf5b859

показыает что на основе этого лейбла его ищет родитеслкий контроллер  Deployment/rss-site

итак selector это хрень на осове которой ты когото ищешь
а label это на основе чего тебя ищут другие

итак selector это чтобы мы могли найти когото
а labels это чтобы могли найти нас

| полезняшка

отличный способ узнать что с контрол панелью куба все окей
надо всего навсего посмотреть успешно ли стартанули все его системные поды

# kubectl get pods  -n kube-system

ну а второй шаг это псмотреть в 

# kubectl describe имя_ноды 

как там с pressure. хватает ли памяти, дисков. и все ли окей с оверлейной сетью.
причем прикол. в kubectl describe имя_ноды может писать что с овеолейной сетю все окей
а состояние системного поды оверлейной сети будет failure так что тут надо с двух 
сторон смотреть

| stuck
| terminate

если под висит бесконечно в статусе "terminate"
то его надо убить рукам 

# kubectl delete pod <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>
или 
# kubectl delete pod POD_NAME --grace-period=0 --force -n NAMESPACE_NAME


| runtime

как узнать какой тип конейнеризации используется на к8. говоря по заумному
кккой рантайм.

ответ надо посмореть в настройках кубелета

/usr/local/bin/kubelet
--v=2
--node-ip=10.42.147.39
--hostname-override=nl-k8-01
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--config=/etc/kubernetes/kubelet-config.yaml
--kubeconfig=/etc/kubernetes/kubelet.conf
--container-runtime=remote
--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
--runtime-cgroups=/system.slice/containerd.service


здесь райнтайм это containerd


| deployment

одна из фишек deploy это 

* Managing the state of an application: Deployments can be paused, edited, and rolled back, so you can make changes with a minimum of fuss.

paused и rollback это прикольно

вторая фишка
* Easily exposing a workload outside the cluster: It might not sound like much, but being able to create a service that connects your application with the outside world with a single command is more than a little convenient.

буду ниже смотреть как это на практике

забегая вперед. под - это рабочий кирпич рабочая машина рабочая программа которая фактичски делает
полезный контент. за ней следит rs. его задача перезапускать упавшие поды.
деплой он автоматизиирует обновление подов  в частности так что числ рабочих подов все время
типа константа во время обновления поэтму редунданси приложения не меняется. делает он это 
через создание +1 новой реплики. а далее во второй прибавляет по 1 поду а встароой убирает. и так 
они их "перекачивает" с одной реплики в другую. по дефолту поды досутпны только по сети только внутри куба 
плюс если под убили а другой создали то поменлся один из IP в этой групппе подов. поэтому service(svc)
дает единый IP кооторый неменяется. плюс он пробрасывает порт наружу хоста.

про деплой. в отличие от rs наш деплой при обновлении шаблона пода  в его манифесте
он сразу начинает обновлять свои поды. и у него есть две стратегии обновления подов.
одна стратегия это когда он полностью удаляет все старые поды
и только после этого начинает создавать новые. эта стратегия в манифесте
прописывается вот так

  replicas: 1  
  strategy:
    type: Recreate
 

вторая стратегия это когда он понемног создает новые поды и понемног удаляет старые
подые. эта стратегия в конфиге прописывется вот так

replicas: 3  
strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0


пр maxsurge и  maxUnavailable 
если кратно для чего они

maxSurge: 2        # how many pods we can add at a time
maxUnavailable: 0  # maxUnavailable define how many pods can be unavailable
                         # during the rolling update

но это если кратно и малопонятно   а так надо говорить отдельно для этого смотри "kubernetes PV.txt" файл, там я это расписал.


по поводу того как откатывать поды в деплое к предыдущему релизу.
с одной стороны можно зайти в манифест деплоя и руками скажем поправить обратно 
версии имаджей на предыдующую версию. по мне метод норм. а можно сделать по другому.
дело в том что деплой  у нас обноавляется как? он создает новый rs в нем приавбяеляет
число реплик в старой rs убавляет до нуля. так вот по окончании процесса обновления
он старую реплику неудаляет. она так и болтается в к8. по дефолту таких реплик деплой
хранит толи 3 толи 10 штук. можно задать чтобы он это хранил и больше.вот
хоршая статья про это (https://learnk8s.io/kubernetes-rollbacks)
так вот можно увелчить это число, для этого в манифесте деплоя надо вписать

 replicas: 3
  revisionHistoryLimit: 100


а вот доказетельство

# kubectl get rs
NAME                                DESIRED   CURRENT   READY   AGE
depl3-599bb89957                    2         2         2       14s
depl3-9759cc7f6                     0         0         0       40h
depl3-fc57c9788                     0         0         0       40h

это rs принадлежщие деплою.видно что висит один рабочий rs с текущими
подами и два старых rs без подов.


инетересыйн вопрос хранит ли в своем манифесте деплой список старых rs
ответ нет. как он их находит  - находит он их просто - во первых он их владелец, 
во вторых они rs имеют теже лейблы которые прописаны в селекторе деплоя.
окей - деплой знает про свою кучу rs. как он определяет в какой последовательности
эти rs он создавал. ответ такой что эта инфа содержится в аннотации rs
в аннотации есть поле "revision"
по нему деплой понимает какой rs был опубликован за каким

это текущий последний rs
# kubectl describe rs  depl3-599bb89957  | head
Name:           depl3-599bb89957
Annotations:    
                deployment.kubernetes.io/revision: 3



это прердыдущий rs
# kubectl describe rs  depl3-9759cc7f6  | head | grep revision
                deployment.kubernetes.io/revision: 2

это самый старый rs
# kubectl describe rs  depl3-fc57c9788  | head | grep revision
                deployment.kubernetes.io/revision: 1


с этим разобрались. теперь вопрос как нам откатсят на преддущую версию подов
используя это ?так занчит еще раз покажу какой rs щас последний и какая версия
имаджа там стоит

# kubectl describe rs  depl3-599bb89957  | grep -E "revision|Image"
                deployment.kubernetes.io/revision: 3
    Image:        nginx:1.7.6


а вотпредыдущая версия rs

# kubectl describe rs  depl3-9759cc7f6  |  grep -E "revision|Image"
                deployment.kubernetes.io/revision: 2
    Image:        nginx:1.7.9


откатвытеися на предудущю весрию подов
# kubectl rollout undo deployment/depl3  --to-revision=2
deployment.apps/depl3 rolled back


вот такая у нас была ситуация

# kubectl get rs
NAME                                DESIRED   CURRENT   READY   AGE
depl3-599bb89957                    2         2         2       14s
depl3-9759cc7f6                     0         0         0       40h
depl3-fc57c9788                     0         0         0       40h


вот такая у нас стала ситуация

# kubectl get rs | grep -E "NAME|depl3"
NAME                                DESIRED   CURRENT   READY   AGE
depl3-599bb89957                    0         0         0       10m
depl3-9759cc7f6                     2         2         2       40h
depl3-fc57c9788                     0         0         0       40h



вот такие номера ревизий были

# kubectl describe rs  depl3-599bb89957  | head
Name:           depl3-599bb89957
Annotations:    
                deployment.kubernetes.io/revision: 3



# kubectl describe rs  depl3-9759cc7f6  | head | grep revision
                deployment.kubernetes.io/revision: 2

# kubectl describe rs  depl3-fc57c9788  | head | grep revision
                deployment.kubernetes.io/revision: 1



а вот такие стали
# kubectl describe rs  depl3-599bb89957  | grep -E "revision|Image"
                deployment.kubernetes.io/revision: 3
    Image:        nginx:1.7.6

# kubectl describe rs  depl3-9759cc7f6  |  grep -E "revision|Image"
                deployment.kubernetes.io/revision: 4
    Image:        nginx:1.7.9



видно что тот rs куда деплой перешел в результате роллоута у него прибавилась 
версия ревизии. была 2 а стала 4. 

вот такая схемотехника. поэтому по номеру ревизии всегда можно 
узнать какая реплика сет былв прошлый раз и позапрошлый раз.
внезависимости скачем ли мы по репликам через роллоут или мы просто накатыавем
новые версии деплоя одну за одной.

тоесть как происходит rollout - деплой просто берет и перекачивает поды из 
одной rs  в другую. вот так просто. тоесть он уменшает число подов в текущй rs
и увелчиивает в одной из старых rs. и прибалвяет номер ревизии в той реплике
которая текущая.

едиснвтенное что над помнить что если после такого роллоута  нам надо 
в нашем текстовом файле манифеста деплоя поменять номера имаджей на текущий.
а то будет несоовтасвие манифетса на к8 и  у нас на компе с которого мы разворачивали
этот деплой.

процесс деплоя и рооллоута в динамике (тоесть сколько подов уже обновлено)
можно наблдать  перидически вызывая 

# kubectl get rs

а можно через комкнду
# kubectl rollout status deployment/depl

елинстевеное что эта команда ебнутая. показыает некоеректные резултатты. 
об этом подробно в "kubernetes PV.txt" я расписал.

поэтому да - вопрс как наблюдать динамику сколько подов обновлено при деплое 
или роллоуте - я рекомендую неюзать команду  kubectl rollout status deployment/depl 
а юзать банально   kubectl get rs

еще вот такая команда есть, это еще одна ебанутая команда
# kubectl rollout history deployment/depl3
deployment.apps/depl3 
REVISION  CHANGE-CAUSE
1         <none>
3         <none>
4         <none>


она по идее должна показывать чтото типа такого
kubectl rollout history deployment/app
REVISION  CHANGE-CAUSE
1         kubectl create --filename=deployment.yaml --record=true
2         kubectl apply --filename=deployment.yaml --record=true

но она такого никогда непокаже потому что для этого надо при накатке 
деплоя в командной строке указать доп флаг --record

# kubectl apply -f deployment.yaml --record
или вот так
# kubectl apply -f deployment.yaml --record=true

так вот проблема в том что если мы хотим задать этот флаг в манифесте деплоя
то в нем такой опции нет. так что пошел нахуй называется.
еще я не знаю нужно ли при каждлом вызове деплоя вставлять этот флаг 
или достатно только один раз.


продолжаю про деплоймент.
важная вещь - положим новая версия деплоя неправильная неудачная.
как ее откатить?

показываю

вот текущая версия имаджа у подов
# kubectl describe rs depl3-fc57c9788 | grep -i image
 Image:          nginx:1.13.10

изменили деплой. накатываем неущствующую версию жинкс


# kubectl get pod | grep -E "NAME|depl3"
NAME                                      READY   STATUS         RESTARTS        AGE
depl3-554ddc4978-4vf4t                    0/1     ErrImagePull   0               5s
depl3-7c7f7bb655-h8465                    1/1     Running        0               2m36s
depl3-7c7f7bb655-jl8xw                    1/1     Running        0               2m24s


смотрим в дескрайб пода
Error: ImagePullBackOff


# kubectl get rs | grep depl3
depl3-554ddc4978                    1         1         0       3m29s
depl3-7c7f7bb655                    2         2         2       6m1s


понятно что где "2" это старая rs а где  "1" 

а вот что к слову показывает status деплоя
# kubectl rollout status deploy depl3
Waiting for deployment "depl3" rollout to finish: 1 out of 2 new replicas have been updated...


откатываем деплой обратно (та самая команда)

# kubectl rollout   undo  deploy depl3
deployment.apps/depl3 rolled back

смотрим что стало с rs


вот так было на момент херового деплоя
depl3-554ddc4978                    1         1         0       3m29s
depl3-7c7f7bb655                    2         2         2       6m1s


вот так стало после rollout
# kubectl get rs | grep depl3
depl3-554ddc4978                    0         0         0       7m42s
depl3-7c7f7bb655                    2         2         2       10m




# kubectl describe deploy depl3 | grep NewReplicaSet:
NewReplicaSet:   depl3-7c7f7bb655 (2/2 replicas created)

таким образом rollout меняет обратно ту активную реплику сет на предыдущую.
вот так работает rolloout. на херовой rs число подов сдувается до нуля. а на предыдущей
rs надувается до максимума.

едиснтвенное что мне непонятно почему при этом неудаляется стремная реплика?


щая я проверю а можно ли таким же rollout октатить успешный деплой?
итак имеем

# kubectl describe deploy depl3 | grep NewReplicaSet:
NewReplicaSet:   depl3-7c7f7bb655 (2/2 replicas created)

# kubectl describe rs  depl3-7c7f7bb655  | grep  -i image
    Image:        nginx:1.13.10

делаем успешный деплой
# kubectl describe deploy depl3 | grep NewReplicaSet:
NewReplicaSet:   depl3-8bfccfcbf (2/2 replicas created)

# kubectl describe rs  depl3-8bfccfcbf    | grep  -i image
    Image:        nginx:1.7.6

откатываем через rollout
# kubectl rollout undo deploy depl3
deployment.apps/depl3 rolled back

проверяем деплой
# kubectl describe deploy depl3 | grep NewReplicaSet:
NewReplicaSet:   depl3-7c7f7bb655 (2/2 replicas created)

# kubectl describe rs   depl3-7c7f7bb655     | grep  -i image
    Image:        nginx:1.13.10

итак да - rollout deploy откатывает версию подов на -1 релиз
внезависиомсти был ли текущий релиз успешным или нет.

получается это просто укороченная версия команды
# kubectl rollout undo deployment/depl3  --to-revision=2

понятно.



продллжаю про депйломент и его свойства
я уже скзаал что деплой обновляет поды по двум сценариям.
повторюсь. одинц сценарйи это recreate
A deployment defined with a strategy of type Recreate will terminate all the running instances then recreate them with the newer version.
тоесть вначале удаляются полнстью все старые поды и  только после этого создаются 
новые поды.
The recreate strategy is a dummy deployment which consists of shutting down version A then deploying version B after version A is turned off. This technique implies downtime of the service that depends on both shutdown and boot duration of the application.

еще про rollout там есть rollout pause

# kubectl rollout pause deploy depl3
deployment.apps/depl3 paused

что это дает. работающие поды будут продолжать работать 
как я понял новые обновления деплоя перестанут работать.
также ( я непроверял) если процесс деплоя идет и в это время ставим пауузу
то как я понимаю то накатка оставшихся подов прекращается.

следущий момент соупуствующий это port-forward
эта штука которая позволяет настроить порт форвардинг проксирования  с локального
локалхоста до пода

вот у меня есть под   whoami

# curl 10.233.80.216
Hostname: whoami-deploy-1-75d55b64f6-gqldp
IP: 10.233.80.216   (<=== ip пода)
RemoteAddr: 10.233.66.128:58432   (<==== src ip нашего пакета)


# kubectl port-forward   whoami-deploy-1-75d55b64f6-gqldp  4545:80
Forwarding from 127.0.0.1:4545 -> 80
Forwarding from [::1]:4545 -> 80
Handling connection for 4545

этой командой мы пробрасываем порт 4545 с нашего локалхоста в порт 80 на удаленном поде.

# netstat -tnlp | grep 4545
tcp        0      0 127.0.0.1:4545          0.0.0.0:*               LISTEN      2395642/kubectl     

#  curl localhost:4545
Hostname: whoami-deploy-1-75d55b64f6-gqldp
IP: 10.233.80.216
RemoteAddr: 127.0.0.1:47906


единственое что мне непоняпонтяно  нахер этот порт форвард надо.
потому что с любой ноды к8 можно напрямую через овелейную сесть достучаться 
до любого пода. непонятно


еще одна полезняшка. как в сервисе изменить селектор под которым он работает
и ищет поды

# kubectl patch service my-app -p '{"spec":{"selector":{"version":"v1.0.0"}}}'

еще полезняшка как зайти в под

# kubectl exec --stdin --tty  название_пода -- /bin/bash

двигаю  дальше.
если унас есть деплоймент. и в нем мы пропишем подключение PV
через PVC а этот PV имеет режим ReadWriteOnce который означает то что
этот PV можно монтировать только на одной ноде и  поды которые крутятся
на этой ноде могут ее себе монтировать. но поды на другой ноде
уже не могут ее монтировать. так вот если запустить деплоймент
и число реплик равно 2 и больше то деплоймент будет стараться раскидвать 
поды по разным нодам и деплой при этом плевал что у нас получится ситуация
что на одной ноде под смортирует себе PV а ана другой ноде под уже получит отлуп
и в логах пода будет запись

Multi-Attach error for volume Volume is already used by pod(s)

так вот это именно потому что деплой плевал на требования PV что все поды 
дожны быть на одной ноде. эта ошибка об этом. реешение такое - надо 
заставить к8 создавать поды на одной ноде. тогда никакой проблемы примонтиовать 
этот PV хоть на 100 000 подов нет. для того чтобы заставить к8 создавать поды
на одной ноде надо в манифест деплоя добавить NodeSelector

пример деплой с NodeSelector

apiVersion: apps/v1
kind: Deployment


metadata:
  name: depl3


spec:
  replicas: 10
  revisionHistoryLimit: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 5
      maxUnavailable: 0
  selector:
    matchLabels:
      app: nginx3


  template:
    metadata:
      labels:
        app: nginx3
    spec:
      nodeSelector:
        kubernetes.io/hostname: nl-test-02
      containers:
      - name: nginx
        image: nginx:1.7.6
        ports:
        - containerPort: 80
        volumeMounts:
        - name: data
          mountPath: "/usr/share/nginx/html"
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-02



