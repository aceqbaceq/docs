kubespray

kubespray

https://github.com/kubernetes-sigs/kubespray.git

самый толковый гид это = https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws

скачиваем с гита
вообще это папка с ансибль плейбуками и раскаткой виртуалок через тераформ.


перед тем как накатывать тераформ надо установить ансибль. потому что 
при накаткет тераформа потом автомтом стартует ансибль. так что надо прежде всего
установит ансибль. в readme.md написано что для этого надо выполнить:
      $ sudo pip3 install -r requirements.txt
я же вместо этого предлагаю ставить ансибль по другому смотри kubespray-ansible.txt

теперь когда ансибль поставили можно ставить виртуалки 
через тераформ.

подпапка в которой лежат тераформ плейбуки
/kubernetes-kubesparay/kubespray-master/contrib/terraform/aws

идем внее.

экспортируем переменные.

export TF_VAR_AWS_ACCESS_KEY_ID="публичный ключ aws token"
export TF_VAR_AWS_SECRET_ACCESS_KEY ="приватный ключ aws tokeen"
export TF_VAR_AWS_SSH_KEY_NAME="имя ssh ключа который будет импортрован внуьрь инстансов"
export TF_VAR_AWS_DEFAULT_REGION="имя региона"

пример

export TF_VAR_AWS_ACCESS_KEY_ID="AKIAXKD3DX4K234234234234"
export TF_VAR_AWS_SECRET_ACCESS_KEY="az3yBNEjWIN4RVrGP1L523423423424323/"
export TF_VAR_AWS_SSH_KEY_NAME="a.krivosheev"
export TF_VAR_AWS_DEFAULT_REGION="eu-west-1"



запускаем тераформ
  $  terraform fmt
  $  terraform validate
  $  terraform init
  $  terraform fmt
  $  terraform validate
  $  terraform plan -out plan.txt
  $  terraform apply  plan.txt
  

вывод на экране после установки


Outputs:

aws_elb_api_fqdn = "kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com:6443"
bastion_ip = "18.202.77.68"
default_tags = tomap({})
etcd = <<EOT
10.250.202.87
10.250.214.169
10.250.194.80
EOT
inventory = <<EOT
[all]
ip-10-250-202-87.eu-west-1.compute.internal ansible_host=10.250.202.87
ip-10-250-214-169.eu-west-1.compute.internal ansible_host=10.250.214.169
ip-10-250-194-80.eu-west-1.compute.internal ansible_host=10.250.194.80
ip-10-250-196-151.eu-west-1.compute.internal ansible_host=10.250.196.151
ip-10-250-222-49.eu-west-1.compute.internal ansible_host=10.250.222.49
ip-10-250-199-7.eu-west-1.compute.internal ansible_host=10.250.199.7
ip-10-250-213-52.eu-west-1.compute.internal ansible_host=10.250.213.52

bastion ansible_host=18.202.77.68

[bastion]
bastion ansible_host=18.202.77.68

[kube_control_plane]
ip-10-250-202-87.eu-west-1.compute.internal
ip-10-250-214-169.eu-west-1.compute.internal
ip-10-250-194-80.eu-west-1.compute.internal

[kube_node]
ip-10-250-196-151.eu-west-1.compute.internal
ip-10-250-222-49.eu-west-1.compute.internal
ip-10-250-199-7.eu-west-1.compute.internal
ip-10-250-213-52.eu-west-1.compute.internal

[etcd]
ip-10-250-202-87.eu-west-1.compute.internal
ip-10-250-214-169.eu-west-1.compute.internal
ip-10-250-194-80.eu-west-1.compute.internal

[calico_rr]

[k8s_cluster:children]
kube_node
kube_control_plane
calico_rr

[k8s_cluster:vars]
apiserver_loadbalancer_domain_name="kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com"

EOT
masters = <<EOT
10.250.202.87
10.250.214.169
10.250.194.80
EOT
workers = <<EOT
10.250.196.151
10.250.222.49
10.250.199.7
10.250.213.52
EOT


потом автоматом запускатеся ансибль то куб неможет поставиться из за того что кэширующий 
dns сервис systemd-resolved имеет неверный dns указанный и надо пойти  на хосты куба  в папку

/etc/systemd/resolved.conf

и там прописать 8.8.8.8

[Resolve]
DNS=8.8.8.8


после того как тераформ прогонится то в папке будет создан иневентори файл hosts для ансибля
/kubernetes-kubesparay/kubespray-master/inventory/

далее надо запустить ансибль потому что мы всего навсего создали только виртуалки.
а куба наних еще нет.

делать это надо только вот так

$ ansible-playbook -i ./inventory/hosts ./cluster.yml -e ansible_user=admin -b --become-user=root --flush-cache

без указания -e ansible_user=admin вобще ничо несработает. будет писать что юзео ansble_user
незадан.


проблемы:
- немогу войти на бастион созданный. = оказалось что ансибль кубспрея после тераформа 
   на иснстанах ключи ssh прописывет не для юзера ubuntu а для юезра admin!!!
   так что на инстансы надо входить как admin


- таже вылезло что непонятно как создавать инстансы сразу в несольких регионах. это уже 
   чисто проблема самого тераформа


далее.
если ансибль у нас все сделал до конца
то в папке кубспрея в корне появится файл

ssh-bastion.conf

это ssh config 
он дает то что теперь мы можем по ssh подклчитья к любому инстансу который мы поставили по ssh
через bastion инстанс как ssh proxy.


ssh -F ./ssh-bastion.conf user@$ip

например 
$ cat ssh-bastion.conf

Host 3.250.187.184
  Hostname 3.250.187.184
  StrictHostKeyChecking no
  ControlMaster auto
  ControlPath ~/.ssh/ansible-%r@%h:%p
  ControlPersist 5m

Host  10.250.199.144 10.250.222.252 10.250.198.134 10.250.201.239 10.250.220.38 10.250.192.227 10.250.214.46
user admin
  ProxyCommand ssh -F /dev/null -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -p 22 admin@3.250.187.184 
krivosheeva@jurvis:~/Terraform/kubernetes-kubesparay/kubespray-master$ 



тогда подключаеиися к одному из инстансов

$ ssh -F ssh-bastion.conf 10.250.198.134


далее. давайте уже наконец покдлючимся к кубу.

идем по ssh на любой мастер



$ ssh -F ssh-bastion.conf 10.250.198.134

там становимся рутом

$ sudo bash

инаконец мы можем проверить состяние куба

# kubectl get nodes

напомню что сертификаты доступа к кубу лежат на мастерах в папке /etc/kubernetes/admin.conf"
и также этот же файл лежит на мастерах в папке 
/root/.kube/config


далее.
хотим чтобы мы могли подлкючаться к кубе прям с нашего ноутбука

$ mkdir -p ~/.kube

копмруем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -F ssh-bastion.conf 10.250.199.144 'sudo chmod 644 /etc/kubernetes/admin.conf'


10.250.199.144 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -F ssh-bastion.conf admin@10.250.199.144:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -F ssh-bastion.conf 10.250.199.144 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com:6443


дело в том что когда мы вывзаем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого

$ LB_HOST=$(cat inventory/hosts | grep apiserver_loadbalancer_domain_name | cut -d'"' -f2)
$ sed -i "s^server:.*^server: https://$LB_HOST:6443^" ~/.kube/config


тут надо сказать что это за хрень kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
откуда она взядась что это такое.
а это тераформ на амазоне создает амазоновский лоад балансер. он создает причем  Classic Load Balancer.
что это за хрень. это доменное имя некоторое.  котооое имеет несколько A IP адресов например

$ nslookup kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Name: kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Address: 52.211.32.9
Name: kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Address: 3.248.96.93

каждый адрес видимо в своей авайлабилити зоне.
и далее также пробрасывается с этих внешних IP внутрь амазона порт уже на кокнетнй инстансы.
тоесть лоадбаласер это как хапрокси который имеет внешний IP и прбосбываем порт с внешнено ip
на бекенд сервера. AWS Classic Load Balancer (CLB) operates at Layer 4. тоесть udp\tcp . means is that the load balancer routes traffic between clients and backend servers based on IP address and TCP port.
для сравнния у амазоне есть еще L7 балансер они его зовут как  Application Load Balancer. 
он подходит для балансировки HTTP трафика. такая же аналогия как haproxy может работать на l4 режиме
а может на L7 режиме.

так вот тераформ куб спрея создает L4 amazon Classic Load Balancer = aws CLB и  с внешних IP пробрасывает 
порт 6443 на мастера куба.  ( про  L4 amazon Classic Load Balancer надо еще дальше поразбираться). 
главная суть что контролплейт куба реально прям выставляется наружу кубспреем во внешний мир.
уж незнаю наколько это безопасно.


что еще интересно. то что хосты кроме бастиона они создаются без внешнего IP. вопрос как тогда 
они в инте выходят например для установки пакетов. ответ для subnet в котоой сидят инстансы  в таблице
маршрутизации для этого subnet  добавлен марщруты вида

10.250.192.0/18 local  
0.0.0.0/0 nat-0110f39cec15f906f  

первый маршрут это чисто внутри площадочный локальный. 
а ip интернетовские - выход на них идет через nat гейтвей. их можно создать в virtual private clooud  - nat gateways. опять же тут неочень пгимаю разницу между virtual private clooud  - internet gateways и 
virtual private clooud  - nat gateways. это тоже надо прояснять.


но зато тпеперь понятно как иснатсны без внешнего ip умудряются выходит в интернет









 это AWS Classic Load Balancer (CLB) operates at Layer 4.





ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes



-----
попробовал увязать тераформ и виртуабокс но к сожалению пока что 
на данный момент провайдер для вирутаобокса гавно.

а так вот огн конфиг для виртуалбокса


r$ cat main.tf 

terraform {
  required_providers {
    virtualbox = {
      source  = "terra-farm/virtualbox"
      version = "0.2.2-alpha.1"
    }
  }
}


resource "virtualbox_vm" "node" {
  count  = 2
  name   = format("node-%02d", count.index + 1)
  image  = "https://app.vagrantup.com/ubuntu/boxes/bionic64/versions/20180903.0.0/providers/virtualbox.box"
  cpus   = 2
  memory = "512 mib"
  // user_data = "${file("user_data")}"


  network_adapter {
    type           = "bridged"
    host_interface = "en0"
  }


}



====
далее. 
рассмотрим установку k8 через kubespray когда виртуалки 
руками раскатаны через vagrant.



скачиваем с гита кубспрей.
$ git clone https://github.com/kubernetes-sigs/kubespray.git

ставим ансибль как прописано в kubespray-ansible.txt

ставим вагрант. конфиг для виртуалок берем из файла kubespray-Vagrantfile.txt
в нем прописано четыре виртуалки
kmaster2	| 100.0.0.3
kmaster2	| 100.0.0.4	
kworker2	| 100.0.0.9
kworker3	| 100.0.0.10

тут важно отметить то что согласно конфигу во все вм будет интегрирован доступ по 
единому ssh ключу из папки ~/.vagrant.d/insecure_private_key.
это важный момент потому что именно этот ключ потом мы будем указывать ансиблю для доступа на виртуалки.


еще важный момент такой что практика показала что первые две виртуалки кубспрей будет 
юзать для мастеров контрол плейна куба. а вот все уже остальные виртуалки кубспрей 
будет использовать уже как дата ноды куба. как это поменять как задать кубспрею сколько 
нод использовать под контрол плейн а сколько под дата ноды пока непонятно. так что 
если хотим иметь хотя бы одну дата ноду то надо создавать минимум три виртуалки.


запускаем виртуалки
$ vagrant up

убеждаемся что мы можем зайти на все эти виртуалки по ключу ~/.vagrant.d/insecure_private_key
чтобы понимать что ансибль тоже туда сможет зайти
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.4
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.9
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.10


идем в папку с кубспреем
и копируем вспомогательные файлы инвентаря из сэмпла

$ cp -rfp inventory/sample inventory/mycluster

но самого файла инвентаря hosts.yml там еще нет.
нам надо его создать 

$ declare -a IPS=(100.0.0.3 100.0.0.4 100.0.0.9 100.0.0.10)
в этой строке мы указываем все ip адреса всех виртуалок которые мы хотим запихнуть в куб

запускаем генерацию файла инвентаря 
$ CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py ${IPS[@]}

если все окей то мы должны получить вот такой файл
$ cat ./inventory/mycluster/hosts.yml 
all:
  hosts:
    node1:
      ansible_host: 100.0.0.3
      ip: 100.0.0.3
      access_ip: 100.0.0.3
    node2:
      ansible_host: 100.0.0.4
      ip: 100.0.0.4
      access_ip: 100.0.0.4
    node3:
      ansible_host: 100.0.0.9
      ip: 100.0.0.9
      access_ip: 100.0.0.9
    node4:
      ansible_host: 100.0.0.10
      ip: 100.0.0.10
      access_ip: 100.0.0.10
  children:
    kube_control_plane:
      hosts:
        node1:
        node2:
    kube_node:
      hosts:
        node1:
        node2:
        node3:
        node4:
    etcd:
      hosts:
        node1:
        node2:
        node3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}


как видно первые две ноды входят в контрол плейн.
насколько я понимаю руками можно скажем добавить еще ноды в группу kube_control_plane
при желании.

этот файл с инвентарем нам нужно немножко подкореектировать. нам нужно указать 
ansible_ssh_user под которым ансибль будет ломиться по ssh на виртуалки и нужно 
указать ансиблю какой приватный ключ.

эти две строки выглядят так
ansible_ssh_user: vagrant
ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key

а откоркетированный файл выглядит так
all:
  vars:
     ansible_ssh_user: vagrant
     ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key
  hosts:
    node1:
      ansible_host: 100.0.0.3
      ip: 100.0.0.3
      access_ip: 100.0.0.3
    node2:
      ansible_host: 100.0.0.4
      ip: 100.0.0.4
      access_ip: 100.0.0.4
    node3:
      ansible_host: 100.0.0.9
      ip: 100.0.0.9
      access_ip: 100.0.0.9
    node4:
      ansible_host: 100.0.0.10
      ip: 100.0.0.10
      access_ip: 100.0.0.10
  children:
    kube_control_plane:
      hosts:
        node1:
        node2:
    kube_node:
      hosts:
        node1:
        node2:
        node3:
        node4:
    etcd:
      hosts:
        node1:
        node2:
        node3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}


далее заходим в наш установленный ансибль через venv
$ source ~/python3/ansible/env/bin/activate

запускаем ансибль плейбук cluster.yml на установку куба на виртуалках

(env)$ ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml

все процесс пошел.
примерно через 20 минут мы получим установленный куб


далее.
хотим чтобы мы могли подлкючаться к кубу с нашего ноутбука
создаем папку для ключа от куба.

$ mkdir -p ~/.kube

копируем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 644 /etc/kubernetes/admin.conf'


100.0.0.3 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -i ~/.vagrant.d/insecure_private_key    vagrant@100.0.0.3:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://100.0.0.3:6443


дело в том что когда мы вызываем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого.

ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes
NAME    STATUS   ROLES                  AGE   VERSION
node1   Ready    control-plane,master   18h   v1.22.5
node2   Ready    control-plane,master   18h   v1.22.5
node3   Ready    <none>                 18h   v1.22.5
node4   Ready    <none>                 18h   v1.22.5

====
следущий этап. апгрейдим предыдущий этап.
делаем тоже самое что делали на предыдущем этапе - тоесть 
разворот куба через кубспрей на вагранте но апгреженный вариант тоесть 
делаем тоже самое но с еще более минимальным ручным участием. чтобы 
это было по максимуму автоматизированный процесс чтобы руками прилось
делать меньше чем в предыдущем варианте.

качаем кубспрей.

$ git clone https://github.com/kubernetes-sigs/kubespray.git

заменяем файл Vagrantfile в папке на kubespray-Vagrantfile2.txt (более продвинутый вагрантфайл
чем в предыдущем случае)

устанавливаем ansible как описано в kubespray-ansible.txt

заходим из под виртуального питона в котором установлен ансибль в папку с кубспреем
и запускаем разворот виртуалок на вагранте. вагрант нетолько  создаст
виртуалки но и сам запустит разворот куба на вирталках через ансибль

(env) vagrant up

важно запускать разворот вагранта из под виртуального питона на котором установлена
нужная версия ансибля для кубспрея потому что вагрант будет запускать ансибль который
именно на хосте и если версия ансибля нета что надо то вагрант напишет про проблемы
с ансиблем и разворота плейбука непроизойдет. я пробовал запускать разворот 
вагранта без нужной версии ансибля на хосте и вагрант вылетал с ошибкой о несовместимости
с версией ансибля. поэтому еще раз подчеркну что чтобы вагрант мог успешно сам запустить ансибль
 и начать разворачивать куб на виртуалках важно чтобы на хосте стоял именно тот ансибль который 
 прописан в requirements.txt, эту версию ансибля можно поставит как общесистемно(чтобы он был доступен всем юзерам хоста) через sudo pip3 ... , а можно локально через виртуальную среду питона(чтобы этот ансибль был доступен только текущему юзеру).


когда все отработает мы получим созданные виртуалки и развернутый на них куб.

остается только руками скопировать ключ с мастер ноды  и поставить kubectl на ноутбуке.

далее.
хотим чтобы мы могли подлкючаться к кубу с нашего ноутбука
создаем папку для ключа от куба.

$ mkdir -p ~/.kube

копируем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 644 /etc/kubernetes/admin.conf'


100.0.0.3 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -i ~/.vagrant.d/insecure_private_key    vagrant@100.0.0.3:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://100.0.0.3:6443


дело в том что когда мы вызываем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого.

ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes
NAME    STATUS   ROLES                  AGE   VERSION
node1   Ready    control-plane,master   18h   v1.22.5
node2   Ready    control-plane,master   18h   v1.22.5
node3   Ready    <none>                 18h   v1.22.5
node4   Ready    <none> 

================================================
следущий этап - попробоват както автоматизировать перетаскивание ключа от куба
с мастер ноды на ноутбук и установку kubectl

делаю этот этап.

напоролся на то что так как я юзаю ансиль из venv питона то  у меня 
когда я пытался использловать модуль apt у ансибля вылезала ошибка:

fatal: [localhost]: FAILED! => {"changed": false, "msg": "Could not import python modules: apt, apt_pkg. Please install python3-apt package."}

это ошибка имено из за того что юзается питон в venv.
решение:
Создайте виртуальную среду без переключателя --system-site-packages . После создания среды перейдите в папку, в которой она была создана. В нем должен быть файл pyvenv.cfg . Отредактируйте этот файл. В нем есть (среди прочего текста) строка
include-system-site-packages = false
Измените эту строку на:
include-system-site-packages = true
после этого ошибка исчезла.

сделал.
в папку с кубспреем нужно положить плейбук phase-II.yml

и запускаем его командой

$ ansible-playbook -i ./.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory phase-II.yml -e ansible_ssh_user=vagrant -e notebook_user=vasya

notebook_user=vasya   это имя линукс юзера под которым мы сидим на ноутбуке. оно нужно для 
того чтобы плейбук знал в какую папку копировать с контрол плейна сертификат от куба.
если мы задали что юзера вася то сертификат будет скопироан в папку /home/vasya/.kube/config

таким образом когда мы разверули куб на вагранте нам ненужно руками перетаскивать на 
ноутбук сертифкат от куба и ставить руками kubectl это все за нас сделает плейбук.

провверяем что все сработало.

$ kubectl get nodes

===================================================
следущий этап - попробовать phase-II.yml интегрировать в Vagrantfile
тоесть чтобы запуск phase-II.yml осуществлялся самим вагрантом а не нашими руками.
походу сработало. теперь фаза2 запускается вагрантом. таким образом вагрант нетолько 
создает виртуалки и разворачивает куб но  и перетаскивает на нуотбук ключ от куба
и устанавливает kubectl.

чтобы все сработало надо положить в папку с кубспреем файлы:
   kubespray-Vagrantfile3.txt
   phase-II.yml

в вагрантфайле появилась новая переменная NOTEBOOK_USER, она описвыает под каким юзером
мы сидим на ноутбуке. чтобы ансибль знал в какую папку копировать ключ от куба.

почему то при работе  с phase-ii.yml 
вылезала ошибка 
   fatal: [localhost]: FAILED! => {"changed": false, "msg": "Failed to update apt cache: unknown reason"}

поэтому пришлось убрать из таска с apt параметр update_cache: yes
думаю это приколы из за python venv

=============================================
следущий шаг - избавиться от NOTEBOOK_USER, определять его автоматически
готово.

для этого в плейбук phase-II.yml 
я добавил такой кусок

  vars:
    - notebook_user: "{{ lookup('env','USER') }}"

 файлы Vagrantfile и phase-ii.yml 
 с этими измнениями можно теперь найти на https://github.com/aceqbaceq/kubespray_vagrant
 номер коммита = e70af45bf070185a159bf0f9f42f66371de30b28 

 далее опять же всплыла почемуто ошибка о том что модуль apt неможет 
 сделать update_cache. ну хер с ним. удалил эту опцию из phase-II.yml
 commit a6912a205248953f7ad409d2272a007a431abe


===========================
следущий этап создать скрипт который 
  загружает кубпспрей, 
  загружает Vagrantfile, 
  загружает  фазу2, 
  ставит ансибль, 
  запускаем вагрант
который  уже все ставит полностью  что уже прям можно 
на ноутбуке будет запустить kubectl get nodes. тоесть чтобы процесс установки куба был автоматический от самого начал до самого конца.

готово. в принципе я все сделал.

$ git clone https://github.com/aceqbaceq/kubespray_vagrant.git
$ ansible-playbook start.yml

но есть пока проблема. если ансиблем запускать vagrant up то ансибль невыводить 
статус выполенения. поэтому последнюю команду прихоится вводить руками в баш после выполнения плейбука

$ source /home/vasya/git/kubespray5/python3/ansible/bin/activate &&  VAGRANT_VAGRANTFILE=/home/vasya/git/kubespray5/Vagrantfile vagrant up


===========================================
следущий этап - надо понять как избавиться от ручного ввода запуска команды
$ source /home/vasya/git/kubespray5/python3/ansible/bin/activate &&  VAGRANT_VAGRANTFILE=/home/vasya/git/kubespray5/Vagrantfile vagrant up


вобщем из того что я нашел = у ансибля сейчас нет такой штуки чтобы можно 
было в режиме realtime получать вывод работающего скрипта. поэтому решил переписать
start.yml через bash.

так - я это сделал.

commit
10002e590884be7f6d1a7b8fb922eecd31add90f

$ git clone https://github.com/aceqbaceq/kubespray_vagrant
$ ./start.bash

ставит куб от в полностью автоматическом режиме от самого начала до самого конца
когда уже можно на нуотбуке ввести kubectl get nodes

 важная хрень с которой столкнулся. когда запускаем vagrant up то самое надежное это перед этим войти через  команду cd в папку где лежит Vagrantfile. 
 если вместо этого пытаться использовать переменную VAGRANT_CWD то это работает очень херово. переменная VAGRANT_CWD согласно докам  позволяет задать 
 папку где vagrant ищет Vagrantfile и где vagrant будет создавать папку .vagrant
 казалось бы эта переменная то что надо. однако увы. по каким то причинам переменная __FILE__
 при этом будет указывать не  на VAGRANT_CWD а на ту папку откуда мы запускаем команду vagrant
  а внутри цикла который разворачивает виртуалки мы вставляем наш код провижионера
  но в нем мы проверяем какая по счету виртуалка только что была запущена. если это последняя
  виртуалка в цикле то мы понимаем что все виртуалки запущены и только тогда if проходит 
  проверку и запускается код провижионера нам то надо чтобы __FILE__ указывал на папку где лежит Vagrantfile.
  пример . мы сидим в папке /scripts и из нее запускаем 
     VAGRANT_CWD=/home/vasya/git/kubespray vagrant up
  так вот вагрант будет искать Vagrantконфиг file в /home/vasya/git/kubespray/Vagrantfile 
  и это да то что нужно. также вагрант создаст папку /home/vasya/git/kubespray/.vagrant
  и это тоже то что нужно. но почему то __FILE__ будет указывать не на папку /home/vasya/git/kubespray/ а на папку /scripts  вот в этом и жопа. поэтому я и говорю что самое наежное
  при запуске vagrant up это руками перейти в папку где лежит Vagrantfile а неиспользовать
  VAGRANT_CWD. переменная VAGRANT_HOME также непомогла. так что самое надежное
         здесь параметры провижионераэто перейти в папку с Vagrantfile 
  и только потом делать vagrant up
===========================================
на данном этапе перезапуск start.bash приводит к тому что папка с кубпсреем полностью 
стирается и деплоится заново с нуля. поэтому конкрено через start.bash увеличить число нод
в кластере неполуится. для этого надо поменять число нод в Vagrantfile и руками 
запустить vagrant up, таким обраом start.bash предназначен для быстрого первоначального
развоорота куба. дальнейшая работа с ним должна идти чисто через vagrant или ансибль.
также еще момент что куб разворачивается без ноды бастион.

============================================

столкнулся с приколом вагранта он делает разворот вируталки а сразу на нее начинает применять
плейбук ансибля несмотря на то что в Vagrantfile стоит цикл на разворот виртуалок а команда
на провижн (запуск ансибля) стоит уже после цикла. в итоге чтобы нам сделать так чтобы вначале
развернуть все виртуалки и только потом начать на них применять ансибль надо применить 
трюк внутри цикла который разворачивает виртуалки мы вставляем наш код провижионера
но в нем мы проверяем какая по счету виртуалка только что была запущена. если это последняя
виртуалка в цикле то мы понимаем что все виртуалки запущены и только тогда if проходит 
проверку и запускается код провижионера


(1..NUM_INSTANCES).each do |i|
  вот здесь кусок который описывает конфиг виртуалки


    # Only execute the Ansible provisioner once, when all the machines are up and ready.
    if i == NUM_INSTANCES
       node.vm.provision "cluster", type: "ansible", run: "once" do |ansible|
       здесь параметры провижионера
        end
     end
end

еще раз скажу если вынести код провижионера просто за цикл который раозворачивает 
виртуалки то почемуто вагранту на это насрать и он начнет применять провижионер в цикле
запуска виртуалок сразу как только будет развернута первая виртуалка что конечно дебилизм.

=============================================================
следущий этап - понять как на вагранте развернуть бастиоn.
на данный момент куб на вагранте разворачивается без бастиона.
надо это исправить.

вобще кажда нода сейчас получется две сет карты. одна NAT карта через которую нода 
выходит в инет. вторая карта это карта локальной сети host-only через которую нода 
общается с другими нодами.


дальше чтобы разрбраться с бастионом придется изучить материал якобы посторонний.
но он будет нужен.

про pipe "|" в bash:
когда в баш мы пишем $ command1 | command2 
то баш создает два процесса command1 и command2 (одновременно и сразу)
потом когда в линуксе созадется процесс то всегда автоматом для этого процесса 
открываются три файла. связь с файлами идет через спец ссылки - файловые дескрипторы(что это 
за хрень вдеталях непонятно). процесс знает о том что у него есть три открытых файла.
ссылки на файлы (дескрпиторы) хранятся в /proc/PID/fd/  в виде /proc/PID/fd/0 , /proc/PID/fd/1, /proc/PID/fd/2 .  используя эти 0, 1, 2 процесс может иметь доступ к этим файлам. причем эти файлы 
по разному открыты. файл 0 открыт на чтение, файл 1 и 2 на запись. а файлы эти непростые файлы на 
диске а особые. файл 0 имеет связь с клавиатурой. когда мы тыкаем по кнопкам то буквы летят
каким то образом в этот файл 0 а из него процесс эти буквы читает\получает. файл 1 связан с экраном
если процесс пишет в файл 1 то буквы из файла 1 автомтом посылатся на экран. файл 2 тоже если в него
писать то буквы автоматом появляются на экране. почемуто чтобы заебать мозги эти файлы зовут такими 
абстрактынми штуками как поток. что такое поток неясно. а вот что такое файл очень даже ясно.
так вот возвращаемся к pipe. вот бащ создал два процесса и линукс автомтом для каждого из них 
создал каждому свой индивуальный набор файлов 0,1,2. так вот баш автоматом что делает - баш 
берет файл 1 от первого процесса и делает бекендом этого файла не экран а буфер в памяти  для первого процесса берет файл  0 и тделает бекендом у этого файла не клавиатура а тот же буфер в памяти от первого процесса. поэтому когда первый
процесс чтото пишет в 1 то данные попадают в память (а ненаэкран) а второй автоматом это получает через свой файл 0. вот такой бекенд работы 
у pipe "|" когда мы его юзаем в баше.

вот как выглядят 0,1,2 для обычного терминала с bash

$ ls -1al /proc/10994/fd
lrwx------ 1 vasya vasya 64 янв 10 14:08 0 -> /dev/pts/57
lrwx------ 1 vasya vasya 64 янв 10 14:08 1 -> /dev/pts/57
lrwx------ 1 vasya vasya 64 янв 10 14:08 2 -> /dev/pts/57
lrwx------ 1 vasya vasya 64 янв 10 14:08 255 -> /dev/pts/57

как видно три файла 0,1,2 у которых бекендом является /dev/pts/57
тоесть и кнопки прилетают с кнопок терминала и вывод идет в  экран темринала и ошибки
выводятся в экран терминала.
непонятно зачем еще линукс создал файл 255 с бекендом терминала с какой целью.


файл 0 обычно называют stdin стандартный ввод. 
файл 1 обычно называют stdout станадартный вывод
файл 2 обычно называюти stderr поток ошибок.

но хочу подчеркнуть что номера файлов ненесут сакрального смысла. тоесть 
если сам процесс захочет читать данные с 1 а не  0 ему никто незапрещает.
главное чтобы у файла был нужный бекенд. скажем можно создать файл с номер 10 , присвоить 
ему бекенд экран терминала и туда выводить буквы и они будут появляться на экране терминала.
так что функционал номров файлов нежестко привязан к тому для чего они могут использоваться.

скажем можно к 1 сделать бекендом /dev/pts/57 и к 255 сделать бекендом /dev/pts/57
и потом посылать буквы и на 1 и на 255 и каждый раз мы будем видеть буквы на экране.

вот на примере выше видно что экран терминала является бекендом и для файла 1 и для файла 255.
так что если мы будем послыать данные в 1 либо в 255 мы каджый раз получим буквы на экране.

проверяем
$ echo "---" >/proc/10994/fd/1
---
$ echo "---" >/proc/10994/fd/255
---

раобтает!

предназначение файлов 0,1,2 это скорее ожидаемый резульат но необязательный. тоест если мы пошлем  у процесса данные в файл 0 то скорей всего можно ожидать что процесс ожидает принятия данных именно
через канал 0. (но это необязательно)
если мы пошлем у процесса данные в канал 1 то скорей всего они выскочать на терминале
на экране где этот процесс крутится ( но это необязательно).

что интересно. что когда мы используем pipe "|" между процессами

$ command1 | command2 

то процессы совершенно не подозревают о том что баш ( да да именно баш) подменил бекенды
у файлов 0 и 1 у этих процессов. для процесса файл 0 это просто конвейр откуда обычно процесс
ожидает влетающих данных но что на том конце конвейера процесс незнает и его это неволгнует 
источник возникновения данных. либо это клава либо это данные из файла на диске либо еще что.
тоже самое для файла 1 . для процесса обычно это конвейер куда процесс кладет данные на отправку 
вовне. но куда этот конвейер ведет процесс незнает и его это неволнует.

поэтому когда мы юзаем pipe то процессы вобще неподозревают что выходной конвейер первого процесса стал входным конвеейером (через буфер в памяти) для второго процесса. это чисто изменение внешней
хрени которая на внутренней механике проецссов никак незатрагивает.

это как благотвориеьный фонд. откуда деньги поступают в него и куда уходят фонд невкурсе.
это как свинья в загоне. откуда приходит жрачка и куда уходит гавно свинью неволнует. 
откуда приходит жрачка это обычно канал 0 . а куда уходит гавно это обычно канал 1 - может 
в канализацию а может в ведро с едой соседней свинье с другом загоне.
я бы еще сказал так. канал 0 это откуда приходит еда для коровы. канал 1 это куда уходит молоко
а канал 2 это куда уходит гавно. корову это неволнует.
и баш через pipe делает  так что  молоко от первой коровы начинает поступаьть в ведро с едой 
для второй коровы. процесс это слепая корова которая стоит в темном загоне  с ведром еды.
трубой под гавно и трубой под молоков. такова внутренняя жизнь процесса.

процесс при желании может окрыть кучу файлов на чтение и запись у которых разные бекенды: файлы
на диске, клавиатуры, экраны терминалов.

вот еще раз возьмем обычный процесс cat
и посмотрим на его файлы 0,1,2

$ ls -1al /proc/11634/fd
lrwx------ 1 vasya vasya 64 янв 10 14:44 0 -> /dev/pts/0
lrwx------ 1 vasya vasya 64 янв 10 14:44 1 -> /dev/pts/0
lrwx------ 1 vasya vasya 64 янв 10 14:44 2 -> /dev/pts/0


что интересно. как видно 0 открыт нетолько на чтение но и на запись. хотя зачем
запись для канала по которому полгается тлоько чиатть данные.
и видим что бекендом для всех трех файлов является терминал /dev/pts/0

далее я послал буквы на все три канала 0,1,2 

$ echo "- - -" > /proc/11634/fd/1
$ echo "- - -" > /proc/11634/fd/2
$ echo "- - -" > /proc/11634/fd/0

и на экране терминала где запущен cat получил каждый раз вывод этих букв.

так вот щас мы запустим два cat ( два процесса неавжно какие конкретно команды) которые 
разделим пайпом

$ cat | cat 

и посмотрим какие теперь бекенды у файлов 0,1,2

$ ls -1al /proc/11721/fd
lrwx------ 1 vasya vasya 64 янв 10 14:49 0 -> /dev/pts/0
l-wx------ 1 vasya vasya 64 янв 10 14:49 1 -> 'pipe:[1835210]'
lrwx------ 1 vasya vasya 64 янв 10 14:49 2 -> /dev/pts/0

$ ls -1al /proc/11722/fd
lr-x------ 1 vasya vasya 64 янв 10 14:49 0 -> 'pipe:[1835210]'
lrwx------ 1 vasya vasya 64 янв 10 14:49 1 -> /dev/pts/0
lrwx------ 1 vasya vasya 64 янв 10 14:49 2 -> /dev/pts/0


и видно что файл 1 у первого процесса ведет в пайп (в буфер в памяти) 'pipe:[1835210]'
и что у второго процесса канал 0 вдете в тот же пайп.
таким ооразом если первый процесс будет писать  в файл 1 то эти данные будут лететь 
в файл 0 второго процесса и если второй процесс заъочет читать из файла 0 то он эти данные
получит.

еще видно что для файла 1 от перовго процесса установлен пермишн только на запись
l-wx------ 1 vasya vasya 64 янв 10 14:49 1 -> 'pipe:[1835210]'

а для файла  0 второго проецесса устанволен пермишн толко на чтениеэ 
lr-x------ 1 vasya vasya 64 янв 10 14:49 0 -> 'pipe:[1835210]'

таким образом один процесс может тлько пиать а второй тлоько читать из этого буфера (пайпа)

откроем еще один терминал и там напишем

$ echo "---" > /proc/11721/fd/1

таким образом мы для вторго cat послали данные через пайп как бутто они были отосланы первым cat

еще интересно так как оба cat запущены из одноо теоминала то в их файлах стоит одинаковый 
терминал dev/pts/0 в бекенде. тоесть обе эти команды могут срать в один терминал на экран.


еще раз про пайп.
раньше было непонятно что это за хрень по своей сути. теперь стало более понятно что 
за этим скрывается. а также за тем как по факту работает pipe в баше.

далее. более того. если мы скажем откроем терминал. потом найдем этот процесс в списке процессов.
скажем новый терминал (bash) имеет номер 9817 то мы можем делать вот такие приколы

$ echo "aaaa" > /proc/9817/fd/1

если мы так сделаем то в экране терминала 9817 мы увидим как появились сами собой символы "aaaa"
тоесть в эти файл без проблем можно посылать данные.
тоже самое будет если сделать

$ echo "aaaa" > /proc/9817/fd/2

мы тоже на экране терминала 9817 увидим что появились сами собой буквы "aaaa"

также если мы в терминале 9817 запустим cat который по умолчанию читает данные из файла 0

9817 $ cat

а потом мы пошлем буквы в этот файл 0 с другого терминала

$ echo "aaaa" > /proc/9817/fd/0

то на экране тмерианала 9817 мы опять же увидим появившиемя буквы "aaaa" потому что эти буквы
прилетели в файл 0 а cat их считал и вывел в файл 1 а оттуда они линуксом были просунуты на экран.

приеольно то что файлы 0 , 1, 2 неявляися как то там узурпированными доступными только их процессу.
это общедостуные файлы. кто хочешь туда тот и сри в них. получается если один процесс знает pid 
другого процесса то первый процесс может без проблем как я понимаю посылать данные другому процессу
через его файл 0 а также красть данные от процесса через его файл 1. 
получается что файл 0 , 1 ,2 это как некие конвейеры рукава ведущие от процесса и к процессу 
но прикол в том что эти рукава являюся общедоступными для всех других процессов. это прикол.
процесс похож на почтампт. а файлы 0 ,1, 2 на коневейер - транспортеры к этому иди от этого процесса. 

получается можно открыть два терминала. скажем второй терминал имеет pid=1234
тогда если начать из второго терминала читать данные из $ cat /proc/1234/fd/1 
то можно будет видеть все то что возникает на экране первого терминала.
точнее нетак - у нас как бы два терминала будут сидеть на одной трубе файла 2. так что 
тот терминал который быстрее успел по статистике то и забирает очередную букву вылетевшую
из файла 2. поэтому буква будет либо показана на терминале 1 либо на терминале 2. на какомто одном.

поэтому один процесс может запросто срать другому процессу на голову. прикольно 
что создатели линукса незащитили файлы 0,1,2 от доступа к ним  от других процессов.

теперь когда мы слыщим заумные слова stdin stdout stderr то теперь можно четко понимать 
что это значит на низовом уровне. что есть процесс. у него есть /proc/pid/fd папка и там
файловые дескприпторы. так вот stdin это файл 0, stdout это файл 1 , stderr это файл 2.
но эти номера это как это быывает по умолчанию но по факту эти номер в общем случае могут 
быть другими. в эти файлы данные могут влетить а могут вылетать. через эти файлы /proc/pid/fd/{0,1,2} процесс общается с внешним миром - получает данные и отправляет данные. в эти файлы
мы можем отправлять данные напрямую через echo "adadff " > proc/pid/fd/{0,1,2}
либо забирать оттуда данные напрямую cat proc/pid/fd/{0,1,2}
Вообще в fd папке находятся файловые дескрипторы всех файлов которые открыл данный процесс



рассмотри еще пример с файлами 0,1,2 и пайпом.
берем команду 
$ pv /dev/urandom
37GiB 0:02:16 [57,7MiB/s] [                   <=>                       ]

если ее так запустить то эта команда читает данные из /dev/urandom а на экране
выводит кучу левых символов и график о том как быстро она эти данные читает

посмотрим что там с открытыми файлами у этой программы


$ ls -1al /proc/11901/fd
lrwx------ 1 vasya vasya 64 янв 10 15:06 0 -> /dev/pts/0
lrwx------ 1 vasya vasya 64 янв 10 15:06 1 -> /dev/pts/0
lrwx------ 1 vasya vasya 64 янв 10 15:06 2 -> /dev/pts/0
lr-x------ 1 vasya vasya 64 янв 10 15:06 3 -> /dev/zero

значит для 0,1,2 бекендом является терминал  /dev/pts/0
через 0 процесс принимает данные с кнопок клавиатуры
через 1 выводит на экран прогресс бар
37GiB 0:02:16 [57,7MiB/s] [                   <=>                       ]
а также все символы что эта команда читает из устройства urandom
через 2 выводит инфо о ощибках на экран
через 3 процесс читает данные всебя.

замечу если мы заюзаем /dev/zero то на экране небудет ничего кроме прогресс бара потому что
/dev/zero выдает нули а нули в ascii на экране невыводят ничего. а вот есл в файл направим
$ cat /dev/zero > /tmp/1.txt то он будет полон точек если начать редактировать . странно. на экране главное ничего  а 
в файле уже точки.
если мы запустим чтение cat  1.txt то на экране будет пусто а неточки.

теперь добавим pipe

$ pv /dev/urandom |  cat > /dev/null

инфо по pv
$ ls -1al /proc/12164/fd
lrwx------ 1 vasya vasya 64 янв 10 15:16 0 -> /dev/pts/2
l-wx------ 1 vasya vasya 64 янв 10 15:16 1 -> 'pipe:[1844455]'
lrwx------ 1 vasya vasya 64 янв 10 15:16 2 -> /dev/pts/2
lr-x------ 1 vasya vasya 64 янв 10 15:16 3 -> /dev/urandom

инфо по cat
$ ls -1al /proc/12165/fd
lr-x------ 1 vasya vasya 64 янв 10 15:16 0 -> 'pipe:[1844455]'
l-wx------ 1 vasya vasya 64 янв 10 15:16 1 -> /dev/null
lrwx------ 1 vasya vasya 64 янв 10 15:16 2 -> /dev/pts/2


мы видим что у pv у файла 1 бекендом явялется пайп 'pipe:[1844455]'
тоесть когда pv пишет в файл 1 то даныне уходят в буфер памяти

мы видим что у cat у файла 0 бекендом вляется тот же пайп 'pipe:[1844455]'
тоесть в cat в файл 0 данные влетают из буфера памяти. 

мы видим что у cat у файла 1 бекендом яляется dev/null поэтому все что cat получил
он шлет в /dev/null и на экране будет чисто и пусто

 мы видим что pv читае данные из файла 3


 а вот как интересно выглядит список окрытых файлов у midnight commander
 как только его запустил

 $ ls -1al /proc/12318/fd
lrwx------ 1 vasya vasya 64 янв 10 15:25 0 -> /dev/pts/1
lrwx------ 1 vasya vasya 64 янв 10 15:25 1 -> /dev/pts/1
lrwx------ 1 vasya vasya 64 янв 10 15:25 2 -> /dev/pts/1
lrwx------ 1 vasya vasya 64 янв 10 15:25 3 -> 'socket:[1837706]'
lrwx------ 1 vasya vasya 64 янв 10 15:25 4 -> /dev/tty
lrwx------ 1 vasya vasya 64 янв 10 15:25 5 -> /dev/ptmx
lrwx------ 1 vasya vasya 64 янв 10 15:25 6 -> /dev/pts/2
lr-x------ 1 vasya vasya 64 янв 10 15:25 7 -> 'pipe:[1837711]'
l-wx------ 1 vasya vasya 64 янв 10 15:25 8 -> 'pipe:[1837711]'

далее
важно отметить что буквы можно посылать нетолько в файлы 0,1,2 но и напрямую в бекенд
в /dev/pts/2

тоест это даст одинаковый результат
$ echo "- - -" > /proc/12164/fd/2
$ echo "- - -" > /dev/pts/2

но в целом это и логично потому что 0,1,2 это всего лишь симлинки.

далее
pipe который мы в баше вставляем называется анонимный pipe.
есть еще иименованные пайпы. это ...

можно слать буквы сразу на /dev/ttyX или /dev/ptsX вместо файл дескриптовров процессов


tty vs pts:
как я щас представляю работает процесс приема букв  с клавы внутрь процесса и отрисовка
букв из процесса на экране: берем более простой случай когда мы работаем не в графической
оболочке а в голом текстовом режиме. (init 3). вот мы запустили команду и создался процесс.
при этом линукс автоматом для этого процесса откроет три файла на чтение и запись.
/proc/pid/fd/0  /proc/pid/fd/1    /proc/pid/fd/2 .  но эти файлы это всего лишь символические
ссылки на файл /dev/tty1 он является бекендом для этих символических ссылок.
далее самое интересное. если  процесс посылает буквы в файл 1 то они соотвественно летят 
в файл /dev/tty1 и попадают в ядро. а дальше ядро там устроено что оно данные которые влетели 
в файл /dev/tty1 отсылает отрисоваыет эти данные на экране. вот такой путь от процесса 
до картинки на экране. более интересно как сигнал от кнопки клавы попадает внутрь процесса.
пока я вижу так: жмем кнопку на клаве это вызывает на цпу срабатываение сигнала на его ноге
происходит hardware interrupt. это порождает то что на это событиие вызывается кусок кода
из ядра который называется interrupt handler. этот кусок понимает что прилетели данные
 с клавы и он их интерпретирует условно говоря в букву "A" далее еще интереснее 
 как я понимаю ядро(хендлер) понимает какой процесс был активен (условно говоря какой баш у нас был на экране когда мы жали кнопку) и ядро шлет эту букву "A" в stdin именно этого процесса!
 шлет эту букву в /proc/pid/fd/0 тоесть в /dev/tty1 таким макаром буква опять попадает 
 в как это ни смешно ядро но в какуб то другую часть этого ядра. и этот кусок кода ядра
 уже пересылает эту букву в кишки нашего процесса. 
 получается линукс так создан что если мы пишем в файл /dev/tty1 то инфо  попадает в ядро(черный ящик для процесса) и 
 ядро послывает эту букву на монитор(каким то хитрым образом). а если мы читаем из файла /dev/tty1
 то инфо попав в ядро им напрявляется в оперативку процесса. но ведь чтобы прочитать чтото 
 из /dev/tty1 надо вначале туда записать. а я уже сказал при записи в /dev/tty1 данные летят на 
 монитор. так при записи в /dev/tty1 данные летят на монитор или в память процесса или в оба места?
монитор разница в том что в /dev/tty1 пишет сам процесс или другой процесс
если сам процесс пишет то инфо идет на экран а если другой процесс то идет в память 
процесса. а может ядро различает что запись идет в /proc/pid/fd/0 а не в /proc/pid/fd/1
и хотя бекенд один и тот же но симлинки разные.

если поставить запусить баш процесс у котооо будет /dev/tty1 а потом откртыь 
второй терминал и там начать чиатть cat /dev/tty1 а потом вернутся на первый терминал
и начать жать на кнопки то часть кнопок будет рисоваться на первом терминале 
а часть на втором

сделал такой эксперимент:
в первом терминале запустил 

$ sleep 120

как sleep работает он неделает ничего в плане вывода на экран. 
при этом /proc/$$/fd/0 и  /proc/$$/fd/1  симлинки ведут на /dev/tty2 

на втором терминале я запустил

$ cat /dev/tty2

далее на первом терминале я начал жать кнопки
asdmsadasdlkasdjlasdjkaslkasdjl

при этом на втором терминале ничего непоявлялось при том что  на первом то терминале я вижу свои 
нажатые кнопки!

потом на первом терминале я нажал enter и очудо на втором терминале появилась вся набранная строка.

что это все значит: тут намешана вкучу несколько штук. вот мы нажали кнопку "a" сработал
сигнал на ноге процессора . был вызван кусок кода ядра и ядро поняло что была нажата кнопка "a"
ядро осознало и получило букву "a" в свою требуху. и внимание ядро отослало эту букву прямиком на экран и при этом внимание в /dev/tty2 ядро ничего непослало. это значит в частнрсти то что 
когда мы тыкаем кнопки в терминале и видим эти буквы в окне терминала это вобще нихера незначит что данные буквы уже долетели до процесса в терминале! он только долетели до ядра. в его буфер.
и еще ядро послало картинку об этих буквах на экран! а процесс в терминале и понятия об 
этих уже нажатых буквах неимеет!!! 
а вот когда мы нажали enter то ядро получило наконец сигнал что все эти буквы из требухи ядра 
надо и можно отосласть наконец процессу! и ядро пишет эти буквы в /dev/tty2 но значит ли это
что наконец процесс из терминала один (который sleep ) получил эти буквы. нет! нихера он 
их неполучил! потому что эти буквы украла команда из второго терминала! для тоого чтбы процесс
их первого терминала получил буквы надо чтобы он их считал их /dev/tty2 я он их читать несобиарлся
потому что это команда sleep. /dev/tty2 похож на почтовый ящик в подьезде. во первых 
получается что нажатые кнопки в сеанса терминала 1 летят ни в коем случае не в почтоый ящик. нет!
они летят чужому дяде на почтовую станцию в ядро. но даже после того как ядро наконец полоижило
буквы в почтовый ящик процес еще должен подойти к ящику и сам вытащит эти буквы. а если их
стырил другой процесс то наш процесс из первого терминала об этом даже и неузнает.
что здесь поразительно. приходим мы в магазин видим продавца и начинаем ему чтото писать на бумажке
мы думаем раз буквы появляютс яна бумаге то продавец их видит. а это нетак. все очено плохо.
мы водим рукой по бумажке. картинка с бумажкой перехватывается демоном. он по своей воле нам в глаза направляет картинку о том что на бумажке появляются буквы. если бы демон незахотел 
то мы бы и неувидели что буквы появляются на бумажке. потому что появляются они на бумажке или
нет зависит от демона ибо он принимаем движения нашей руки а рисует на бумажке уже сам демон.
и показать нам бумажку зависит тоже от демона. только он невидим и мы о нем незнаем. 
нам кается что от движегия нашей рукаи на бумажке реально и появлятися буквы. на самом деле
мы тлько водим рукой. демон фиксирует наши движения и уже сам начинает на бумаге рисовать
потму что у нас по факту нет доступа к бумаге. есть только у демона. а нам кажется что 
это у нас есть доступ к бумаге. ну вот демон нарисовал и даже нам в глаза показал что 
на бумажке нарисовано. далее мы думаем что раз продавец рядом то и он уже это видит. а это 
тоже нетак. увидит продавец или нет зависит  от демона. и когда мы на бумажке ставим enter
то демон тогда только показываем эту бумажку продавцу! точнее даже хуже. демон кладет эту 
бумажку продавук в пчтоый ящик. если продавец туда незаглянет то он и неузнает а если другой
процесс туда заглянет то ону эту бумажку стырит.! нихрена себе коленкор!

еще раз когда мы тыкаем кнопки в терминале то нажатие кнопок до процесса в терминале 
недоходит. потому что все данные от железа в том числе и от кнопок влетают прежде всего 
в ядро. ядро фикисрует букву которая нажата и сразу ее выводит на экран. 
и нам кажется что это процесс уже получил эту букву. ниехера. буква всего навсего нарисована 
ядром на экране и лежит вбуфере ядра. и только когда мы тыкнули enter тогда ядро
берет все буквы из буфера и посылает на /dev/tty2. буфер и tty2 это конечно разные вещи(так на 
всякий случай). от того что буквы постуаили в /dev/tty2 это незначит что буквы уже оказались в 
памяти процесса. нет! они упрощенно говоря просто скажем типа записаны на диск в файл. 
а процессу чтобы получить эти буквы еще нужно совершить акт. их нужно считать оттуда!
sleep по своей природе ничего нечитает из /dev/$$/fd/0 ему нахер ненадо поэтому если мы 
читаем из tty2 другим процессом то мы без проблем эти буквы и увидим через чтение из другого
процесса. если мы sleep автоматом читал буквы из /dev/$$/fd/0 то их бы там уже неоказалось
когда мы полезли из другого процесса читать из   /dev/$$/fd/0
этим я еще раз хочу подчеркнуть что из за того что данные упали в /dev/$$/fd/0 это еще незначит
что данные попали в память процесса. она просто лежат в почтовом ящике.
и тут возникает еще один очень очень важный вопрос! у нас tty2 прописан и для /dev/$$/fd/0 и 
для /dev/$$/fd/1 у первого процесса. но мы же знаем что если бы процесс сделал запись в /dev/$$/fd/1 то эти буквы нарисовались бы на мониторе первого процесса! а этого непроисходит. как же так.
(кстаи еще подчеркну что когда ядро из своего внутренего буера пишет в /dev/tty2 то данные в 
конечном итоге опять же попадают в ядро только в другой буфер. хаахаха.) так вот я думаю разгадка
почему когда ядро пишет в tty2 у нас непроисходит вывод на экран потому что в ядре видимо 
прописано что если в tty2 идет запись из процесса то надо выводить на экран. а если в tty2 идет 
запись из ядра то выводить на экран ненужно!!!!

я еще сделал эксеримент.
в первом терминале я запустил

$ cat

во втором

$ cat /dev/tty2 

 в первом терминале  я тыкаю кнопку 'a'
 она появляется на экране. но появляется она только один раз. потому что 
 сигнал от клавы пришел в ядро. оно приняло. и послало картинку на монитор.
 но в tty2 ядро ничего непослало. поэтому cat неотобразил "a" второй раз.
 первое 'a' отобразилось потому что ядро приняло кнопку и послало картинку на экран.

 далее я тыкаю enter. теперь ядро берет букву 'a' и шлет ее на tty2
 а далее я наблюдал два возможных варианта - с одной стороны cat из первого терминала
 читает данные из tty2 с другой стороны с того же самого tty2 читает данные cat из второго 
 терминала. так вот в зависимости кто из этих двух программ успевал первым прочитать 
 данные из tty2 на том термиале буква 'a' и появлялась. 

 из этого  я постулирую как в линуксе устроен вывод на экран в консоли:
кхм.. и тут опять я малек со стороны начну. фундаментальный вопрос - как происходит связь 
между процессом и тем что в него прилетают с кнопок буквы и что процесс выводит на экран
буквы. пока я нерасматриваю когда у нас запущен X11. пусть чисто без графического 
режима. надо начать с загрузки линукса. вот граб , потом загрузилось ядро. в код ядра встроен
програмный эмулятор терминала vt102. что за хрень терминал. раньше компы были мейнфреймами 
у которых небыло условно говоря своего монитора и своей клавы. точнее монитор и клава 
были реализованы в форме единого ящика мониторо-клава. эта штука называлась hardware терминал.
этот ящик по com порту подключался к мейнфрейму. и таких ящиков было дохера. как бы если сейчас 
клавитура это отдельное устройство а монитор отедьное устройство да еще и монитор поключается
к видеоакрте то раньше  монитор и клава это был единый ящик который полключался к компу по com порту. и вот этот ящик для мейнрейма выглядел порсто как нечто удаленное поключенное к нему 
через com порт. поэтому терминал это было сверх тупое устройство предназнчаенное только 
для того чтобы посылать кнопки в мейнфрейм  в форме символов условно гвооря и обратно принимать 
символы которые терминал показывал на экране. а до этого небыло и монитора вместо него символы
печатались на принтере! тоесть ввод шел а кнопок а вывод шел на принтер. монитора небыло вобще.
 я  это к тому что черный экран это совсем не нутро процесса скажем так. это не его черное 
 загадочное брюхо. процесс просто срет иногда буквами  . буквы это какашки процесса но никак не 
 кишки процесса и монитор в текстовом режиме просто эти буквы показывает. если сравнить процесс 
 с человек. то процесс это человек который сидит в темной комнате и и ничего об оакужабщем мире незнает и человек иногда пишет письма на stdoout в форме просто букв и вот эти буквы по почте прилетают ( по com порту) прилетали в терминал в форме просто букв и терминал эти буквы показывал
 на экране или печатал на принтере. так что черный экран монитора это характеристика самого экрана
 если у принтера лист белый то у экрана лист черный но это все неимеет никакой связи с
 самим процессом с его кишками. они невидны. бквы на экране это письмо от процесса но не лицо
 процесса. получается когда клава+монитор были единым ящиком через который связь шла 
 по единому com порту то мейнферейм знал что чтобы чтото появилось на экране терминала
 надо просто на tty файл послать буквы вот все. когда на терминал через /dev/tty приходили
 буквы то терминал их рисовал на экране. тоесть на стороне мейнфрейма был com порт и /dev/tty
 файл. темринал подключался к мейнферйму как мышка.  данные которые шли от терминала в мейнфрейм
через /dev/tty обозначали буквы от клавы. а данные которые шли обратно через /dev/tty означали
ответ от мейнфрейма который надо рисовать на экране. ровно по такой схеме работаео досих пор 
подключение к циско условно говоря через тот самый консольный порт. это com порт.
туда у нас летят буквы с клавы а обратно летят буквы нам наш монитор в текстовом режиме. 
едиснвенное что мы поключаемся к циско не через желехный терминал а через его програмный 
эмулятор. он умеет посылать буквы на com порт циски и умеет принимать от нее буквы 
чтобы их потом показать на экране.
двигаем дальше. потом нестало этих железных терминалов этих коробок нестало.
 вместо них появилось две отдельных коробки - отдельно клава и отдельно монитор. и каждый поключается по отдельному порту к системной блоку да еще и на монитор идет не текстовая информация а уже графическая хрень с видеокарты. в мейнфреймах небыло никаких видеокарт
 также как их нет в цисках сейчас. информация для монитора шла буквально в форме букв которые
 монитор и показывал. но главное еще раз что информация для монитора шла в режиме буквально буквенном текстовом.  поэтому com порт /dev/tty отлично подходил и для связи к компу ( от клавы) и для обратной связи от мейнфрейма к монитору тоже в форме букв. суть потока была одна и таже
 это буквы. итак железные терминалы сдохли  но с точки зрения линукса все осталось по прежнему. 
 только теперь терминал стал виртуальным . теперь терминал вмонтирован в код ядра. 
тоесть попрежнему ядро общается с юзером через терминал. а это терминал уже хитрым образом
преобразуется в сигнал на монитор от видеокарты.

как я это вижу:

раньше было вот так
 процесс ---буквы-->/dev/tty -->ядро -->com порт ---->провод ->com порт -->буквы ->терминал -->картинка
  терминал приняв буквы умел их преобразоавыать в физическую картнку на мониторе. 
  тоесть терминал в себя принимал галимые буквы и уже сам терминал делал магию по преобразованию
  букв в графические символы на экране. или вместо монитора мог быть принтер. принтер даже лучше
  описывает суть ситуации. получается кстати видеокарта была встроена в сам терминал а не в сист блок как это есть сейчас.

еще получается такое оступление что можно было послать на терминал опрееленные буквы и они 
могли менять на терминале детали вывода на экран. сообщить терминалу например что слово надо 
сделать цветным. и эти комбинации управляющих букв у терминалов были разные потому что было
несколько видов терминалов. тоесть в потоке шли нетолько буквы но и управляющие комбинации 
букв.

так вот когда теперь терминал стал виртуальным стал куском кода в ядре то 
картина вывод на экран стала по мне такая
  процесс ---буквы-->/dev/tty -->ядро --> кусок кода в ядре эмулирует терминал -->видеокарта -->картинка

железный терминал исчез из реальной жизни и его тогда подселили в ядро как програмную 
эмуляцию. условно говоря раньше у хлебозавода была сеть хлебных магазинов а сейчас у хлебозавода
один свой хлебный магазин на выходе из хлебозавода. может в какойто стеени это как если раньше
ходить в качалку на районе а потом купить себе тренажеры в квартиру. 

получается линукс когда грузится после граба то загружается ядро оно инициализируется и оно делает вывод данных в эмулятор терминал который тоже ялвяется куском кода ядра и этот эмулятор уже 
преобразует данные и выводит их на видеокарту и экран монитора служит как бы экраном железного
терминала а клава служит как бы клавой железного терминала. далее ядро запускает юзерский про
цесс init и этот процесс от ядра получает три открытых файла /proc/1/fd/0 /proc/1/fd/1 /proc/1/fd/2 у которых бекендом является /dev/tty1  и init начинает делать вывод в  /proc/1/fd/1 букв
тоесть идет запись в /dev/tty1 буквы поступают в ядро. в ядре они поступают в эмулятор терминала
эмулятор преобразует буквы и выводит через видеокарту их на монитор. таким образом процесс
с его точки зрения и с точки зрения ядра отправляет данные выходные на терминал по порту /dev/tty1 по протоколу терминала в буквенном виде. тоесть процесс общается с внешним миром как и встарину
как бутто через терминал через com порт. как бутто к компу через ком через /dev/tty1 подключен 
некий удаленный железный терминал с клавой+экран.

далее как я говорил интерснейший момент. получается раньше во времена железных терминалов
юзер который сидел за терминалом он мог через этот терминал иметь связь только с одним процессом
на мейнфрейме.  скажем на мейнфрейме есть com порт который в системе виден как /dev/tty56
на мейнфрейме создавался процесс скажем bash который прикреплялся к этому /dev/tty56
в том плане что процесс bash имел три файла /proc/1/fd/0 /proc/1/fd/1 /proc/1/fd/2 
бекендом которых был /dev/tty56
получетя что человек когда сидел за терминалом и нажимал кнопки и тыкал enter то то он послылал 
эти буквы этом конкретно процессу bash , ну тоесть буквы конечно же прилетали в ядро линукса 
мейнфрейма а ядро их переправляло уже в /dev/tty56   после этого опять же буквы попадали в ядро
но уже в другое его место и поскольку источником букв было тоже ядро то буквы непосылались 
обратно в com порт а оставались в буфере /dev/tty56 и если процесс баш был настроен принимать читать данные из /proc/1/fd/0 ( а баш конечно же был так настроен) то данные наконец от клавы
терминала попадали в опертивную память процесса баша и он мог начать на них реагировать.
но вот просто что здесь важно отметить это то что человек сидя за терминалом как я понимаю
работал с мейнфреймом в однозадачном режиме. мог взаимодействовать тоько с одним выделенным
процессом на мейнфрейме. вопрос а если ему хотелось сразу и в текстовом редакторе чтото набирать
и в баше чтото запускать  и скажем почту (в текстовом виде) отправлять то как это можно 
было сделать через железный терминал. ведь дело в том что сейчас когда мы линуксе я 
открываю несколько башей в разных окнах (ctrl alt f1, ctrl alt F2 итп) то каждое такое
окно привязано к отдельному эмулируемумому терминалу и отдедьному /dev/ttyX
приколно также что процесс досих пор по архитектуре может получать отправлять данные 
либо на терминал (/dev/tty /dev/pts ) либо в файл на диске  вот и все варианты связи процесса
с внешним миром. либо экран монитора либо файл на диске.... интеерсно.
процесс похож на человека в тюрьме в камере который там чтото делает и его никуда невыпускают
и он имеет связь с внешним миром только через записки. также кстати между терминалом и мейнфреймом
использовался модем. так что можно было сидеть на удаленке легко даже в те времена.
Также еще раз хочется отметить когда мы вбиваем буквы сидя в линуксе и они появляются на экране
при нажатии то эти буквы еще никоим образом недобрались до процесса к которому привязан 
на той стороне /dev/tty на мейнфрейме. если бы мы сидели на реаьном железном терминале
как я понимаю то отображение букв при нажатии это было чисто функционал самого терминала.
тоесть мы могли включить его врозетку и быть неподключенными ни к какому мейнфрейму и начать
нажимать кнопки на экране и терминал был так сделан что он нажатие кнопки на экране автоматом
выводил эту букву на экран и этот вывод никак не связан с мейнфреймом и процессом который 
на нем там сидит и следит за /dev/tty56 к которму наш терминал мог быть привязан. и только когда
мы на темринале тыкали enter только тогда терминал отпрпавлял эту строку на удаленный мейнфрейм , далее интересно когда данные прилетают в com порт то они попадают там в регистры ком порта или 
чтото типа того и на цпу происходит активация на его ножке hardware interrupt. на это дело цпу 
запускает кусок кода ядра обработчик прерывания на com порту и данные которые прилетели 
обработчик засасывает в ядро в буфер. тут заметим важную вещь что в процессе получаения ядром
данных из com порта неучаствуют никакие /dev/tty56 штуки. потому что /dev/tty56 это хрень 
выглядывающая из ядра чтобы программа процесс работающий в юзерсейсе мог забрать из ядра
данные которые в ядро поступили из com порта. таким образом в ядро данные из ком порта поступают
совершенно одним путем а в юзерский процесс они попадают совершенно другим путем.
условно говоря товары в магазин поступают бесплатно из черного хода их разгружаютс  грузовика
а к пользователю они попадают через парадный вход с полки да еще за деньги. совершенно разные 
пути и механизмы. 
  По поводу того как ядро общается с железками\устройствами. всякие там /dev/... это все 
для програм которые работают в юзерспейсе. /dev/... это хреновины через которые программа
из юзерспейса может обратиться к ядру чтобы прочитать\записать данные в железку. самое же
ядро общается с железками соверешенно внезависимости от /dev/... файлов. как оно это делает.
из беглого чтения складывается такая картина. в режиме ядра доступ к устройствам 
выглядит так : прежде всего доступ цпу к периферийным устройствам : есть два метода доступа цпу к устройствам memory mapped io и port mapped io. насколько я понял это зависит от модели цпу. 
метод зашит в железе. в конкретном цпу зашит либо один метод либо другой. прежде чем расказать как работают эти два метода надо сказать про системную шину цпу. она состоит из трех подшин - шина адреса, шина данных и шина контроля (data bus, address bus, contro bus). на адресной шине
цпу выставляет адрес по которому он хочет обратится к RAM, по дата шине идут уже байты информации
либо из цпу  в память либо из памяти в цпу. на шине данных цпу выставляет операцию которую 
он хочет сделать с памятью - считать с памяти либо записать в память. 

  так вот при memory map методе (вот описание охиренное https://www.youtube.com/watch?v=2Cbcb2yGjiM)во первых control шина это условно говоря 1 провод который идет от цпу к модулям памяти 
  по этому проводу от цпу к памяти идет сигнал о том что мы хотим считать из памяти или записать
  в память. таким макаром происходит управление памятью. шина данных согласно видео это 8 бит.
  шина адреса это 32 бита (32 провода) из них от цпу к модулям памяти идет только 28 проводов.
  а 4 провода шины адреса идут в спец микросхему memory decoder. также из мемори декодера
  идет по одному проводу в каджый модуль памяти и также идут провода в периферийные устройства.
  что это дает. согласно видео
  если у нас установлен первый бит на шине адреса (тоесть первый из четырех проводов который идет из цпу к мемеори декодеру) то тогда  мемори декодер посылает в первую микросхему сигнал 
  о том что эта микросхема должна стать активна а другим модулям памяти и перифериным устройствам
  мемеори докодер шлет сигнал чтоб они должны отключить от шины данных. дело в том что все модули 
  памяти и все перифериные устройства параельельно сидят на шине данных поэтому если мы хотим считать через шину данных данные из первой микросхемы памяти то она должна быть подключена 
  к шине данных а остальные устройства должны заткнуться и отключить от шины данных.
  и вот этим процессом занимается мемеори декодер. если первый бит шины адреса установлен
  то меморидекодер шлет сигнал первой микросхеме памяти что ей можно поключаиться к шине
  данных а остальных устройствам нужно отключить от шины данных. если от цпу на мемори декедоер 
  придет сигнал что активирован второй бит шины данных то тогда мемори докодер актиивирует второй
  модуль памяти а всем остальным устройствам скажет заткнуться.  если третий бит шины данных
  будет установлен то мемори декодер активириует первое периферийное устройства на шине
  данных и по шине данных из периферинго устройства будет считать или записан байт данных.
  при этом с точки зрения процессора все будет выглядеть как бутто процессор считал\записал
  байт данных из памяти  потому что для этого процесса будет заюзана стандартная команду 
  цпу на чтение\запись из RAM. поэтому это называется memory mapping io. когда цпу использует
  с его точки зрения механизм для обращение по некоторому адерсу к RAM ячейке но данные 
  идут из ячейки на периферийном устройстве. при этом надо почеркнуть что вот такого непроисходит что данных из перифериного устройства пеерекачиваются в  ячейку RAM а из нее в цпу . нет
  такого не происходит.  


  ncurses 













 









in-kernel VT102 emulator which is also known as “linux kernel console” 
init стартует getty он стартует login а он стартует bash


dev/vcsN is a character device i.e. it refers to the memory of the currently displayed virtual console terminal.
# cat /dev/vcs

программа evetest нетребует графическоого режима и позволяет считвать коды кнопок
клавы

есть утилита grc (apt get install grc)  с помощью ее можно получиить на экране 
раскрашенный вывод любой стандартной команды. например
         $ grc cat /var/log/syslog
         $ grc ps au
ну это уддобно если в логах чтото глазами искать.











====
далее в проект kubespray входит свой Vagrantfile. надо разобраться  с ним.
но он очень сложный я мало что понял оттуда. 
и если его запустить то весь разоворот в конце концов закан(1..NUM_INSTANCES).each do |i|
  вот здесь кусок который описывает виртуалки


    # Only execute the Ansible provisioner once, when all the machines are up and ready.
    if i == NUM_INSTANCES
       node.vm.provision "cluster", type: "ansible", run: "once" do |ansible|
        end
     end
чendивается ошибками
так что их ний вагрант файл неработает из коробки.

Vagrantfile который идет с коробки с кубспрей там для убунту18 указан имадже вида
generic/ubuntu18. это не имадж а гавно потому что у него есть dnsmask кэширующий dns
сервер и он с коробки неработает поэтому из за него резолвинг dns в ip неработате
поэтому надо в Vagrantfile заменить имя имаджа на "hashicorp/bionic64
c этим имаджем нет никаких проблем

какой прикол я обнаружил. если мы в vagramtfile пропишем запуск ансибля
то вагрант сам создаст инвентори файл для ансибля что очень круто.
создаст он его в папке     папка_где_лежит_Vagrantfile/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory    и при этом ансибль провижинеру в Vagrantfile ненужно руками 
указывать на этот инвентори файл вагрант сам его по дефолту и начнет юзать.
и тут есть одна огромная проблема - в ансибле нам нужен нетолько файл с инвентори но и 
group_vars папка. как ее подсунуть ансибль провижинеру в vagrantfile? говоря в целом
на первый взгляд никак. 

absible_ssh_host deprecreate вместо него ansible_host и он ознаает ip по которому ансибль
дож=лжен стучаться на виртуалку

переменная acess_ip это несистемная перпеменная аснибля а кастомная переменная плейбука.
ообонает ip по которому другие ноы дожжны стучаться на эту ноду.

ip: в invemtory ---> etcd_metrics_address в плейбуках

vagrantfilr сильно осрован на ruby.
дикшонари или как его зовут еще hash обозначаетя в ruby как

 vasya = { "one" => "eins", "two" => "zwei", "three" => "drei" }

ещенадо добаить ip, access_ip в конфиг вагранта в hosts_vars
если сравнить инвентарь гененируемый питоном и инвентарь генеируремыый вагрантом
то станет видно что в вагрнет инвертатрере нехаватвтает acccess_ip и ip (который ничто иное как
etcd_metrics_address)

значит после скачки кубсперя нужна символическя ссылка на инветнарь. и поправить 
вагрантафайл с ip, access_ip

допилить чтобы ненужно было руками скачивать сертификат от к8 на ноутбук

\\ вспомогательный скрипт
emp$ cat 3.rb 
#!/usr/bin/ruby

require 'fileutils'

# Variables
INSTANCE_NAME_PREFIX = "k8s"
NUM_INSTANCES = 5
KUBE_MASTER_INSTANCES = 2
INVENTORY_FOLDER = "inventory/sample"
ETCD_INSTANCES ||= NUM_INSTANCES
host_vars = {}


# делаем симлинк с group_vars ансибля в папку вагранта куда он по умолчанию создает файл инвентаря
  VAGRANT_ANSIBLE   = File.join(File.dirname(__FILE__), ".vagrant", "provisioners", "ansible")
  VAGRANT_INVENTORY = File.join(VAGRANT_ANSIBLE,"inventory")
if ! File.directory?(VAGRANT_INVENTORY)
  FileUtils.mkdir_p(VAGRANT_ANSIBLE)
  FileUtils.ln_s(File.absolute_path(INVENTORY_FOLDER), VAGRANT_INVENTORY)
end
\\ конец = вспомогательный скрипт 


вагрант конфиг написан на ruby.
тоесть поддерживает его синтакс


===

