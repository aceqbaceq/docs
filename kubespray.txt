kubespray

kubespray

https://github.com/kubernetes-sigs/kubespray.git

самый толковый гид это = https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws

скачиваем с гита
вообще это папка с ансибль плейбуками и раскаткой виртуалок через тераформ.


перед тем как накатывать тераформ надо установить ансибль. потому что 
при накаткет тераформа потом автомтом стартует ансибль. так что надо прежде всего
установит ансибль. в readme.md написано что для этого надо выполнить:
      $ sudo pip3 install -r requirements.txt
я же вместо этого предлагаю ставить ансибль по другому смотри kubespray-ansible.txt

теперь когда ансибль поставили можно ставить виртуалки 
через тераформ.

подпапка в которой лежат тераформ плейбуки
/kubernetes-kubesparay/kubespray-master/contrib/terraform/aws

идем внее.

экспортируем переменные.

export TF_VAR_AWS_ACCESS_KEY_ID="публичный ключ aws token"
export TF_VAR_AWS_SECRET_ACCESS_KEY ="приватный ключ aws tokeen"
export TF_VAR_AWS_SSH_KEY_NAME="имя ssh ключа который будет импортрован внуьрь инстансов"
export TF_VAR_AWS_DEFAULT_REGION="имя региона"

пример

export TF_VAR_AWS_ACCESS_KEY_ID="AKIAXKD3DX4K234234234234"
export TF_VAR_AWS_SECRET_ACCESS_KEY="az3yBNEjWIN4RVrGP1L523423423424323/"
export TF_VAR_AWS_SSH_KEY_NAME="a.krivosheev"
export TF_VAR_AWS_DEFAULT_REGION="eu-west-1"



запускаем тераформ
  $  terraform fmt
  $  terraform validate
  $  terraform init
  $  terraform fmt
  $  terraform validate
  $  terraform plan -out plan.txt
  $  terraform apply  plan.txt
  

вывод на экране после установки


Outputs:

aws_elb_api_fqdn = "kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com:6443"
bastion_ip = "18.202.77.68"
default_tags = tomap({})
etcd = <<EOT
10.250.202.87
10.250.214.169
10.250.194.80
EOT
inventory = <<EOT
[all]
ip-10-250-202-87.eu-west-1.compute.internal ansible_host=10.250.202.87
ip-10-250-214-169.eu-west-1.compute.internal ansible_host=10.250.214.169
ip-10-250-194-80.eu-west-1.compute.internal ansible_host=10.250.194.80
ip-10-250-196-151.eu-west-1.compute.internal ansible_host=10.250.196.151
ip-10-250-222-49.eu-west-1.compute.internal ansible_host=10.250.222.49
ip-10-250-199-7.eu-west-1.compute.internal ansible_host=10.250.199.7
ip-10-250-213-52.eu-west-1.compute.internal ansible_host=10.250.213.52

bastion ansible_host=18.202.77.68

[bastion]
bastion ansible_host=18.202.77.68

[kube_control_plane]
ip-10-250-202-87.eu-west-1.compute.internal
ip-10-250-214-169.eu-west-1.compute.internal
ip-10-250-194-80.eu-west-1.compute.internal

[kube_node]
ip-10-250-196-151.eu-west-1.compute.internal
ip-10-250-222-49.eu-west-1.compute.internal
ip-10-250-199-7.eu-west-1.compute.internal
ip-10-250-213-52.eu-west-1.compute.internal

[etcd]
ip-10-250-202-87.eu-west-1.compute.internal
ip-10-250-214-169.eu-west-1.compute.internal
ip-10-250-194-80.eu-west-1.compute.internal

[calico_rr]

[k8s_cluster:children]
kube_node
kube_control_plane
calico_rr

[k8s_cluster:vars]
apiserver_loadbalancer_domain_name="kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com"

EOT
masters = <<EOT
10.250.202.87
10.250.214.169
10.250.194.80
EOT
workers = <<EOT
10.250.196.151
10.250.222.49
10.250.199.7
10.250.213.52
EOT


потом автоматом запускатеся ансибль то куб неможет поставиться из за того что кэширующий 
dns сервис systemd-resolved имеет неверный dns указанный и надо пойти  на хосты куба  в папку

/etc/systemd/resolved.conf

и там прописать 8.8.8.8

[Resolve]
DNS=8.8.8.8


после того как тераформ прогонится то в папке будет создан иневентори файл hosts для ансибля
/kubernetes-kubesparay/kubespray-master/inventory/

далее надо запустить ансибль потому что мы всего навсего создали только виртуалки.
а куба наних еще нет.

делать это надо только вот так

$ ansible-playbook -i ./inventory/hosts ./cluster.yml -e ansible_user=admin -b --become-user=root --flush-cache

без указания -e ansible_user=admin вобще ничо несработает. будет писать что юзео ansble_user
незадан.


проблемы:
- немогу войти на бастион созданный. = оказалось что ансибль кубспрея после тераформа 
   на иснстанах ключи ssh прописывет не для юзера ubuntu а для юезра admin!!!
   так что на инстансы надо входить как admin


- таже вылезло что непонятно как создавать инстансы сразу в несольких регионах. это уже 
   чисто проблема самого тераформа


далее.
если ансибль у нас все сделал до конца
то в папке кубспрея в корне появится файл

ssh-bastion.conf

это ssh config 
он дает то что теперь мы можем по ssh подклчитья к любому инстансу который мы поставили по ssh
через bastion инстанс как ssh proxy.


ssh -F ./ssh-bastion.conf user@$ip

например 
$ cat ssh-bastion.conf

Host 3.250.187.184
  Hostname 3.250.187.184
  StrictHostKeyChecking no
  ControlMaster auto
  ControlPath ~/.ssh/ansible-%r@%h:%p
  ControlPersist 5m

Host  10.250.199.144 10.250.222.252 10.250.198.134 10.250.201.239 10.250.220.38 10.250.192.227 10.250.214.46
user admin
  ProxyCommand ssh -F /dev/null -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -p 22 admin@3.250.187.184 
krivosheeva@jurvis:~/Terraform/kubernetes-kubesparay/kubespray-master$ 



тогда подключаеиися к одному из инстансов

$ ssh -F ssh-bastion.conf 10.250.198.134


далее. давайте уже наконец покдлючимся к кубу.

идем по ssh на любой мастер



$ ssh -F ssh-bastion.conf 10.250.198.134

там становимся рутом

$ sudo bash

инаконец мы можем проверить состяние куба

# kubectl get nodes

напомню что сертификаты доступа к кубу лежат на мастерах в папке /etc/kubernetes/admin.conf"
и также этот же файл лежит на мастерах в папке 
/root/.kube/config


далее.
хотим чтобы мы могли подлкючаться к кубе прям с нашего ноутбука

$ mkdir -p ~/.kube

копмруем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -F ssh-bastion.conf 10.250.199.144 'sudo chmod 644 /etc/kubernetes/admin.conf'


10.250.199.144 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -F ssh-bastion.conf admin@10.250.199.144:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -F ssh-bastion.conf 10.250.199.144 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com:6443


дело в том что когда мы вывзаем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого

$ LB_HOST=$(cat inventory/hosts | grep apiserver_loadbalancer_domain_name | cut -d'"' -f2)
$ sed -i "s^server:.*^server: https://$LB_HOST:6443^" ~/.kube/config


тут надо сказать что это за хрень kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
откуда она взядась что это такое.
а это тераформ на амазоне создает амазоновский лоад балансер. он создает причем  Classic Load Balancer.
что это за хрень. это доменное имя некоторое.  котооое имеет несколько A IP адресов например

$ nslookup kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Name: kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Address: 52.211.32.9
Name: kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Address: 3.248.96.93

каждый адрес видимо в своей авайлабилити зоне.
и далее также пробрасывается с этих внешних IP внутрь амазона порт уже на кокнетнй инстансы.
тоесть лоадбаласер это как хапрокси который имеет внешний IP и прбосбываем порт с внешнено ip
на бекенд сервера. AWS Classic Load Balancer (CLB) operates at Layer 4. тоесть udp\tcp . means is that the load balancer routes traffic between clients and backend servers based on IP address and TCP port.
для сравнния у амазоне есть еще L7 балансер они его зовут как  Application Load Balancer. 
он подходит для балансировки HTTP трафика. такая же аналогия как haproxy может работать на l4 режиме
а может на L7 режиме.

так вот тераформ куб спрея создает L4 amazon Classic Load Balancer = aws CLB и  с внешних IP пробрасывает 
порт 6443 на мастера куба.  ( про  L4 amazon Classic Load Balancer надо еще дальше поразбираться). 
главная суть что контролплейт куба реально прям выставляется наружу кубспреем во внешний мир.
уж незнаю наколько это безопасно.


что еще интересно. то что хосты кроме бастиона они создаются без внешнего IP. вопрос как тогда 
они в инте выходят например для установки пакетов. ответ для subnet в котоой сидят инстансы  в таблице
маршрутизации для этого subnet  добавлен марщруты вида

10.250.192.0/18 local  
0.0.0.0/0 nat-0110f39cec15f906f  

первый маршрут это чисто внутри площадочный локальный. 
а ip интернетовские - выход на них идет через nat гейтвей. их можно создать в virtual private clooud  - nat gateways. опять же тут неочень пгимаю разницу между virtual private clooud  - internet gateways и 
virtual private clooud  - nat gateways. это тоже надо прояснять.


но зато тпеперь понятно как иснатсны без внешнего ip умудряются выходит в интернет









 это AWS Classic Load Balancer (CLB) operates at Layer 4.





ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes



-----
попробовал увязать тераформ и виртуабокс но к сожалению пока что 
на данный момент провайдер для вирутаобокса гавно.

а так вот огн конфиг для виртуалбокса


r$ cat main.tf 

terraform {
  required_providers {
    virtualbox = {
      source  = "terra-farm/virtualbox"
      version = "0.2.2-alpha.1"
    }
  }
}


resource "virtualbox_vm" "node" {
  count  = 2
  name   = format("node-%02d", count.index + 1)
  image  = "https://app.vagrantup.com/ubuntu/boxes/bionic64/versions/20180903.0.0/providers/virtualbox.box"
  cpus   = 2
  memory = "512 mib"
  // user_data = "${file("user_data")}"


  network_adapter {
    type           = "bridged"
    host_interface = "en0"
  }


}



====
далее. 
рассмотрим установку k8 через kubespray когда виртуалки 
руками раскатаны через vagrant.



скачиваем с гита кубспрей.
$ git clone https://github.com/kubernetes-sigs/kubespray.git

ставим ансибль как прописано в kubespray-ansible.txt

ставим вагрант. конфиг для виртуалок берем из файла kubespray-Vagrantfile.txt
в нем прописано четыре виртуалки
kmaster2	| 100.0.0.3
kmaster2	| 100.0.0.4	
kworker2	| 100.0.0.9
kworker3	| 100.0.0.10

тут важно отметить то что согласно конфигу во все вм будет интегрирован доступ по 
единому ssh ключу из папки ~/.vagrant.d/insecure_private_key.
это важный момент потому что именно этот ключ потом мы будем указывать ансиблю для доступа на виртуалки.


еще важный момент такой что практика показала что первые две виртуалки кубспрей будет 
юзать для мастеров контрол плейна куба. а вот все уже остальные виртуалки кубспрей 
будет использовать уже как дата ноды куба. как это поменять как задать кубспрею сколько 
нод использовать под контрол плейн а сколько под дата ноды пока непонятно. так что 
если хотим иметь хотя бы одну дата ноду то надо создавать минимум три виртуалки.


запускаем виртуалки
$ vagrant up

убеждаемся что мы можем зайти на все эти виртуалки по ключу ~/.vagrant.d/insecure_private_key
чтобы понимать что ансибль тоже туда сможет зайти
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.4
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.9
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.10


идем в папку с кубспреем
и копируем вспомогательные файлы инвентаря из сэмпла

$ cp -rfp inventory/sample inventory/mycluster

но самого файла инвентаря hosts.yml там еще нет.
нам надо его создать 

$ declare -a IPS=(100.0.0.3 100.0.0.4 100.0.0.9 100.0.0.10)
в этой строке мы указываем все ip адреса всех виртуалок которые мы хотим запихнуть в куб

запускаем генерацию файла инвентаря 
$ CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py ${IPS[@]}

если все окей то мы должны получить вот такой файл
$ cat ./inventory/mycluster/hosts.yml 
all:
  hosts:
    node1:
      ansible_host: 100.0.0.3
      ip: 100.0.0.3
      access_ip: 100.0.0.3
    node2:
      ansible_host: 100.0.0.4
      ip: 100.0.0.4
      access_ip: 100.0.0.4
    node3:
      ansible_host: 100.0.0.9
      ip: 100.0.0.9
      access_ip: 100.0.0.9
    node4:
      ansible_host: 100.0.0.10
      ip: 100.0.0.10
      access_ip: 100.0.0.10
  children:
    kube_control_plane:
      hosts:
        node1:
        node2:
    kube_node:
      hosts:
        node1:
        node2:
        node3:
        node4:
    etcd:
      hosts:
        node1:
        node2:
        node3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}


как видно первые две ноды входят в контрол плейн.
насколько я понимаю руками можно скажем добавить еще ноды в группу kube_control_plane
при желании.

этот файл с инвентарем нам нужно немножко подкореектировать. нам нужно указать 
ansible_ssh_user под которым ансибль будет ломиться по ssh на виртуалки и нужно 
указать ансиблю какой приватный ключ.

эти две строки выглядят так
ansible_ssh_user: vagrant
ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key

а откоркетированный файл выглядит так
all:
  vars:
     ansible_ssh_user: vagrant
     ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key
  hosts:
    node1:
      ansible_host: 100.0.0.3
      ip: 100.0.0.3
      access_ip: 100.0.0.3
    node2:
      ansible_host: 100.0.0.4
      ip: 100.0.0.4
      access_ip: 100.0.0.4
    node3:
      ansible_host: 100.0.0.9
      ip: 100.0.0.9
      access_ip: 100.0.0.9
    node4:
      ansible_host: 100.0.0.10
      ip: 100.0.0.10
      access_ip: 100.0.0.10
  children:
    kube_control_plane:
      hosts:
        node1:
        node2:
    kube_node:
      hosts:
        node1:
        node2:
        node3:
        node4:
    etcd:
      hosts:
        node1:
        node2:
        node3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}


далее заходим в наш установленный ансибль через venv
$ source ~/python3/ansible/env/bin/activate

запускаем ансибль плейбук cluster.yml на установку куба на виртуалках

(env)$ ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml

все процесс пошел.
примерно через 20 минут мы получим установленный куб


далее.
хотим чтобы мы могли подлкючаться к кубу с нашего ноутбука
создаем папку для ключа от куба.

$ mkdir -p ~/.kube

копируем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 644 /etc/kubernetes/admin.conf'


100.0.0.3 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -i ~/.vagrant.d/insecure_private_key    vagrant@100.0.0.3:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://100.0.0.3:6443


дело в том что когда мы вызываем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого.

ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes
NAME    STATUS   ROLES                  AGE   VERSION
node1   Ready    control-plane,master   18h   v1.22.5
node2   Ready    control-plane,master   18h   v1.22.5
node3   Ready    <none>                 18h   v1.22.5
node4   Ready    <none>                 18h   v1.22.5

====
следущий этап. апгрейдим предыдущий этап.
делаем тоже самое что делали на предыдущем этапе - тоесть 
разворот куба через кубспрей на вагранте но апгреженный вариант тоесть 
делаем тоже самое но с еще более минимальным ручным участием. чтобы 
это было по максимуму автоматизированный процесс чтобы руками прилось
делать меньше чем в предыдущем варианте.

качаем кубспрей.

$ git clone https://github.com/kubernetes-sigs/kubespray.git

заменяем файл Vagrantfile в папке на kubespray-Vagrantfile2.txt (более продвинутый вагрантфайл
чем в предыдущем случае)

устанавливаем ansible как описано в kubespray-ansible.txt

заходим из под виртуального питона в котором установлен ансибль в папку с кубспреем
и запускаем разворот виртуалок на вагранте. вагрант нетолько  создаст
виртуалки но и сам запустит разворот куба на вирталках через ансибль

(env) vagrant up

важно запускать разворот вагранта из под виртуального питона на котором установлена
нужная версия ансибля для кубспрея потому что вагрант будет запускать ансибль который
именно на хосте и если версия ансибля нета что надо то вагрант напишет про проблемы
с ансиблем и разворота плейбука непроизойдет. я пробовал запускать разворот 
вагранта без нужной версии ансибля на хосте и вагрант вылетал с ошибкой о несовместимости
с версией ансибля. поэтому еще раз подчеркну что чтобы вагрант мог успешно сам запустить ансибль
 и начать разворачивать куб на виртуалках важно чтобы на хосте стоял именно тот ансибль который 
 прописан в requirements.txt, эту версию ансибля можно поставит как общесистемно(чтобы он был доступен всем юзерам хоста) через sudo pip3 ... , а можно локально через виртуальную среду питона(чтобы этот ансибль был доступен только текущему юзеру).


когда все отработает мы получим созданные виртуалки и развернутый на них куб.

остается только руками скопировать ключ с мастер ноды  и поставить kubectl на ноутбуке.

далее.
хотим чтобы мы могли подлкючаться к кубу с нашего ноутбука
создаем папку для ключа от куба.

$ mkdir -p ~/.kube

копируем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 644 /etc/kubernetes/admin.conf'


100.0.0.3 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -i ~/.vagrant.d/insecure_private_key    vagrant@100.0.0.3:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://100.0.0.3:6443


дело в том что когда мы вызываем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого.

ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes
NAME    STATUS   ROLES                  AGE   VERSION
node1   Ready    control-plane,master   18h   v1.22.5
node2   Ready    control-plane,master   18h   v1.22.5
node3   Ready    <none>                 18h   v1.22.5
node4   Ready    <none> 

================================================
следущий этап - попробоват както автоматизировать перетаскивание ключа от куба
с мастер ноды на ноутбук и установку kubectl

делаю этот этап.

напоролся на то что так как я юзаю ансиль из venv питона то  у меня 
когда я пытался использловать модуль apt у ансибля вылезала ошибка:

fatal: [localhost]: FAILED! => {"changed": false, "msg": "Could not import python modules: apt, apt_pkg. Please install python3-apt package."}

это ошибка имено из за того что юзается питон в venv.
решение:
Создайте виртуальную среду без переключателя --system-site-packages . После создания среды перейдите в папку, в которой она была создана. В нем должен быть файл pyvenv.cfg . Отредактируйте этот файл. В нем есть (среди прочего текста) строка
include-system-site-packages = false
Измените эту строку на:
include-system-site-packages = true
после этого ошибка исчезла.

сделал.
в папку с кубспреем нужно положить плейбук phase-II.yml

запускаем его командой

$ ansible-playbook -i ./.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory phase-II.yml -e ansible_ssh_user=vagrant -e notebook_user=vasya

notebook_user=vasya   это имя линукс юзера под которым мы сидим на ноутбуке. оно нужно для 
того чтобы плейбук знал в какую папку копировать с контрол плейна сертификат от куба.
если мы задали что юзера вася то сертификат будет скопироан в папку /home/vasya/.kube/config

таким образом когда мы разверули куб на вагранте нам ненужно руками перетаскивать на 
ноутбук сертифкат от куба и ставить руками kubectl это все за нас сделает плейбук.


===================================================
следущий этап - попровать phase-II.yml интегрировать в Vagrantfile
заапгреженный разворот к8 из вагранта.
заапгржеенный в том сммысле что нужно еще меньше делать все руками. 
и в вагранте число нод леко можно увеличить.


столункулся с приколом вагранта что он делает провижн сразу после деплоя вм 
даже если провижн указан уже после все деплоев так что нужно юзать трюк.



====
далее в проект kubespray входит свой Vagrantfile
но он очень сложный я мало что понял оттуда. 
и если его запустить то весь разоворот в конце концов заканчивается ошибками
так что их ний вагрант файл неработает из коробки.

Vagrantfile который идет с коробки с кубспрей там для убунту18 указан имадже вида
generic/ubuntu18. это не имадж а гавно потому что у него есть dnsmask кэширующий dns
сервер и он с коробки неработает поэтому из за него резолвинг dns в ip неработате
поэтому надо в Vagrantfile заменить имя имаджа на "hashicorp/bionic64
c этим имаджем нет никаких проблем

какой прикол я обнаружил. если мы в vagramtfile пропишем запуск ансибля
то вагрант сам создаст инвентори файл для ансибля что очень круто.
создаст он его в папке     папка_где_лежит_Vagrantfile/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory    и при этом ансибль провижинеру в Vagrantfile ненужно руками 
указывать на этот инвентори файл вагрант сам его по дефолту и начнет юзать.
и тут есть одна огромная проблема - в ансибле нам нужен нетолько файл с инвентори но и 
group_vars папка. как ее подсунуть ансибль провижинеру в vagrantfile? говоря в целом
на первый взгляд никак. 

absible_ssh_host deprecreate вместо него ansible_host и он ознаает ip по которому ансибль
дож=лжен стучаться на виртуалку

переменная acess_ip это несистемная перпеменная аснибля а кастомная переменная плейбука.
ообонает ip по которому другие ноы дожжны стучаться на эту ноду.

ip: в invemtory ---> etcd_metrics_address в плейбуках

vagrantfilr сильно осрован на ruby.
дикшонари или как его зовут еще hash обозначаетя в ruby как

 vasya = { "one" => "eins", "two" => "zwei", "three" => "drei" }

ещенадо добаить ip, access_ip в конфиг вагранта в hosts_vars
если сравнить инвентарь гененируемый питоном и инвентарь генеируремыый вагрантом
то станет видно что в вагрнет инвертатрере нехаватвтает acccess_ip и ip (который ничто иное как
etcd_metrics_address)

значит после скачки кубсперя нужна символическя ссылка на инветнарь. и поправить 
вагрантафайл с ip, access_ip

допилить чтобы ненужно было руками скачивать сертификат от к8 на ноутбук

\\ вспомогательный скрипт
emp$ cat 3.rb 
#!/usr/bin/ruby

require 'fileutils'

# Variables
INSTANCE_NAME_PREFIX = "k8s"
NUM_INSTANCES = 5
KUBE_MASTER_INSTANCES = 2
INVENTORY_FOLDER = "inventory/sample"
ETCD_INSTANCES ||= NUM_INSTANCES
host_vars = {}


# делаем симлинк с group_vars ансибля в папку вагранта куда он по умолчанию создает файл инвентаря
  VAGRANT_ANSIBLE   = File.join(File.dirname(__FILE__), ".vagrant", "provisioners", "ansible")
  VAGRANT_INVENTORY = File.join(VAGRANT_ANSIBLE,"inventory")
if ! File.directory?(VAGRANT_INVENTORY)
  FileUtils.mkdir_p(VAGRANT_ANSIBLE)
  FileUtils.ln_s(File.absolute_path(INVENTORY_FOLDER), VAGRANT_INVENTORY)
end
\\ конец = вспомогательный скрипт 


вагрант конфиг написан на ruby.
тоесть поддерживает его синтакс


===

следущий этап - понять как на вагранте развернуть бастион






