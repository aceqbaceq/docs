kubespray

kubespray

https://github.com/kubernetes-sigs/kubespray.git

самый толковый гид это = https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws

скачиваем с гита
вообще это папка с ансибль плейбуками и раскаткой виртуалок через тераформ.


перед тем как накатывать тераформ надо установить ансибль. потому что 
при накаткет тераформа потом автомтом стартует ансибль. так что надо прежде всего
установит ансибль. в readme.md написано что для этого надо выполнить:
      $ sudo pip3 install -r requirements.txt
я же вместо этого предлагаю ставить ансибль по другому смотри kubespray-ansible.txt

теперь когда ансибль поставили можно ставить виртуалки 
через тераформ.

подпапка в которой лежат тераформ плейбуки
/kubernetes-kubesparay/kubespray-master/contrib/terraform/aws

идем внее.

экспортируем переменные.

export TF_VAR_AWS_ACCESS_KEY_ID="публичный ключ aws token"
export TF_VAR_AWS_SECRET_ACCESS_KEY ="приватный ключ aws tokeen"
export TF_VAR_AWS_SSH_KEY_NAME="имя ssh ключа который будет импортрован внуьрь инстансов"
export TF_VAR_AWS_DEFAULT_REGION="имя региона"

пример

export TF_VAR_AWS_ACCESS_KEY_ID="AKIAXKD3DX4K234234234234"
export TF_VAR_AWS_SECRET_ACCESS_KEY="az3yBNEjWIN4RVrGP1L523423423424323/"
export TF_VAR_AWS_SSH_KEY_NAME="a.krivosheev"
export TF_VAR_AWS_DEFAULT_REGION="eu-west-1"



запускаем тераформ
  $  terraform fmt
  $  terraform validate
  $  terraform init
  $  terraform fmt
  $  terraform validate
  $  terraform plan -out plan.txt
  $  terraform apply  plan.txt
  

вывод на экране после установки


Outputs:

aws_elb_api_fqdn = "kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com:6443"
bastion_ip = "18.202.77.68"
default_tags = tomap({})
etcd = <<EOT
10.250.202.87
10.250.214.169
10.250.194.80
EOT
inventory = <<EOT
[all]
ip-10-250-202-87.eu-west-1.compute.internal ansible_host=10.250.202.87
ip-10-250-214-169.eu-west-1.compute.internal ansible_host=10.250.214.169
ip-10-250-194-80.eu-west-1.compute.internal ansible_host=10.250.194.80
ip-10-250-196-151.eu-west-1.compute.internal ansible_host=10.250.196.151
ip-10-250-222-49.eu-west-1.compute.internal ansible_host=10.250.222.49
ip-10-250-199-7.eu-west-1.compute.internal ansible_host=10.250.199.7
ip-10-250-213-52.eu-west-1.compute.internal ansible_host=10.250.213.52

bastion ansible_host=18.202.77.68

[bastion]
bastion ansible_host=18.202.77.68

[kube_control_plane]
ip-10-250-202-87.eu-west-1.compute.internal
ip-10-250-214-169.eu-west-1.compute.internal
ip-10-250-194-80.eu-west-1.compute.internal

[kube_node]
ip-10-250-196-151.eu-west-1.compute.internal
ip-10-250-222-49.eu-west-1.compute.internal
ip-10-250-199-7.eu-west-1.compute.internal
ip-10-250-213-52.eu-west-1.compute.internal

[etcd]
ip-10-250-202-87.eu-west-1.compute.internal
ip-10-250-214-169.eu-west-1.compute.internal
ip-10-250-194-80.eu-west-1.compute.internal

[calico_rr]

[k8s_cluster:children]
kube_node
kube_control_plane
calico_rr

[k8s_cluster:vars]
apiserver_loadbalancer_domain_name="kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com"

EOT
masters = <<EOT
10.250.202.87
10.250.214.169
10.250.194.80
EOT
workers = <<EOT
10.250.196.151
10.250.222.49
10.250.199.7
10.250.213.52
EOT


потом автоматом запускатеся ансибль то куб неможет поставиться из за того что кэширующий 
dns сервис systemd-resolved имеет неверный dns указанный и надо пойти  на хосты куба  в папку

/etc/systemd/resolved.conf

и там прописать 8.8.8.8

[Resolve]
DNS=8.8.8.8


после того как тераформ прогонится то в папке будет создан иневентори файл hosts для ансибля
/kubernetes-kubesparay/kubespray-master/inventory/

далее надо запустить ансибль потому что мы всего навсего создали только виртуалки.
а куба наних еще нет.

делать это надо только вот так

$ ansible-playbook -i ./inventory/hosts ./cluster.yml -e ansible_user=admin -b --become-user=root --flush-cache

без указания -e ansible_user=admin вобще ничо несработает. будет писать что юзео ansble_user
незадан.


проблемы:
- немогу войти на бастион созданный. = оказалось что ансибль кубспрея после тераформа 
   на иснстанах ключи ssh прописывет не для юзера ubuntu а для юезра admin!!!
   так что на инстансы надо входить как admin


- таже вылезло что непонятно как создавать инстансы сразу в несольких регионах. это уже 
   чисто проблема самого тераформа


далее.
если ансибль у нас все сделал до конца
то в папке кубспрея в корне появится файл

ssh-bastion.conf

это ssh config 
он дает то что теперь мы можем по ssh подклчитья к любому инстансу который мы поставили по ssh
через bastion инстанс как ssh proxy.


ssh -F ./ssh-bastion.conf user@$ip

например 
$ cat ssh-bastion.conf

Host 3.250.187.184
  Hostname 3.250.187.184
  StrictHostKeyChecking no
  ControlMaster auto
  ControlPath ~/.ssh/ansible-%r@%h:%p
  ControlPersist 5m

Host  10.250.199.144 10.250.222.252 10.250.198.134 10.250.201.239 10.250.220.38 10.250.192.227 10.250.214.46
user admin
  ProxyCommand ssh -F /dev/null -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -W %h:%p -p 22 admin@3.250.187.184 
krivosheeva@jurvis:~/Terraform/kubernetes-kubesparay/kubespray-master$ 



тогда подключаеиися к одному из инстансов

$ ssh -F ssh-bastion.conf 10.250.198.134


далее. давайте уже наконец покдлючимся к кубу.

идем по ssh на любой мастер



$ ssh -F ssh-bastion.conf 10.250.198.134

там становимся рутом

$ sudo bash

инаконец мы можем проверить состяние куба

# kubectl get nodes

напомню что сертификаты доступа к кубу лежат на мастерах в папке /etc/kubernetes/admin.conf"
и также этот же файл лежит на мастерах в папке 
/root/.kube/config


далее.
хотим чтобы мы могли подлкючаться к кубе прям с нашего ноутбука

$ mkdir -p ~/.kube

копмруем на наш комп ключ доступа от куба с мастера куба
для этого вначале на мастере надо расширить права на чтение на файл с ключом 
поомуто что по деолфту оно есть только у root

$ ssh -F ssh-bastion.conf 10.250.199.144 'sudo chmod 644 /etc/kubernetes/admin.conf'


10.250.199.144 = ip мастера куба

копируем теперь ключ на наш комп
$ scp -F ssh-bastion.conf admin@10.250.199.144:/etc/kubernetes/admin.conf ~/.kube/config

меняем обратно на мастере пермишнсы на 600

$ ssh -F ssh-bastion.conf 10.250.199.144 'sudo chmod 600 /etc/kubernetes/admin.conf'

далее надо малек подкоретировать ~/.kube/config
заменить в нем строки вида

    server: https://127.0.0.1:6443

на

    server: https://kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com:6443


дело в том что когда мы вывзаем kubectl то он внутри ~/.kube/config файла ищет урл 
по которому стучаться на куб.  по дефлоту это 127.0.0.1 так что нам надо это скоректрвать
для этого

$ LB_HOST=$(cat inventory/hosts | grep apiserver_loadbalancer_domain_name | cut -d'"' -f2)
$ sed -i "s^server:.*^server: https://$LB_HOST:6443^" ~/.kube/config


тут надо сказать что это за хрень kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
откуда она взядась что это такое.
а это тераформ на амазоне создает амазоновский лоад балансер. он создает причем  Classic Load Balancer.
что это за хрень. это доменное имя некоторое.  котооое имеет несколько A IP адресов например

$ nslookup kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Name: kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Address: 52.211.32.9
Name: kubernetes-elb-devtest-1645838512.eu-west-1.elb.amazonaws.com
Address: 3.248.96.93

каждый адрес видимо в своей авайлабилити зоне.
и далее также пробрасывается с этих внешних IP внутрь амазона порт уже на кокнетнй инстансы.
тоесть лоадбаласер это как хапрокси который имеет внешний IP и прбосбываем порт с внешнено ip
на бекенд сервера. AWS Classic Load Balancer (CLB) operates at Layer 4. тоесть udp\tcp . means is that the load balancer routes traffic between clients and backend servers based on IP address and TCP port.
для сравнния у амазоне есть еще L7 балансер они его зовут как  Application Load Balancer. 
он подходит для балансировки HTTP трафика. такая же аналогия как haproxy может работать на l4 режиме
а может на L7 режиме.

так вот тераформ куб спрея создает L4 amazon Classic Load Balancer = aws CLB и  с внешних IP пробрасывает 
порт 6443 на мастера куба.  ( про  L4 amazon Classic Load Balancer надо еще дальше поразбираться). 
главная суть что контролплейт куба реально прям выставляется наружу кубспреем во внешний мир.
уж незнаю наколько это безопасно.


что еще интересно. то что хосты кроме бастиона они создаются без внешнего IP. вопрос как тогда 
они в инте выходят например для установки пакетов. ответ для subnet в котоой сидят инстансы  в таблице
маршрутизации для этого subnet  добавлен марщруты вида

10.250.192.0/18 local  
0.0.0.0/0 nat-0110f39cec15f906f  

первый маршрут это чисто внутри площадочный локальный. 
а ip интернетовские - выход на них идет через nat гейтвей. их можно создать в virtual private clooud  - nat gateways. опять же тут неочень пгимаю разницу между virtual private clooud  - internet gateways и 
virtual private clooud  - nat gateways. это тоже надо прояснять.


но зато тпеперь понятно как иснатсны без внешнего ip умудряются выходит в интернет









 это AWS Classic Load Balancer (CLB) operates at Layer 4.





ключ пеертянули. тпепеерь ставим kubectl

$ sudo apt-get install -y apt-transport-https ca-certificates curl
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg

$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

$ sudo apt-get update
$ sudo apt-get install -y kubectl

ну все. можно проверить что мы видим кластер куба

$ kubectl get nodes



-----
попробовал увязать тераформ и виртуабокс но к сожалению пока что 
на данный момент провайдер для вирутаобокса гавно.

а так вот огн конфиг для виртуалбокса


r$ cat main.tf 

terraform {
  required_providers {
    virtualbox = {
      source  = "terra-farm/virtualbox"
      version = "0.2.2-alpha.1"
    }
  }
}


resource "virtualbox_vm" "node" {
  count  = 2
  name   = format("node-%02d", count.index + 1)
  image  = "https://app.vagrantup.com/ubuntu/boxes/bionic64/versions/20180903.0.0/providers/virtualbox.box"
  cpus   = 2
  memory = "512 mib"
  // user_data = "${file("user_data")}"


  network_adapter {
    type           = "bridged"
    host_interface = "en0"
  }


}



====
далее. 
рассмотрим установку k8 через kubespray когда виртуалки 
руками раскатаны через vagrant.



скачиваем с гита кубспрей.
$ git clone https://github.com/kubernetes-sigs/kubespray.git

ставим ансибль как прописано в kubespray-ansible.txt

ставим вагрант. конфиг для виртуалок берем из файла kubespray-Vagrantfile.txt
в нем прописано четыре виртуалки
kmaster2	| 100.0.0.3
kmaster2	| 100.0.0.4	
kworker2	| 100.0.0.9
kworker3	| 100.0.0.10

тут важно отметить то что согласно конфигу во все вм будет интегрирован доступ по 
единому ssh ключу из папки ~/.vagrant.d/insecure_private_key.
это важный момент потому что именно этот ключ потом мы будем указывать ансиблю для доступа на виртуалки.


еще важный момент такой что практика показала что первые две виртуалки кубспрей будет 
юзать для мастеров контрол плейна куба. а вот все уже остальные виртуалки кубспрей 
будет использовать уже как дата ноды куба. как это поменять как задать кубспрею сколько 
нод использовать под контрол плейн а сколько под дата ноды пока непонятно. так что 
если хотим иметь хотя бы одну дата ноду то надо создавать минимум три виртуалки.


запускаем виртуалки
$ vagrant up

убеждаемся что мы можем зайти на все эти виртуалки по ключу ~/.vagrant.d/insecure_private_key
чтобы понимать что ансибль тоже туда сможет зайти
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.3
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.4
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.9
$ ssh -l vagrant -i ~/.vagrant.d/insecure_private_key 100.0.0.10


идем в папку с кубспреем
и копируем вспомогательные файлы инвентаря из сэмпла

$ cp -rfp inventory/sample inventory/mycluster

но самого файла инвентаря hosts.yml там еще нет.
нам надо его создать 

$ declare -a IPS=(100.0.0.3 100.0.0.4 100.0.0.9 100.0.0.10)
в этой строке мы указываем все ip адреса всех виртуалок которые мы хотим запихнуть в куб

запускаем генерацию файла инвентаря 
$ CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py ${IPS[@]}

если все окей то мы должны получить вот такой файл
$ cat ./inventory/mycluster/hosts.yml 
all:
  hosts:
    node1:
      ansible_host: 100.0.0.3
      ip: 100.0.0.3
      access_ip: 100.0.0.3
    node2:
      ansible_host: 100.0.0.4
      ip: 100.0.0.4
      access_ip: 100.0.0.4
    node3:
      ansible_host: 100.0.0.9
      ip: 100.0.0.9
      access_ip: 100.0.0.9
    node4:
      ansible_host: 100.0.0.10
      ip: 100.0.0.10
      access_ip: 100.0.0.10
  children:
    kube_control_plane:
      hosts:
        node1:
        node2:
    kube_node:
      hosts:
        node1:
        node2:
        node3:
        node4:
    etcd:
      hosts:
        node1:
        node2:
        node3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}


как видно первые две ноды входят в контрол плейн.
насколько я понимаю руками можно скажем добавить еще ноды в группу kube_control_plane
при желании.

этот файл с инвентарем нам нужно немножко подкореектировать. нам нужно указать 
ansible_ssh_user под которым ансибль будет ломиться по ssh на виртуалки и нужно 
указать ансиблю какой приватный ключ.

эти две строки выглядят так
ansible_ssh_user: vagrant
ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key

а откоркетированный файл выглядит так
all:
  vars:
     ansible_ssh_user: vagrant
     ansible_ssh_private_key_file: ~/.vagrant.d/insecure_private_key
  hosts:
    node1:
      ansible_host: 100.0.0.3
      ip: 100.0.0.3
      access_ip: 100.0.0.3
    node2:
      ansible_host: 100.0.0.4
      ip: 100.0.0.4
      access_ip: 100.0.0.4
    node3:
      ansible_host: 100.0.0.9
      ip: 100.0.0.9
      access_ip: 100.0.0.9
    node4:
      ansible_host: 100.0.0.10
      ip: 100.0.0.10
      access_ip: 100.0.0.10
  children:
    kube_control_plane:
      hosts:
        node1:
        node2:
    kube_node:
      hosts:
        node1:
        node2:
        node3:
        node4:
    etcd:
      hosts:
        node1:
        node2:
        node3:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}


далее заходим в наш установленный ансибль через venv
$ source ~/python3/ansible/env/bin/activate

запускаем ансибль плейбук cluster.yml на установку куба на виртуалках

(env)$ ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml

все процесс пошел.
примерно через 20 минут мы получим установленный куб


====




