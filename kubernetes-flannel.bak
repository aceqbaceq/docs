прежде всего напонять что такое бридж.

для начала вспомним как работает свич.

у нас есть комп А с одним сетевым портом А
у нас есть комп Б с сет портом Б

порт А имеет мак адрес и он шлет в сеть
фрейм у которого | src МАК-А | dest МАК-Б

фрейм прилетает в порт свича.
и тут очень важно скзаать что порт свича неимеет свой собственный мак
адрес. а если и имеет это в процессе передачи этого фрейма неучаствует 
вообще! разве это не шок? мы имеем сетевой порт на железке но 
этот порт не имеет или по крайней мере вообще неучаствует в процессе
передачи фрейма.
клмпьютер а ничего незнает о свиче и его портах никак это неисопльзует
при подгтовке фрейма при отправке и тому подобное.

фрейм засосался внуьрь свича. если свич уже знает за каким портом 
сидит комп Б то он этот фрейм выплюнет без изменений ровно как он 
и пришел от компа-А в этот порт. причем выходной порт опять же необязан
иметь никакой свой МАК адрес. разве это не шок? 
если свич незнает за каким портом сидит комп-Б то он выплюнет этот фрейм
во все порты свои.

важно тут увидеть что порты свича хоть и участвуют в процессе передачи фрейма
но они совершенно обезличены ибо не имеют или необязаны иметь свой
мак адрес.

это шок.

ведь мы привыкли что если порт эзернет то он обязан иметь мак адрес
чтобы участвоват в процессе передачи информации.

шок.

теперь таже предстваим что в какойто порт-1 свича-1 воткнут не конечный компьюер
а другой свич-2. это значит что в порт-1 свича-1 постоянно влетает 
не фреймы от одного компа у которого один src MAC а постоянно влетают
фреймы с кучей разных src MAC. 
о чем это говорит. это говорит о том что порт свича работает в режиме 
promiscious mode. он принимает фреймы с любыми src MAC. засасывает в себя все.

это тоже шок.
потому что это тоже совершенно отичается от работы эзернет порта на компе.

итак еще посмотрим на различия как работает эзернет порт на компе и свиче.

на компе порт обязан иметь МАК адрес.
на свиче это необязан

на компе порт принимает только фреймы преднезначенные для его мак адреса
на свиче порт принимает все фреймы с любыми мак адресами
(броадкастный трафик понятно что принимают и комп тоже)

на компе комп отправляет в сеть фреймы только с одним src MAC - своей карты
на свиче порт плюет в сеть фреймы с совершенно разными src mac адресами.

вот такая огромная разница. хотя и там и там эзернет порт.


получается если мы имеем на компе два эзернет порта 
и хотим заставить комп работать как свич то надо 
чтобы софт на компе умел переключать порты и использовать их 
в другом режиме в режиме свича а не врежиме компа.

то есть порт на компе должен начать принимать весь трафик (promiscou mode)
порт на компе должен начать отправлять в сеть фреймы с разными src MAC 
и плевать какой мак имеет сам порт. он вообще неважен и ненужен.
потому что если комп А отделен от компа Б свичом или свичами то они никак
неменяют src\dest MAC так что фрейм летит от компа А до компа Б через все 
свичи безизмененнно.

таким образом сразу видно что чтобы комп смог работать в режиме свича
это надо заствить его сетевые порты работать в совершенно
другом режиме чем сетевые порты обычного компа.

вот это очень важно было хотя просто осознать
придать этому занчение.е
обратить на это внимание.

какя понимаю бридж это свич на 2-4 порта.
и внутри он попроще устроен.
просто витоге маленький простой свич. это бридж.


значит если мы говорим о софтовом бридже.
то появляется некое непонятная хрень которой нет в физ бридже.

а именно. в линуксе среди реальных физ портов появляется 
+1 вирт эзернет порт.

захрена это сделано непонятно.
ибо  в нем нет необходимости.

в реальном бридже скажем есть только два порта один 
в одну сеть второй во вторуб сеть и никаких доп портов специального
назначения в физ бридже нет.
поэтому захрена было в линуксе лепить этот порт непонятно.
в нем нет реального физ смысла.   


далее опять же обращу внимание что когда мы в линуксе 
обявляем тот или иной физ порт частью бриджа то этот порт необязан 
иметь ни мак адрес ни IP адрес. они также как на свиче 
абсолютно этим не будут пользоваться. это чисто физ безликие 
порты без всяких опознавательых знаков. в моем понимании.

слово добавить порт в бридж опять же я считаю сверх неудачным.
я бы сказал обьявить порт переклчит порт в режиме бриджа. вот так.
так вот в линуксе когда мы переключаем порт в режим бриджа и если порт
имеет назначенный IP адрес то либо будет ошибка либо автоматом у порта будет
стерт ip адрес. так вот я считаю что кроме этого надо автоматом удалять и
мак адрес на порту. тогда будет все по фэншуй.  если уж делать все 
как надо то доконца. но мак адрес на порту остается хотя смысла в нем 
никакого нет. порты на свичах не имеют своих индивиудальных 
мак адресов. едиснвтенное насколько я понял если мы активиурем STP
то он требует вот только я непонял толи чтобы весь свич имел один мак адрес
по которрому его можно идентифициаровать то ли чтобы порт каждый имел 
свой мак адрес. если же мы неиспольщуем STP то порты на свиче 
мак адреса иметь необязаны.
точка.

далее следущий важный момент для понимания.
как контейнерные сетевые карты прикрепляются к бриджу.


контейенер имеет сетевую карту уровня L3,
у нее есть IP и mac адрес.
эта шарманка сидит в своем сетевом неймспейсе.

[  сетевой неймспейс 1 ( сетевая карта контейнера MAC+IP ) ] 

эта сетевая карта связана с veth портом который сидит в другом сетевом 
неймспейсе , нашем обычном сетевом неймспейсе init. veth порт имеет мак 
хотя по мне он ему ненужен. как я понял все фреймы что он получает он 
неотбрасывает и неразбирает он их сразу посылает на карту контейнера.

так вот этот veth докер  переключает в режиме бриджа. то есть все что влетает 
в порт он пропускает через себя в обе стороны. то есть то что карта контейнера
из себя плюет и оно прилетает на veth он через себя пропускает дальше.
и то что снаружи прилетает в veth он через себя пропускает и плюет в карту.

за veth сидит ядро. то есть цепочка такая

ядро <-> veth <-> сет карта контейра (IP+MAC)

просто veth одном неймспейсе а карта контейнера в другом сет неймспейсе 
поэтому процесс контейнера видит только свою карту и не видит veth карту
а процесс init видит veth и невидит карту контейнера.

что происходит когда у нас несколько veth и может быть несколько 
реальных физ портов переключены в режим бриджа (или как они говорят 
подключены к софт бриджу).  как это выглядит

		|=
		|<-> eth0
ядро<->	|<-> veth1 - сет карта контейнера 1
		|<-> veth2 - сет карта контейнера 2
		|<-> eth1
		|=

что мы тогда имеем. 
1 каждая карта работает в promiscious mode и она всасывает в себя
абсолютно все фреймы что к ней долетают
2 всосав фрейм карта его тут же передает дальше через себя в ядро.

если в ядре есть уже понимание за какой картой сидит dest MAC 
фрейма то этот фрейм направляет конкретно в тот один порт.
а если такой информации нет то (внимание) фрейм отправляется НА ВСЕ ПОРТЫ
РАБОТАЮЩИЕ В РЕЖИМЕ БРИДЖА. другими словами фрейм флудится на все порты
бриджа. таким образом например если контейнер 1 направляет фрейм на 
контейнер 2 и ядро еще незнает что  сет карта контейнера 2 сидит 
за за veth2 портом то этот фрейм (внимание) полетит в том числе и на eth0
и на eth1 и те его выплюнут в реальные сети.
также (важно) броадкаст фрейм влетевший в один порт бриджа будет тутже 
зафлужен на все порты бриджа.

не все карты позволяют переключатся в режиме бриджинга.
например вай фай карты этого сделат ненадут. например они недают отправлять
в сеть фреймы с мак адресом отличным от мак адреса карты.

внутри линукса можно создать несколько бриджев.
таким образом одна группа портов в режиме бриджинга будет назависима
от другой группы портов в режиме бриджинга. 

при создании бриджа на линуксе создается порт brctl L2 
с мак аресом. еще раз какой смысл физический в нем - по моему его нет.
на фреймы пересылаемые он никак не влияет. 
его мак адрес тоже никому не всрался.
единственное что может быть это делается если мы будет использовать в этом
бридже еще и протокол STP. которым я не будут пользоваться.
опять же этому порту можно даже IP назначить. 
захера тоже непонятно.

важно конечно понимать что бридж это нетолько порты которые все пропускают
снаружи вовнутрь и изнутри наружу. это еще и софт который обрабатывет 
эти фреймы когда они влетели из порта внутрь ядра и этот софт решает
куда эти фреймы отправлять дальше.также как в физ коробке это есть.
то есть фрейм влетел из порта внутрь коробки а дальше там же коробка смотрит
есть у  нее информация за каким портом находистя dst mac и если есть то бридж
плюет фрейм невовсе порты свои а только в один.

порт <-> внутренности коробки <-> порт(ы)


вот как для меня сейчас выглядит бридж  линукса


eth0 ----  |
eth1 ----  |
veth0 ---- | ядро (софт) | ---- brctl0
veth ----  |

и получается что этот brctl0 он ни к селу ни к городу. как бы в него ниоткуда 
немогут никакие фреймы влететь потому что он ни к чему не подключен 
ни вылететь. ну окей влететь могут из ядра.но что дальше. лететь то некуда.

или что там другая схемп что фреймы от всех портов влетают в ядро через brctl0? так что ли?

eth0  <--->|
eth1  <--->| <-> brctl0 <-> | ядро
veth0 <--->|
veth1 <--->|


но это бред какойто. это представить что есть порты на свиче
и от них фреймы летят несразу внутрь железки а на какойто внутренний
невидимый снаружи но такойже по факту ethernet порт а уже из него внутрь 
железки. ну и получается что 10х1ГБ портов идут в 1 порт 1ГБ.
дебилизм какойто.

дальше.
представим мы имеем комп. на нем две физ карты

комп   
eth0 |---------> порт 1 свича  
eth1 |---------> порт 2 свича 
                 порт 3 свича <---------- другой комп
				 

мы их воткнули в обычный свич.
что будет если в свич в порт 3 от другого компа прилетает броадкастный фрейм. 
свич этот фрейм выплюнет во все порты (кроме 3). то есть этот 
фрейм полетит в eth0 и в eth1.

комп засосет эти фреймы и от eth0 и от eth1.
ну и поскрипит софтом и на этом все закончится.

другое дело если мы на компе включили eth0 и eth1 в наш
софт бридж.
когда фрейм влетит в eth0 то софт свича направит его во все остальные порты
свича. то есть он его выплюнет в eth1 обратно в сеть.

 также когда фрейм влеит в eth1 то он его засосет 
 и софт выплюнет пришедший фрейм во все остальные порты бриджа
 то есть в eth0. 
 
 когда обратно в физ свич прилетит броадкастный фрейм в порт1. то он его 
 выплюынет во все порты кроме порта1.
 тоже самое касается когда в физ свич прилетит обратно фрейм в порт2.
 он его выплюнет во все порты кроме порта2.
 таким образом видно что начнется адский ад. на всех портах
 ибо в отличие от IP пакета эзернет фреймы неимеют TTL.
 
 
 вернемся обратно к контейнерам.
 важно понимать что карты контейнеров неявляются портами бриджа.
 это абсолютный мисконепшн. портами бриджа являются veth порты.
 а уже они соединены с портами контейнеров.
 таким образом контейнерные порты имеют уровень L3 с IP
 а veth имеют уровень L2
 
 veth как быявляются теми физ портами на реальных свичах
 куда по проводам подключаются порты компов.
 
 порты компов это порты контейнерров. порты veth это физ порты свичей.
 
 порты veth неимеют IP адресов могут и мак неиметь. 
 порты контейров имеют MAC+IP
 
 |			  veth0|<--------->|if1 карта контейнера1	
 |софт бридж 	   | 
 |			  veth1|<--------->|if2 карта контейнера1


и тут мне кажется важно четко понимать разницу между портами 
которые составляют сам свич и портами которые воткнуты в эти порты.

между ними огромадная разница.


если ты добавляешь какойто порт в свич то ему это недает ничего.
он просто ставноится транзитным портом через который все просто пролетает туда и сюда. 

а вот если ты какойто порт подключаешь к порту бриджа то это уже дает.
дает то что наш порт теперь может связываться с другим портами подключенными 
к бриджу через L2 на уровне фреймов.


походу я понял что дает brctl. на уровне L2 когда у него только мак 
он недает ничего и никак. а вот когда ему присваивают IP адрес. 
то это позволяет наш свич превратить в рутер. вот походу его смысл.

картинка

комп if1|<----->|veth1				  |      
комп if2|<----->|veth2  бридж   brctl0|<----->интернет 
комп if3|<----->|veth3 				  |

 наличие brctl0 порта уровня L3 
 дает то что свич превращается врутер.
 вот походу смысл этого порта.
 
 через brctl0 бридж может выводить компы на схеме в инет если 
 они сидят в одной IP сети.
 
 или если компы сидят в разных IP сетях то через brctl0 они 
 могут общатся друг с другом на основе рутинга на бридже.
 
 то есть еесли комп1 имеет ip = 192.168.1.10
 если комп2 имеет ip = 192.168.2.10
 
 а на свиче мы имеем что 
 brctl0 
 ip1=192.168.1.1
 ip2=192.168.2.1
 
 ну тоесть что brctl0 имеет два ip.
 то тогда комп1 и комп2 смогут пинговать друг друга.
 
 тоесть без l3 brctl0 бридж дает связность компами на уровне фреймов
 уровне l2
 при наличии L3 brctl0 ну я уже описал свич превращается в рутер.
 
 собственно ровно это мы и имеем в домашних рутерах.
 ряд свичевых портов. плюс виртуальный то есть без реального 
 физ интерфейса внутренний L3 lan порт ну и плюс 1 L3 внешний порт
 к провайдеру.
 
 вобщем brctl0 нужен только для одного - превраттить бридж в рутер.
 вот походу его смысл.
 
 это совсем неовидно изначально. и нигде необьяснено
 
 
 возвращается к контейнерам.
 изначальная задача поместить контейнеры каждый в свой независмый L2 
 сетевой стек. это мы сделлаи с помощью индивидуалных сетевых неймспейсов.
 каждому контейнеру по своей независимой сет карте.
 
 далее мы хотим соединить ряд контейнеров друг с другом на уровне L2.
 при этом мы нехотим переделыать их сетевые неймспейсы. пусть они так и сидят
 в своих неймспейсах. с незавиимыми карточками.
 значит их нужно воткнуть в свич виртуальный. вирт свич должен имет какието
 свои свичевые обезличенные порты. 
 такие придумали = veth
 
 veth мы делаем портами свича = получили свич с портами.
 также veth по дизайну соединяются с портами контенеров.
 
 таким образом мы обьеидинили карты контейнеров через свич.
 
 на уровне l2 они могуо друг с другом контактировать.
 
 возникает вопрос вот мы обединли карты контейнеров 
 другс другом через veth-ы через свич софт.
 
 это связь по L2.
 
 а есть ли связь между картами контейнеров и 
 физ картой сервера через которую он выходит в сеть на уровне L2.
 
 то есть понятно что вот эти veth-ы они образуют некий виртуальный свич
 внутри линукса. вопрос в том что есть ли аплинк между этим вирт свичем
 и физ картой сервера
 
 
 eth0(192.168.1.10)----????-----|(аплинк) вирт бридж  veth1|<----->|контейнер1
													  veth2|<----->|контейнер2
													  veth3|<----->|контейнер3
													  

есть ли внутри линукса аплинк L2 между реальной физ картой
и софт бриджом по дизайну?

думаю все таки что его нет. потому что мак адреса которые генерирует 
линукс для вирт карт контейнеров они могут совпать с мак адресами
реальных физ устройство в сети за компом. 

поэтому надо заставить чтобы если мы выпускаем пакеты из контейнера в реальную
сеть за компом чтобы происходил рутинг. то есть чтобы внутри линукса сдирался
фрейм контейнера , одевался фрейм данного компе его карты eth0 (пакет Ip от контейнера при этом остается без изменений конечно) и только тогда выпускался 
в сеть. так что я думаю что на уровне фреймов есть связь только между veth.
 а послать фрейм от контейнера через eth0 во внешнююю сеть нельзя.
 
 я думаю что если контейнеру дать ip сети 192.168.1.0 и сделать 192.168.1.10
 гейтвеем то я думаю что такой пакет успешно покинет комп ( при условии
 что на компе разрешен форвардинг). 
 
 но при такой ситуации когда мы дали контейнеру IP из сети хоста это нам приенсет кучу мудежа только.
 если мы захотим достучаться до компа 192.168.1.100 который лежит несреди
 наших контейнеров а где то в сети хоста. то нужно будет укзаать 
 на контейнере статический маршрут что мол если тебе нужен 1.100 то ищи его
 не среди соседей контейнеров ибо тогда ненужен гейтвей а ищи его за гейтвеем
 
 route add 192.168.1.100 через 192.168.1.1
 
 а на 192.168.1.100 надо тоже будет прописыать обратный стат маршрут 
 что мол если ты ищешь 1.х то посылай на 1.10
 
 тоже самое касается выйти в интернет. надо будет на интернет шлюзе прописываь
 стат маршурт к 1.х через 1.10
 
 причем такие стат маршруты на интернет шлюзе или соседях
 сервера нужно будет на каждом из них прописывать индидуалный маршрут 
 для каждого контейнера. вобщем ужас.
 
 окей. итого понятно что ip сеть под контейнеру надо выбрать совсем другую
 чем сеть хоста. например 192.168.2.0
 
  теперь контейнеры имеют свою сеть а сервер и его соседи по физ сети
 имеют другую сеть.
 
 теперь достаточно на гейтвее физ сети прописать ровно один маршрут 
 для всех контейнеров что мол сеть 192.168.2.0 надо переправлять 
 на 1.10 и все. то есть из физ сети понятно где контейнеры искать.
 
 теперь остается контейнеры выпоустить в сеть. и тут нам на помощь 
 приходит brctl0 L3
 
 
 физ сеть| <---> |eth0 компа (сеть 192.168.1.10) <----> ядро <---->| brctl0(192.168.2.1) вирт свич veth1|<---->|контейнер if1 (192.168.2.10)
 
 получается brctl0 это порт свича L2 причем с L3 фукцией.
 
 мы назначаем конейтнерам гейтвей 192.168.2.1
 
 и у нас пакеты теперь ходят из физ сети к контейнерам и обратно.
 (на компе мы активировали форвардинг и на рутере физ сети прописали 
 стат марушрут что 2.0 надо искать на 1.10)
 
 так вот docker0 это brctl0 порт.
 
 заметим что пока NAT даже непахнет. и это хорошо ибо нат это cpu затратная
 процедура
 
 
 если мы при прохождении eth0 зададим NAT правило. то тогда мы можем подменять 192.168.2.0 через 192.168.1.10 и тогда на рутере физ сети
 прописывать стат маршрут непридется.
 
 
 но отмотаем обратно. итак чтоже нам дает вирт бридж.итоги
 самое главный вопрос  - что будет если я какойто  порт включаю в состав бриджа 
 
 # brctl docker0 addif eth0
 
 docker0 название бриджа
 eth0 порт который мы включаем в состав бриджа
 
 1. eth0 ПОРТ СТАНОВИТСЯ ПОЛНОСТЬЮ ТРАНЗИТНЫМИ
 2. ТО ЧТО ВЛЕТАЕТ В ОДИН ТАКОЙ ПОРТ ПЕРЕНАПРАВЛЯЕТСЯ СРАЗУ ВО ВСЕ ДРУГИЕ
 3. НЕ ПОРТЫ П.1 А ТЕ ПОРТЫ КОТОРЫЕ ПОДКЛЮЧЕНЫ К ПОРТАМ П.1 ПОЛУЧАЮТ 
 СВЯЗЬ ДРУГ С ДРУГОМ. 
 4. СВЯЗЬ УРОВНЯ L2
 5. еще раз важно различать порты которые являются непосредственно частью
 бриджа. в данном случае это eth0. и порты которые неявляются частью бриджа
 но которые соединены с портами бриджа. в нашем случае это порты контейров.
 порты контейнеров неявляются частью самого бриджа. но они соединены с портами бриджа. порты самого бриджа являются транзитными и безликими.
 порты которые подключаются к портами бриджа уже являются полноценными
 и имеют мак+IP. мы настраиваем бридж чтобы через безликие порты создать 
 связь между полноценными L2+L3 портами.
 
 
 что дает бридж разобрались.
 
 и вот теперь мы переходим к задаче что нам надо что контейнеры
 могли друг до друга достучаться между отдельными физ серверами.
 
 есть такой еще момент. если brctl0 это гейтвей для всех карточек контейнеров
 то тогда их срупут упирается в срупут brctl0. получается что скорость каждой карточки условно говоря 1ГБ их 100 штук и все они выходят наружу
 через 1ГБ. это как то пахнет усзким местом. но пока непонятно 
 так как карточки и контейнеров и brctl0 они виртуальные.
 
 без проблема можно создать карточку сетевую с таким вот именем
 
  ip link add petya.1.1  type dummy
  
 # ip -c link show
 
 15: petya.1.1: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000

отсюда приходит понимание что интерфейс фланнеля с именем flannel.1
это не какой субинтерфейс (точка один) а просто такое дурацкое
название этого интерфейса.
 
3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default


 прикольно что карточка в контейнера сетевая она ничто иное как по своей
 сути тоже veth интерфейс. то есть veth он спокойно может быть L3
 
 
 veth напраник = down
 но в нашем неймспейсе мы все равно пингуем veth1
 
 veth link = down. но все равно пингуется
 поразщительно
 
 что dummy что veth оним когда down то при этом саойокной 
 пингуются.
 если сосдем veth = down то его другой сосдет все равно пингутеся.
 
 еще прикол. если бридж не имеет ниодного порта то его brctl интерфейс
 будет оставаться в состоянии down. 
 
 # brctl create docker2
 и он будет оставться в down и его будет неподнять.
 
 ip route get IP
 
 
 если настоящий лупбек погасить то он непигуется
 
 # ip route get  172.16.0.1
local 172.16.0.1 dev lo  src 172.16.0.1
    cache <local>

очень странный маршрут

также если мы от процесса пигуем lo панпример непонятно какой он 
ставит тгда source IP в пакет.наверно того же самого лупбека. 

то есть у нас же должна быть source карта  и source Ip. 
процесс обращается к какрте и с нее пингует ее же.


процесс если он пингует другую карту он обязан иметь исходную карту
когда мы пингуем лупюбек интервейс какая карта является исходной?

если я создам dummy который не лупбэек.
и есл я его погашу то он все равно пингуется.

значит у менять есть lo = 127.0.0.1 UP
есть dummy = 172.16.0.1  UP

я пингую 172.16.0.1 он пингуется
 но самое интересное какой маршрут выбирет линукс
 
 # ip route get  172.16.0.1
local 172.16.0.1 dev lo  src 172.16.0.1
    cache <local>

ия в шоке. строка означает что пинг до 172.16.0.1 пойдет 
через loopback интерфнйс при этом src ip = 172/16/0/1
как бы што? посчему через лупбек?
если мы погаим дамми то все равно 172 16 0 1 пингкуется
а вот если мы погасим лупбек то нифига не будет пинговаться

возможно лупбек интерфейс и дамми интерфейсы сидят на одной "шине" в линуксе.
но тогда получктся вопрос как тогда пакеты из физ интерфейса могут дойти 
до лупюбек интерфейса

пакеты швыряются исключетльно исходя из таблицы маршутизации
прежде всего  а потом все остальвное

если мы veth назначим ip то рут к нему будет проходить через lo интерфейс.

 япрописал маршрут
 
 172.15.0.0/24 dev veth0  scope link  src 172.15.0.2

veth0 неимеет ip

ping 172.15.0.1

цепочка процесс шлет чрез veth0(l2 без ip) на veth1 в другой неймспейс
а оттуда до 172.15.0.1 все дошел отуда оброатно он прилетет в 
veth0 и вопрос что с ппакетом происходит дальше. он что сразу отдается 
процессу ? стоп. он же недошел до 172.15.0.2 
а как до него дойти ? он же недошел до карты назначения.

и причем пинг доходит хотя dummy с ip = 172.15.0.2 находистя down

яя проерил через tcpdump когда мы пингуем veth то на сам veth вробще
ничего не прилетает. вообще.
все летит на lo и на этом все заканчивается

 ip route get  172.14.0.1
local 172.14.0.1 dev lo  src 172.14.0.1
    cache <local>

то есть летит как напписано в таблице марщутизации. 
а на veth вобще не прилетает. бред
не понимаю пакет недоставлен до карты. а он говорит чт доставил.


цепочка. есть карта дамми vasya с 172.15.0.1

есть veth0 который вдетв в другой спейс на другом конце карта с 172.15.0.10

я задал в роутинге

172.15.0.0/24 dev veth0  scope link  src 172.15.0.1

и пинги пошли.

они вообще неисходили из васи и не входили в него. и не входили в лупбек.
они только проходили через veth0 и все.
и как бутто отуда сразу в процесс

ощущение что все дамми они по факту являются алиасами лупбека.
я убрал ручной маршрут 172.15.0.0/24 dev veth0  scope link  src 172.15.0.1

и остался дефолтовый маршрут

172.15.0.0/24 dev vasya  proto kernel  scope link  src 172.15.0.1

через dummy .

и пинги не проходят.

получатеся что пакет испускается из дамми ииии ????? улетает в ядро
и ничео не происходит.

хотя веф это вирт инт как и дамми но они несоидениеы.

 я провел эксперимент все локальные ip адреса достигаются 
 только через lo интерфейс
 
 ~# ip -c route get 172.14.0.1
local 172.14.0.1 dev lo  src 172.14.0.1
    cache <local>
root@test-kub-05:~# ip -c route get 172.16.102.35
local 172.16.102.35 dev lo  src 172.16.102.35
    cache <local>

~#  ip -c route get 10.5.98.1
local 10.5.98.1 dev lo  src 10.5.98.1
    cache <local>


поразительно! неважно пингуешь ты реальную физ карту
или виртвальную - при пинге своих IP доставка идет всегда вседа
всегда через lo интерфейс. и если его погасить то ты несможешь
пинговать свои собстыенные карочки и неважно она физическая или 
вирталнаяю!


шок!

ну понятно что в физ карту ты неможешь сам засунуть пакети снаружи
но вирт карты то ....

вобщем механизм связи с собсвтенными карточками супер странный нефизичный.
значит когда пингуешь свои IP то пингуешь лупбек интерфейс по факту.

когда я пингую свой внутрений docker0 интерфейс то по факту пакеты ходят 
толькон на lo интерфейсе а на docker0 tcpdump неловит ни-че-го

полусется пактеы летят в lo интерфейс и летят непонятно куда но 
до программы долетают. рисует такая картин

процесс  - lo - ядро. ядро говорит все окей и типа ядро обратно в lo
пихает пакет с обратным порядком IP.
то есть типа на другой стороне провода от лупбек адаптера сидит ядро
и принимает пакеты как сетевая карта безадреса. получает пакеты
формирует обратный пакет и посылает его обратно в lo

как lo L3 имеет на другом конце L2 порт который уходит прямиком в ядро.
и ядро типа через lo принимает пакеты на все IP которые есть на компе на
всех картах.

lo это L3 порт который ведет в другой порт L2 который уже воткнут прям 
в ядро.

еще можно по другому представить. LO это L3 порт который
через провод ведет в L3 порт который имеет все IP карт на компе.


а что если представить что линукс это рутер с кучей L3 портов.
LO это тоже L3 порт входящий в состав рутера.
фишка его в том что процессы которые крутятся внутри рутера-линукса
им тоже типа надо дать возможность как то посылать пакеты на L3 порты
через некоторый api. как бы сами порты сидят в ядре. и вот надо 
юзерспейсу дать возможность блядь посылать пакеты к своим портам 
которые снаружи.

цепочка

получается что lo это как бы порт на невидимый на рутере и сидязий внутри корпуса линукса. зато так как он внутри к нему могут подключаться внутренние
процесыы.

цепочка 

процесс -> LO -> ядро рутера линукс. -> внещние порты в том числе как 
это ни смешно и вирт порты линукса кроме lo.

представим мы шлем пинг на IP физ порта.
имеем цепочку

процесс шлет пакет через -> LO -> он попадает в ядро ведь конечная цель
чтобы пакет попал в ядро. и ядро пакет для физ карты уже получило. его 
отправлять на физ карту на его драйвер неимеет смысла ибо он оттуда попал бы в ядро а он уже в ядре. ядро формирует ответ и отправляет обратно через lo.
и мы поэтому с одной стороны видим пинги а сдругой стороны мы ничего невидим
в тисипидамп на физ порту.
тоесть порт это устройство чтобы пакет попал в ядро. если он уже в ядре 
зачем его направлят в порт.

насколько я понимаю когда пакет влетает в порт "снаружи" то влетев в порт
он попадает в ядро. а из ядра в юзерский процесс

пакет -> порт -> ядро -> процесс

почему нерабтате когда мы пигуес IP карты компа 
но эта ккрта лежит вдруо неймспейсе.
потому что пает идет через lo а на том конце там толко IP 
карт которые в этом неймспейсе.

 в итоге 



возможно почему мы невидим в tcpdump чтобы н


поток идет через brctl0 хотя он для него даже не предназначен.

если  дать lo ip = 10.5.98.5/24 
то будет успешно пинговаться ЛЮБОЙ АДРЕС ИЗ СЕТИ 10.5.98.0\24
это пищлец

дале у меня на компе карта veth1 = 10.5.98.3/24
есть маршрут

10.5.98.0/24 dev veth1  proto kernel  scope link  src 10.5.98.3

однако если ввести команду

ip route get 10.5.98.9 
то она выдааст совсес нето что в таблице маршрутизации а

local 10.5.98.10 dev lo  src 10.5.98.5
    cache <local>


то есть игнор таблицы мраутизации и поход через lo.

это пиздец.

я предлагаю на lo невещать ничего кроме 127.0.01
иначе заебешься вычичлять приколы на ровном месте.

 у меня lo во втором неймспейсе имел ip = 10.5.98.9
 и veth1 во втором нейиспейсе имел ip = 10.5.98.10
 
 я из первого неймспейса из veth0 пытался до пинговться
  до 10.5.98.10 и никак не мог! погасил lo непомогает
  удалил ip с lo и только тогда заработало. 
  какой то п..ц с этим lo.
  
  ужас. как он работает логика непонятина.
  
  лучше lo вообще нетрогать и про него забытьб
  
  lo дебилнейщий интерфейс и ни коммутация ни рутинг с ним неопнятне
  тидет не поправилам. лучше его вообще нетрогать. и никакие ip адреса 
  ему недавать кроме дефолтовго
  
  к соажалениею  brctl0 это не просто L3 порт бриджа 
  это еще и транзитный порт через который как я понял протекает транзитно буквально каждый пакет всего бриджа. так что если его погасить ниодлтин порт
  бриджа не сможет связаться с другим портом бриджа. это дебилизм. полнейший.
  я считаю что brctl0 должен быт влкючен только если мы хотим его L3 функционал 
  заюзать. а так это узкое место .
поэтлому brctl погаситл и весь бридж остаовился

ответ на мой изначальный вопрос
взяли пару вефов. 
один оставил а другой зауснули в другой неймспейс и дали ему IP
вопрос что нам надо сделать чтобы тот IP пинговать.

ответ - этому вефу назначить IP из той же сети и пинги пошли.
 аесли у нас два 10 контейнеров как тогда их пинговать. 
удобнее всего тогда наши вефы этого спейса сделать частью бриджа.
и самоу бриджу дать L3 ip. тогда можно будет их всех достать для пинга.

так будет минималная заморочка с этим.
потому что чтобы пиновать без гейтвея надо иметь порт L3 из тожй е сети 
что и порты те. и чтобы наш L3 порт имел связт с каждым тем портом на уровне L2. 

без свича нужно каждому концу нашего веф назначать ip индивдуальный.
ипрописывать инд маршрут к тому концу. 10 контейнеров это 10 IP с нашей стороны назначь + 10 маршрутов пропши до их ip. ужас.
поэтом свич это выход. если бы на самом свиче нельзя был назначать один порт 
 с IP то тогда нужно было бы создать +1 пару веф. оба оствит внашем неймспейсе
 один вотнуть в свич а второй невтыкать и второму дать IP. 
 
 
 
 ЕДИНСТВЕННОЕ ИЗЗА ЧЕГО НЕРАБОТАЛА СВЯЗЬ МЕЖДУ НОДАМИ ДЛЯ ПОДОВ
 ЭТО ЧТО НА ДАТАТ НОДАХ БЫЛ ВЫКЛЮЧЕН IP FORWARDING.!!
 
 ОСТАЕТСЯ деталеьно выяснить как комбтнация фланеля и iptables
 дает связь
 
 я нашел в инете инфо что лупбэк в линуксе и лупбэк в циске 
 работают совсем неодинаково. опа! подстава!
 я нашел что если лупбек имеет IP то для этих IP трафик должен ходить 
 только внутри машины. поясненеие если линукс увидит трафик вроде бы
 который влетает снаружи и при этом имеет IP из сети lo то линукс
 его будет дропаь. пример
 
 lo = 192.168.1.1/24 - есл итак назнваить 
 то любой входящий в комп пакет с src ip = 192.168.1.0/24 комп будет его дропать.
 а мол если мы хотим lo такой же как у циски можно заюзать dummy 
 интерфейсы. жуть
 
 # ip route
192.168.12.0/24 dev eth0  proto kernel  scope link  src 192.168.12.101
default via 192.168.12.1 dev eth0

proto kernel означает что маршрут был создан линуксом
автоматом


посдтава1 
в iprpute2 нет алиасов
едиснтвенное что работает это 


auto ens160
iface ens160 inet static
        address 172.16.102.19
        netmask 255.255.255.0
        network 172.16.102.0
        broadcast 172.16.102.255
        gateway 172.16.102.1
        scope link
        # dns-* options are implemented by the resolvconf package, if installed
        dns-nameservers 172.16.101.2 172.16.101.7
        dns-search mk.local

iface ens160 inet static
        address 172.16.102.20
        netmask 255.255.255.0

второй момент про scope.
нельщя дать два IP адреса с одной подсети и с одним scope



 

про фланель
 
 видно что заведен бридж cni0 который никак не связан 
 с докер нетворкс командой
 
 
  root@test-kub-02:~# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.5efab67e726a       no              veth6b8aeca2
                                                        vethde173048
docker0         8000.0242f0b9d9c1       no              veth45c266d
root@test-kub-02:~# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
1b537e52e590        bridge              bridge              local
bbffd9dc86b7        host                host                local
ce3ae03bbaa1        none                null                local
root@test-kub-02:~#

--
что заметил инеерсно
как и следовало ожидать veth 
интерфейсы работабт в режиме promiscuity

 vethc8d37526@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 62:80:ae:f3:64:cb brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 1
    veth
    bridge_slave state forwarding priority 32 cost 2 hairpin on guard off root_block off fastleave off learning on flood on
    inet6 fe80::6080:aeff:fef3:64cb/64 scope link
       valid_lft forever preferred_lft forever


параметр promiscuity 1

---

что выяснилось в целом.

берем чисто докер.

его все вот эти команды работы с сетью

docker netwrork ls \create

они не делаюи никакой магии. они испольщуют на бекенде

ip link add ....

напрмиер docker0 это бридж.

тоесть бекенд сетей докера опирсатеся на понятный прозрачный функционал.


docker0  это бридж порт


фланнель создает в системе интерфейс который тоже создатся через 
ip link add vxlan1 type vxlan id 1 remote 172.31.0.107 dstport 4789 dev eth0

вот это сейчас более детально изучим

ближайшая тема vxlan в линукс

--
интересно интфрейс vxlan ненужно обьединять скажем через бридж
с выходным интерфейсом

пример
на компе физ интерфейс
ens160

vxlan интерейс
vxlan.50

каждый из них самостоятельно болтется на компе. 
они никак не обьеидинены на уровне L2 

однако фрейм входя в vxlan.50 далее транзитрруется на ens160 
ив сеть.

хотя по мне надо было создать пару veth.
ксати при этом на коме форвардинг неактивироан.
возникет вопрос как фреймы пакеты перелетаюти с vslan.50 на ens160

vxlan интерфейс это как бы NAT, при проходжении пакета через vxlanx 
интрфейс у него подменяется dst IP адрес. ну типа того.
обыччный nat подменяет src ip. 
а тут одевается сверху новая оболочка и подменеяется src ip dest ip

еще раз у нас есть L2 интерфейс 
vxlan.50

у нас естть маршрут

10.12.0.0/24  proto kernel  scope link  src 10.12.0.1 dev vxlan.50

который говорит пакеты на 10.12.0.0 плюй в сет через vxlan.50 карту.
и используй впакетах src ip = 10.12.0.1

трафик для 10.12.0.10 согласно маршруту  направляется на vxlan.50 
с параметрами 
src ip = 10.12.0.1
dst ip = 10.12.0.10

входит в  vxlan.50 , на пакет одевается новая оболочка
и dst ip заменятеся на 172.16.102.20 ибо так прописано в насйтроках vxpan.50

 vxlan.50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 
    vxlan id 50 remote 172.16.102.20 
    
	
	далее видимо он ищет на компе с какого реавльного физ карты
	он может выплюнуть dst ip 172.16.102.20 нахоит ens160 с его 172.16.102.19
	и вперед использует ens160 и src ip = 172.16.102.19
	
	
	---
	создадим vxlan интерфейс
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.19 remote 172.16.102.20 dstport 4789
	
	
	вот так он выглядит
	
	13: vxlan.50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450
     vxlan id 50 remote 172.16.102.20 local 172.10.0.10 dev ens160 dstport 4789 
    
	
	значит что он дает. весь поток который будет в него направлен 
	будет обернут снаружи новым ip пакетом в котороом
	dst_ip=172.16.102.20 (адрес в локалке того компа)
	src_ip=172.10.0.10   (адрес в локалке нашего компа)
	dev ens160 = указана физ карточка через которую "преобразованный пакет"
	выплевывать в физ сеть
	
	то есть
	remote
	local
	dev
	это параметры пакета в который оборачивается исходный ip пакет.
	
	
	по сути vxaln тоннельный интерфейс.
	
	прикольно что vxlan никак не увязывается через бридж с ens160.
	сточеи зрения линукса мы никак не увязываем vxlan интерфейс и ens 
	интрефейс. поток если попадает в vxlan то каким то неведомым
	ненастроенным образом попадает в esn160. даже без бриджевой
	связм между ними или как у veth пары.
	
	
	преобразование настроено.
	остается прописать какой поток направлят на vxlan.50
	это делается через маршрут в таблице марршрутизации
	
	10.12.0.0/24  proto kernel  scope link  src 10.12.0.10 dev vxlan.50
	
	марушрут - куда, через что, какой src ip.
	
	оно говорит трафик c dst_ip=10.12.0.0/24 шли через карту vxlan.50
	при этом используй src_ip=10.12.0.10
	
	как организовать src_ip=10.12.0.10
	можно либо его присвоить самому vxlan.50
	а можно vxlan.50 оставит как L2 интерфейс и вставить его в бридж.
	а уже какому то другому порту в бридже прсвоить L3 = 10.12.0.10
	
	если мы вставим vxlan.50 в бридж то в маршруте  для vxlan.50
	тогда можно src ip и не указывать
	
	пример
	
	vxlan.50 ---> brctl0 ----> veth0-->|другой неймспейс---> if0(L3)=10.12.0.20
	
	виртуалка когда будет слать пакеты то он автоматом будет прописывать 
	src_ip = 10.12.0.20
	
	кодга vxlan.50 всталвен в бридж тода по идее и маршрут ненужен
	пакеты попадают  в vxan не через маршуртизацию 
	а через бриджизацию.
	
	--
	
	схема
	
	
	другой комп --vxlan.50(10.12.0.2) -- ens160(172.16.102.20) -- (LAN) -- мой комп -- ens160( 172.16.102.19) --vxlan.50 -- brctl0- veth0 -- veth10(10.12.0.100)
	
	
	мой комп 
	настройки
	
	14: vxlan.50: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 
    link/ether 8e:78:77:73:5c:d3 brd ff:ff:ff:ff:ff:ff promiscuity 1
    vxlan id 50 remote 172.16.102.20 local 172.16.102.19 dev ens160 srcport 0 0 dstport 4789 ageing 300
    bridge_slave state forwarding priority 32 cost 100 hairpin off guard off root_block off fastleave off learning on flood on
    inet6 fe80::8c78:77ff:fe73:5cd3/64 scope link
       valid_lft forever preferred_lft forever

	как видно vxlan.50 не имеет IP
	и вставлен в бридж
	
	18: veth10@veth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 
    link/ether 22:e3:ec:3d:49:76 brd ff:ff:ff:ff:ff:ff promiscuity 0
    veth
    inet 10.12.0.100/24 scope global veth10
       
	
	и единственный маршрут
	
	10.12.0.0/24 dev veth10  proto kernel  scope link  src 10.12.0.100
	
	как видно когда vxlan.50 вставлен в бридж 
	то на него специально маршрут персонально заворачивающий на него трафик нернужен то ест ьненужен маршрут с параметром  dev vxlan.50 мол такой то
	трафик пускай через карту vxlan.50
	вместо этоого достаточно просто того маршрута что я указал.
	
	в итоге будет работать пинг  между 10.12.0.100 и 10.12.0.2 (адрес vxlan сети другого компа)
	
	# ping 10.12.0.2 
	
	как видно самому интерфейсу вообще по барабану внутренность ip
	пакета который в него придлетает. он просто преобразует все что в него
	прилетает - одевает сверху новый ip пакет и вставляем фиксирванный
	dst_ip=172.16.102.20 и src_ip=172.16.102.19
	
	остатется просто направтиь трафик на этот интерфейс.
	через маршрут в таблице маршрутизации
	либо через бридж
	
	так постпенно мы подходми к понимаю как работает фланнель
	
	
	получается кстати такой простецкий впн условно говоря.
	но штука в том что  в интернет его направялть нельзя
	только внутри площадки 
	так как он незашифрован.
	
	надо понять вот этот вот сервиф фланнеля который висит в кубернетесе
	он както сам поток обрабатывает или он просто нужен как автоматизатор.
	
	линукс почемуто теперь позволяет присваивать дурацкий ip
	интерфейсам. например ip = 10.5.98.0/32 какой смысл такого ip?
	он же вообще некорректен
	
	прикольно у нас получается.
	если  у нас в свич воткнуто два компа.
	один комп имеет 192.168.1.1
	вторйо комп имеет 192.168.2.1
	но походу пьесы связь между ними должна работаь на уровне ip
	так как на уровне mac они могут общаться без проблем.
	на первом компе типа надо написать
	ip route add 192.168.2.0/24 via 192.168.2.1 dev ens160 onlink
	и нахер
	
	// линукс bond из нескольких бриджей есть ли смысл//
	
	
	
	раскрыта тайна как сделать чтобы можно было связать через vxlan 
	несолкьо сервреов через юникаст.
	
	оказыется параметр remote в команде
	ip link vxlan
	это параметр для самого проестецкого случая. когда у нас два сервера.
	по факту это параметр вносит данные в другое место
	в forwarding таблицу для vxlan.
	так вот через ip link связь на есколько серверов никак не нстроить
	по юникасту.
	это делатеся  через другу команду которая позвяолеся напрмяму вносит
	данные в таблицу fdb для vxlan
	
	bridge fdb append to 00:17:42:8a:b4:05 dst 192.19.0.2 dev vxlan0
	
	или 
	
	bridge fdb append to 00:00:00:00:00:00 dst 192.19.0.2 dev vxlan0
	bridge fdb append to 00:00:00:00:00:00 dst 192.19.0.3 dev vxlan0
	
	эти две команды создадут два endpointa для vxlan.
	
	как  я понимаю ровно это иделает flannel.
	
	теперь vxlan знает что у него две точки . куда надо все слать
	
	вместо 00Ж00Ж00Ж можно прописать концено конкретные mac-и.
	нгасколь я понимаю если просиать нули то каждвый раз информация 
	будет шарашить сразу на два сервера  засирая сеть
	
	теперь походу понятно как работает фланнель.
	он накажом хосте просто правильно прописывает 
	flannel.1
	и
	bridge fdb add to 00:00:00:00:00:00 dst 192.19.0.2 dev vxlan0
	
	позволятет это елать прозарвачно ватоматично вместро руками работы
	только и всего.
	
	документацтия  у фланнеля на уровне ноль
	
	--
	я прочитал в статье что на каждом сервере ееный vtep клиент в процессе
	того что через него пролетают пакеты с фреймами изучает инфо и 
	записывает в fdb таблицу информацию вида 
	mac, vtep IP, vtep mac, vxlan id.
	
	как понимаю когда vtep незнает за каким vtep соседом находится вирталка
	с MAc он шлет пакет ко всем соседям. а вот когда знает то только тому
	за которым сидит виртуалка с MAc ( в общем такая же логика как и у свича обычного)
	
	----
	----
	!!! теперь надо для тех нод руками настроить vxlan
	
	имею три хоста с IP
	172.16.102.19
	172.16.102.20
	172.16.102.21
	
	создадим на них vtep клиенты L2 
	и назначим им IP.
	172.20.0.19/24
	172.20.0.20/24
	172.20.0.21/24
	
	
	
	host-(test-linux-01)
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.20  dstport 4789
	
	ens160 и 172.16.102.20 это карта через которую пакеты будут транзитться
	в физ сеть и какой src_ip будет у обертки от исходного пакета
	
	
	как видно я не использовал параметр remote потому что сейчас у нас будет
	ни оди сервер сосед а два соседа. прараметр remote здесь никак не поможет
	
	
	удаленные vtep соседи прописываются вот именно так
	
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.19 dev vxlan.50
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.21 dev vxlan.50
	
	append это опция добавить строчку для мак адреса который уже есть.
	работает только для vxlan и ряда других случаев.
	
	неважно какой на vxlan.50 mac у соседа неважно знаем мы его 
	заранее или нет мы всегда указыаем (непонятно почему) именно 00:00:00:00:00:00
	а dst ip указывается именно внешней карточки сервера
	
	
	fdb таблица. когда сквозь свич пролетает мак
	то свич смотрит его src mac и заносит в таблицу fdb
	мол из такого то порта прилетел  такой то mac
	теперь свич знает что фрейм с таким то маком надо слать 
	на такойто порт
	
	так вот по приколу в fdb вносится нетолько информация вида
	мак-порт
	в эту же таблицу вносится информация по vxlan настройкам.
	если в fdb внести мак=00-00-00-00-00-00 vxlan-id dst IP
	то линукс будет эту инфо использовать для доступа к VTEP соседу.
	и в отличие от remote в команде ip link таких соседов можно указать неодин
	а много.
	
	также я прочитал вот здесь https://vincent.bernat.ch/en/blog/2017-vxlan-linux
	что даже если я знаю заранее мак адреса от карточек vxlan
	соседей то все равно в fdb маки должны быть в виде 00-00-00-00-00-00
	
		для меня это еще вопрос почему мы неиспользуем mac от ens160
	
	для простоты добавим IP прям на vxlan.50
	
	# ip addr add 172.20.0.20/24 dev vxlan.50
	# ip link set vxlan.50 up
	
	
	
	host-(test-ansible)
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.19  dstport 4789
	
	
    # bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.20 dev vxlan.50
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.21 dev vxlan.50
	
	
	# ip addr add 172.20.0.19/24 dev vxlan.50
	# ip link set vxlan.50 up
	
	картчока vxlan это ничто иное как vtep клиент в терминологии 
	из доков.
	
	
	host-(test-linux-02)
	
	# ip link add vxlan.50 type vxlan id 50 dev ens160 local 172.16.102.21  dstport 4789
	
	
    # bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.19 dev vxlan.50
	# bridge fdb append to 00:00:00:00:00:00 dst 172.16.102.20 dev vxlan.50
	
	
	# ip addr add 172.20.0.21/24 dev vxlan.50
	# ip link set vxlan.50 up
	
	
	пинги между компами работают
	
	# ping 172.20.0.{19,20,21}
	
	----
	замечу что на дефолтовых линуксах чтобы раотал vpx lan
	ненужно актиививровать ip форвардинг.
	а вот на хостах с кубернеретесом нужно.
	
	--
	еще что необьяснимо
	если vpxlan это карта неважно l2 или l3 уровня то как 
	трафик влетая в эту карту потом магическим образом транзитируется в 
	ens160. если они несвязаны друг с другом по L2. 
	тогда надо хотя бы чтобы какойто veth тоннель был между ними или что.
	или маршрут. а так получается хрень. в одну влетел а потом оказался
	вдругой и вылетел. а между ens160 и vpxlan нет связи как это есть 
	у vethh пар.
	это херовая архитерктура
	
	---
	
	далее надо понять как  вкубренретенс фланнель настроек
	
	например вот этот странный ip у карты фланнеля
	
	10.252.0.0/32
	что за чушь
	
	4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 
    link/ether 0e:d8:dc:0e:7c:cb brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 1 local 172.16.102.31 dev ens160 srcport 0 0 dstport 8472 nolearning ageing 300
    inet 10.252.0.0/32 brd 10.252.0.0 scope global flannel.1
      
	  
	 также флаг nolearning - почему?
	 
	 также почему то vte клиенты вбиты 
	 
	 ~# bridge fdb show | grep flannel
22:1b:9f:bc:ef:45 dev flannel.1 dst 172.16.102.32 self permanent
12:f3:4d:91:f4:05 dev flannel.1 dst 172.16.102.33 self permanent
16:74:4c:0f:6f:6e dev flannel.1 dst 172.16.102.32 self permanent
d2:f7:5f:37:16:2b dev flannel.1 dst 172.16.102.33 self permanent

не в виде 00:00:00:00:00

	---
	
	про brctl
	
	как я понял в чем его минус.
	транзит пакетов между его портами идет не некоей общей шине а
	непосредственно через 1 порт brctl0 поэтому сумматрная проепускная способоость бриджа равна всего авсего скорости 1 порта brctl0
	
	про фланнель
	ка я понял точка в названии карты ничего незначтт это не алиас и ничего такого это ничо незначтт просто символ в имени поэтому 
	имя карты
	
	flannel.1 это просто имя карты.
	
	как я понял теерь в линуксе интрефейсу можно прсиваимтиь бессмысленный
	ip адрес не юникаст. например  
	
	ip=192.168.0.0\24 
	
	правда непонятно что с таким адресом линукс может делать
	 а так понка соображение что это просто адрес для справки
	 но именно такой дурацкий адрес имеет фланнель
	 
	 когда мы приаттачиваем контейнерв  в докере через docker networkds
	 то мы всего навсего меняем у контейнера сетевой неймспейс.
	 
	 а docker netowrk create bridge это просто напросто создатеся
	 brctl бридж в линуксе
	 
	 ----
	 
	надо будет вспомнить почему дофолтовыу куб не смог 
	успешно завести фланнель сеть что не было пингов
	ответ - на нодах был выключен айпи форвардинг
	
	
--
 
 
 

  FLANNEl.
  
 
начнем с фланнеля + докер. 
а потом уже расмотрим куб+фланнель+докер.

начнем просто с фланнеля.
написано что своим бекендом он может использовать VxLAN.
это ровно то что мы будем использовать для начала.

VxLAN использует как и по аналогии с VLAN неокторый идентификатор
VNI который отличает один Vx от другого.

Vx использует UDP. 
для UDP нужно указать dest IP. причем  я не понял если наш хост
сидит в одном VNI то ему нужно указыавть все хосты со всеми VNI
или только хосты с VNI на нашем хосте.

но наш случай самый простой - все хосты будут иметь один VNI.
ну и все равно возникает вопрос. вот у нас три хоста. один 
из хостов шлет пакет он что каждый раз слать на оба оставшихся хоста?.

как я понял нет не каждый раз.

каждый раз когда на хост прилетает vxalan хрень то комп запоминает 
mac адрес удаленного компа, его VNI , ну и может быть его внешний IP.
таки образом когда мы захотим отправить vxlan с таким же маком и таким
же VNI то мы пошлем сразу на тот один сервер.

пока на этом детали vxlan оставляем. 
на стороне свичей железных ничего настраивать ненужно.
для них полностью прозрачная технология.

нам только нужно будет указать все сервера кластера


root@test-kub-02:~#  nsenter --net=/proc/3148/ns/net ip -c -f  inet address show eth0
3: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default  link-netnsid 0
    inet 10.252.1.4/24 brd 10.252.1.255 scope global eth0
       valid_lft forever preferred_lft forever


возможно поды невиын на другх хостах
изза неактивации форвардинга.
а также надо в свойстваох iptables  в  правильных местах
поставить ACCEPT

вопрос - если кубернетес создает контейнеры через докер
то как он умудряетмся подключать контейенеры к сети друогго бриджа (не докер0 ) cni0 
и при этом cni0 его нет в docker networks

список какие порты сидят на бриждами

test-kub-02:~# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.5efab67e726a       no              veth6b8aeca2
                                                        vethde173048
docker0         8000.0242f0b9d9c1       no              veth45c266d


также в свойствах линка указано за каким бридждлм он сидит


7: vethde173048@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 46:98:0c:b8:67:d6 brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::4498:cff:feb8:67d6/64 scope link

master cni0

       valid_lft forever preferred_lft forever
9: veth45c266d@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default
    link/ether fa:24:30:ee:84:a1 brd ff:ff:ff:ff:ff:ff link-netnsid 2
    inet6 fe80::f824:30ff:feee:84a1/64 scope link

master docker0

containerd-shim  = это какой то спец процесс к каждому контейнеру что бы чтото там удобнее работало
на уровне процессов. надо уточнять


kubectl exec -ti kube-system etcd-test-kub-01 -- ps aux


для начала научиться чтоб связка работала докер+ фланнель
и тока потом пытться понять связку кубер+фланнель+докер

стаивм etcd
пототом фланнель



фланнель+докер заработал(прозрачная связь между подами между серверами 
работате)


надо записать всю установку и осознать как это работает от и до.
----------
чтобы vpxlan заработал надо такие вещи
соззать vpxlan интерфейс
в нем указать local ip . это адрес внешней карточки сервера.
он будет служить как src_ip когда исходные пакеты будут оборачиваться в
новый ip пакет.
надо указать адреса соседей которые тоже имеют vpxlan интерфейс.
указываются они либо через параметр remote в ip lab но это работает тлоько
если у нас vpxaln сосед один (их еще зовут vtep сосед)
если их несколько то надо указать сосейде через 

bridge fdb appnd 00:00:00...:00 ip-адрес-соседа

и нужно каким то образом заставить трафик втекать в vpxlan интерфейс.
например с помощью маршрута в табблице маршрутизации.например

10.252.2.0/24  dev vpxlan-if 

поток будет входит в vpxlan-if и оттуда прозрачно транзитироваться в выходную
физ карту ens160 и вылетатть в сеть преобразованный

настривать какую то L2 связь между vpxlan интерфейсом и физ интерфейсом ненужно


насколько я понимаю процесс выгдлядит так 
скажем до входа в vpxlan интерфейс пакет выглядит как

dst_ip=10.252.2.10 src_ip=10.252.2.1

когда поток попадает в vpxlan на него сверху одвется новый ip пакет с
dst_ip=(внешний ip адрес соседа)=192.168.1.20
src_ip=(внешний ip адрес нашего сервера)=192.168.1.10
внутри этого нвоого пакета сидит UDP с dest_port=4872


потом этот пакет просто всовывается во внешнюю физ карту которая с ним 
ничего неделает кроме как тупо пропускает через себя и выплевывает 
наружу в физ сеть.


когда пакет долетает до соседского компьютера он входит в физ карту.
комп вскрывает ip пакет видит внутри UDP с портом 4872 и он ничего неделает
с этим пакетом кроме как он тупо его транзитирует передает на внутреннюю
vpxlan карту.

внутреняя карта вскрывает внешний пакет и видит внутри пакет с
dst_ip=10.252.2.10 src_ip=10.252.2.1 
и она передает этот пакет уже внуьрь кишков компа

если мы направляеи поток на vpxlan карту и хотим указать 
гейтвей на стороне соседа то гейтвеемм можеть быть только походу
ip адрес vpxlan карты соседа.
хоть vpxlan карат и делает преобразование ip адресов когда поток проходит 
сквозь нее но она в тоже время об этом сама незнает. поэтому не может работать  в плане маршрутизаии с внешними ip сетями компа а толко внутренними
сетями компа. как то думаю так

если у нас поток вставляется в vpxlan карту с помощью свича то 
маршрут который бы вставлял поток в vpxlan карту писать ненужно. и 
тогда сама vpxlan карта необязана иметь IP адрес. пример


veth0(172.16.0.10)->veth1 L2-> bridge brctl0-> vpxlan -> физ карта(192.168.1.10) -> сеть->
->физ картадруого компа(192.168.1.2) -> vpxlan ->bridge->veth L2-> veth(172.16.0.2) 

чтоюы попасть с 172.16.10 на 172.16.0.2

нам достаточно иметь на исходном компе маршрут

172.16.0.0\24 dev veth0 src 172.16.0.10

как видно персонализированного маршрута вида 
IP dev vpxlan
который специально вставляет поток в карту vpxlan ненужно
получается поток в vpxlan не маршрутизируется а коммутируется.

посмотрим другой вариант.

в рамках свичинга в рамках L2 домена в компе есть такой тракт
контейнер veth0(172.16.0.10)---> veth1---> бридж docker0 (172.16.0.1) 

и отдельно в компе болтается вот такое
vpxlan (172.20.0.1) который ни с кем несвязан L2 доменом
отдельно также
физ карта ens160 (192.168.0.10) -> сеть -> физ карт другго компа ->vpxlan(172.20.0.10)

как мы уже знаем vpxlan несвязан L2 доменом или маршрутом с ens160
но каким то магическим образом пакеты входящие в vpxlan транзитируются
на ens160 и он их транзитиррует в сеть

как я понимаю если на компе ip форвардинг выключен то тогда 
пакеты с src_ip=172.16.0.10 dst=172.20.0.10 и гейтвей = 172.16.0.1
они дойдут до гейтвея и все . потому что дальше надо 
перебросить пакет с интерфейса docker0 на vpxlan 
а это есть форвардинг а он выключен

форвардинг это когда ядро перебрасывает пакет с карточки на карточку
которая не входит в dest сеть пакета

\\разобраться как работает форвардоинг на компе из 2 карт.
ест ли особая стрка маршрута связывающая две карты друг с другом

\\ расмотеть случай когда две карты каждя в одной сети
192ю168ю1ю10
192ю168ю1ю100
явяется бриджом то есть по обе стороны сеть 192ю168ю1ю0
и посмтреть при каком условии может комп с одной стороны пингвать 
комп с другойс стороны
посмотреть при прохождении компа его МАК фрейм адреса менаяются или нет

значит я сделал комп1 имеет две карты 
которые принадлежат одной IP сети

комп1 
ens160 = 172.16.102.20  
ens192 = 172.16.102.22

при этом ens160 воткнут в свич LAN
а ens190 воткнут в другой свич

в другой свич также воткнут комп2
ens160 = 172.16.102.21


lan switch - ens160 (комп1)  ens 192 -  another swicth - ens160(комп2)

карта второго компа такжепринадлежит той же IP сети.

что я делаю.

1) с компы 2 я пингую 172.16.102.1
при этом на компе 1 на ens192 я фиксирую arp запросы 
мол какой mac имеет комп 172.16.102.1
при этом на компе 1 на ens160 я этого запроса невижу.
тоесть по дефолту линукс нефорвардит фреймы с карточки на карточку.

и я понима сейчас почему.
по дефолту в линуксе порты имеют роль L3.
то есть это порты рутера. и неважно что два порта принадлежат
одной IP сети и неважно что два порта принадлежат одлному вилану.
главное что порты уровня L3 как у рутера. а порты L3 у рутера 
всегда работают вот как - в порт влетает фрейм. 
он сдирает обертку с фрейма и остается голый L3 пакет.

и прикол в том что данный пакет будет направлен в другой порт рутера
только если есть маршрут для этого пакета направляющий его в друго порт.

поэтому ARP влетая в порт нифига больше никуда невылетает.
развернуто все проанализируем. почему ARP хрень непередается на другой 
порт линукса.
что такое локальная сеть. 
что такое рутер. 
что такое порт такого то уровня
что такое протокол такого то уровня

локальная сеть - есть сетевые порты(сетевые карточки). каждая карточка 
имеет адрес. адрес именно карточки. его называют MAC адрес.условно говорят она его получает на 
заводе и его нельзя сменить. да щас это можно суть главного тут в том что
адрес адресует именно карточку устройство. далее мы берем группу таких 
портов и обьединяем их через некоторую проводящую среду. и карточки могут
друг  с другом связываться адресовываться на основе вот этих вот своих 
адресов карточных. это и есть локальная сеть.
получается если есть один порт с адресом и другой порт с адресом
и они немогут друг с другом связаться через свои адреса - значит 
они сидят уже в разных локальных сетях.
когда порт может связываться с другим портом используя свой карточный адрес
это назыавется связь на уровне L2 osi. и порты имеют уровень L2.
также это значит что софт который сидит который обеспечивает работу портов
он умеет обрабатывать поток на уровне L2, это значит что софт воспринимает 
поток как кусочки - фреймы. в которых обязан присуствовать как я понимаю
по крайней мере один MAC. (а чаще и два отправителя и получателя).
и вот порты они оперируют этими фреймами. они их получают, отправляют 
и переправляют с порта на порт. каждый фрейм идентифицируется наличием
MAC адреса. соотсвтеенно фрейм имеет формат вида MAC адрес часть еще 
какойто служебной информации и полезная нагрузка. софт который умеет
рабоать с такого рода фреймами - получать отправлять перенаправлять назщыается софт уровня L2. 
когда мы говорим про поток на уровне L2. это значит что поток состоит
из фреймов. каждый фрейм был испущен каким то портом и предназначен 
какому то порту. когда фрейм влетает в порт уровня L2 значит софт порта
понимает поток на уровне фреймов и задача софта  принять этот фрейм либо
игнорировать либо принять и переправить в другой порт.  это и есть основная работа и задача порта уровня L2 и его софта.
порт уровня L2 это сама железная часть и ее софт.

протокол уровня L2 это значит такой софт который помогает настроить 
передачу данных в локальной сети между портами. то есть задача софта уровня
L2 чтобы локальная сеть работала. он ей в этом помогает. главные тут слова локальная и работала. протокол уровня L2 также понимает что поток разбит на  фреймы то есть на  кусочки идентифицируемые наличием MAС адреса и полезных данных. 

протокол ARP относится к протолку уровня L2. он помогает найти в локальной 
сети найти адрес то есть MAC некоего порта который обладает специфицическими
характеристиками. мы называем ARP протоколу характеристику что мы ищем
а он нам возвращает MAC адрес такого порта. в локальной сети. 

порт уровня L3 автоматом также является портом L2.
потому что L3 использует функционал L2 как бекенд.

когда в порт уровня L3 влетает поток то софт порта вначале распознает в нем 
отдельные фреймы (уровень L2) берет этот фрейм. вскрывает его  и передает
софту уровня L3. софт L3 нахоит в переданной информации информацию уровня L3.
для софт уровня L2 эта информация просто набор бессмысленный нулей и единиц
как для русского англиская речь это просто набор звуков.

протокол уровня L3 позволяет вкладывать внутрь фреймов уровня L2 некую
полезную информацию которая позволяет обмениваться данными портам 
между локальными сетями. ключевое слово между.

если у нас коробка в которой два порта уровня L3 то 
формально у нас за каждым портом лежит своя локальная сеть 
а эта коробка нам должна позволить связать эти две локальные сети.

рутер это корробка у которой есть минимум два порта уровня L3.
и цель этой коробки обеспечить связь локальных сетей лежащих за 
этими портами. потому что напрямую то есть через MAC адрес порта в одной
сети никак не связать с портом который имеет MAC2 адрес в другой локальной
сети. рутер получается оновременно принаддлежит нескольким локальным сетям.

фишка софта L3 и протоколов уровня L3 что они позволяют назначить портам
адреса уровня L3 которые позволяют адресовать эти порты несмотря на то что
они лежат в разных L2 сетях. с точки зрения L3 при этом порты например
могут принадлежать одной L3 сети. на этом фишки уровня L3 конечно
не заканчиыаются 
 
если мы берем коробку и она имеет сетевые порты и эти порты типа заявлено
что они уровня L2 это значит что 
1) данные порты предназначены для связи с другими устройствами в рамках одной
локальной сети
2) данные порты могут обрабатывать поток на уровне фреймов основываясь 
на MAC адресах. основываясь на них они посылают поступающие фреймы в тот 
или иной порт или во все порты
основная задача коробки рассылать входящий поток в порты в те или иные
порты

в чем еще прикол рутера и его L3 портов.
когда фрейм из одной сети попадает внутрь рутера.
он вытаскивает данные из фрейма а служебную част фрейма выкидыает в мусор.
в том числе и MAC информацию содержащуюся во фрейме так как за его портами
лежат независимые локальные сети и MAC информация фрейма из одной сети
непригодна ни вкаком виде для адресации фрейма в другой сети.
далее рутер упаковывает извлеченную информацию в новый фрейм с новым MAC 
актуальным для другой сети и выплевывает новый фрейм с исходным payload
из старого фрейма в другую локальную сеть из другого своего порта. 
также рутер имеет понимание предназначалась ли информция из payload
для другой сети. если нет то он данные из этого фрейма никуда пересылать
небудет.

возвращаюсь к ARP и линуксу.
линукс обладает портами L3.
это значит что за каждой карточки формально сидит своя независимая 
L2 сеть. которые он обьединяет находясь одновременно в обоих. принадлежа обоим. в него через ens192 влетает фрейм внутри которого находится ARP payload. ARP как я уже сказал предназначен для поиска некоей  заданной 
сетевой карты в рамках локальной сети. поэтому ARP запрос не имеет 
никакого отношения ко второй локальной сети лежащей за ens160 за втрой картой.
поэтому ARP запросы по дефолту линуксом не перекидываются из ens192 на ens160.
они к сети ens160 не имеют никаого отношения.

напомню конфиг что выше

комп1 
ens160 = 172.16.102.20  
ens192 = 172.16.102.22

при этом ens160 воткнут в свич LAN
а ens190 воткнут в другой свич

в другой свич также воткнут комп2
ens160 = 172.16.102.21


lan switch - ens160 (комп1)  ens 192 -  another swicth - ens160(комп2)

у нас ens160 компа1 и ens160 компа2 хотя принадлежат одной IP сети ( сети уровня 3) но разным локальным сетям уровня L2 поэтому 
по дефолту связь между этими портами неработает.

чтобы она заработала нужно либо на компе 1 два порта уровня L3 
превратит в два порта уровня только L2 тогда это будет означать
что сети за этими портами являются одной L2 сетью. и тогда ARP запрос
от компа 2 пройдет сквозь комп1.
ибо я с компа2 пытаюсь пинговать комп2 условно говоря который сидит за компом1.

еще вариант нужно на компе2 и компе 3 прописать маршрут.
вопрос что делает линукс когда получает пакет у котрого ip 
принадлежит сети которая у этого линукса лежит не за 
одним портом а за несколькими.
также понятно что нужно будет активиать форвардинг на линуксе.
то есть передачу пакетов между портами линукса которые не предназначены
самому этому хосту линукса. я проверил этот вариант работает.



\\важные моменты за L3 портами лежат по орпделению разные L2 сети.
поэтому фреймы внутри которых лежат L2 протоколы не форвардятся не 
транзитятся между L3 портами
L2 протокол это протокол обеспечивающий или помогающий работаь передавать
данные между портами внутри локальной сети. фреймы с запросами в payload от протокола L2 одной локальной сети нельзя выпускать в другую локальную сеть так как они к ней неимеют никакого отношения. фреймы с запроами в payload от протокола L2 имеют отношение только к данной локальной сети. вне ее они не имеюи никакого смысла.типа никакого права там оказаться\\

 в целом это рельная плохая практика иметь два и более
 L3 интерфейсов на рутере из одной IP сети.
 потому что например когда пакет влетел в рутер и его надо выплюнуть
 в сеть от которой у нас на рутере два порта то непонятно из какого 
 порта выплевывать. скорее всего линукс возьмет первый в списке из двух.
 и через него плюнет. выяснят из какого он будет плевать я не буду
 ибо как сказал это неправильная конфтигурация рутера когда 
 два L3 порта сидят в одной ip сети. должно быть каждый L3 интерфейс
 в другой уникальнрой ip сети.
 а если надо несколько портов засунуть в одну L2 сеть к примеруто порты надо 
 обьединять в бридж бондинг или типа того.
 опять же это совсем другая ситацяи когда у нас 1 порт имеет несколько 
 ip из одной сети. потому что в этом случае нет неоднозначности. а есть 
 полная однозначность что за таким то портом  такая то сеть. 
 а за другим портом другая сеть.
 
 тогда по мне по хорошему если пакет влетел в vxlan интерфейс то 
 для того чтобы он после этого вылетел из ens160 надо чтобы на 
 линуксе был во первых маршрут для этого во вторых активирован
 форвардинг. потому что перебрасывание транзитно пакетов с интерфейса
 на интерфейс это точно форвардинг. а в лиунксе это работает "просто так"
 как кастомное правило. исключеие.
 
 
 








 



---------------



----------------------------
фланнель + кубернетес.

для начала я пытаюсь понять откуда фланнель
читает настройки из кубернетеса

в тупорылой документации от фланнеля они пишут
если фланнель запущен с параметром --kube-subnet-mgr то фланнель читает 
конфиг из своего локального файла /etc/kube-flannel/net-conf.json

если этот параметр при запуске демона фланнеля незадан то фланнель
читает параметры из etcd из его ключа /coreos.com/network/config
опять же если при старте фланнеля указать ключ --etcd-prefix то он будет 
читать из etcd из другого пути.

смотрим как кубернетес запустил фланнель

3983 это pid контейнера фланнеля на мастер ноде

# nsenter --mount=/proc/3983/ns/mnt ps aux
PID   USER     TIME  COMMAND
    1 root      3:32 /opt/bin/flanneld --ip-masq --kube-subnet-mgr

итак фланнель запущен с ключем --kube-subnet-mgr
значит он читает свой конфиг из файла /etc/kube-flannel/net-conf.json
смотрим что в этом файле

# nsenter --mount=/proc/3983/ns/mnt cat /etc/kube-flannel/net-conf.json
{
  "Network": "10.244.0.0/16",
  "Backend": {
    "Type": "vxlan"
  }
}

щас я не буду подробно описыать но этот файл понятное дело проброшен
с файловой системы хоста

теперь посмотрим в итоге какие параметры использует фланнель по факту 
в работе

/# nsenter --mount=/proc/3983/ns/mnt cat /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.252.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true

и тут я вижу совершенно непонятную херню.
что хотя ему задано чтобы он как сеть  использовал 10.244.0.0/16
он в качестве подсетей для хостов использует совершенно другую подсеть
10.252.0.1/24

совершенно ясно что 10.252.0.1/24 не является подсетью.
так что за херня? фланнелю указали какую сеть использовать 
а он откуда то с воздуха взял другую сеть и начал ее использовать.

кстати еще раз замечу что если ключ --kube-subnet-mgr неуказан 
то фланнель ищет конфиг на etcd. а если указан то на своей локальной
ФС. так вот у нас не указан значит он конфиг свой ищет на ФС.
так вот я сделал свою мануальную инсталляцию фланнеля на отдельной 
системе и там фланнель использует etcd для считывания конфига 
так вот при этом фланнель нетолько читает из etcd но и пишет туда данные.
вот какая фланнелю была задана сеть изначлаьно там
~# etcdctl get /coreos.com/network/config
{ "Network": "10.5.0.0/16", "Backend": {"Type": "vxlan"}}

а вот что фланнель потом уже сам дописал
# etcdctl ls /coreos.com/network/subnets
/coreos.com/network/subnets/10.5.29.0-24
/coreos.com/network/subnets/10.5.98.0-24

тоесть фланнель дописал какие подсети этой сети он будет использовать.

при этом еще замечу что фланнель написал что будет использовать две 
подсети но почему то тут он указывает только одну из них

~# cat /var/run/flannel/subnet.env
FLANNEL_NETWORK=10.5.0.0/16
FLANNEL_SUBNET=10.5.29.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=false

фланнельо однозначно имеет дебильную неполную документацию. козлы
получается что sybnet.env неотображает все подсети которые фланнель
использует а только одну.


в случае же кубернетеса еще раз скажу фланнель запущен так 
чтобы он читал данные слокальной фс а не из etcd

это еще можно доказать и так что в кубернетесовом etcd 
нет никакого ключа по пути /coreos.com/network/config
и фланнель в контенере запущен без ключом --etcd-prefix который бы мог 
переопределить путь по которому читать конфиг из etcd кубернетеса.

витоге остается загадкой вот как так фланнель считал конфиг с сетью
но по факту использует другую подсеть
# nsenter --mount=/proc/3983/ns/mnt cat /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.252.0.1/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
 

откуда вцелом взялась сеть  10.252.0.1/24 понятно. я ее указал 
при создании мастер ноды 

# kubeadm init  --pod-network-cidr=10.252.0.0/16 --apiserver-advertise-address=172.16.102.31

но опять же я задал маску \16 а в фланнель написано \24
все указывает на то что фланнель использует FLANNEL_NETWORK=10.252.0.0/16
но абсолютно непонятно как из kubeadm параметр попал во фланнель

далее я полез в etcd кубернетеса 
и стал там искать эту сеть в какой переменной она прописана

прежде надо сказать что данные в etcd кубернетес 
хранит в некоем хитрожопом формате и нужно поставить прогу которая
позволит их декодировать.

# mkdir /tmp/1
# git clone https://github.com/jpbetz/auger
# cd auger
# make release
# cp ./release/build/auger /usr/local/bin
все декодировщик поставили

далее уже можно читать из etcd  на мастере

# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /registry/clusterrolebindings/flannel --prefix -w simple | auger decode


получить список всех ключей

# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /  --prefix --keys-only

далее. у меня не получилось с помощью auger декодировать сразу 
весь список переменных. 
получается только запрашивать конктерный ключ и смотреть его значение

далее. методом тыка я нашел где в etcd хранится 10.244.0.0/16 которые
фланнель читает но неиспользует

/registry/configmaps/kube-system/kube-flannel-cfg


# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /registry/configmaps/kube-system/kube-flannel-cfg  --prefix | auger decode

apiVersion: v1
data:
...
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
....

ну и вобщем 10.252.0.0/16 мы вбиваем в kubeadm.
я нашел где в кубадм хранится эта цифра

/# nsenter  --mount=/proc/3042/ns/mnt   /usr/local/bin/etcdctl --endpoints=https://localhost:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key   get /registry/configmaps/kube-system/kubeadm-config --prefix | auger decode

...
podSubnet: 10.252.0.0/16
...


в общем в итоге соверщенно непонятно откуда фланнель получает данные
чтоб пользовать сеть 10.252.0.0\16

но по факту на серверах в итоге именно фланнель создаеть
 интерфейс flannel.1
вот такой

на мастере
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP>
       inet 10.252.0.0/32 brd 10.252.0.0 scope global flannel.1

на дата ноде
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP>
       inet 10.252.1.0/32 brd 10.252.1.0 scope global flannel.1


а далее (это уже нефланнель а сам кубернетес) создается 
еще интерфейс cni0

на мастере
5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> 
    inet 10.252.0.1/24 brd 10.252.0.255 scope global cni0


на дата ноде
5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP>
    inet 10.252.1.1/24 brd 10.252.1.255 scope global cni0

это бридж интерфейс.
и именно к нему приконнекчены veth-ы от контейнеров.

таким образом на мастер ноде контейнеры созданные кубернетесом 
имеют ip из сети 10.252.0.0\24

контейнера созданные кубернетосом на дата ноде имеют ip из
сети  10.252.1.0\24

(почему я особо подчеркивают контейнера созданные кубернетесом потому что 
если на мастер ноде или дата ноде создать контейнер с помощью докера
то его контейнеры будут сидеть в другой сети за бриджм docker0 и 
будут иметь еще раз как я сказал другие ip из другой сети. но об этом потом)

так вот как работает проход пакета из контейнера на дата ноде 
к контейнеру на мастер ноде ( вы скажете на мастер ноде неожет быть контейнеров - нет это брехня там неможет быть пользовательских контейнеров
их туда куб недеплоит но на мастере прекрасно сидят системные контейнеры)

так вот как работает проход пакета с дата ноды в контенер на мастер ноде
например надата ноде сидит контейнер с 
ip(дата) =  10.252.1.8
а на мастере есть контейнер с ip
ip(мастер) =  10.252.0.21


сетевая карта контейнера if(дата) -> veth -> cni0  (это все идет
коммутация) дальше пакет попадаетв ядро.

и в ядре ядро смотритв таблицу маршрутизации а
там видит маршрут

10.252.0.0/24 via 10.252.0.0 dev flannel.1 onlink

который говорит трафик идущий к 10.252.0.0\24 пихай в карточку dev flannel.1

NAT в меконтейнрорной связи неучастует ! этого требует 
техзадание от куба

для передачи пингов от нашего контейнера до контейнера
в соседнем хосте на flannel.1 нужен IP  то есть нам недостаточно
L2 flannel.1 нам нужен именно L3 flannel.1 с IP.

щас разбереем почему

но прикол в том что на flannel.1 дебильный IP

почему он дебильный потому что он = 10.252.1.0/32

щас мы это тоже обсьудим

значит что нужно обязательно чтобы была связь
1) на flannel.1 должен быть IP
2) должен быть правильный маршрут
3) на компе должен быть прописан mac адрес от flannel.1 от 
соседнего сервера куда будем пинговать
4) еще раз отмечу что NAT никак ненужен никак неучаствует в процессе. ура!


если хотя бы один пункт отстуствует то связи между подами неработает

пример
интерфейс с дебильным IP
# ip -c a s dev flannel.1
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP>
    link/ether 62:33:33:4a:27:86 brd ff:ff:ff:ff:ff:ff
    inet 10.252.1.0/32 brd 10.252.1.0 scope global flannel.1

маршрут    
# ip r l dev flannel.1
10.252.0.0/24 via 10.252.0.0 onlink
гейтвеем указан дебильный ip адрес flanneля соседнего сервера.
мало того что тут указан дебильный ip адрес.
так еще важно то что 
у одного соседа flannel.1=10.252.0.0/32
у второго соседа flannel.1=10.252.2.0/32

комп а
контейнер ip = A

комп б
контейнер ip = б

контейнер б  видит пинги от контейнара а как src_ip= A
это понятно птому что котейнер А имеет одну карту и он посылает пинг с нее
поэтому когда пакет долетает до falnnel.1 то у пактеа у же есть src_ip
поэтму линуксу ненужно решать как src ip посдавтиь в пакет.
 если же мы пингуем контейнер на сосденем сревререерер просто из консоли 
баша на сервререе то линуксу надо решить а как src ip подставить. так вот 
линукс пощствляелт src_ip=адрес flannel.1 нащшего сервера.
поэтому контейнер б увидит пинг с консоли баш с src_ip=10.252.1.0
.. да уж тот самый деьильный ip адрес.

 
 вопрос вот в чем енпонятно как работает связь со вторым сервером 
 сосдеом
 
 у второго соседа flannel.1=10.252.2.0/32
 
 у нас интерфей фланнеля невставлен в бридж поэтому в него поток 
 нужно засоывать чреез маршрут но это еще пол дела 
 нужно в маршруте указывать гейтвей. потому что 
 наодном хосте поды сидят в 10.252.0.0\24
 на втором хосте поды сидят в 10.252.1.0\24
 на третьртье хосте поды содтят в 10.252.2.0\24
 поэтому чтобы пакеты ходили между этими подами где то на пути 
 нужна маршрутизация. поэтому в маршрутек flannel.1 нужно указать гейтвей.
 
 тепреь вопрос .... аааа я понял
 
 для кажого соседа свой маршрут
 
 к одному соседу
 10.252.0.0/24 via 10.252.0.0 onlink

ко второйму сосдеу
10.252.2.0/24 via 10.252.2.0 dev flannel.1 onlink 
 
 сотвтетсвенно получается что гейтвеем как я уже сказал указан 
 ip от flannel.1 на соседе.
 и надо еще прописать для кждого такого гейтвея статистческий MAC
 
 
 вот так одбдодлвятеся стат MAc адрес
 
 # ip neighbor add 10.252.2.0 dev flannel.1 lladdr 4e:4e:0c:ee:ca:3b
 
 а чрез arp команду так нифига несделать
 
 
 итак сетевой тракт от одного пода на первом мерверере до дроугог
 пода на другом сереререр выгляждит так
 
 iptables в этом неучаствует никак и это отлично
 
 еще замечу что vxplan - vxplan между среверами канал 
 это канал уровня L2 в целом поэтому весь L2 трафик L2 прротоколы запросы
 от одного сервера прилетят на другой сервер. типа vpxlan это вирт свитч
 между серверами. 
 но конкретно в нашем случае kuberntes  делает vpxlan канал как L3.
 это значит что между серверами пролетает только L3 трафик но не L2.
 тоесть только содержимое IP пакета долетает от сервера до сервреа.
 L2 трафик непролетает. это птому что flannel.1 (vpxlan интерфейс 
 на серверах невсталвен нив какой свич. поэтому трафик на них нужно рутить
 а не коммутировать)
 
 значит что я выяснил.
 
 если я удалил ip с flannel.1 то удаляется и маршрут и 
 стат мак адрес для соседа.
 
 так вот как нужно это воостатанавливать.
 
 первое = надо назначить ip для flannel.1
 второе  = нужно прописатат стат ip адрес от flannel.1 соседа
 третье = прописать маршрут  к flannel.1 соседа
 
 что существенно если поменять пункты три и два
 то после доавбдлениямаршррута кога будешь прбовать происать стат MAC 
 система выдаст ошибку.
 
 поэтому важно вначале пррописать стат MAC адрес. и толко потом 
 прописывтаь маршрут.
 
 также я проверил если прописать маршрут и не прописать стат мак адрес
 то ичего работать небудет. система сама неможет найти MAC адрес 
 соседа.
 
 
 так вот как выгляидит тракт от пода до пода
 
 карта пода L3 (src 10.252.1.11/24 dst 10.252.2.3)-> vethL2-> cni0 ->  
 
 самое прикольно что сеть на поде \24
 так что когда он стучит на 10.252.2.3 тодля него это нелокльая сеть
 а другая сеть в которую он лезет чере прописанный на поде 
 маршррут
 
 default via 10.252.1.1 dev eth0
 
 10.252.1.1 = это адрес cni0
 
 а я то думал что на всех подах сеть \16 и под лезет через коммутацию
 безгейтвея думая что тот под лежит с ним водной LAn сети
 
 ан нет.
 
 получаетс поды сидят в одной сети только в рамкх сервера.
 а на другом сервере побы в другой сети.
 
 едиснвтенное в чем есть плюс это то что в связи неучаствует iptables и\или NAT
 
 
 пакет долетает до cni0 как гейтвей. 
 далее пакет оказывется в ядре линукса хоста
 
 хост смотрим в таблицу рутов
 и находит
 10.252.2.0/24 via 10.252.2.0 dev flannel.1 onlink
  

ядро пихает пакет в flanel.1  
одевает его во фрейм в котором dst MAC=MAC карты фланннеля на той стороне
ибо он указан гейтвеем в маршруте

далее flannel.1 одевает сверху еще один IP пакет в котром
dst_ip= внешний IP соседа, src_ip= внешний ip нашего сервера

и пакет пихается в ens160 а тот его без имзенееий просто выплеквыет в сеть

рисую

->cni0->ядро ->flannel.1->ens160-> сеть -> ens160 того компа -> flannel.1

на том компе пакет прилает и ens160 его тупо передает на flannel.1
 
 flannel.1 того компа снимает внешний ip пакет
 и видит что 
 
 dst_ip=пода src_ip=пода нашего компа dst_MAC=MAC фланнель
 
 далее на том компе ядро смотрит маршрут
 
 10.252.2.0/24 dev cni0  proto kernel  scope link  src 10.252.2.1
 
 тоесть ядро видит что этот пакет нужно всунуть в cno0
 рисуюб а тот его передает через коммутацию в под
  
 ->flannel.1->cni0-> pod
 
 походу наконец выяснено как устроена бекенд связи в кубернетесе 
 между подами
 
 поучется это классическая марушттизация, софт бриджи, vpxlan, стат MAC
 
 готово
 
 
 
 -----
 бльше всего выывает вопрос зачем фланнель.1 было давать IP такой страный и урщдодский а не обычный IP unicast
 
 прчием все эти 
 10.252.0.0
 10.252.1.0
 10.252.1.0
 

они  спокойно пиннгуются линуксом

# ping 10.252.0.0
# ping 10.252.1.1
# ping 10.252.3.0

как вообще линукс такоедопускаетт ??
============

далее нужно понять откуда берутся IP для сервисов и деплойментов
==
следущий этап попробвать устанвоить эалстик
которая упирется в helm и persiustent volume

===




статический MAC от flannel.1 на том конце (у соседнего сервера)
# ip neighbor show
10.252.0.0 dev flannel.1 lladdr 0e:d8:dc:0e:7c:cb PERMANENT


arp команду нужно забыть. она старая тупая. она многочего неоможет
она имеет дебилное описание вместо нее испольуем ip neigh....


удаляем его потом ыптвется восстановить связь


arp кэш на компе удаляется совсем не компндйр arp 
а

ip -s -s neigh flush all

--||
 ip route add 10.252.0.0/24 via 10.252.0.0 dev flannel.1 onlink

~# arp
Address     HWtype  HWaddress           Flags Mask       Iface
10.252.0.0  ether   0e:d8:dc:0e:7c:cb    CM              flannel.1



настроить самому руками связь между 3 серверами через vpxlan без фланнеля
с использованием дебильным IP адресов для vpxlan интерфейсов.
убедиться что сам по себе сервис фланнеля не играет никакой роли











фланнель мы ставим на мастер 
а потом куб сам ставит фланнель на дата ноды коогда мы их джойним
к кубу
таким образом фланнель пристуствует на всех нодах

как работает фланнель.
настройки он читает из etcd
там всего лищь настройки

из настроек он считывает вобщемто только сеть

(считать ее из живого etcd кубернетеса)

обычно там есть с префиксом 16

далее фланнель для каждого хоста из этой сети выделяет подлсеть размером \24
пример

далее фланнель на хосте создает vpxlan интерфейс с названием flannel.1

в этом интерфейсе он указыать local IP , транзитный dev
и с помощью bridge fdb фланнель прописывает адрес удаленных vtep 
соседей

странный IP на flannel,1

и посмтреть как пакеты направлвяются в фланнель


-
IP сети для сервисов и деплойментов
--

 (кстати прикол если контейнер создан через сам docker run то связь с ним
идет через процесс docker-proxy тоесть по другому чем когда контейнер
созда кубом разобрать потом)

===
