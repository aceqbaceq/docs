прежде всего напонять что такое бридж.

для начала вспомним как работает свич.

у нас есть комп А с одним сетевым портом А
у нас есть комп Б с сет портом Б

порт А имеет мак адрес и он шлет в сеть
фрейм у которого | src МАК-А | dest МАК-Б

фрейм прилетает в порт свича.
и тут очень важно скзаать что порт свича неимеет свой собственный мак
адрес. а если и имеет это в процессе передачи этого фрейма неучаствует 
вообще! разве это не шок? мы имеем сетевой порт на железке но 
этот порт не имеет или по крайней мере вообще неучаствует в процессе
передачи фрейма.
клмпьютер а ничего незнает о свиче и его портах никак это неисопльзует
при подгтовке фрейма при отправке и тому подобное.

фрейм засосался внуьрь свича. если свич уже знает за каким портом 
сидит комп Б то он этот фрейм выплюнет без изменений ровно как он 
и пришел от компа-А в этот порт. причем выходной порт опять же необязан
иметь никакой свой МАК адрес. разве это не шок? 
если свич незнает за каким портом сидит комп-Б то он выплюнет этот фрейм
во все порты свои.

важно тут увидеть что порты свича хоть и участвуют в процессе передачи фрейма
но они совершенно обезличены ибо не имеют или необязаны иметь свой
мак адрес.

это шок.

ведь мы привыкли что если порт эзернет то он обязан иметь мак адрес
чтобы участвоват в процессе передачи информации.

шок.

теперь таже предстваим что в какойто порт-1 свича-1 воткнут не конечный компьюер
а другой свич-2. это значит что в порт-1 свича-1 постоянно влетает 
не фреймы от одного компа у которого один src MAC а постоянно влетают
фреймы с кучей разных src MAC. 
о чем это говорит. это говорит о том что порт свича работает в режиме 
promiscious mode. он принимает фреймы с любыми src MAC. засасывает в себя все.

это тоже шок.
потому что это тоже совершенно отичается от работы эзернет порта на компе.

итак еще посмотрим на различия как работает эзернет порт на компе и свиче.

на компе порт обязан иметь МАК адрес.
на свиче это необязан

на компе порт принимает только фреймы преднезначенные для его мак адреса
на свиче порт принимает все фреймы с любыми мак адресами
(броадкастный трафик понятно что принимают и комп тоже)

на компе комп отправляет в сеть фреймы только с одним src MAC - своей карты
на свиче порт плюет в сеть фреймы с совершенно разными src mac адресами.

вот такая огромная разница. хотя и там и там эзернет порт.


получается если мы имеем на компе два эзернет порта 
и хотим заставить комп работать как свич то надо 
чтобы софт на компе умел переключать порты и использовать их 
в другом режиме в режиме свича а не врежиме компа.

то есть порт на компе должен начать принимать весь трафик (promiscou mode)
порт на компе должен начать отправлять в сеть фреймы с разными src MAC 
и плевать какой мак имеет сам порт. он вообще неважен и ненужен.
потому что если комп А отделен от компа Б свичом или свичами то они никак
неменяют src\dest MAC так что фрейм летит от компа А до компа Б через все 
свичи безизмененнно.

таким образом сразу видно что чтобы комп смог работать в режиме свича
это надо заствить его сетевые порты работать в совершенно
другом режиме чем сетевые порты обычного компа.

вот это очень важно было хотя просто осознать
придать этому занчение.е
обратить на это внимание.

какя понимаю бридж это свич на 2-4 порта.
и внутри он попроще устроен.
просто витоге маленький простой свич. это бридж.


значит если мы говорим о софтовом бридже.
то появляется некое непонятная хрень которой нет в физ бридже.

а именно. в линуксе среди реальных физ портов появляется 
+1 вирт эзернет порт.

захрена это сделано непонятно.
ибо  в нем нет необходимости.

в реальном бридже скажем есть только два порта один 
в одну сеть второй во вторуб сеть и никаких доп портов специального
назначения в физ бридже нет.
поэтому захрена было в линуксе лепить этот порт непонятно.
в нем нет реального физ смысла.   


далее опять же обращу внимание что когда мы в линуксе 
обявляем тот или иной физ порт частью бриджа то этот порт необязан 
иметь ни мак адрес ни IP адрес. они также как на свиче 
абсолютно этим не будут пользоваться. это чисто физ безликие 
порты без всяких опознавательых знаков. в моем понимании.

слово добавить порт в бридж опять же я считаю сверх неудачным.
я бы сказал обьявить порт переклчит порт в режиме бриджа. вот так.
так вот в линуксе когда мы переключаем порт в режим бриджа и если порт
имеет назначенный IP адрес то либо будет ошибка либо автоматом у порта будет
стерт ip адрес. так вот я считаю что кроме этого надо автоматом удалять и
мак адрес на порту. тогда будет все по фэншуй.  если уж делать все 
как надо то доконца. но мак адрес на порту остается хотя смысла в нем 
никакого нет. порты на свичах не имеют своих индивиудальных 
мак адресов. едиснвтенное насколько я понял если мы активиурем STP
то он требует вот только я непонял толи чтобы весь свич имел один мак адрес
по которрому его можно идентифициаровать то ли чтобы порт каждый имел 
свой мак адрес. если же мы неиспольщуем STP то порты на свиче 
мак адреса иметь необязаны.
точка.

далее следущий важный момент для понимания.
как контейнерные сетевые карты прикрепляются к бриджу.


контейенер имеет сетевую карту уровня L3,
у нее есть IP и mac адрес.
эта шарманка сидит в своем сетевом неймспейсе.

[  сетевой неймспейс 1 ( сетевая карта контейнера MAC+IP ) ] 

эта сетевая карта связана с veth портом который сидит в другом сетевом 
неймспейсе , нашем обычном сетевом неймспейсе init. veth порт имеет мак 
хотя по мне он ему ненужен. как я понял все фреймы что он получает он 
неотбрасывает и неразбирает он их сразу посылает на карту контейнера.

так вот этот veth докер  переключает в режиме бриджа. то есть все что влетает 
в порт он пропускает через себя в обе стороны. то есть то что карта контейнера
из себя плюет и оно прилетает на veth он через себя пропускает дальше.
и то что снаружи прилетает в veth он через себя пропускает и плюет в карту.

за veth сидит ядро. то есть цепочка такая

ядро <-> veth <-> сет карта контейра (IP+MAC)

просто veth одном неймспейсе а карта контейнера в другом сет неймспейсе 
поэтому процесс контейнера видит только свою карту и не видит veth карту
а процесс init видит veth и невидит карту контейнера.

что происходит когда у нас несколько veth и может быть несколько 
реальных физ портов переключены в режим бриджа (или как они говорят 
подключены к софт бриджу).  как это выглядит

		|=
		|<-> eth0
ядро<->	|<-> veth1 - сет карта контейнера 1
		|<-> veth2 - сет карта контейнера 2
		|<-> eth1
		|=

что мы тогда имеем. 
1 каждая карта работает в promiscious mode и она всасывает в себя
абсолютно все фреймы что к ней долетают
2 всосав фрейм карта его тут же передает дальше через себя в ядро.

если в ядре есть уже понимание за какой картой сидит dest MAC 
фрейма то этот фрейм направляет конкретно в тот один порт.
а если такой информации нет то (внимание) фрейм отправляется НА ВСЕ ПОРТЫ
РАБОТАЮЩИЕ В РЕЖИМЕ БРИДЖА. другими словами фрейм флудится на все порты
бриджа. таким образом например если контейнер 1 направляет фрейм на 
контейнер 2 и ядро еще незнает что  сет карта контейнера 2 сидит 
за за veth2 портом то этот фрейм (внимание) полетит в том числе и на eth0
и на eth1 и те его выплюнут в реальные сети.
также (важно) броадкаст фрейм влетевший в один порт бриджа будет тутже 
зафлужен на все порты бриджа.

не все карты позволяют переключатся в режиме бриджинга.
например вай фай карты этого сделат ненадут. например они недают отправлять
в сеть фреймы с мак адресом отличным от мак адреса карты.

внутри линукса можно создать несколько бриджев.
таким образом одна группа портов в режиме бриджинга будет назависима
от другой группы портов в режиме бриджинга. 

при создании бриджа на линуксе создается порт brctl L2 
с мак аресом. еще раз какой смысл физический в нем - по моему его нет.
на фреймы пересылаемые он никак не влияет. 
его мак адрес тоже никому не всрался.
единственное что может быть это делается если мы будет использовать в этом
бридже еще и протокол STP. которым я не будут пользоваться.
опять же этому порту можно даже IP назначить. 
захера тоже непонятно.

важно конечно понимать что бридж это нетолько порты которые все пропускают
снаружи вовнутрь и изнутри наружу. это еще и софт который обрабатывет 
эти фреймы когда они влетели из порта внутрь ядра и этот софт решает
куда эти фреймы отправлять дальше.также как в физ коробке это есть.
то есть фрейм влетел из порта внутрь коробки а дальше там же коробка смотрит
есть у  нее информация за каким портом находистя dst mac и если есть то бридж
плюет фрейм невовсе порты свои а только в один.

порт <-> внутренности коробки <-> порт(ы)


вот как для меня сейчас выглядит бридж  линукса


eth0 ----  |
eth1 ----  |
veth0 ---- | ядро (софт) | ---- brctl0
veth ----  |

и получается что этот brctl0 он ни к селу ни к городу. как бы в него ниоткуда 
немогут никакие фреймы влететь потому что он ни к чему не подключен 
ни вылететь. ну окей влететь могут из ядра.но что дальше. лететь то некуда.

или что там другая схемп что фреймы от всех портов влетают в ядро через brctl0? так что ли?

eth0  <--->|
eth1  <--->| <-> brctl0 <-> | ядро
veth0 <--->|
veth1 <--->|


но это бред какойто. это представить что есть порты на свиче
и от них фреймы летят несразу внутрь железки а на какойто внутренний
невидимый снаружи но такойже по факту ethernet порт а уже из него внутрь 
железки. ну и получается что 10х1ГБ портов идут в 1 порт 1ГБ.
дебилизм какойто.

дальше.
представим мы имеем комп. на нем две физ карты

комп   
eth0 |---------> порт 1 свича  
eth1 |---------> порт 2 свича 
                 порт 3 свича <---------- другой комп
				 

мы их воткнули в обычный свич.
что будет если в свич в порт 3 от другого компа прилетает броадкастный фрейм. 
свич этот фрейм выплюнет во все порты (кроме 3). то есть этот 
фрейм полетит в eth0 и в eth1.

комп засосет эти фреймы и от eth0 и от eth1.
ну и поскрипит софтом и на этом все закончится.

другое дело если мы на компе включили eth0 и eth1 в наш
софт бридж.
когда фрейм влетит в eth0 то софт свича направит его во все остальные порты
свича. то есть он его выплюнет в eth1 обратно в сеть.

 также когда фрейм влеит в eth1 то он его засосет 
 и софт выплюнет пришедший фрейм во все остальные порты бриджа
 то есть в eth0. 
 
 когда обратно в физ свич прилетит броадкастный фрейм в порт1. то он его 
 выплюынет во все порты кроме порта1.
 тоже самое касается когда в физ свич прилетит обратно фрейм в порт2.
 он его выплюнет во все порты кроме порта2.
 таким образом видно что начнется адский ад. на всех портах
 ибо в отличие от IP пакета эзернет фреймы неимеют TTL.
 
 
 вернемся обратно к контейнерам.
 важно понимать что карты контейнеров неявляются портами бриджа.
 это абсолютный мисконепшн. портами бриджа являются veth порты.
 а уже они соединены с портами контейнеров.
 таким образом контейнерные порты имеют уровень L3 с IP
 а veth имеют уровень L2
 
 veth как быявляются теми физ портами на реальных свичах
 куда по проводам подключаются порты компов.
 
 порты компов это порты контейнерров. порты veth это физ порты свичей.
 
 порты veth неимеют IP адресов могут и мак неиметь. 
 порты контейров имеют MAC+IP
 
 |			  veth0|<--------->|if1 карта контейнера1	
 |софт бридж 	   | 
 |			  veth1|<--------->|if2 карта контейнера1


и тут мне кажется важно четко понимать разницу между портами 
которые составляют сам свич и портами которые воткнуты в эти порты.

между ними огромадная разница.


если ты добавляешь какойто порт в свич то ему это недает ничего.
он просто ставноится транзитным портом через который все просто пролетает туда и сюда. 

а вот если ты какойто порт подключаешь к порту бриджа то это уже дает.
дает то что наш порт теперь может связываться с другим портами подключенными 
к бриджу через L2 на уровне фреймов.


походу я понял что дает brctl. на уровне L2 когда у него только мак 
он недает ничего и никак. а вот когда ему присваивают IP адрес. 
то это позволяет наш свич превратить в рутер. вот походу его смысл.

картинка

комп if1|<----->|veth1				  |      
комп if2|<----->|veth2  бридж   brctl0|<----->интернет 
комп if3|<----->|veth3 				  |

 наличие brctl0 порта уровня L3 
 дает то что свич превращается врутер.
 вот походу смысл этого порта.
 
 через brctl0 бридж может выводить компы на схеме в инет если 
 они сидят в одной IP сети.
 
 или если компы сидят в разных IP сетях то через brctl0 они 
 могут общатся друг с другом на основе рутинга на бридже.
 
 то есть еесли комп1 имеет ip = 192.168.1.10
 если комп2 имеет ip = 192.168.2.10
 
 а на свиче мы имеем что 
 brctl0 
 ip1=192.168.1.1
 ip2=192.168.2.1
 
 ну тоесть что brctl0 имеет два ip.
 то тогда комп1 и комп2 смогут пинговать друг друга.
 
 тоесть без l3 brctl0 бридж дает связность компами на уровне фреймов
 уровне l2
 при наличии L3 brctl0 ну я уже описал свич превращается в рутер.
 
 собственно ровно это мы и имеем в домашних рутерах.
 ряд свичевых портов. плюс виртуальный то есть без реального 
 физ интерфейса внутренний L3 lan порт ну и плюс 1 L3 внешний порт
 к провайдеру.
 
 вобщем brctl0 нужен только для одного - превраттить бридж в рутер.
 вот походу его смысл.
 
 это совсем неовидно изначально. и нигде необьяснено
 
 
 возвращается к контейнерам.
 изначальная задача поместить контейнеры каждый в свой независмый L2 
 сетевой стек. это мы сделлаи с помощью индивидуалных сетевых неймспейсов.
 каждому контейнеру по своей независимой сет карте.
 
 далее мы хотим соединить ряд контейнеров друг с другом на уровне L2.
 при этом мы нехотим переделыать их сетевые неймспейсы. пусть они так и сидят
 в своих неймспейсах. с незавиимыми карточками.
 значит их нужно воткнуть в свич виртуальный. вирт свич должен имет какието
 свои свичевые обезличенные порты. 
 такие придумали = veth
 
 veth мы делаем портами свича = получили свич с портами.
 также veth по дизайну соединяются с портами контенеров.
 
 таким образом мы обьеидинили карты контейнеров через свич.
 
 на уровне l2 они могуо друг с другом контактировать.
 
 возникает вопрос вот мы обединли карты контейнеров 
 другс другом через veth-ы через свич софт.
 
 это связь по L2.
 
 а есть ли связь между картами контейнеров и 
 физ картой сервера через которую он выходит в сеть на уровне L2.
 
 то есть понятно что вот эти veth-ы они образуют некий виртуальный свич
 внутри линукса. вопрос в том что есть ли аплинк между этим вирт свичем
 и физ картой сервера
 
 
 eth0(192.168.1.10)----????-----|(аплинк) вирт бридж  veth1|<----->|контейнер1
													  veth2|<----->|контейнер2
													  veth3|<----->|контейнер3
													  

есть ли внутри линукса аплинк L2 между реальной физ картой
и софт бриджом по дизайну?

думаю все таки что его нет. потому что мак адреса которые генерирует 
линукс для вирт карт контейнеров они могут совпать с мак адресами
реальных физ устройство в сети за компом. 

поэтому надо заставить чтобы если мы выпускаем пакеты из контейнера в реальную
сеть за компом чтобы происходил рутинг. то есть чтобы внутри линукса сдирался
фрейм контейнера , одевался фрейм данного компе его карты eth0 (пакет Ip от контейнера при этом остается без изменений конечно) и только тогда выпускался 
в сеть. так что я думаю что на уровне фреймов есть связь только между veth.
 а послать фрейм от контейнера через eth0 во внешнююю сеть нельзя.
 
 я думаю что если контейнеру дать ip сети 192.168.1.0 и сделать 192.168.1.10
 гейтвеем то я думаю что такой пакет успешно покинет комп ( при условии
 что на компе разрешен форвардинг). 
 
 но при такой ситуации когда мы дали контейнеру IP из сети хоста это нам приенсет кучу мудежа только.
 если мы захотим достучаться до компа 192.168.1.100 который лежит несреди
 наших контейнеров а где то в сети хоста. то нужно будет укзаать 
 на контейнере статический маршрут что мол если тебе нужен 1.100 то ищи его
 не среди соседей контейнеров ибо тогда ненужен гейтвей а ищи его за гейтвеем
 
 route add 192.168.1.100 через 192.168.1.1
 
 а на 192.168.1.100 надо тоже будет прописыать обратный стат маршрут 
 что мол если ты ищешь 1.х то посылай на 1.10
 
 тоже самое касается выйти в интернет. надо будет на интернет шлюзе прописываь
 стат маршурт к 1.х через 1.10
 
 причем такие стат маршруты на интернет шлюзе или соседях
 сервера нужно будет на каждом из них прописывать индидуалный маршрут 
 для каждого контейнера. вобщем ужас.
 
 окей. итого понятно что ip сеть под контейнеру надо выбрать совсем другую
 чем сеть хоста. например 192.168.2.0
 
  теперь контейнеры имеют свою сеть а сервер и его соседи по физ сети
 имеют другую сеть.
 
 теперь достаточно на гейтвее физ сети прописать ровно один маршрут 
 для всех контейнеров что мол сеть 192.168.2.0 надо переправлять 
 на 1.10 и все. то есть из физ сети понятно где контейнеры искать.
 
 теперь остается контейнеры выпоустить в сеть. и тут нам на помощь 
 приходит brctl0 L3
 
 
 физ сеть| <---> |eth0 компа (сеть 192.168.1.10) <----> ядро <---->| brctl0(192.168.2.1) вирт свич veth1|<---->|контейнер if1 (192.168.2.10)
 
 получается brctl0 это порт свича L2 причем с L3 фукцией.
 
 мы назначаем конейтнерам гейтвей 192.168.2.1
 
 и у нас пакеты теперь ходят из физ сети к контейнерам и обратно.
 (на компе мы активировали форвардинг и на рутере физ сети прописали 
 стат марушрут что 2.0 надо искать на 1.10)
 
 так вот docker0 это brctl0 порт.
 
 заметим что пока NAT даже непахнет. и это хорошо ибо нат это cpu затратная
 процедура
 
 
 если мы при прохождении eth0 зададим NAT правило. то тогда мы можем подменять 192.168.2.0 через 192.168.1.10 и тогда на рутере физ сети
 прописывать стат маршрут непридется.
 
 
 но отмотаем обратно. итак чтоже нам дает вирт бридж.итоги
 самое главный вопрос  - что будет если я какойто  порт включаю в состав бриджа 
 
 # brctl docker0 addif eth0
 
 docker0 название бриджа
 eth0 порт который мы включаем в состав бриджа
 
 1. eth0 ПОРТ СТАНОВИТСЯ ПОЛНОСТЬЮ ТРАНЗИТНЫМИ
 2. ТО ЧТО ВЛЕТАЕТ В ОДИН ТАКОЙ ПОРТ ПЕРЕНАПРАВЛЯЕТСЯ СРАЗУ ВО ВСЕ ДРУГИЕ
 3. НЕ ПОРТЫ П.1 А ТЕ ПОРТЫ КОТОРЫЕ ПОДКЛЮЧЕНЫ К ПОРТАМ П.1 ПОЛУЧАЮТ 
 СВЯЗЬ ДРУГ С ДРУГОМ. 
 4. СВЯЗЬ УРОВНЯ L2
 5. еще раз важно различать порты которые являются непосредственно частью
 бриджа. в данном случае это eth0. и порты которые неявляются частью бриджа
 но которые соединены с портами бриджа. в нашем случае это порты контейров.
 порты контейнеров неявляются частью самого бриджа. но они соединены с портами бриджа. порты самого бриджа являются транзитными и безликими.
 порты которые подключаются к портами бриджа уже являются полноценными
 и имеют мак+IP. мы настраиваем бридж чтобы через безликие порты создать 
 связь между полноценными L2+L3 портами.
 
 
 что дает бридж разобрались.
 
 и вот теперь мы переходим к задаче что нам надо что контейнеры
 могли друг до друга достучаться между отдельными физ серверами.
 
 есть такой еще момент. если brctl0 это гейтвей для всех карточек контейнеров
 то тогда их срупут упирается в срупут brctl0. получается что скорость каждой карточки условно говоря 1ГБ их 100 штук и все они выходят наружу
 через 1ГБ. это как то пахнет усзким местом. но пока непонятно 
 так как карточки и контейнеров и brctl0 они виртуальные.
 
 без проблема можно создать карточку сетевую с таким вот именем
 
  ip link add petya.1.1  type dummy
  
 # ip -c link show
 
 15: petya.1.1: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000

отсюда приходит понимание что интерфейс фланнеля с именем flannel.1
это не какой субинтерфейс (точка один) а просто такое дурацкое
название этого интерфейса.
 
3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default


 прикольно что карточка в контейнера сетевая она ничто иное как по своей
 сути тоже veth интерфейс. то есть veth он спокойно может быть L3
 
 
 veth напраник = down
 но в нашем неймспейсе мы все равно пингуем veth1
 
 veth link = down. но все равно пингуется
 поразщительно
 
 что dummy что veth оним когда down то при этом саойокной 
 пингуются.
 если сосдем veth = down то его другой сосдет все равно пингутеся.
 
 еще прикол. если бридж не имеет ниодного порта то его brctl интерфейс
 будет оставаться в состоянии down. 
 
 # brctl create docker2
 и он будет оставться в down и его будет неподнять.
 
 ip route get IP
 
 
 если настоящий лупбек погасить то он непигуется
 
 # ip route get  172.16.0.1
local 172.16.0.1 dev lo  src 172.16.0.1
    cache <local>

очень странный маршрут

также если мы от процесса пигуем lo панпример непонятно какой он 
ставит тгда source IP в пакет.наверно того же самого лупбека. 

то есть у нас же должна быть source карта  и source Ip. 
процесс обращается к какрте и с нее пингует ее же.


процесс если он пингует другую карту он обязан иметь исходную карту
когда мы пингуем лупюбек интервейс какая карта является исходной?

если я создам dummy который не лупбэек.
и есл я его погашу то он все равно пингуется.

значит у менять есть lo = 127.0.0.1 UP
есть dummy = 172.16.0.1  UP

я пингую 172.16.0.1 он пингуется
 но самое интересное какой маршрут выбирет линукс
 
 # ip route get  172.16.0.1
local 172.16.0.1 dev lo  src 172.16.0.1
    cache <local>

ия в шоке. строка означает что пинг до 172.16.0.1 пойдет 
через loopback интерфнйс при этом src ip = 172/16/0/1
как бы што? посчему через лупбек?
если мы погаим дамми то все равно 172 16 0 1 пингкуется
а вот если мы погасим лупбек то нифига не будет пинговаться

возможно лупбек интерфейс и дамми интерфейсы сидят на одной "шине" в линуксе.
но тогда получктся вопрос как тогда пакеты из физ интерфейса могут дойти 
до лупюбек интерфейса

пакеты швыряются исключетльно исходя из таблицы маршутизации
прежде всего  а потом все остальвное

если мы veth назначим ip то рут к нему будет проходить через lo интерфейс.

 япрописал маршрут
 
 172.15.0.0/24 dev veth0  scope link  src 172.15.0.2

veth0 неимеет ip

ping 172.15.0.1

цепочка процесс шлет чрез veth0(l2 без ip) на veth1 в другой неймспейс
а оттуда до 172.15.0.1 все дошел отуда оброатно он прилетет в 
veth0 и вопрос что с ппакетом происходит дальше. он что сразу отдается 
процессу ? стоп. он же недошел до 172.15.0.2 
а как до него дойти ? он же недошел до карты назначения.

и причем пинг доходит хотя dummy с ip = 172.15.0.2 находистя down

яя проерил через tcpdump когда мы пингуем veth то на сам veth вробще
ничего не прилетает. вообще.
все летит на lo и на этом все заканчивается

 ip route get  172.14.0.1
local 172.14.0.1 dev lo  src 172.14.0.1
    cache <local>

то есть летит как напписано в таблице марщутизации. 
а на veth вобще не прилетает. бред
не понимаю пакет недоставлен до карты. а он говорит чт доставил.


цепочка. есть карта дамми vasya с 172.15.0.1

есть veth0 который вдетв в другой спейс на другом конце карта с 172.15.0.10

я задал в роутинге

172.15.0.0/24 dev veth0  scope link  src 172.15.0.1

и пинги пошли.

они вообще неисходили из васи и не входили в него. и не входили в лупбек.
они только проходили через veth0 и все.
и как бутто отуда сразу в процесс

ощущение что все дамми они по факту являются алиасами лупбека.
я убрал ручной маршрут 172.15.0.0/24 dev veth0  scope link  src 172.15.0.1

и остался дефолтовый маршрут

172.15.0.0/24 dev vasya  proto kernel  scope link  src 172.15.0.1

через dummy .

и пинги не проходят.

получатеся что пакет испускается из дамми ииии ????? улетает в ядро
и ничео не происходит.

хотя веф это вирт инт как и дамми но они несоидениеы.

 я провел эксперимент все локальные ip адреса достигаются 
 только через lo интерфейс
 
 ~# ip -c route get 172.14.0.1
local 172.14.0.1 dev lo  src 172.14.0.1
    cache <local>
root@test-kub-05:~# ip -c route get 172.16.102.35
local 172.16.102.35 dev lo  src 172.16.102.35
    cache <local>

~#  ip -c route get 10.5.98.1
local 10.5.98.1 dev lo  src 10.5.98.1
    cache <local>


поразительно! неважно пингуешь ты реальную физ карту
или виртвальную - при пинге своих IP доставка идет всегда вседа
всегда через lo интерфейс. и если его погасить то ты несможешь
пинговать свои собстыенные карочки и неважно она физическая или 
вирталнаяю!


шок!

ну понятно что в физ карту ты неможешь сам засунуть пакети снаружи
но вирт карты то ....

вобщем механизм связи с собсвтенными карточками супер странный нефизичный.
значит когда пингуешь свои IP то пингуешь лупбек интерфейс по факту.

когда я пингую свой внутрений docker0 интерфейс то по факту пакеты ходят 
толькон на lo интерфейсе а на docker0 tcpdump неловит ни-че-го

полусется пактеы летят в lo интерфейс и летят непонятно куда но 
до программы долетают. рисует такая картин

процесс  - lo - ядро. ядро говорит все окей и типа ядро обратно в lo
пихает пакет с обратным порядком IP.
то есть типа на другой стороне провода от лупбек адаптера сидит ядро
и принимает пакеты как сетевая карта безадреса. получает пакеты
формирует обратный пакет и посылает его обратно в lo

как lo L3 имеет на другом конце L2 порт который уходит прямиком в ядро.
и ядро типа через lo принимает пакеты на все IP которые есть на компе на
всех картах.

lo это L3 порт который ведет в другой порт L2 который уже воткнут прям 
в ядро.

еще можно по другому представить. LO это L3 порт который
через провод ведет в L3 порт который имеет все IP карт на компе.


а что если представить что линукс это рутер с кучей L3 портов.
LO это тоже L3 порт входящий в состав рутера.
фишка его в том что процессы которые крутятся внутри рутера-линукса
им тоже типа надо дать возможность как то посылать пакеты на L3 порты
через некоторый api. как бы сами порты сидят в ядре. и вот надо 
юзерспейсу дать возможность блядь посылать пакеты к своим портам 
которые снаружи.

цепочка

получается что lo это как бы порт на невидимый на рутере и сидязий внутри корпуса линукса. зато так как он внутри к нему могут подключаться внутренние
процесыы.

цепочка 

процесс -> LO -> ядро рутера линукс. -> внещние порты в том числе как 
это ни смешно и вирт порты линукса кроме lo.

представим мы шлем пинг на IP физ порта.
имеем цепочку

процесс шлет пакет через -> LO -> он попадает в ядро ведь конечная цель
чтобы пакет попал в ядро. и ядро пакет для физ карты уже получило. его 
отправлять на физ карту на его драйвер неимеет смысла ибо он оттуда попал бы в ядро а он уже в ядре. ядро формирует ответ и отправляет обратно через lo.
и мы поэтому с одной стороны видим пинги а сдругой стороны мы ничего невидим
в тисипидамп на физ порту.
тоесть порт это устройство чтобы пакет попал в ядро. если он уже в ядре 
зачем его направлят в порт.

насколько я понимаю когда пакет влетает в порт "снаружи" то влетев в порт
он попадает в ядро. а из ядра в юзерский процесс

пакет -> порт -> ядро -> процесс

почему нерабтате когда мы пигуес IP карты компа 
но эта ккрта лежит вдруо неймспейсе.
потому что пает идет через lo а на том конце там толко IP 
карт которые в этом неймспейсе.

 в итоге 



возможно почему мы невидим в tcpdump чтобы н


поток идет через brctl0 хотя он для него даже не предназначен.

если  дать lo ip = 10.5.98.5/24 
то будет успешно пинговаться ЛЮБОЙ АДРЕС ИЗ СЕТИ 10.5.98.0\24
это пищлец

дале у меня на компе карта veth1 = 10.5.98.3/24
есть маршрут

10.5.98.0/24 dev veth1  proto kernel  scope link  src 10.5.98.3

однако если ввести команду

ip route get 10.5.98.9 
то она выдааст совсес нето что в таблице маршрутизации а

local 10.5.98.10 dev lo  src 10.5.98.5
    cache <local>


то есть игнор таблицы мраутизации и поход через lo.

это пиздец.

я предлагаю на lo невещать ничего кроме 127.0.01
иначе заебешься вычичлять приколы на ровном месте.

 у меня lo во втором неймспейсе имел ip = 10.5.98.9
 и veth1 во втором нейиспейсе имел ip = 10.5.98.10
 
 я из первого неймспейса из veth0 пытался до пинговться
  до 10.5.98.10 и никак не мог! погасил lo непомогает
  удалил ip с lo и только тогда заработало. 
  какой то п..ц с этим lo.
  
  ужас. как он работает логика непонятина.
  
  лучше lo вообще нетрогать и про него забытьб
  
  lo дебилнейщий интерфейс и ни коммутация ни рутинг с ним неопнятне
  тидет не поправилам. лучше его вообще нетрогать. и никакие ip адреса 
  ему недавать кроме дефолтовго
  
  к соажалениею  brctl0 это не просто L3 порт бриджа 
  это еще и транзитный порт через который как я понял протекает транзитно буквально каждый пакет всего бриджа. так что если его погасить ниодлтин порт
  бриджа не сможет связаться с другим портом бриджа. это дебилизм. полнейший.
  я считаю что brctl0 должен быт влкючен только если мы хотим его L3 функционал 
  заюзать. а так это узкое место .
поэтлому brctl погаситл и весь бридж остаовился

ответ на мой изначальный вопрос
взяли пару вефов. 
один оставил а другой зауснули в другой неймспейс и дали ему IP
вопрос что нам надо сделать чтобы тот IP пинговать.

ответ - этому вефу назначить IP из той же сети и пинги пошли.
 аесли у нас два 10 контейнеров как тогда их пинговать. 
удобнее всего тогда наши вефы этого спейса сделать частью бриджа.
и самоу бриджу дать L3 ip. тогда можно будет их всех достать для пинга.

так будет минималная заморочка с этим.
потому что чтобы пиновать без гейтвея надо иметь порт L3 из тожй е сети 
что и порты те. и чтобы наш L3 порт имел связт с каждым тем портом на уровне L2. 

без свича нужно каждому концу нашего веф назначать ip индивдуальный.
ипрописывать инд маршрут к тому концу. 10 контейнеров это 10 IP с нашей стороны назначь + 10 маршрутов пропши до их ip. ужас.
поэтом свич это выход. если бы на самом свиче нельзя был назначать один порт 
 с IP то тогда нужно было бы создать +1 пару веф. оба оствит внашем неймспейсе
 один вотнуть в свич а второй невтыкать и второму дать IP. 
 
 
 
 ЕДИНСТВЕННОЕ ИЗЗА ЧЕГО НЕРАБОТАЛА СВЯЗЬ МЕЖДУ НОДАМИ ДЛЯ ПОДОВ
 ЭТО ЧТО НА ДАТАТ НОДАХ БЫЛ ВЫКЛЮЧЕН IP FORWARDING.!!
 
 ОСТАЕТСЯ деталеьно выяснить как комбтнация фланеля и iptables
 дает связь
 

про фланель
 
 видно что заведен бридж cni0 который никак не связан 
 с докер нетворкс командой
 
 
  root@test-kub-02:~# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.5efab67e726a       no              veth6b8aeca2
                                                        vethde173048
docker0         8000.0242f0b9d9c1       no              veth45c266d
root@test-kub-02:~# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
1b537e52e590        bridge              bridge              local
bbffd9dc86b7        host                host                local
ce3ae03bbaa1        none                null                local
root@test-kub-02:~#






 
 
 
 

  FLANNEl.
  
 
начнем с фланнеля + докер. 
а потом уже расмотрим куб+фланнель+докер.

начнем просто с фланнеля.
написано что своим бекендом он может использовать VxLAN.
это ровно то что мы будем использовать для начала.

VxLAN использует как и по аналогии с VLAN неокторый идентификатор
VNI который отличает один Vx от другого.

Vx использует UDP. 
для UDP нужно указать dest IP. причем  я не понял если наш хост
сидит в одном VNI то ему нужно указыавть все хосты со всеми VNI
или только хосты с VNI на нашем хосте.

но наш случай самый простой - все хосты будут иметь один VNI.
ну и все равно возникает вопрос. вот у нас три хоста. один 
из хостов шлет пакет он что каждый раз слать на оба оставшихся хоста?.

как я понял нет не каждый раз.

каждый раз когда на хост прилетает vxalan хрень то комп запоминает 
mac адрес удаленного компа, его VNI , ну и может быть его внешний IP.
таки образом когда мы захотим отправить vxlan с таким же маком и таким
же VNI то мы пошлем сразу на тот один сервер.

пока на этом детали vxlan оставляем. 
на стороне свичей железных ничего настраивать ненужно.
для них полностью прозрачная технология.

нам только нужно будет указать все сервера кластера


root@test-kub-02:~#  nsenter --net=/proc/3148/ns/net ip -c -f  inet address show eth0
3: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default  link-netnsid 0
    inet 10.252.1.4/24 brd 10.252.1.255 scope global eth0
       valid_lft forever preferred_lft forever


возможно поды невиын на другх хостах
изза неактивации форвардинга.
а также надо в свойстваох iptables  в  правильных местах
поставить ACCEPT

вопрос - если кубернетес создает контейнеры через докер
то как он умудряетмся подключать контейенеры к сети друогго бриджа (не докер0 ) cni0 
и при этом cni0 его нет в docker networks

список какие порты сидят на бриждами

test-kub-02:~# brctl show
bridge name     bridge id               STP enabled     interfaces
cni0            8000.5efab67e726a       no              veth6b8aeca2
                                                        vethde173048
docker0         8000.0242f0b9d9c1       no              veth45c266d


также в свойствах линка указано за каким бридждлм он сидит


7: vethde173048@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default
    link/ether 46:98:0c:b8:67:d6 brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::4498:cff:feb8:67d6/64 scope link

master cni0

       valid_lft forever preferred_lft forever
9: veth45c266d@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default
    link/ether fa:24:30:ee:84:a1 brd ff:ff:ff:ff:ff:ff link-netnsid 2
    inet6 fe80::f824:30ff:feee:84a1/64 scope link

master docker0

containerd-shim  = это какой то спец процесс к каждому контейнеру что бы чтото там удобнее работало
на уровне процессов. надо уточнять


kubectl exec -ti kube-system etcd-test-kub-01 -- ps aux


для начала научиться чтоб связка работала докер+ фланнель
и тока потом пытться понять связку кубер+фланнель+докер

стаивм etcd
пототом фланнель



фланнель+докер заработал(прозрачная связь между подами между серверами 
работате)


надо записать всю установку и осознать как это работает от и до.




 