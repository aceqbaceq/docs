| kubernetes certificates


проблема кога проутхкли сертфиикаты на к8

---

root     12406 22.8  1.4 10612728 89528 ?      Ssl  21:17   0:38 etcd --advertise-client-urls=https://192.168.1.50:2379 
--cert-file=/etc/kubernetes/pki/etcd/server.crt 
--client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://192.168.1.50:2380 --initial-cluster=mk-kub2-01=https://192.168.1.50:2380 
--key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.50:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.1.50:2380 
--name=mk-kub2-01 
--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt 
--peer-client-cert-auth=true 
--peer-key-file=/etc/kubernetes/pki/etcd/peer.key 
--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt 
--snapshot-count=10000 
--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt



---
 systemctl stop kubelet

 kubeadm alpha  certs check-expiration
 
 kubeadm alpha  certs renew all


rm /etc/kubernetes/kubelet.conf
rm /var/lib/kubelet/pki/*....
kubeadm init phase kubeconfig kubelet


@@@# kubeadm join --token=jw88ph.o61raxsf77sd0h8y  172.16.1.50:6443 --node-name mk-kub2-06@@

kubeadm token create jw88ph.o61raxsf77sd0h8y --print-join-command



kubeadm join 192.168.1.50:6443 --token e53g96.fwdik1j99lm016eo  --node-name mk-kub2-06 --discovery-token-ca-cert-hash sha256:b75814e298b4f94e6da46e58eed04964a337189491bcaf52dc0fb49ef96e71ad



kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:mk-kub2-06 > /root/temp/06/kubelet.conf
kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:mk-kub2-09 > /root/temp/09/kubelet.conf



client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
client-key: /var/lib/kubelet/pki/kubelet-client-current.pem




systemctl start kubelet
---
$ sudo mv /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.key.old

$ mv /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.crt.old

$ mv /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.crt.old

$ sudo mv /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.key.old

$ sudo mv /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.crt.old

$ sudo mv /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.key.old
====

$ kubeadm alpha phase certs apiserver --apiserver-advertise-address 192.168.1.50
$ sudo kubeadm alpha phase certs apiserver-kubelet-client
$ sudo kubeadm alpha phase certs front-proxy-client
---


----
s# cat kube-apiserver.yaml | grep advert
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.1.50:6443
    - --advertise-address=192.168.1.50
root@mk-kub2-01:/etc/kubernetes/manifests#
----
mv /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.key.old
mv /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.crt.old
mv /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/apiserver-kubelet-client.crt.old
mv /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.key.old
mv /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/front-proxy-client.crt.old
mv /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.key.old


kubeadm alpha phase certs apiserver  --config /root/kubeadm-kubetest.yaml
kubeadm alpha phase certs apiserver-kubelet-client
kubeadm alpha phase certs front-proxy-client
 
mv /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.crt.old
mv /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/apiserver-etcd-client.key.old
kubeadm alpha phase certs  apiserver-etcd-client


mv /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.crt.old
mv /etc/kubernetes/pki/etcd/server.key /etc/kubernetes/pki/etcd/server.key.old
kubeadm alpha phase certs  etcd-server --config /root/kubeadm-kubetest.yaml

mv /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/etcd/healthcheck-client.crt.old
mv /etc/kubernetes/pki/etcd/healthcheck-client.key /etc/kubernetes/pki/etcd/healthcheck-client.key.old
kubeadm alpha phase certs  etcd-healthcheck-client --config /root/kubeadm-kubetest.yaml


mv /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/peer.crt.old
mv /etc/kubernetes/pki/etcd/peer.key /etc/kubernetes/pki/etcd/peer.key.old
kubeadm alpha phase certs  etcd-peer --config /root/kubeadm-kubetest.yaml

*)  Backup old configuration files
mv /etc/kubernetes/admin.conf /etc/kubernetes/admin.conf.old
mv /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.old
mv /etc/kubernetes/controller-manager.conf /etc/kubernetes/controller-manager.conf.old
mv /etc/kubernetes/scheduler.conf /etc/kubernetes/scheduler.conf.old

kubeadm alpha phase kubeconfig all  --config /root/kubeadm-kubetest.yaml

mv $HOME/.kube/config .$HOMEkube/config.old
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
chmod 777 $HOME/.kube/config
export KUBECONFIG=.kube/config




----
$ cd /etc/kubernetes/pki/
$ mv {apiserver.crt,apiserver-etcd-client.key,apiserver-kubelet-client.crt,front-proxy-ca.crt,front-proxy-client.crt,front-proxy-client.key,front-proxy-ca.key,apiserver-kubelet-client.key,apiserver.key,apiserver-etcd-client.crt} ~/
$ kubeadm init phase certs all --apiserver-advertise-address <IP>
$ cd /etc/kubernetes/
$ mv {admin.conf,controller-manager.conf,kubelet.conf,scheduler.conf} ~/
$ kubeadm init phase kubeconfig all
$ reboot

----

----
root@mk-kub2-01:~# ps aux | grep kube
root      1375  1.2  1.6 977608 98012 ?        Ssl  19:48   0:02 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2
root      2508  0.8  1.1 810316 70292 ?        Ssl  19:49   0:01 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --node-cidr-mask-size=24 --port=0 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true
root      2571  1.2  0.7 746404 43724 ?        Ssl  19:49   0:01 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true --port=0
root      2854 34.5  1.6 10613112 98004 ?      Ssl  19:49   0:54 etcd --advertise-client-urls=https://192.168.1.50:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://192.168.1.50:2380 --initial-cluster=mk-kub2-01=https://192.168.1.50:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.50:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.1.50:2380 --name=mk-kub2-01 --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt



root      4684 17.6  1.1 817840 72272 ?        Ssl  19:51   0:00 kube-apiserver --advertise-address=192.168.1.50 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key



root      4767  0.0  0.0  14224   920 pts/0    S+   19:51   0:00 grep --color=auto kube
root@mk-kub2-01:~# ps aux | grep kube-apiserver
root      4684  7.3  1.1 817840 72272 ?        Ssl  19:51   0:00 kube-apiserver --advertise-address=192.168.1.50 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root      4805  0.0  0.0  14224  1012 pts/0    S+   19:51   0:00 grep --color=auto kube-apiserver
root@mk-kub2-01:~#
root@mk-kub2-01:~#
root@mk-kub2-01:~# ps aux | grep kube-apiserver
root      4684  4.8  1.1 817840 72272 ?        Ssl  19:51   0:00 kube-apiserver --advertise-address=192.168.1.50 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub -
----

openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text |grep ' Not '

-----

mkadm@mk-kub2-01:~/temp/0002/interfaces$
mkadm@mk-kub2-01:~/temp/0002/interfaces$ kubectl describe  statefulset.apps/elastic-prod-interface  -n elastic-prod
Name:               elastic-prod-interface
Namespace:          elastic-prod
CreationTimestamp:  Thu, 03 Dec 2020 19:35:16 +0300
Selector:           app=es-interface
Labels:             <none>
Annotations:        <none>
Replicas:           0 desired | 0 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=es-interface
  Init Containers:
   fix-permissions:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      chown -R 1000:1000 /usr/share/elasticsearch/data
    Limits:
      cpu:     200m
      memory:  100M
    Requests:
      cpu:        100m
      memory:     50M
    Environment:  <none>
    Mounts:
      /usr/share/elasticsearch/data from data (rw)
   increase-vm-max-map:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sysctl
      -w
      vm.max_map_count=262144
    Limits:
      cpu:     200m
      memory:  100M
    Requests:
      cpu:        100m
      memory:     50M
    Environment:  <none>
    Mounts:       <none>
  Containers:
   elasticsearch:
    Image:       docker.elastic.co/elasticsearch/elasticsearch:7.7.1
    Ports:       9200/TCP, 9300/TCP
    Host Ports:  0/TCP, 0/TCP
    Limits:
      memory:  27Gi
    Requests:
      cpu:     1
      memory:  24Gi
    Environment:
      cluster.name:                         <set to the key 'clustername' of config map 'confmap'>  Optional: false
      node.name:                             (v1:metadata.name)
      discovery.seed_hosts:                 <set to the key 'discovery_seed_hosts' of config map 'confmap'>  Optional: false
      node.master:                          false
      node.voting_only:                     false
      node.data:                            false
      node.ingest:                          false
      node.ml:                              false
      node.attr.rack_id:                     (v1:spec.nodeName)
      xpack.monitoring.enabled:             true
      xpack.monitoring.collection.enabled:  true
      xpack.monitoring.exporters.id1.type:  http
      xpack.monitoring.exporters.id1.host:  192.168.0.231:9200
      xpack.security.enabled:               false
      ES_JAVA_OPTS:                         -Xms20g -Xmx20g
    Mounts:
      /usr/share/elasticsearch/data from data (rw)
  Volumes:  <none>
Volume Claims:
  Name:          data
  StorageClass:  pv-localdisks-1g
  Labels:        app=es-interface
  Annotations:   <none>
  Capacity:      1Gi
  Access Modes:  [ReadWriteOnce]
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulDelete  12m   statefulset-controller  delete Pod elastic-prod-interface-2 in StatefulSet elastic-prod-interface successful
  Normal  SuccessfulDelete  12m   statefulset-controller  delete Pod elastic-prod-interface-1 in StatefulSet elastic-prod-interface successful
  Normal  SuccessfulCreate  7m8s  statefulset-controller  create Pod elastic-prod-interface-0 in StatefulSet elastic-prod-interface successful
  Normal  SuccessfulDelete  32s   statefulset-controller  delete Pod elastic-prod-interface-0 in StatefulSet elastic-prod-interface successful
mkadm@mk-kub2-01:~/temp/0002/interfaces$
--------------

получетс вот что. 

у меня есть точки монттрования на дата нодах в особой папке.
их наодит локал профижонер и создает PV
они должны выгляеть так 


$ kubectl get pv | grep -i -E "Claim|Avail"
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM      STORAGECLASS         REASON   AGE
local-pv-28eaa6e9   295Gi      RWO            Delete           Available              pv-localdisks-300g            3y223d
local-pv-3328560f   295Gi      RWO            Delete           Available             pv-localdisks-300g            3y223d

здесь важно два стобика
CLAIM - должен быть пустой
STATUS должен быть AVAIL


у нас есть stefuull set . 

он автомтатом создает CLAIMS (pvc) , они в свою очередь итщут в списке PV 
те который AVAIL и столбят их! а поды которые плодит SS  они ищут PV через PVC

так вот! если я уменьшу число реплик на SS
скажем было 3  а стало 2

kubectl scale statefulset  elastic-prod-interfac  -n elastic-prod  --replicas=2

то у нас исчезает -1 под. при этом claim никуда автоматом не исчезает!
а если он не исчезает то PV который он застолоибил будет в состоянии BOUND
тоест занят!
так вот 
если я обратно расширю SS на +1

kubectl scale statefulset  elastic-prod-interfac  -n elastic-prod  --replicas=3

то под сука не сможет и не будет юзать тот PV которым ползовался! 
нужно удалить PVC руками 

 2125  kubectl delete pvc -n   elastic-prod data-elastic-prod-interface-0


после этого нужно еще пдождать секунд 30 потому что наш PV после этого 
вначалле предйтет в STATUS=RELEASED , потом он вобще исчезанет. а потом повится
снова с STATE=AVAIL
и вот только теперь можно увеличватрелпику на +1 потому что под наш под повился
свобдный PV !!!!

я с этим надоблался на примреер интерфейсных подов. тоесть на примерер 

SS =  elastic-prod-interface  -n elastic-prod

  
----
# kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:mk-kub2-08 > ~/temp/08/kubelet.conf


---

===============
ссылки:
https://discuss.kubernetes.io/t/solved-api-server-not-starting-error-context-deadline-exceeded/22286/4
https://github.com/kubernetes/kubernetes/issues/96263
https://medium.com/@sunilmalik12012/renew-expired-k8s-cluster-certificates-manually-e591ffa4dc6d

https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired
https://knowledge.broadcom.com/external/article/382787/error-failed-to-run-kubelet-unable-to-lo.html
https://github.com/kubernetes/kubernetes/issues/84252
https://github.com/kubernetes/kubeadm/issues/581
https://github.com/kubernetes/kubeadm/issues/2352

https://stackoverflow.com/questions/49885636/kubernetes-expired-certificate
https://github.com/kubernetes/kubeadm/issues/581
https://stackoverflow.com/questions/61023319/where-i-can-find-kubeadm-config-yaml-on-my-kubernetes-cluster
https://stackoverflow.com/questions/56320930/renew-kubernetes-pki-after-expired/56334732#56334732
https://stackoverflow.com/questions/49885636/kubernetes-expired-certificate
https://stackoverflow.com/questions/41144589/how-to-redirect-docker-container-logs-to-a-single-file
https://github.com/kubernetes/kubeadm/issues/2814
https://kubernetes.io/docs/tasaks/administer-cluster/kubeadm/kubeadm-certs/
https://stackoverflow.com/questions/60293189/reset-work-node-when-to-rejoin-to-master-using-kubeadm

https://stackoverflow.com/questions/45913034/readd-a-deleted-node-to-kubernetes
https://kubernetes.io/docs/tasks/tls/certificate-rotation/
https://github.com/canonical/microk8s/issues/2489
https://serverfaulat.com/questions/1068751/var-lib-kubelet-pki-kubelet-crt-is-expired-how-to-renew-it
https://stackoverflow.com/questions/51027036/what-is-the-difference-between-apiserver-kubelet-client-apiserver-and-kubelet-c
https://github.com/kubernetes/kubeadm/issues/2186

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert
https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/
https://docs.amd.com/r/en-US/ug1656-onload-operator-for-kubernetes-user-guide/All-Resources-in-All-Namespaces
https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/
https://kodekloud.com/blog/kubernetes-pods-stuck-in-terminating-a-resolution-guide/
https://kubesphere.io/blogs/restart-k8s-cluster/
https://jhooq.com/get-yaml-for-deployed-kubernetes-resources/

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/
https://sbytestream.pythonanywhere.com/blog/Renewing-k8s-certificates

https://discuss.kubernetes.io/t/multi-master-cert-renewal-help/24767https://discuss.kubernetes.io/t/multi-master-cert-renewal-help/24767https://discuss.kubernetes.io/t/multi-master-cert-renewal-help/24767
https://discuss.kubernetes.io/t/solved-api-server-not-starting-error-context-deadline-exceeded/22286
https://github.com/kubernetes/kubernetes/issues/96263
