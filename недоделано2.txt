-----
info
info info
man man

-----
logrototate = как он заставляет приоложение закрыть файл в который приложение пишет
umask = как это работает
rename = какой сисколл
rm = какой сисколл

как работает эта хрень что место на дсике освооождается
когда закрыт последний файловый десприптор к файлу


touch использует openat()
openat(AT_FDCWD, "1.txt", O_WRONLY|O_CREAT|O_NOCTTY|O_NONBLOCK, 0666) = 3



прикол udev /dev хосится в памяти
# df .
Filesystem     1K-blocks  Used Available Use% Mounted on
udev             8033820     0   8033820   0% /dev

и туда спокойно можно сождававть папки и писать обычные файлы



tty - controlling terminal - controlling process - session leader - session - process group - 

uid, euid, ruis, fuid , ouid, suid
$ ps o uid,euid,ruid,fuid,ouid,suid  -p $$
  UID  EUID  RUID  FUID OWNER  SUID
 1000  1000  1000  1000 1000   1000


 Когда вы применяете sticky bit, пользователь может удалять файлы, только если выполняется одно из следующих условий:

Пользователь является владельцем файла;
Пользователь является владельцем каталога, в котором находится файл.



создал папку ./root c правами
Access: (0754/drwxr-xr--)  Uid: (    0/    root)   Gid: (    0/    root)
существенно то что для others разрешено только чтение (запись и execute запрешена)

далее юзером васей пытаюсь читать список файлов внутри этой папки и получаю:
$ ls  -1 ./root
ls: cannot access './root/root2.txt': Permission denied
ls: cannot access './root/root.txt': Permission denied
ls: cannot access './root/root-dir': Permission denied
root2.txt
root-dir
root.txt


$ ls  -1al ./root
ls: cannot access './root/root2.txt': Permission denied
ls: cannot access './root/.': Permission denied
ls: cannot access './root/root.txt': Permission denied
ls: cannot access './root/root-dir': Permission denied
ls: cannot access './root/..': Permission denied
total 0
d????????? ? ? ? ?            ? .
d????????? ? ? ? ?            ? ..
-????????? ? ? ? ?            ? root2.txt
d????????? ? ? ? ?            ? root-dir
-????????? ? ? ? ?            ? root.txt


тоесть он список файлов читает из папки но неможет прочитать пермишнсы от них

и только если я на папку ./root добавляю пермишнс execute
только тогда он читает и и список файлов и их пермишнсы

$ ls  -1al ./root
total 12
drwxr-xr-x 3 root  root  4096 июн 16 11:15 .
drwxrwxrwx 3 vasya vasya 4096 июн 16 11:11 ..
-rw-r--r-- 1 root  root     0 июн 16 11:15 root2.txt
drwxr--r-- 2 root  root  4096 июн 16 11:15 root-dir
-rw-r--r-- 1 root  root     0 июн 16 11:14 root.txt


походу я понял.
папка это по факту файл. файл этот хранит список файлов
тоесть наша папка это на диске файла типа папка.txt с сооедеоржмимым внутри:
root2.txt
root-dir
root.txt

а как же тога мы проваливаемся в папку что происходит?
по факту мы неможем никуда перейти потому что папка это файл. мы только можем
считать файл но никак туда не"перейти"
когда мы тыкаем в mc войти в папку это по факту мы никуда непроваливаеммся а мы читаем
содержимое файла-папка и получаем список файлов и он этот спмсок отрсиоываыет а нам кажется
что мы провалились в папку. а  по факту мы просто считали содежимре файла.
так вот 

read на папку  - дает то что мы можем читать содержимое этого файла тоесть прочитать солежимое афайла. поскольку внутри файла спсок то мы читая получим список файлоы

так почему же мы неможем при это увидеть пермишнсы на файлы. потому что 
наша папка-файл хранит только имя файлы и его иноду. а пермишнсы от файла они хранятся внутри иноды а не внутри папкифайла. тоест внутри папки файла нет информации о пермишнсах на файл.

тость читай нечитай наша файл-папка в нем этой инфо просто нет!!!!
чтобы пполучиить пермишнсы на файл надо перейти на иноду на диске и считать с нее пермишнсы
на файл.

(найти имя файла по его иноде.)

(прикол еще в том что файл по факту это инода + тело. поэтому файл понятие неимеет о том 
в какой папке о нем упоминается. это как стихотворение в книге. книга и стиъотвоерниея понятия неимют в какой газете о них пишут. вот такая же савязь между файлом и записью о нем в папках)

(интересня штука

fstat(1, {st_dev=makedev(0, 24), st_ino=185, st_mode=S_IFCHR|0620, st_nlink=1, st_uid=1000, st_gid=5, st_blksize=1024, st_blocks=0, st_rdev=makedev(136, 182), st_atime=1655368744 /* 2022-06-16T11:39:04.464846020+0300 */, st_atime_nsec=464846020, st_mtime=1655368744 /* 2022-06-16T11:39:04.464846020+0300 */, st_mtime_nsec=464846020, st_ctime=1655366519 /* 2022-06-16T11:01:59.464846020+0300 */, st_ctime_nsec=464846020}) = 0

fstat - значит смотрели свойства файла. 
а что же это за файл?

лежит этот файл на устройстве с мажром 0 и минором 24 =  st_dev=makedev(0, 24)



)



(sort


$ cat 04.txt; echo "========"; cat 04.txt | sort -k 2.2  -b
111      215    456
222      123    678
111      123    090
222      419    965

========

111      215    456
222      419    965
111      123    090
222      123    678
$ cat 04.txt; echo "========"; cat 04.txt | sort -k 2.2  
111      215    456
222      123    678
111      123    090
222      419    965

========

111      123    090
222      123    678
111      215    456
222      419    965

опчему ??

дефолтовый разделитель полей (fileeds sepratator) это   
 -t, --field-separator=SEP
              use SEP instead of non-blank to blank transition

$ cat 04.txt; echo "========"; cat 04.txt | sort  -t"*" -k2.3
111***!15
222***123
111***123
222***459


========


111***123
222***123
111***!15
222***459
vasya@vasya-Lenovo-IdeaPad-L340-15IWL:~/bash$ 
vasya@vasya-Lenovo-IdeaPad-L340-15IWL:~/bash$ cat 04.txt; echo "========"; cat 04.txt | sort  -t"*" -k2.4
111***!15
222***123
111***123
222***459


========


111***!15
111***123
222***123
222***459
vasya@vasya-Lenovo-IdeaPad-L340-15IWL:~/bash$ cat 04.txt; echo "========"; cat 04.txt | sort  -t"*" -k2.5
111***!15
222***123
111***123
222***459


========


111***123
222***123
111***!15
222***459

непонятно почему ! стоит ниже чем 1 ведь ! в ascii\utf-8 имеет более маленький код



LC_CTYPE=C.UTF-8

! = 041h
1 = 061h
: = 072h
> = 076h
? = 077h

LC_CTYPE=en_US.UTF-8

! = 041h
1 = 061h
: = 072h
> = 076h
? = 077h

ASCII

! = 021h
1 = 031h
: = 03Ah
> = 03Eh
? = 03Fh



хм.. коды хуйняя. так что онипоказывают?
в чем разница между locale и utf-8 ?


ТЕМА UNICODE:

	- man unicode ?

лбщая канва 

ld /dev | sort --> sort ---> c.utf-8 vs en_us.utf-8 ---> locale --> locale vs utf-8 -->
locale vs encoding vs charset ?

все руские буквы в utf-16

$ LC_CTYPE=ru_RU.utf8; for i in {0..47}; do  printf "\xd0\x$(echo "obase=16; ibase=10; 144+$i"| bc)";  done;  LC_CTYPE=ru_RU.utf8; for i in {0..15}; do  printf "\xd1\x$(echo "obase=16; ibase=10; 128+$i"| bc)";  done; echo

АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюя


utf8 vs utf16 vs unicode ?


17 16-bit "planes"‡‡ (log₂(17×2¹⁶)≈20.1bits).

unicode:
$ builtin echo -e "\u0428"
Ш

$ builtin echo -e "\u0410"
А

$ builtin echo -e "\u0048 \u0045 \u004C \u004C \u004F"
H E L L O

u0048 = code point. ( по мне это буква другими словами)

на сайте юникода я вижу что code point имеет вид 4 или 5 цифр в hex формате
5 цифра в hex это = 1 048 575 букв закодировано уже в юникод

еще нашел такое определение юникода : Unicode - a mapping of characters to integers ("code points") in the range 0 through 1,114,111 (откуда эта цфиар незнаю)

итак юникод это маппинг буквы в число. для каждой буквы (символа) задается число. 
называется это число code point. обозначается U-XXXXX где X это цифра из hex формата.

Unicode provides a unique number for every character, no matter what the platform, no matter what the program, no matter what the language.

encoding? codepage? charset?

в чем разница преоюрвзаования unicode в utf8 против utf16?
буква Ё
Unicode  = U-0401
UTF-8 = D081h	
UTF-16BE = 0401h
UTF-16LE = 0104h


$echo -e "\u0401"
Ё
$echo -e "\xd0\x81"
Ё

так насколко я понял из pdf документа о юникоде code points они начнаются с U-0000
и заканчиваются U-10FFFF
получается что этим можно закодировать 1 114 112 букв(символов)

как я понял unicode он делает соотвествие между множеством символов и целыми числами.
и это чисто такая математическая абстракция без привязки к компам.
и юникод это не кодировка.

а далее вступает в бой уже кодировка. такая как utf-8,utf16,utf32.
что они делают. они берут это целое число из предыдущего пункта и трансформируют его 
в другое число котороу уже привязано к компьютеру.

в чем прикол такой схемы в том что мы ввели дополнтельный уровень абстракции. 
что это дает. так как кодировка в той или иной мере завязана на подробности компа 
то если архитетктура компа меняется то у нас прерыдущий пункт осатется без изменений
а тольк меняется вот этот пункт которйый нижележащий. и еслискажем программа работает на уровне
предыдущего пунка то ее переписыать ненадо ибо от нее нижний уровень абстракции скрыт.
пока как то так.

As of Unicode version 14.0, there are 144,697 characters
чем более старшая версия юникода там больше там символов появляется.

 ISO/IEC 10646-1: 1993 полностью совпдатает с unicode 2.0

 вобщем насколко я понял юникод что он по сути делает он берет символ и дает ему номер порядковый - называется эникод число и обрзначается U-число. число обычно в hex формате.
 номера идут в диапазоне 0-10FFFF (10FFFF это почти три байта) это где то лимон символов можно закодировать. 
 но это чисто математическая хрень к применению на компе это еще негодится. далее юникод 
 предлагает дополниельное преобразование(преобразования бывают UTF-8, UTF-16LE, UTF-16BE, UTF-32 и они прописаны в стандарте юникода) которое берет юникод число и преобрзует в другое
 число и вот это уже число предлгается использовать для обрзначения символ в текстовых 
 файлах на компе. еще я хочу сказать что полный набор символов юникода напоминает язбуку 
 только мировую. туда собраны все символы всех алфавитов мира. полнйы набор символов юникода 
 от U-0 до U-10FFFF называется репертуаром.
  итак еще раз выглядит это так


 символ Ё --> U-0401 ---> (доп преобразование) ----> 81d0
и именно вот этот 81d0 можно использоать в текстовых файлах для обохначение буквы Ё.




 	- зачем двойное преобразование , почему вместо 0x81d0 неиспользовать в текстовых файлах сразу число 0x0401. резонный вопрос. 

 	- есть несколько преобразований. зачем несколко ?
 	- вопросы echo и printf они какой  поддерживают utf8, utf16, utf32?
 	- utf8 он до 1 до 4 байт. utf16 он 2 или 4 байта utf32 он 4 байта толко.
 все они поддерживают кодирование всех U- символов. тоесть полный набор (или говоря языком
 юникод документа весь репертуар). UTF-8 он может для кодирования одного символа использовать
 то 1 до 4 байт. дада. для каких то символов хватит и одного байта а для каких то целых четыре.
UTF-16 кодирует символ либо через 2 байта либо через 4. UTF-32 кодирует символ всегда через 4 байта. нафига такая чехарда. каждый encoding имеет свои плюсы и минусы. расмотрим UTF-8 его фишка в том что если у нас текст сосотоит из английских символов то UTF-8 кодирует каждый английскй симол используя только один байт. если бы мы использовали порядковый номер юникода 
то нам бы нужно было для каждого символа использовать три байта а тут для английских симвлов всего один . получаем сумашедшу экономию в три раза дискового места. плюс коды символов 
в UTF-8 для английских смиволов совпаюают с кодами английских символов в ASCII encoding
таким образом если мы закодировали анлийский текств UTF-8 и отсолали на комп который неумеет 
читать UTF-8 а умеет только ASCII то он покажет текст абсолютно коректно! в этом тоже 
чудесная особенность UTF-8 его преимущество. буквы руского алфавта занимв ают по 2 байта на символ это уже конечно больше чем один байт на символ для анлглийской буквы но это занимает 
меньше чем три байта если бы мы испльзовали в качестве байтов unicod номер который три байта длинной. если мы насуем в одини тот же текстовый файл буквы из английского и руского алфавита
в кодировке UTF-8 и пошлем на комп который только умеет ASCII показывать то человек там увидит 
только ангийские буквы коректно. так что то что символы U-0 U-127 которые отвечают за английский алфавит они кодирутся всего одним байтом и эти байты совпадают с кодиовкой ASCII это очень даже большой плюс.
>B чем прикол utf16. во первых она уже несовместима с ASCII. упс! несмертельно но неприятно. 
у нас могут быть старые проги которые выдают поток байтов наружу думая что надо выдавать ASCII. поэтому программ будт думать что она выдает буквы AB а UTF16 будет думать что надо печататаь на экране QW. прикол UTF16 в том что  в юникоде весь набор симоволов все множество (или как они говорят репертуар) он разделен на planes. каждый plane это 65536 символов. так вот  в plane 0 который зовется BMP они включили самые часто используемые символы.  а так как 
65536 это 16бит то получается что через UTF-16 весь этот набор символов BMP он вмещается
в кодировку через два байта. это дает то что если у нас тексты многоязычные и мы будем 
их кодировать через UTF-8 и UTF-16 то скорей всего на UTF-8 с одной стороны часть символов 
будет представлена через однобайтное выражение (английские символы) что более выигрышно
по сравннеию с двух байтным в UTF-16 но часть символов будет идти в трехбайтном представлении
 что менее выгрыно по сравнеию с UTF-16. конечно же часть символов в UTf8 будет идти в двухбайтном выражении также как это есть в UTF-16 что неделает разницы. таким образом
 на какихто текстах и каких то обьемах UTF16 может дать меньший обьем на диске в случае когда тексты многоязычные. я так понимаю. но конешно очень серьезные минусы UTF-16  в том что
 если тексты ангийский то он будет занимать в 2 раза больше чем в UTF8. и то что он ASCII несовместим. есть правда очень сильный плюс utf16  в том что если мы хотим напечатать букву из какого то языка то с огромной доолей вероятности она вхоит в plane0 и это значит что 
 номер в формате юникода U-1234 автоматом нам дает байтовое представление в кодировке UTF16
 потому что она будет точно такая же. примеры:
буква  Ё 
Unicode: U-0401
UTF-16 Encoding:	0x0401

арабская буква ؈
Unicode:U-0608
UTF-16: 0x0608

таким зная порядковый номер буквы в юникоде мы сразу с огромной долей вероятности можем
коректно налету получить байтовое представлени в UTF16. потому что оно ровно такоеже.
Для сравнения двухбайтовое представление в UTF-8 оно коренным образом непростое из юникод номера. например буква Ё(U-0401) в UTf-8 имеет вид 0xD081. согласись что 0401 в D081 никак налету неполучить в уме. поэтому в этом огромная фишка UTF-16.
 >UTF32 дает толлько то что все аюсолютрно символы занимают одинаковый размер 4 байта. хотя мне непонятно зачем было 4 байта когда можно было 3 байта сделать ведь на дайннйы момент
 весь набор эникода это 10FFFF что составляет три байта в ширину. 
 > в целом прикол юникода в том (неважно в какой кодировке UTF8\16\32 хотя utf8 по мне самая сильная кодировка) у нас в тексте который закодирован в utf8\16\32 могут идти слова предложения и буквы сразу из всех языков мира одновременно.
 В любом случае теперь видно почему на компы в текстовые файлы не стали шарашит порядковые
 номера юникода для символов. потому что еси ввести допонительный слой абстракции\преобразования из юникод номера в некоторый новый номер через UTF-8\16\32 то  в определенных случаях у нас получаются более выигрышные по обьему тексты (более маленькие) плюс обратная совместимость с ASCII. и вобще кто знает какой в будущем более выгодный encoding вместо UTF-8\16\32 придумают но при этом то класно непридется менять юникод номера. 
 это очень умно на будущее придумано. дополниельный уровень абстракции на нижнем уровне позоволяет играться с ним менять в будущем и при этом оставить без измеений верхний уровень. очень умно.  также предвариетльно насколько я понял фишка этого энкодинга в том что 
 байтовое представление из юникод номера ровно такое же самое как юникод номер.тоесть зная юникод номер мы сразу знаем предствлени этого символ в utf32 байтовом виде. тоесть для примера
 символ 🌈
Unicode: U-1F308
UTF-32 Encoding:	0x0001F308
ну что очень удобно.

>далее. про терминологию юникода:
code point - 
я узнал как назыается вот это число после U-0401. число называется code point.
unicode character -
символ который  поставлен в соотвествие code point называется unicode character. тоесть берется unicode character ( по нашему символ) и ему в соотвествие ставится
unicode code point
code unit -
значит  у нас есть символ в форме юникод числа (code point) и мы берем encoding (UTF8\16\32) и натравливаем на code point и получаем на выходе набор байтов один или  несколько тоесть некоторое множество байтов. так вот в этом наборе байтов один или несколько байтов назыается
code unit. размер code unit (тоесть сколько байтов он поразмеру) зависит от той encoding который мы использовали для преобразования. тоесть в UTF8 размер code unit = 1 байт.
в UTF16 размер code unit = 2 байта. в utf32 размер code unit = 4 байта. еще раз подчеркну что размер code unit завиисит от конкретного encoding. но в рамках этого encoding он фиксированный. тоесть в UTF8 code unit = 1 байт. в рамках UTF8 он всегда 1 байт  и никак иначе. в рамках UTF16 он всегда 2 байта и никак иначе. в рамках UTf32 он всегда 4 байта и никак иначе. ты меня спросишь но в UTF8 бывают character который занимает 4 байта например символ символ 🌈 (Unicode code point: U-1F308) у него UTF-8 Encoding =	0xF0 0x9F 0x8C 0x88 как это связать с однобайтовым code unit. связать так что поскольку символ занимает четыре байта а размер code unit для utf8 один байт это значит что символ занимает 4 code unit. поскольку размер code unit завиисит от encoding то когда мы говорим про то сколько code unit загимает символ то надо всегда упоминать в каком encoding. иначе нет смысла. итак в нашем случае мы имеем
символ: 🌈
юникод chataracter code point:  U-1F308
encoding UTF8: 0xF0 0x9F 0x8C 0x88, поскольку code unit (UTF8) = 1 байт то символ занимает 4 code unit в UTF8
encoding UTF16: 0xD83C 0xDF08, поскольку code unit(UTF16)= 2 байта то символ занимает 2 code point в UTF16
UTF-32 Encoding:	0x0001F308, поскольку code unit(UTF32) = 4 байта то символ занимает 1 code unit.

физический смысла code unit в том что это минимальный размер (в байтах) в котором в данном encoding может быть закодирован целиком символ. но это верно не всегда для всех символов.
в UTF32 всегда code unit кодирует целый символ. а в UTF8 code unit кодирует для ряда символов
символ целиком а для ряда символов только часть символа. в UTF16 code unit для ряда символов
кодирует в себе символ целиком а для ряда символов только часть символа. 
пример.

символ: 🌈
code point:  U-1F308
encoding UTF8: 0xF0 0x9F 0x8C 0x88,

в этом примере code point кодирует в себе только часть символа

символ !
code point = U-0021
encoding UTF-8 = 0x21

а в этом примере code point в себе закодировал символ целиком!

в этом и есть физический смысла code point.
по своей сути code point это один или несколко байтов.
с другой стороны смысл этого code point  в том что внутри него ( в этих байтах) закодирован
либо символ целиком  либо часть символа(зависит от конкретного символа). 

таким образом если мы знаем размер code point в байтах для данной кодировки то мы знаем
минимальный размер в байтах который нужен этой кодировке чтобы закодировать на диске один символ. да это верно порой не для всех символов но хотя бы для части это верно. размер code point нам дает предсталвение о прожорливости encoding при сохранении символов.



это один или несколько байтов который encoding (UTf8\16\32) использует для того чтобы преобразовать code point (символ) в набор байтов.  
значит в каждой кодировке наш character(символ) имеет вид набора байтов. так вот каждый байт 
называется code unit.

 	 - вопрос как же было раньше до юникода? как в тексте можно было использовать несколько
 	 языков. скажем три. 

 	- почему utf8 без endieness. ведь у него до 4 байт.да ивообще а если 1 байт то что ненадо?
	- обьяснить 
		$ hexdump /tmp/r.txt
0000000 9ff0 888c                              
0000004
$ man hexdump
$ hexdump -C /tmp/r.txt
00000000  f0 9f 8c 88                                       |....|
00000004
	- page vs encoding?
	- /bin/echo vs bash builtin echo они какие из utf-8\16\32 поддерживают? 
тожесамое про printf. у него  как с этим.

берем пример
Ё:
Unicode: U-0401
UTF-8 Encoding:	    0xD0 0x81
UTF-16 Encoding:	0x0401
UTF-32 Encoding:	0x00000401

смотрим что написано в 
$ man bash
echo
              \xHH   the eight-bit character whose value is the hexadecimal value HH (one or two hex digits)
              \uHHHH the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHH (one to four hex digits)
              \UHHHHHHHH
                     the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHHHHHH (one to eight hex digits)

значит я разобрался что они хотели этим сказать:
\xHH  = если у нас echo идет вот в  виде echo e "\xHH\xHH\xHH\xHH..."
то ожидается что текс  в кодировке UTF8. 
вот пример
символ 🌈
Unicode: U-1F308
UTF-8 Encoding:	0xF0 0x9F 0x8C 0x88
UTF-16 Encoding:	0xD83C 0xDF08
UTF-32 Encoding:	0x0001F308

$ builtin echo -e "\xF0\x9F\x8C\x88"
🌈

из примера видно что байтовое представление символа в utf16\32 абсолютно несовпадает
с UTF8 однако именно UTF8 байтовое представлеие нам дало символ.
итак если мы вбиваем в echo некий набор байтов в hex виде то echo ожидает что это UTF8 кодировка. 

\uHHHH, \UHHHHHHHH = остальные два способа передачи инфо о символе \uHHHH и \UHHHHHHHH они не про UTF16 или UTF32.
с этими кодировками builtin echo работать неумеет! в опция \uHHHH и \UHHHHHHHH вбиваеися не байтовое представление из какой либо кодировки а непосредственно сам ЮНИКОД номер!! единственное что если юникод номер от одного до четырех символов то можно использовать
как форму \uHHHH так и форму \UHHHHHHHH а вот если юникод номер более четырех символов 
то можно использоваьт только форму \UHHHHHHHH. еще раз хочу подчеркнуть что в эти форму вставляется не байты из кодировок UTF8\16\32 . нет!  в эти формы вставляется непосредтсенно
юникод номер.примеры
символ 🌈
Unicode: U-1F308
UTF-8 Encoding:	0xF0 0x9F 0x8C 0x88
UTF-16 Encoding:	0xD83C 0xDF08
UTF-32 Encoding:	0x0001F308

поскольку юникод номер больше чем 4 цифры то использум форму с большой буквой U тоесть 

$ builtin echo -e "\U1F308"
🌈

хотя надо вот что еще упомянуть поскольку кодировка UTF32 она имеет вид ровно такого же числа
что и code point хотя в байтовом виде они конечно оличаются потому что хотть число и одно и тоже но code point занимает 3 байта а utf32 занимает 4 байта. я это к чему что вот эта штука
\UHHHHHHHH трудно понять все таким это UTF32 или это code point представлен. и вообще еще более того непонятно они пишут что 8 цифр можно вбить. это 4 байта. так вот юнико код point он имеет в длинну максимум 3 байта ну сам посуди 10FFFF макс число это же три байта. яеще потом 
в другом места в вики нашел что числа после \u или \U это именно точно code point юникода.

возврашаюсь еще раз к 
$ man bash
echo
              \xHH   the eight-bit character whose value is the hexadecimal value HH (one or two hex digits)
              \uHHHH the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHH (one to four hex digits)
              \UHHHHHHHH
                     the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHHHHHH (one to eight hex digits)


значит что ожидается вот в этой опции вводить \xHH. Ожидается что HH это hex байт полученный 
при операции encoding UTF8. что значит операция encoding UTF8. берем символ , находим для него юникод code point далее натравливаем на этот code point функцию преоброзования UTF8. на выходе получаем некоторый поток байтов. вот эти байтв в hex виде и надо вставлять в echo  в виде \xHH\xHH\xHH... и тогда на экране мы получим напечаттанный символ.
примеры

символ:  Ё
code point: U-0401
UTF8 (поток байтов): 0xd0 0x81 

$ builtin echo -e "\xD0\x81"
Ё

поток байтов после операции encoding называется по научному Code Unit Sequence (CSE).
будем далее так и именовать.

По поводу процесса энкодинга. можно его делать руками ибо есть правило преобразования.
на практике более просто это зная code point зайти в интернет и найти UTF8 CSE в таблице.

Тоесть алгоритм такой: 
хотим напечатать символ --> ищем на сайте юникода какой у символа code point ---> либо руками делаем преобразование code point в UTF8\16\32 CSE либо зная code point ищем в таблице. ---> юзаем echo -e "\xHH\xHH..." ---> готово

Еще примеры:
символ:  🌝
code point: U-1F31D
UTF8 (поток байтов): 0xf0 0x9f 0x8c 0x9d 

$ builtin echo -e "\xf0\x9f\x8c\x9d"
🌝


еще раз  скажу про 

$ man bash
echo
              \xHH   the eight-bit character whose value is the hexadecimal value HH (one or two hex digits)
              \uHHHH the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHH (one to four hex digits)
              \UHHHHHHHH
                     the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHHHHHH (one to eight hex digits)



навскидку неочень понятно что же надо вводить в HHHHH числах - где вводить UTF CSE (набор байтов после энкодинга) а где нужно вводить юникод code point. это ведь совсем разные вещи.
юникод code point это целое число с компом вобщемто несвязанное которое имеет символ
потом на этот code point можно натравить процессо энкодинга UTF 8\16\32 и получить 
некоторый набор байтов который называется UTF CSE. так вот надо различать толи данный ключ
требует ввести code point либо набор UTF CSE.
попробуем разобраться.

берем символ  🌈 RAINBOW , у него code point = 1F308
а его UTF8 CSE = f0 9f 8c 88

подставляем f0 9f 8c 88 в ключ \xHH

$ strace -xx -e write echo -e "\xf0\x9f\x8c\x88"
write(1, "\xf0\x9f\x8c\x88\x0a", 5)     = 5
🌈 

видно что в терминал ушел именно тот поток который мы вставили плюс радугу
на экране получили. значит все верно и значит что ключ \xHH он ожидает что мы в него
будем вставлять поток UTF-8 CSE.(причем важно что именно UTF-8 CSE а не UTF-16 CSE или UTF-32 CSE. это можно проверить подставить UTF16\32 CSE и увидеть что на экране будет мусор а не
радуга)

берем следущий ключ \uHHHH
есть подозрение что в него надо вставлять не UTF-8\16\32 CSE а code point. Единственное
что его размер должен быть не более 4 символа.
проверяем .берем символ Ё , его code point U-0401


$ builtin echo -e "\u0401"
Ё

получили букву Ё.


далее я запустил для этой команды strace
$ sudo strace -xx -f -e write -p  18167
write(1, "\xd0\x81\x0a", 3)             = 3

и тут видно что приложение посылает в терминал не code point а уже UTF8 CSE
\xd0\x81\x0a

\x0a  = это код новой строки. его добавила сама команда echo так что отбрасываем
и получаем что UTF8 CSE для буквы Ё это \xd0\x81 и это реально так.

так что получается мы ввели в echo code point его программа echo преобразовала в UTF8 CSE
и уже этот поток байтов шлет на fd=1 который ведет на /dev/pts/ короче в терминал.
тоесть в сам терминал юникод коды нелетят. 
итак HHHH ключа \uHHHH является юникод code point. 
хотя для некоторой части code point UTF-16 CSE имеет ровно такой же вид как и code point.
пример
символ: Ё 
code point: (U+0401)
UTF-16 CSE: 0x0401

как видим они совпдают но всеже я думаю что HHHH это все таки code point.


переходим к ключу \UHHHHHHHH

запускаем команду
$ builtin echo -e "\U1F308"
🌈

и трейсим ее
$ sudo strace -xx -f -e write -p  18167
write(1, "\xf0\x9f\x8c\x88\x0a", 5)   = 5

во первых мы получили радугу значит команда верная и значит что HHHHHHHH в ключе \UHHHHHHHH
это тоже юникод code point. (разница между \uHH и \UHHHHHHHH  втом что второй понимает code point более четырех HHHH).

во вторых видно что опять же команда echo она трансформировала код поинт а на терминал уже
послала UTF8 CSE. потому что согласно таблице f0 9f 8c 88 это UTF8 CSE для U-1F308
(\x0a UTF-8 CSE для символа перенос строки )

что меня смущает что \UHHHHHHHH позволяет ввести 4 байта а макс code point это 10FFFF который три байта длинной. хотя с друго стороны его конечно можно легко дополнит до 4 байта 0010FFFF.
а вот есть другая хрень которая 4 байта длинной это UTF32 CSE. и он четко равен code point
тоесть  к примеру.

code point = U-401
UTF32 CSE = 0x00000401

но опять же я смысла в четрыех байтах в UTF32 CSE непонимаю ибо даже для макс code point он выглядит как 0x0010FFFF тоесть он три байта данных и один байт пустой.
вобщем точно непонятно что же явлется смыслом HHHHHHHH для \UHHHHHHHH толи code point
толи UTF32 CSE. хотя они с другой стороны они всегда совпадают.

что еще прикольно. возможностью принимать не UTF8 CSE а code points умеет делать
echo который встроен в bash
а вот /bin/echo который другая программа он неумеет с code point работать.
он только умеет принимать и передавать просто hex числа без изменний тоесть 
по сути только UTF8 CSE

$ man echo
\xHH   byte with hexadecimal value HH (1 to 2 digits)

проверка
$ strace -xx -e write /bin/echo -e  "\xd0\x81"
write(1, "\xd0\x81\x0a", 3)             = 3
Ё

да сработало. получили букву Ё и байты переданы без изменений.

кстати и встроенный в bash echo также раотает с таким ключом - он посто передает без измеений
raw поток hex байтов в файл fd=1 и все расчитывая что мы знаем что мы делаем. и это уже 
дело драйвера терминала в ядре понять что мы ему шлем поток UTF-8 CSE.

$ /bin/echo -e  "\u0401"
\u0401

это мы убелиись что /bin/echo нееумеет работаь с code point.

а вот такой еще эксперимент

$ strace -xx -e write /bin/echo "🌈"
write(1, "\xf0\x9f\x8c\x88\x0a", 5)     = 5
 🌈

тоесть если через copy\past символ вставить то его /bin/echo умеет печатать.


далее printf.
что встроенный в баш что отдельная прграмма обе умеют принимать юникод code point


$ builtin printf '%b\n' \\U1F308
🌈

а вот тот printf котоырй внешнаяя программа он почему то ругается.
$ /usr/bin/printf '%b\n' \\U1F308
/usr/bin/printf: missing hexadecimal number in escape

ну  и хер с ним нехочу разбираться.


запускаю strace убедиться втом что builtin printf тоже принимает code point
трансформирует его в UTF-8 CSE и уже его шлет в fd=1

$ strace ...
write(1, "\xf0\x9f\x8c\x88\x0a", 5)     = 5

все так убедились.

больше нехочется разбтраться с printf.  нахер.

зато суть понятна вот какая - на входе в прогу может быть что угодно что UTF CSE что code point но в терминал мы шлем исключетльно UTF-8 CSE.

далее рассмотрим printf тот который невстроенный в баш а тот который /usr/bin/printf
так вот у него дебильный man
заявлен команды формат

printf FORMAT [ARGUMENT]...

идалее написано 
Print ARGUMENT(s) according to FORMAT.

так вот это наебалово. печатается именно FORMAT а аргумент это просто блядь опция.
второй важный момент это то что FORMAT должен быть заключен в кавычки!!! иначе ничего
вобще небудет работать!. об этом в man нет вобще нихера!

так что формат printf такой

$ env printf "тут мы всталвяем то что хотим напечатаать"

почему я юзаю env printf а не просто printf потому что это гарантирует что 
мы запустим внешнюю программу а не builtin башевский. (про команду env смотри файл env.txt)

далее в man printf написано как внутри кавычек вставлять спецсимволы
приведу часть из них

  		\"     double quote   = печать кавычек

       \\     backslash			= печать бекслеша

       \a     alert (BEL)		= непонял

       \b     backspace		 	= стереть один символ слева

       \c     produce no further output = все что после этого символ небудет напечатано

       \e     escape		= непонял

       \f     form feed 	= непонял

       \n     new line 		= Enter (это сумма \f\r)

       \r     carriage return = перенести курсор вначале этоже строки


вот пруф что \n = \f + \r

$ env printf "aaaa\f\rbbbb\f\r"
aaaa
bbbb


       \t     horizontal tab

       \v     vertical tab


непонял разницу между \f и \v

$ env printf "aaaa\vbbbb\n"
aaaa
    bbbb
$ env printf "aaaa\fbbbb\n"
aaaa
    bbbb



       \NNN   byte with octal value NNN (1 to 3 digits)

эта хрень позволяет нам указать команде printf какие символы мы хотим печатать в форме
нелитер а в форме голых байтов. поскольку в линуксе encoding UTF8 значит голые байты
нужно пихать в энкодинге UTF8. голые байты этот ключ принимает в формате octal

       \xHH   byte with hexadecimal value HH (1 to 2 digits)

тоже самое тлоько голые байты мы подсовываем в hex форме.

проверяем. в UTF-8 знак "!" имеет CUS (энкодинг code unit sequence) 0x21

$ env printf "\x21  \n"
!  

в UTF-8 знак  🌈 имеет CUS = 0xF0 0x9F 0x8C 0x88
проверяем
$ env printf "\xF0\x9F\x8C\x88  \n"
🌈  




       \uHHHH Unicode (ISO/IEC 10646) character with hex value HHHH (4 digits)

здесь мы вводим code point. но неболее 4 символов которые.
прикол в том что нужно обязательно указать неменее 4 символа.

$ env printf "\uA1  \n"
printf: missing hexadecimal number in escape
$ env printf "\u00A1  \n"
¡  
$ env printf "\u0401 \n"
Ё 
вывод: работает 

также прикол этой опции в том что она пишет ошибку если мы ей даем point code из первых
127 символов - те которые совпдаают  с ascii  и те которые кодируются одним байтом в UTF8
$ env printf "\u0021  \n"
printf: invalid universal character name \u0021


       \UHHHHHHHH
              Unicode character with hex value HHHHHHHH (8 digits)
тут тоже вводится юникод code point но важная особоенность в том что нужно обязательно 
вводить только 8 символов и неменьше.

$ env printf "\U060f \n"
printf: missing hexadecimal number in escape
$ env printf "\U0000060f \n"
؏ 

но есть еще прикол. обе опции \uHHHH  \UHHHHHHHH неумеют печатать code поинты у которых
в utf8 один байт. тоесть те код поинты которые совпадают с ASCII. тоесть с 0 до 0x7F(127dec)  символа - они выдают ошибку а могут печатать только с тех символов у которых энкодинг UTF8 хотя бы два байта.



$ env printf "\u21  \n"
printf: missing hexadecimal number in escape
$ env printf "\u0021  \n"
printf: invalid universal character name \u0021
$ env printf "\U00000021  \n"
printf: invalid universal character name \U00000021

первый печатный символ у которого два байта занимает энкодинг UTF8 это U-00A1
$ env printf "\U000000A1  \n"
¡ 
$ env printf "\u00A1  \n"
¡  

вобщем я бы сказал что сэтой командой много ебалы. с echo проще.

далее рассмотрим башевский встроенный printf
про него дает очень хорошее описание:
printf [-v var] format [arguments]

 The format is a character string which contains three types of objects: plain characters, which are simply copied to standard output, character escape sequences, which are
              converted and copied to the standard output, and format specifications, each of which causes printing of the  next  successive  argument. 


хотя только опытный глаз доагдается что character string это значит что format
 нужно заключать в кавычки "format"

printf [-v var] "format" [arguments]

тестирую

$ printf "\u21 \n"
! 
$ printf "\u401 \n"
Ё 
$ printf "\U21 \n"
! 
$ printf "\U401 \n"
Ё 


навскидку башевский printf в 100 раз лучше. им можно пользватся без мозгоебки.
он все печаатает на ура.


)




	-  ascii,charsets(7),  iso_8859-1(7),  iso_8859-10(7),  iso_8859-11(7),  iso_8859-13(7),    iso_8859-14(7),  iso_8859-15(7),  iso_8859-16(7),  iso_8859-2(7),  iso_8859-3(7),  iso_8859-4(7),   iso_8859-5(7), iso_8859-6(7), iso_8859-7(7), iso_8859-8(7), iso_8859-9(7), utf-8(7)

	- я чтото непонял aSCII это энкодинг или charset ?

	- unicode vs encoding vs charsets vs font ?

code unit - когда мы берем code point и натравливаем на него encoding алогоритм то на выходе
мы получаем некую послоедовательность байтов. причем бывают энкодинги у которых число байт на выходе разное непостоянное и зависит от конкретного code point. тоесть для каокго то code point энкодинг выдаст 1 байт а для какого то 2 байта а для какого то 4 байта. так вот code unit это минимальное число байтов которое получается на выходе при энкодинге при кодировании символа.меньше этого числа байт данный энкодинг никогда невыдаст. code unit всегда привязан к типу энкодинга тоест когда мы говорим про code unit то сразу надо уточнять а для какого энкодинга. теперь примеры. у энкодинга UTF-8 размер code unit =1байт. у UTF-16 размер code unit= 2 байта у UTF-32 размер энкодинга равен 4 байта. это значит что еси мы взяли code point и натравили на него utf8 то на выходе мы получим минимум 1 байт. если мы натравили UTF16 то на выходе получим минимум 2 байта. если натраваили UTF32 то получим минимум 4 байта которые кодируют этот символ. также совершенно понятно что на выходе из энкодинга может вылезти несколько штук code points. тоесть при энкодинге через UTF8 вылезет миниум один code unit (котоырй равен 1 байт) но может вылезти и несколько code units , по факту у UTF8 при кодировании символа может вылезти аж 4  байта. также надеюсь понятно то что на выходе из энкодинга байты будут вылезать кратным размер к code unit. тоест если у UTF16 размер code unit = 2 байта. то значит что из него вылезет либо 2 байта либо 4 байта  и никогда 3 байта. потому что 3 байта никак ненкратно 2 байтам (чему равен размер code unit у UTF16) 

encoding это утилита станадарт правило которое переводит (mapping) одно число в другое число
или числа. UTf8 переводит число code point в один или несколько байтов (которые тоже числа)

unicode это и есть charset . charset это character set.
charset он делает мапиинг соотвествие числа и символа. символ имеется ввиду как абстрактное
понятие несвязанное с компьютером. символ получаетс может быть описан просто словами.

0401 ---> русская буква Ё

получается

charset(unicode) ---> encoding ---> font --> экран

мы указем в echo unicode номер (code point) , echo обращается к encoding и он переводит
code point число в набор байтов ,далее передаем набор байтов в font он на основе этого числа
находит в своей таблице некией векторный вид для символа и теперь система может на основе
этого вектороной инфрмации нарисовать символ на экране.

так я думаю ?

согласно вики:
The Universal Coded Character Set (UCS, Unicode) is a standard set of characters defined by the international standard ISO/IEC 10646

итак unicode относится к charavcer set. 
charater set он делаем маппинг связку соотвествие между символом в его абстрактном описании
и некоторым целым числом.

целое число из character set уже можно использовать в компе. можно совать в текстовый файл
это число. кокнетно в случае юникода это неделают а используют доаполниеттбную трансформацию
этого числа в другие числа а точнее даже байты и уже их хранят в текстовых файлах. как я писал выше это слой виртуализации\абстркции вводят для того чтобы можно было при необходимости менять
экодинг а charset оставлвался тем же самым тоесть простым языком чтобы целые числа в char set оставались теми же самыми для символа "A" а на компе при необходимости чтобы можно было менять
каким набором байтов оно сохраняется в текстомо файле.
значит encoding нам позволяет получить набор байтов которые и будут храниться в текстовом файле. а для отображения на мониторе экрана нужен доп компонент это font. он мапит коды энкодинга в векторные хрени которые используются при работе с видеокартой. он позоволяет прервратиь байты которые видео карте похер в те примитивы которые используются видеокартой для отрисовки. условно гвооря font мапит байты энкодинга и команды головки принтера для печати.

truetype шрифт может содержть в себе только 65 535 симоволов. так что нет такого шрифта
который в себе сдрежит все юникож симоволы. ха. получается что все труды юникода обломались
об шрифты. что делать потому что я с этм столкнулся на терминале гнома  я ввржу code point а на экране фига. теперь я понял виноват шрифт.? если для данного encoding байтов  в шрифте
нет инфо то на экране обычно вводится квадратик. видимо как некий дефолтовый экшн.

утиилита gucharmap позволяет посмотреть какие символы есть в шрифте.

	- шрифты и юникод
про шрифты
скачиваем шрифт.
обычно это целая папка потому что всякие вот эти штуки типа курсивный шрифт
или жирный это не свойства шрифта а это отдельный шрифт. так что если мы ставим шрифт Terminus
то в его папке будет сразу несколько разновидностей  - обычный, жирный , курсивный.

как ставить шрифт.

ставим если лично для своего акаунта то :

создаем папку в ~

$ mkdir ~/.fonts

копируем папку в папку ~/.fonts
copy dir to ~/.fonts


пересоздаем кэш шрифов (fc-cache это команда из пакета fontconfig)
$ fc-cache -f -v

и он покажет что подцепил папку ~/.fonts

вот и все.

также интересно:
как посмотреть свойства отдельного шрифта

$ fc-cat | grep -i oxygen-sans.ttf

"Oxygen-Sans.ttf" 0 "Oxygen\\-Sans:familylang=en:style=Sans-Book:stylelang=en:fullname=Oxygen Sans Book:fullnamelang=en:slant=0:weight=80:width=100:foundry=newt:index=0:outline=True:scalable=True:charset=20-7e a0-122 124-137 139-140 143-152 154-17e 1c4-1cc 1f1-1f5 200-21b 237 2c6-2c7 2d8-2dd 307 30f 311 326 394 1e02-1e03 1e0a-1e0b 1e1e-1e1f 1e40-1e41 1e56-1e57 1e60-1e61 1e6a-1e6b 1e80-1e85 1ef2-1ef3 2013-2014 2018-201a 201c-201e 2020-2022 2026 2030 2039-203a 2044 2074 20ac 2122 2206 220f 2212 2215 2219 221e 222b 2248 2260 2264-2265 25ca fb00 fb03-fb04:lang=aa|af|ay|bi|br|bs|ca|ch|cs|cy|da|de|en|eo|es|et|eu|fi|fj|fo|fur|fy|ga|gd|gl|gv|ho|hr|hu|ia|id|ie|io|is|it|ki|la|lb|lt|mg|mh|mt|nb|nds|nl|nn|no|nr|nso|ny|oc|om|pt|rm|ro|se|sk|sl|sma|smj|smn|so|sq|ss|st|sv|sw|tk|tl|tn|tr|ts|uz|vo|vot|wa|wo|xh|yap|zu|an|crh|fil|ht|jv|kj|ku-tr|kwm|lg|li|ms|na|ng|pap-an|pap-aw|rn|rw|sc|sg|sn|su|za:fontversion=262144:capability=otlayout\\:DFLT otlayout\\:cyrl otlayout\\:grek otlayout\\:latn:fontformat=TrueType:decorative=False:postscriptname=Oxygen-Sans-Book:color=False:symbol=False"

lang=aa|af|ay|bi|br|bs|ca|ch|cs|cy|da|de|en|eo|es|et|eu|fi|fj|fo|fur|fy|ga|gd|gl|gv|ho|hr|hu|ia|id|ie|io|is|it|ki|la|lb|lt|mg|mh|mt|nb|nds|nl|nn|no|nr|nso|ny|oc|om|pt|rm|ro|se|sk|sl|sma|smj|smn|so|sq|ss|st| = эта хрень походу показывает алфавиты каких языков есть в шрифте

charset=20-7e a0-122 124-137 139-140 143-152 154-17e 1c4-1cc 1f1-1f5 200-21b 237 2c6-2c7 2d8-2dd 307 = это походу показывает какие code point юникода есть в шрифте. точнее старшие четыре hex числа. дело в том что в pdf от юникода обычно приводятся таблицы в виде когда
слева идет столбец от 0 до F а вверху идет строка со старшими hex цифрами.
тогда code point символа получается как то что мы берем верхнее число и приставляем справа левое число. пример.

русская буква Ё. для нее слева в столбце идет 1 а сверху 040 таким образом полный code point будет равен 0401. таким образом возвращаяесь обратно вот к этому charset=20-7e a0-122 124-137 139-140.  я думаб что если в шрифте указано например 139 то это значит что в шрифте есть все символы от 1390 до 139F тоесть каждое число обозначает целый столбик символов.



$ fc-list | grep -i term
/home/vasya/.fonts/terminus/TerminusTTF-Bold Italic-4.49.2.ttf: Terminus (TTF):style=Bold Italic
/home/vasya/.fonts/terminus/TerminusTTF-Bold-4.49.2.ttf: Terminus (TTF):style=Bold
/home/vasya/.fonts/terminus/TerminusTTF-4.49.2.ttf: Terminus (TTF):style=Medium
/home/vasya/.fonts/terminus/TerminusTTF-Italic-4.49.2.ttf: Terminus (TTF):style=Italic
vasya@vasya-Lenovo-IdeaPad-L340-15IWL:~/.fonts/terminus$ h


графическая утилита посмотреть каквыглядят символы в шрифте это утиилита gucharmap (это в командной строке) а в меню имеет имя "character map"

походу что я еще понял это то что gnome-terminal да и вообще любой терминал может 
использовать только mono шрифт. это не название шрифта а название свойства шрифта. суть свойства в том что все буквы внем имеют одинаковую ширину.

так можно посмотреть все моношрифты установлленные

$ fc-list | grep -i mono

еще что я понял это то что в терминале шрифт относится к настройками именно железки терминал 
и неимеет отношения к процессам который пускают инфо в терминал. ибо они пускают только потоки текста а уж каким шрифтом этот текст будет отрисован это уже забота железки терминал.
тоесть условно говоря прогама mc или bash понятия неимеют каким шрифтом отрисовано то что они посылают в терминал.

еще есть одна программа просмотра шрифтов = gnome-font-viewer

далее что интересно.
вот мы имеем символ Ё

символ Ё:
code point: U+0401

запускаю команду на его печать:
$ builtin echo -e "\u0401"

и далее через strace смотрю какие сисколлы будут использованы для его печати и вижу:
write(1, "\320\201\n", 3)

что это как это оно откуда и почему и зачем?

тот же символ но вводим в явном виде
$ builtin echo -e "Ё"
Ё
ее strace:
write(1, "\320\201\n", 3) 

видим что сисколл используется ровно также.

для сравнения символ "!"
$ builtin echo -e "!"
!
и ее strace:
write(1, "!\n", 2)

тотже символ но через юникод code point:
$ builtin echo -e "\u21"
!
strace:
write(1, "!\n", 2)                      = 2



тотже символ через hex
$ builtin echo -e "\x21"
!
strace:
write(1, "!\n", 2)                      = 2

прикольно ключ strace -T будем показыать сколлько времени система выполнялся сисколл.
например
write(1, "!\n", 2)                      = 2 <0.000125>
тоесть 0.000125секунды чтоли.

итак. я в итоге раскрыл секрет вотт этих записей \320\201 это байты которые записан в 
octal форме. тоесть 
\320 = 0xd0 
\201 = 0x81

а 0xd0 0x81 ничто иное как UTF8 для буквы Ё. так откуда взялся этот сраный octal?
как я понял это дефолтная настройка strace - что для всех стрингов которые не ASCII писать
их в octal виде. так вот у strace есть ключ -x который гооврит чтобы если сттринг не ASCII то пиши его в hex виде. пример
$ strace -x  ./hello.exe
execve("./hello.exe", ["./hello.exe"], 0x7fff9f60d3f8 /* 81 vars */) = 0
write(1, "\xd0\x81\x0a", 3Ё

а есть еще более прикольный ключ -xx он говорит что все стринги ( и те которые ASCII )
пиши в hex виде на экране

$ strace -xx -e write ./56.exe
write(1, "\x21\x0a", 2!
)                 = 2

x21 - это ASCII для "!"

таким образомтеперь понятно почему для оодних сисколлов write у нас на экране 
был символ
write(1, "!\n", 2)

для других сисколлов был octal вид байтов вместо символльного вида
write(1, "\320\201\n", 3)

здесь суть такая  в регистрах лежат параметры для сисколла. вызывается сисколл.
он читает параметры из регистров. и strace он нам показывает какой сисколл был вызыван 
и чему было равно значение параметров в регистрах для сисколла. возникает вопрос в каком 
виде показывать значения параметров. по дефолту strace берет значение из регистра и смотрит
есть ли у данного байта соотвствущий символ согласно ASCII encoding. если есть то strace
на экране рисует символ а если нет то выдает просто содержимое байта в octal виде.
как уже сказал выше ключами -x или -xx можно заставить starce показывать  байты  в hex виде.
вопрос почему strace неконерветирует байты в utf8. ответ думаю так проще.типа если просто символ то покажу а елси какойто мудленый то покажу просто байт.

в итоге я понял что физически при вызове сисколла write() в rsi хранится ссылка на адрес памяти
где лежит стринг который мы хотим напечатать. так вот у меня был вопрос в какой кодировке
там должен лежать стринг. ответ уже очевидно - в UTF8.
но я еще раз проверил. взял программу чисто на асемблере

$ cat hello.asm
section .text
global _start

_start:
    mov rdi, 0x1
    mov rsi, hello
    mov rdx, helloLen
    mov rax, 0x1
    syscall

    xor rdi, rdi
    mov rax, 60d
    syscall

section .data
    hello db "Ё", 0xa
    helloLen equ $-hello

она печатает букву Ё.
это задача компилятора (как я понимаю) на стадии создания бинарника 
перекодировать символ Ё в UTF8 бинарный вид.

 $ nasm -g -f elf64 -l hello.lst  hello.asm
 $ ld -o hello.exe hello.o


(gdb) disassemble _start
Dump of assembler code for function _start:
   0x00000000004000b0 <+0>:	mov    edi,0x1
   0x00000000004000b5 <+5>:	movabs rsi,0x6000d8
   0x00000000004000bf <+15>:	mov    edx,0x3
   0x00000000004000c4 <+20>:	mov    eax,0x1
   0x00000000004000c9 <+25>:	syscall 
   0x00000000004000cb <+27>:	xor    rdi,rdi
   0x00000000004000ce <+30>:	mov    eax,0x3c
   0x00000000004000d3 <+35>:	syscall 
End of assembler dump.
(gdb) x /3xb 0x6000d8
0x6000d8 <hello>:	0xd0	0x81	0x0a
(gdb) 

и мы видим что в памяти лежат коды символов
0xd0	0x81	0x0a

где 0xd0 0x81 это Ё в кодировке UTF8
0x0a это знако новой строки в UTF8

а это что показывает strace
write(1, "\xd0\x81\x0a", 3) = 3

таким образом ответ на вопрос в какой кодировке нужно класть в память байты чтобы
их понял сисколл который пишет в файл. ОТВЕТ = В UTF8.

тоесть сисколуу write  ядра информацию про строчку которую мы хотим напечатать надо передавать в формате UTF8

почему в utf8 наверное потому что наша настройка locale имеет utf8
$ locale
LANG=en_US.UTF-8

думаю поэтому. 
если бы наша локале была чтото типа koi8-R то наверное тогда в write надо было бы передавать
даныые в кодировке koi8-r.

многое прояснилось многое стало понятно.

как  я понимаю как работает strace.
перед тем как начинается исполнению syscall или стразу после того как я думаю что strace
смотрит во все регистры которые отвечают за передачу параметров сисколлу. итаким макаром 
он определяет и что за сисколл по номеру и чему равны его паарметры.
тоесть этого сисколл
write(1, "\xd0\x81\x0a", 3) = 3

я думаю что strace это увидел потому что 
mov    eax,0x1  = в rax лежала 1 значит это write()
mov    edi,0x1  = это дескриптор куда пишем fd=1 (stdout)
movabs rsi,0x6000d8  = ссылка адрес где лежит стринг в виде энкодинга
mov    edx,0x3  = это длинна текста сколько оттуда байт читать. три байта. и дейтствтельно
хотя у нас  символа два "Ё"" и "новая строка" но в кодировке это три байта.

насколько я понимаю в asm тексте можно вставить символы сразу ввиде utf8 энкодинга.
попробуем в программе что выше заменим строку

 hello db "Ё", 0xa

 на

  hello db 0xd0,0x81, 0xD0,0x9B, 0xD0,0x9A, 0xD0,0x90, 0xa

где

D081 = Ё
D09B = Л
D09A = К
D090 = А

получаем:
$ ./hello.exe
ЁЛКА

директива db в NASM означает что мы вбиваем данные в форме байта. 
а скажем директива dw говорит что мы вбиваем данные в форме слова. в 64бит слово это 8 байт.
тогда заменим еще раз строку

берем число D0
как оно будет побитно записано в регистре в цпу

регистр

старший бит ---> 1101 0000 <----- младщий бит

число в регистре  D    0                  


в памяти у нас нумерация битов в ячейкках идет наоборот
поэтому число в битовом виде будет зеркально наоборот

память

младший бит--->  0000  1011 < ---- старший бит
                   0     D

 какое это имеет значение? если мы хотим прочитать из памяти число однобайтовое то тогда это все неважно.
 а вот если мы хотим прочитаь число двух байтовое трехайтовое итд то это имеет уже огромное
 значение. берем двух байтовое число D0A0. пишем в память по одному байту.
 вначале пишем D0 а потом пишем A0
 получаем в памяти

 
 ячейка 0 младший бит ----> 0000  1011 < -- старший бит
                              0     D

 ячейка 1 младший бит ----> 0000  0101  < -- старший бит
                              0     A

  ---------------куда растет память ---------------->


  теперь я нарисую две ячейки рядом
 
   яч 0       яч 1 
  00001011  00000101
    0   D     0  A

далее мы говорим считать двухбайтовое число и даем ячейку 0 в качестве адреса.
возникает вопрос первый байт это старший байт или младший байт?
цпу знает что первые биты всегда хранят младшие биты информации. раз у нас  16 бит число
значит первый бит в яч0 то самый левый это самый младший бит 16битной конструкциии
а самый правый бит в ячейке 1 это самый старший бит в 16бит конструкции

                 яч 0       яч 1 
 младщий бит--->00001011  00000101 <--- старший бит
                  0   D     0  A


итак цпу понял где у этой конструкции самый старший бит. он справа. 
а я напомню что в регистре самый старший бит он слева

регистр

старший бит ---> xxxxxxx 


тогда в регистре наш поток битов из ячейки 0 и ячейки 1 будет записан так чтобы старший
бит ячейки 1 стал старшим битом регистра. тоесть пофакту битовая запись будет повернута 
зеркально тоесть 

вот как она выглдяит в памяти

                яч 0       яч 1 
 младщий бит--->00001011  00000101 <--- старший бит
                  0   D     0  A

и вот как эта битовая карта будет запсана в регистр

регистр
старший бит---->10100000 11010000
                 A    0    D   0

и тут мы видим самое главное и самое страшное что оказавшись в регистре мы получили 
число A0D0 а хотели получить D0A0. о чем это говорит? о том что если мы пишем в память побайтово а потом будем считыать эту конструкция как многобайтовое число (двух байтовое трехбайтовое восьми байтовое ) то при записи в память надо первым писать самый младший байт, потом более старший, потом еще более старший и самым последним надо писать самый старший байт.
тогда при счиытании будет все коретно. мы же сделали наоборот мы начали писать в память с самого старшего байта. как это легко все запомнить?
смотрим вот имеем число D0A0 это наша бумажная форма написания числа. при этом совершенно
точно понятно что чем левее буква тем она обозначает более высокий класс числа. тоесть
1234 и 4321 есть же разница. чем более левее цифра тем у нее больше вес . потому что бумажное
обозначание чисел это раскладка по базису 10^2*k+10^1*p+10^0*r итак понятно что чем более 
левее число тем оно весомее при записи на бумаге

бумага
			 			D0A0  <---- самая незначительная цифра

ровно такая же хрень внутри регисра. у него самый младший бит справа
регистр
      		10100000 11010000 <--- самый незначительный бит

а у памяти все наоборот у нее всегда слева младший бит (цпу интел пишет в память вначале млаший бит битового потока)
память
младший бит ----> 0010100101010101010101010101


именно поэтому если мы пишем в память байт за байтом а потом будем считыать не байт за байтом
а сразу в форме многбайтового числа то надо писать вначале самый младший байт. а последним
писать самый старший байт.


к чему  в план нашей конкретике вся эта лекция. к тому что 

мы хотим в память записать в UTF8 слово ЁЛКА

D081 = Ё
D09B = Л
D09A = К
D090 = А


0xD081  = 11010000 10000001
старший бит => 11010000 10000001 
а таком битовом виде биты будут лежать только в регистрах цпу. а если мы запишем это число
из регистра в память то оно там будет лежать в зеркальном виде.

100000001 00001011 <--- старший бит

так вот если мы записали D081 в память а потом начинаем читать побайтово.
счтываем первый байт 

100000001 <---- старший бит

получаем 81

считываем следущий байт

00001011 <--- старший бит

получаем D0

и тогда что мы видим что если мы в регистр положили D081 
и записали из него в память (через mov двухбайтовое число из регистра в память) то в памяти байты меняются местами старший с младшим 

ячейка 0   ячейка 1
   81        D0 

поэтому если мы в asm файле укажем вот такую строку

 hello dw 0xd081, 0xD09B, 0xD09A, 0xD090

(директива dw пишет в память двухбайтовые числа)

 то в памяти по факту байты будут лежать подругому

 81 d0, 9B D0, 9A D0, 90 D0

 а нам это ненадо потому что сисколл будет считывть байты побайтово инам надо чтобы в памяти
 байты лежали вот так

 0xD0 0x81 0xD0 0x9B 0xD0 0x9A 0xD0 0x90

еще раз напомню utf8 коды букв

D081 = Ё
D09B = Л
D09A = К
D090 = А


поэтому я говорю здесь несколько вариантов. 
первый вариант писать по байтово в память

 hello db 0xd0, 0x81, 0xD0 0x9B, 0xD0, 0x9A, 0xD0, 0x90

(директива db пишет в память побайтово)


второй вариант писать в память как двухбайтовые числа но тогда 
надо их модифицировать (переставить младший байт и старший)
 
hello dw  0x81D0, 0x9BD0, 0x9AD0, 0x90D0

поскольку они будут писатьс в память как двухбайтоое число то младший
и старший байт будут переставлены в памяти

вот хороший пример который показывает

dw    0x1234              ; в памяти будет 0x34 0x12 

а вот еще пример

dd    0x12345678          ; в памяти 0x78 0x56 0x34 0x12

(dd это директива double word четрые байта по факту)


и чисто для тренировки вот как надо написать если писать в память в форме четырехбайтового числа


hello dd 0x9BD081D0,  0x90D09AD0 ;  в памяти будет D0 81 D0 9B, D0 9A D0 90 

( вот так нам надо в памяти 0xD0 0x81 0xD0 0x9B 0xD0 0x9A 0xD0 0x90 )

вроде совпадает.

суем все все три строки в  программу

$ cat 40.asm
section .text
global _start

_start:
    mov rdi, 0x1
    mov rsi, hello1
    mov rdx, hello1Len
    mov rax, 0x1
    syscall

    mov rdi, 0x1
    mov rsi, hello2
    mov rdx, hello2Len
    mov rax, 0x1
    syscall

    mov rdi, 0x1
    mov rsi, hello3
    mov rdx, hello3Len
    mov rax, 0x1
    syscall


    xor rdi, rdi
    mov rax, 60d
    syscall

section .data
    hello1 db 0xd0, 0x81, 0xD0, 0x9B, 0xD0, 0x9A, 0xD0, 0x90, 0x0A
    hello1Len equ $-hello1

    hello2 dw  0x81D0, 0x9BD0, 0x9AD0, 0x90D0, 0x000A
    hello2Len equ $-hello2

    hello3 dd 0x9BD081D0,  0x90D09AD0, 0x0000000A
    hello3Len equ $-hello3

$ nasm -g -f elf64 -l 40.lst  40.asm
$ ld -o 40.exe 40.o
$ ./40.exe
ЁЛКА
ЁЛКА
ЁЛКА

$ strace -xx -e write ./40.exe 1>/dev/null
write(1, "\xd0\x81\xd0\x9b\xd0\x9a\xd0\x90\x0a", 9) = 9
write(1, "\xd0\x81\xd0\x9b\xd0\x9a\xd0\x90\x0a\x00", 10) = 10
write(1, "\xd0\x81\xd0\x9b\xd0\x9a\xd0\x90\x0a\x00\x00\x00", 12) = 12
+++ exited with 0 +++

( еще раз напомню вот так нам надо в памяти 0xD0 0x81 0xD0 0x9B 0xD0 0x9A 0xD0 0x90 )

отлично видно те последовательности которые были переданы в write()

все сработало! теория совпала с практикой.

 кстати заметим что наши стринги он небыли сохранены ни встек ни в heap
потому что мы их обьявили в  .data секции.

получается если нехочешь ошибится то вбивай utf8 коды в побайтовом виде в программу.

И еще раз самый главный важный вывод что если процесс пишет в терминал а наш процесс 
писал в терминал потому что он писал в fd=1 а он уходит в терминал

$ ls -1al /proc/self/fd
lrwx------ 1 vasya vasya 64 июн 19 00:15 1 -> /dev/pts/204

то тогда надо в терминал передавать информацию о символах в кодировке UTF8.
вот это у меня был центровой вопрос в каком виде инфо из процесса летит в терминал.
наш процесс писал через сисколл write в файл fd=1 который ведет в /dev/pts/204 который представляет собой гейтвей на терминал в формате UTF8 энкодинга.

итак еще раз важно - терминал ожидает поступлени в себя данных в формате UTF8.
это очень важная инфо.

ну кстати вот еще такой пример

$ strace -xx -e write echo -e "\xd0\x81\xd0\x9b\xd0\x9a\xd0\x90\x0a" > /dev/pts/204
write(1, "\xd0\x81\xd0\x9b\xd0\x9a\xd0\x90\x0a", 9) = 9

Когда в терминал ( устройство железку ) поступает поток UTF8 байтов CSE то терминал (который виртуальное устройство в виде кода ядра) печататет на экране символ используя шрифт  терминала. И тут фишка в том что шрифт может в себе иметь максимум 65535 символов так что так может статься что в шрифте прописанном в настройках терминала может не оказаться этого символа. поэтому если вместо символа на экране квадрат а с другими симаолваим все окей
то надо смотреть в сторону проверки поддерживает ли этот шрифт тот символ. или стоит поменять шрифт на другой.




- 	ручной процесс экодинга с юникода в UTF-8\16\32
в книге по юникоду "The Unicode® Standard Version 14.0 – Core Specification" 
я нашел таблицу для для преобразования юникод code point в UTF-8 CSE
____________________________________________________________________________
Unicode code point			|					UTF-8 CSE
----------------------------|-----------------------------------------------
Scalar Value                |First Byte  Second Byte  Third Byte  Fourth Byte
00000000 0xxxxxxx           |0xxxxxxx
00000yyy yyxxxxxx           |110yyyyy    10xxxxxx
zzzzyyyy yyxxxxxx           |1110zzzz    10yyyyyy     10xxxxxx
000uuuuu zzzzyyyy yyxxxxxx  |11110uuu    10uuzzzz     10yyyyyy    10xxxxxx



посмотрим на первую строку
Scalar Value                |First Byte  Second Byte  Third Byte  Fourth Byte
00000000 0xxxxxxx           |0xxxxxxx


0xxxxxxx => макс число с таким видом это 01111111 = 127(dec)=0x7F

получается UTF8 CSE для unicode code points в диапазоне 0h-7Fh имеет однобайтовый вид.
( и кстати совпадает с ASCII CSE).

например 
символ !
code point = U-21
проверяем, печатаем через юникод code point
$ builtin echo -e "\u21"
!
а теперт проверяем печатаем через UTF8 CSE
UTF8 CSE = 0x21
$ echo -e "\x21"
!
совпало. значит UTF8 CSE найдем верно.




смотрим дальше
Unicode code point			|					UTF-8 CSE
----------------------------|-----------------------------------------------
Scalar Value                |First Byte  Second Byte  Third Byte  Fourth Byte
00000yyy yyxxxxxx           |110yyyyy    10xxxxxx

code point вида 00000yyy yyxxxxxx имеет макс номер 0000011111111111 = 7FF
минимальны номер ноль но мы помним что предыдущий диапазон заканчивается на 7F 
значит этот начинается с 7F+1=0x80
итак диапазон code point  80h-7FFh
в этом диапазоне UTF CSE имеет вид двухбайтного множества.

проверяем. 
символ Ё
code point U-401

$ builtin echo -e "\u401"
Ё

преобразуем согласно схеме:
code point = 0х401 = 10000 000001
тогда UTF8 CSE = 11010000 10000001 = 0xD0(first byte) 0x81(second byte) = 0xD0 0x81

first byte это значит он отсылается программе первее или хранится на диске первее.
а second byte вторым.

проверяем

$ builtin echo -e "\xD0\x81"
Ё

сработало. значит UTF8 CSE мы нашли верно!
так что с ручным UTF8 encoding процессом разобрались.


	- Далее тонкий момент про многобайтный UTF CSE  и cpu endiness







