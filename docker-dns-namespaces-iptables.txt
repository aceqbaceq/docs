| docker
| iptables
| route
| dns
| namespaces

все началось вот с чего.
я создал докер контейнер. 

$ docker network create lan-2

$ docker run  -it --network lan-2 --name debian3 --cap-add=NET_ADMIN --cap-add=SYS_MODULE --privileged debian bash


и я постмрел а какой днс сервер указан внутри
контейнера

# cat /etc/resolv.conf | grep -v '#'

nameserver 127.0.0.11
options ndots:0


и я подумал что по идее трафик днс реквестов
течет в конечном итоге на докер сервер. 
оставалось только понять каким маккаром
поток попадает чисто технически на докер сервис

флаги --cap-add=NET_ADMIN --cap-add=SYS_MODULE --privileged 
нужны для того чтобы внутри контейнера мы запустили
iptables утииту и она показала бы набор правил которые
рабтают внутри контйенера . без этих флагов внутри
контйенера иптбейлс правла работат также вгутри контейнера
но посмреть эти правила хуй получится.

итак вот что  я внутри контйенера обнаружил
что днс сервер это 127.0.0.11


# cat /etc/resolv.conf | grep -v '#'
nameserver 127.0.0.11
options ndots:0


далее вот такие иптбейлс правила


/# iptables-save
# Generated by iptables-save v1.8.9 (nf_tables) on Sun Jul 27 20:35:57 2025
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:DOCKER_OUTPUT - [0:0]
:DOCKER_POSTROUTING - [0:0]
-A OUTPUT -d 127.0.0.11/32 -j DOCKER_OUTPUT
-A POSTROUTING -d 127.0.0.11/32 -j DOCKER_POSTROUTING
-A DOCKER_OUTPUT -d 127.0.0.11/32 -p tcp -m tcp --dport 53 -j DNAT --to-destination 127.0.0.11:35569
-A DOCKER_OUTPUT -d 127.0.0.11/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:54826
-A DOCKER_POSTROUTING -s 127.0.0.11/32 -p tcp -m tcp --sport 35569 -j SNAT --to-source :53
-A DOCKER_POSTROUTING -s 127.0.0.11/32 -p udp -m udp --sport 54826 -j SNAT --to-source :53
COMMIT
# Completed on Sun Jul 27 20:35:57 2025


и вот такой набор слушающих сокетов

# ss -tnlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
LISTEN      0           4096                127.0.0.11:35569                0.0.0.0:*    


# ss -unlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
UNCONN      0           0                   127.0.0.11:54826                0.0.0.0:*                    

тут правда невидно какой процесс слушает на этих 
сокетах. чтобы это узнать надо подругому посмотреть




$ sudo nsenter -t 6556 -n -p  ss -tnlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
LISTEN      0           4096                127.0.0.11:35569                0.0.0.0:*         users:(("dockerd",pid=2404,fd=84))


$ sudo nsenter -t 6556 -n -p  ss -unlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
UNCONN      0           0                   127.0.0.11:54826                0.0.0.0:*         users:(("dockerd",pid=2404,fd=83))


значит суть получается какая.
положим что наша юзер программа типа dig
через udp смотрит на resolv.conf 
и видит там 127.0.0.11 и делает запрос по UDP 
на 127.0.0.11 и порт 53


эта хрень из сокета приложения попдает в иптббейлс
и там срабатывает правило

-A DOCKER_OUTPUT -d 127.0.0.11/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:54826

и у этого пакета дст_порт меняется с 53 на 54826


и после этого пакет попадает в сокет 
от процесса докер сервера

$ sudo nsenter -t 6556 -n -p  ss -unlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
UNCONN      0           0                   127.0.0.11:54826                0.0.0.0:*         users:(("dockerd",pid=2404,fd=83))


который принимает этот пакет. и обслуживает его
как днс сервер. 


теперь я расскажу важные детали.
первый момент. 
нахуй нужны роуты в таблице маршруизации. что 
они дают. дело в том что если пакет уже влетел входящий
из сети то для его принятия и обаботки роуты нахуй не 
нужны. ротуы нужны для исходящих пакетов.
когда наша прога хочет послать пакет на адрес 1.1.1.1
на порт 53 по протоколу UDP то она больше ничего ядру
несообщает. а ядру чтобы сделать эту передачу ему 
еще нужно определить какой в пакет вставить сорс_ИП
и сорс_ПОРТ и  через какой физ порт этот пакет нужно
сунуть в сеть. так вот ядро это все опредляет из 
записи в таблице маршрутиазции. без маршрута ядро 
не сможет это ничего определить. и пакет исходящий
отравить не сможет! итак если нет маршрута то пакет
отправить не полуится никога. невозможно.

теперь вспомим что на компе есть вирт сет карта LO

# ip -c a sh dev lo
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo


вотличии от реальной физ карты в котору если снуть
пакет то он полетит в релаьну сеть а если сунуть пакет
в lo порт то он полетит в особый кусок ядра который
эмулирует сеть. 

вот у lo порта назаначен вот такой ип

  127.0.0.1/8


тоесть утверждается что за этим портом сидит куча
компов с ип адресами в диапзааоне 

  127.0.0.1 - 127.255.255.254

если я суну в этот порт пакет с ип из этой сети
тоесть например пингану комп 127.0.0.100 то пакет
улетит в этот порт и попадает в особый кусок ядра
который эмулирует эту сеть и ядро в обратку пришлет
ответный пакет якобы с того удаленного компа.

но! как я уже сказал что от тго что я назначил ип
для сет карты это мне нихуя не дает для исходящих
пакетов. нужен обязательно маршрут.
если я посмотрю список маршуртов

# ip -c r sh
default via 172.20.0.1 dev eth0 
172.20.0.0/16 dev eth0 proto kernel scope link src 172.20.0.2 

то я тут нхуя не увижу никкго маршрута ведущего на
lo порт. ЧТО ЗА ПЗДЦ?
оказывается что маршруты которые ведут на lo порт 
они лежат в ДРУГОЙ ТАБЛИЦЕ МАОШРРУТИЗАЦИИ!


# ip -c r sh table local
local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 
local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 


итак получается если я из проги хочу пигануть комп
с ип 127.0.100.100 то ядро смотрит в таблицу маошутиацзии
local и ищет там маршрут

# ip -c r get 127.0.100.100
local 127.0.100.100 dev lo src 127.0.0.1 uid 0 
    cache <local> 

и ядро из таблицы маршрутизации получает инфо 
о том что данный пакет нужно совать в LO порт
и что у этого пакета нужно использовать src_ip=127.0.0.1


вот так оно работает. и оно реально работает

$ ping -c1 -4 127.0.100.100
PING 127.0.100.100 (127.0.100.100) 56(84) bytes of data.
64 bytes from 127.0.100.100: icmp_seq=1 ttl=64 time=0.164 ms


тоесть моя прога создает сокет. и в нем 
указывает что dst_ip=127.0.100.100
далее ядро из таблицы мащуриации узнает что в сокет
нужно дописать src_ip=127.0.0.1
и потом берет пакет и пихает его в lo порт  при этом
протаскивая пакет по иптбейлс цепочками таблицам.
далее из порта пакет попадет не в сеть  а в особый
кусок ядра которйы эмулирует сеть за этим портом.
ядро принимает пакет и формирует обратный пакет.
и он поступает на вход порта LO как бутто из внещнего
мира. 

теперь я коснусь такого момента что каков формат
имеют записи  в таблице local табтицы маршутизации.


# ip -c r sh table local
local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 
local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 
broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 

local 172.20.0.2 dev eth0 proto kernel scope host src 172.20.0.2 
broadcast 172.20.255.255 dev eth0 proto kernel scope link src 172.20.0.2 



первое что я хочу отметить что слева стоит слово 
либо local либо broadcast
а в обычно таблице слева ничего не стоит. есди 
ничего не стоит то на самом деле это подразумевает
что стоит слово unicast просто лиункс нихуя его не печа
тает! слово local как я понимаю означает что трафик
суется не  в  реальную карту а в LO порт. 

следущий важный момент что в обычгой таблице марщру
тизации (которая зовется main) в ней если мы видим
слово dev то оно означает в какой порт нужно сунуть
данный трафик. в local таблице слово dev имеет 
совршенно другое значение а именно  - неважно какой
порт указан  в поле dev трафик данный по любому
будет засунуть в порт LO ! и это шок!!

тоесть наример вот эта строка

local 172.20.0.2 dev eth0 proto kernel scope host src 172.20.0.2 

означает что если я буду пинговать  172.20.0.2
то несмотря на то что dev eth0 то данный пакет
будет сунут в LO порт!

я могу создать любой ип или сеть и присовиить еей
статус local и тогда при пинге хостов этой сети
ядро будет совать пакеты в LO порт. и посылать обртные
пакеты из "сети" на мой lo порт как бутоо те хосты
реально сидят за LO портом. пример

# ip route add local 23.23.23.0/24 dev lo table local


# ip -c r sh table local
local 23.23.23.0/24 dev lo scope host 


теперь пигуем хост 23.23.23.10


#  ping -c1 -4 23.23.23.10
PING 23.23.23.10 (23.23.23.10) 56(84) bytes of data.
64 bytes from 23.23.23.10: icmp_seq=1 ttl=64 time=0.174 ms

проверяю что поток идет именно через lo порт

# tcpdump -pnv -i lo icmp
tcpdump: listening on lo, link-type EN10MB (Ethernet), snapshot length 262144 bytes
21:15:18.263559 IP (tos 0x0, ttl 64, id 17422, offset 0, flags [DF], proto ICMP (1), length 84)
    23.23.23.10 > 23.23.23.10: ICMP echo request, id 5, seq 1, length 64
21:15:18.263599 IP (tos 0x0, ttl 64, id 17423, offset 0, flags [none], proto ICMP (1), length 84)
    23.23.23.10 > 23.23.23.10: ICMP echo reply, id 5, seq 1, length 64


значит когда у меня на компе есть физ порт eth0.
и я ему даю ип адрес 172.16.20.2 то ядро автоматом заности
маршурут до этого ип адреса в local таблицу.
и если я пингую из проги на компе этот ип 172.16.20.2
то из за этого пакет исходщий ядром будет засунут не
на карту eth0 а в порт lo ! так ядро всегда делает.

например вот физ карта

# ip -c a sh dev eth0
2: eth0@if19: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether c6:da:9a:3c:8d:82 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.20.0.2/16 brd 172.20.255.255 scope global eth0

и вот ядро создат автоматом вот такой маршрут

# ip -c r sh table local
local 172.20.0.2 dev eth0 proto kernel scope host src 172.20.0.2

причем замечу что укзывается dev eth0 а не dev lo
хотя по факту на основе этого маршрута ядро при 
пинговании с компа 172.20.0.2 будет совать пакет 
в LO порт а не eth0 порт!


следущий прикольный момент.
я привык вот к чему - у меня есть порты на компе
они имеют ип ареса. я в проге создаю слушающий сокет
и я его bind() к ип адресу который назначен на один 
из портов. то можно привзять сокет к ип который
назначен на сет карту на компе. но нельзя сделать
bind() для произвольного ип. ядро выдаст ошибку.
это понятно это привычно. но! это работает совершенно
по другому в случае LO порта. 
а именно.


вот у меня есть ип на lo порту

  127.0.0.1\8


так вот так как LO порт особенный то я могу 
навесить bind() для сокета нетолко на ип 127.0.0.1
НО И НА ЛЮБОЙ ИП ИЗ СЕТИ 127.0.0.1\8
тоесть я могу навесить сокет на ип 

   127.0.0.100

и типа никаких прблем. хотя формально на нашем компе
данный ип неимеет ни один порт!
но так как это Lo порт то для него это работает!
хотя формально комп с адресом 127.0.0.100 это не наш
комп а комп внешний который лежит за портом LO
более того так можно сделать для любого ип который 
принадлежит LOCAL таблице маршуртиацзии!!


# ip -c r sh table local
local 23.23.23.0/24 dev lo scope host 
local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 
local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 
broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 
local 172.20.0.2 dev eth0 proto kernel scope host src 172.20.0.2 
broadcast 172.20.255.255 dev eth0 proto kernel scope link src 172.20.0.2 


например я могу навесить на ип 23.23.23.10

вот такой прикол.
тоесть чисто фрмально наш сокет будет получать 
пакеты приходящие на сет карту которая лежит не
на нашем компе а на компе леежащем во внешней сети.
вот такой прикол.

положим у меня есть прога-А и я навешиваю ее слушающий
сокет на адрес 127.0.0.11 порт 53
и работает это так. если прога-Б шлет пакет 
на 127.0.0.11 порт 53 то ядро сует этот пакет
в LO порт. оттуда он подает в ядро. ядро смотрит
а нет ли сокета который навешен на один из ип адресов
которые входят в LOCAL таблицу маршуртиазции. и если
есть то ядро этот пакет сует в этот сокет и он 
поадает в сокет программы-А вот такой прикол. формально
ип 127.0.0.11 лежит вне нашего компа на каком то 
удаленом другом компе. но по факту пакет напрплный на 
тот комп в итоге поападает в сокет програмы нашего
компа.
так вот именно этот трюк исползует докер демон сервис!
он вешает свой слушающий сокет на 127.0.0.11 UDP порт  54826

# ss -unlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
UNCONN      0           0                   127.0.0.11:54826                0.0.0.0:*              


также докер демон вешает еще один слушающий сокет
на TCP протоколе на 127.0.0.11 TCP порт 35569

# ss -tnlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
LISTEN      0           4096                127.0.0.11:35569                0.0.0.0:*     


(далее я буду рассатривать только UDP сокет)

таким образом если моя прога пошлет пакет на UDP 127.0.0.11
порт 54826 то в итоге этот пакет будет ядром
сунут в LO порт а оттуда он будет сунуть в сокет
докер демона. 

но тут еще есть хитрость. у меня данный LO порт лежит
в сетевом неймсейсе контейнера. а докер демон процесс
он лежит в другом сетовом неймсейсе спрашивается 
а как это он смог навесить свой сокет на LO порт
сет неймсейса контейнра. что за хуйня. ОКАЗЫВАЕТСЯ!
процесс с рут правами может перклються на любой сетевой
немспейс. совтетвенно уивидеть сет порты этого 
неймсеса. затем навесить свой слушающий сокет на 
один из ип одного и портов. получить от ядра дескриптор
а далее этот процесс может переключться на другой
сетевой неймсейс и при этом полученный декриптор
будет по прежнему связан с сокетом который связан с
тем сетвым неймспейсом. и этот дексрпитор будет 
реально прдолжать раобтать!  кстати узнать текущий 
номер неймпейса у прцоесса можно через 

  /proc/pid/ns/...


таким чудным обрзаом раотет связь между сокетом
привзяным к 127.0.0.11 порт 54826 и докер демоном
коорый лежит в другом сетеовм неймспйесе.


можно заметит что при запуске комады в сет неймсейсе
контейнера я не вижу PID пцроесса котоырй владеет 
этим сокетом

# ss -unlp
State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
UNCONN      0           0                   127.0.0.11:54826                0.0.0.0:*              



этоп отмуо что докер демеон также лежит в другом
pid неймспйесе поэтому егоного пид вообще нет
в пид нейсмсейсе контйера. поэтому в свотсвах сокета
из пид неймпейса конейнера невоможно отобразит какой
у него пид. потому что его пид в пид неймпейсе 
контейнера просто остутвтует. вот такой тоже пиздец
и прикол.

как же я узнаю егоный пид? 
а я пользуюсь тем что  пид неймсейсы они как пцроессы
имеют древовидную стркутуру. тоесть есть родтельский
пид неймсейс. и он порждает дочерний неймспейс. 
ну тчнее у нас ест родительный процесс и он порждлвает
дочерний процесс но этот процесс также получает свой
новый пид неймспейс. при этом я вижу pid дочрернего 
процесса в своем родитестком неймпейсе.  а дочерний
процесс видит себя под другим pid обычно pid=1
и он невидит родителсьаий процесс и вобещ все прцессы
котоыре принадлелжать пид неймейсу в которомо
лжеит рдитель.
поэтому я иду на хостоый пид неймспйес и там ищу
какой PID имеет мой докер процесс с точки зрения
пид неймпейса. и далее я запукаю nsnenter кторый
входит в тот же сетеовой неймсейс что и мой докер
процесс но при этом он входит в пид неймсейс
котоыйр имеет хост. и в этой комбинации неймсейсов
я запускаю комаду ss которая рисует мне список
сокетов в этом сеетовом неймпейсе и при этом она мне
нарисует владельцев (пид процессов) с точки зрения
пид неймпейса хоста. и так как докер демон лежит в 
пид неймейса хоста то пид тут же обнаружится

 $ docker inspect debian3 | grep -i pid
            "Pid": 6556,


$ sudo nsenter --net=/proc/6556/ns/net --pid=/proc/1/ns/pid ss -unlp

State       Recv-Q      Send-Q           Local Address:Port            Peer Address:Port     Process     
UNCONN      0           0                   127.0.0.11:54826                0.0.0.0:*         users:(("dockerd",pid=2404,fd=83))


и вот я обнаруживаю

(("dockerd",pid=2404,fd=83))


то есть я обанруживаю что сокетом который привяан
к сет карте сетевого неймспейса конейнера владеет
процесс dockerd и что  деескриптор который связан с 
этим сокетом имеет номер 83

я почти все обьяснил кроме еще одного момента.
у меня полуается докерд слушает сокет 


127.0.0.11:54826

а клиент делая днс запрос будет обрааться на 
порт 53 . как же этот момент свзыватется? а вот
чрез иптбейлс приавило внутри контейнера


-A DOCKER_OUTPUT -d 127.0.0.11/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:54826


тоесть когда моя прога сделает запрос к 
dest_ip=127.0.0.11 UDP dest_port=53
то иптбейлс заменит в пакете дест_порт на 54826
и потом этот пакет она сунет в LO порт.
и он уже долетит до сокета 127.0.0.11 UDP dest_port=54826
и он успешно ядро будет в сунуть в этот сокет
и чрез дескиптор в докерд.который и начнет
как днс сервер обрабтыать этот запрос.
когда днс сервер то бишь докер сервис сунет в 
дескриптр 83 обратный пакет и он полетит обратно
то в ип тбейлс правило для обратного преоабозования
пакета из порта 54826 в порт 53 добавлять не нужно
это автоматом сделает сам иптбейлс на основе записи
в conntrack таблице! в этом плане работа таблицы NAT
в иптбейлс отличается от таблицы FILTER.
если в табице фильтр ей плевать на то что если мы
разрешили пролет пакета ТУДА то таблица filter
автоматом не будет пропускать ОБАРТНЫЙ ПАКЕТ! для этого
нужно в таблице FIELTER здавать правило и ТУДА и 
ОБРАТНО либо для обратного тарифка здавать правило 
вида

-A OUTPUT   -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

а вот таблица NAT она работает по другому. если у нас
пакета в одну сторону прошел. то автмоатом обратный пакет
будет обработан нужным образом тоесть порт или порты
и или ип адреса будут для пакета в обранутную сторону
летяющего заменены без всяких доп правил!



тоесть вот мы иеем в таблцие NAT праивло

-A DOCKER_OUTPUT -d 127.0.0.11/32 -p udp -m udp --dport 53 -j DNAT --to-destination 127.0.0.11:54826

оно делает то что если изнутри компа в порт LO 
у нас летит пакет то у него будет заменен dst_port с 53
на 54826
если у нас в LO прилеит входяий пакет обратный
то нат автмоатом на оснвое это же правила и коннтрак
таблцы сделает обмен с 54826 на 53


я поти все разобрал о том как же рабоатет днс
резволинг в докер контейнере. а точнее получается
как же днс запрос из контейнера долетает до докер
демона. кстати когда запрос долетате до докер демона
то он его далее резволвит через сетевой неймпейс уже
хоста и оратно через тот сокет которыимеет декриптор 83
сует обратно овтет. тоесть с точки зрения tcpdump
днс запрос от моей программы летит на LO порт
и ответ прилетает обратно из LO порта.

/# tcpdump -pnv -i lo icmp
tcpdump: listening on lo, link-type EN10MB (Ethernet), snapshot length 262144 bytes
21:15:18.263559 IP (tos 0x0, ttl 64, id 17422, offset 0, flags [DF], proto ICMP (1), length 84)
    23.23.23.10 > 23.23.23.10: ICMP echo request, id 5, seq 1, length 64
21:15:18.263599 IP (tos 0x0, ttl 64, id 17423, offset 0, flags [none], proto ICMP (1), length 84)
    23.23.23.10 > 23.23.23.10: ICMP echo reply, id 5, seq 1, length 64


тоесть через порт eth0 внутри компььюетра днс
трафик ни в коем случае не летает!

тоесть 



сет неймпейс контейнера  | сет неймсейс хоста
                         |
моя прога ---> LO порт   | 
---> ядро --->сокет  ----|--------> докер демон --> systemd
                         |                          resolved ---> wlp2s0 (сет карта хоста) ----> 8.8.8.8
                         |



и обратный ответ от докер демона летит
в контейнер вот так

сет неймпейс контейнера  | сет неймсейс хоста
                         |
моя прога <--- LO порт   | 
<--- ядро <---сокет  ----|<-------- докер демон <-- systemd
                         |                          resolved ---> wlp2s0 (сет карта хоста) <---- 8.8.8.8
                         |



тоесть сточки зрения сет карт
внутри контейнера весь обмен идет тоько
через LO порт. и более никак.



все я разобрал но осттеется еще один вопросик
нахуй  в иптбейлс стоит вот это правило


-A DOCKER_POSTROUTING -s 127.0.0.11/32 
   -p udp -m udp --sport 54826 -j SNAT --to-source :53


для успешного преобразования портов для обратного
пакета с овтетом от докер демаона это праивло
нахер ненужно! оно и так автоматом срабатывает.
это правило нужно толко если мой докер демон
сам ицнииирует udp конект через данный сокет
кудато там но я чтото не вижу ситации когда бы 
днс резолвер(докер демон) должен был бы инциировать
сам куда то конект. поэтому практическая польза
от этого правила мало ясна.
этот как если systenmd-resolved на компе вдруг
зачемто стало бы инциироват конект к каому то сервису
внутри компа. 
да безуслвоно systend-resolved он ицнииурет сам
конект к вненему резволверу 8.8.8.8 но это 
неимеет значения для нашего кейса потому что докерд
сервис он ицнииурет конект к внешнему днс резволверу
через соверешенно другой сокет который лежит в
стевом неймейсе хоста а не контейнера! вот!


тем немееенее я считаю что тема а как же рабоатет
днс резволинг внутри конейтрера  прояснена. вывдедена
на свет!

так что днс резволинг рабатет внутри конетйнера
не на основе модифициированных hosts файлов а на 
основе того что докерд демон выполянеяет роль 
днс сервера\днс резволвера. сам же докерд когда ему
нужно чтото зареволить как я понима смотрит в 
/etc/resolv.conf на хосте и обращается туда.
если же мы из конейтерра делаем запрос про DNS имя
которое равно имени контейнера то докерд никуда не 
обаращается а просто выдает ответ из своей локальной
базы из его головы.



