>>
одна из дебильных архитектурных реешений эластика в том что 
он свои настройки хранит нетольк в elasticsearch.yml файлах нет.
еще часть настроек он хранит в том же районе где и данных индексов
называется эта часть настроек как persistent настройки
так что чтобы увидеть эту часть настроек надо делать запрос

curl -s -X GET 192.168.0.221:9200/_cluster/settings/?pretty



>>
как сделать чтобы при создании нового индекса у него было заданное
число шардов и реплик чтобы каждый раз невспоминать и не задавать 
руками

	(см. "elastic задать дефолтовое число шардов и реплик для индексов.txt" )

>>
elastic сеть смотри отдельно ("elastic сеть.txt")


>>
интерфейсная нода занимает 32КБ места на диске 
мастер нода занимает 120КБ места на диске


>>
при развороте эластика на дата нодах надо указывать две 
очень важные переменные.

	cluster.initial_master_nodes
	discovery.seed_hosts


в первой cluster.initial_master_nodes мы обязаны перечислить все ноды которые являются мастерами 
на начальной стадии создания кластера снуля.
поулчается если на начальной стадии 10 мастеров надо все 10 указать
в этой переременной. это неудрбно однозначно. причем это не DNS имена нет.
это эластиковвские имена которые к dns неимеют нгикакого отношения.
на каждой ноде в конфиге эластика elastricseerhc.yml  оно указывется в строке

	node.name master-01

тоесть для этой переменной както красиво выкрутится не получается.
но единственное у меня есть мысль о том что возможно можно указать три мастера и этого уже хватает для отказоустойчивости а то что другие будут неуказаны это мне видится абсолютно неважно. ведь мастеров можно динамически прибавлятьи удалять а для началной стадии хватит указать и три.

а вот со второй переменной 	discovery.seed_hosts
все круто. в ней мы указываем всех мастеров в форме либо IP адресов либо их dns имен (эластик автоматом преврратит dns имена в ip адреса).
тогда для этой переменной мы можем указать просто DNS A запись например
	discovery.seed_hosts=master-hosts.local

и все. далее на нашем dns сервере мы можем добавлять или удалять 
кучу A записей с IP мастеров. при этом в конфиги эластика лазить и чтото 
менять в этой переменной неприедтся!!! дело в том что если у нас несколько A записей на dns сервере вида
	IP1	A master-hosts.local
	IP2	A master-hosts.local
	IP3	A master-hosts.local
то эластик при обращениик dns северу получит все IP причем при каждом запросе обычно dns сервер меняет порядок какой ip отдать первым. плюс 
как написано в доке эластика он умеет работать с этим слоучаем когда dns имя резволится в кучу IP и эластик засосет все IP которые получитиз dns сервера . что это дает. это дает то что в итоге эластик успешно засосет все ip для всех мастеров. таким оразом очень удобно с этой перменной. потому что в конфиге мы указыаем запись которая неменяется потом никогда и ни для какой ноды (нам ненужно менять конфиг на нодах при ихменении числа мастеров и ненужно преезапускать службу эластика на нодах что просто пркрасно). а все изменения делаем на внешнем dns сервере.

если мы ставим эластик на куб. то  в стейтфулл сет 
эти перменные выглядят так

$ cat  el-1-data.yaml
...
 - name: cluster.initial_master_nodes
            value: "elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,elasticsearch-master-3,elasticsearch-master-4,elasticsearch-master-5,elasticsearch-master-6,elasticsearch-master-7,elasticsearch-master-8,elasticsearch-master-9,"
   
		  - name: discovery.seed_hosts
            value: "elasticsearch-master-headless"
          

тоесть как я и сказал.  для первой переменной приходится указывать вот прям все начальные мастера. которых в случае 10 мастеров аж 10 штук
для второй переменной мы просто указываем одну запись 					elasticsearch-master-headless 
это dns имя headless сервиса. оно резолвится тоесть содержит в себе
сразу все IP всех мастер подов что нам и нужно.

было бы круто както чтобы эластик изменисял чтобы и для первой переменной
можно было указать dns А записть чтобы не прописывать конркетно имена
нод а указать просто некую переменную
тогда бы конфиг независел от имзенений в числе мастеров.


>>
возможно надо еще учитывать размер места на PV
чтобы на кластере можно было реплики создавать  новые
рпелика невсегда создается оптимально. она быает создается на однй
ноде а потом перемещсется на другую.бывает какието PV переполенны
рпеликами а какието пустые и потом они уже более равномерно
расрпделяются


>>
еще идея как планировать кластер эластика.

планировать исходя из 
			числа серверов
			фактора резилиенси
			числа макс подов на серверах
			
скажем имеем 5хсерверов
			также хотим чтобы при выпадении 2хсерверов все работало
			и еще хотим чтобы при выпадении одного или двух нагрузка по потоку ложиласт равномерно
			
положим вначале что на кажом сервере по 1хподу.
тогда всего подов 5 как и сереров.

(под-1) (под-2) (под-3) (под-4) (под-5)

положим что на поде n шардов

(n) (n) (n) (n) (n)

посмотрим какое должно быть число n чтобы при падении одного сервера
на оставшихся четырех поток распределился равномерно. а поток распредилится тогда когда число шардов поесле перераспределения было 
одинаковое.
	при падении одного сервера его n шардов должны равномерно нацело 
	расползтись по 4 серверам. 
	
значит условие №1. 
	n:4=N , где N - целое число

когда шарды расползлись то на каждом серверре будет n+n:4 шардов

(n+n:4) (n+n:4) (n+n:4) (n+n:4)

положим что еще один сервер упал. нам опять же надо чтобы располшиеся с четвертого сервера шарды нацело распределились по оставшимся трем
серверам. 

значит условие №2
	(n+n:4):3 =N2, где N2 - целое число

получаем два уравеннеия

n:4=N
(n+n:4):3=N2

выражаем n из первого подставляем во второе

5N:3=N2

при N=1, 5:3 <> целое
при N=2, 10:3 <> целое
при N=3, 5 = целое. нашли 

тогда n=4N=12, где n - число шардов на одном сервере. значит общее число
шардов = 60 , так как серверов 5.

значит изначально на кажом сервере по 12 шардов. после первой аварии
на каждом будет по 15 шардов. и после второй аварии на каждом будет по 20 шардов. 

ну и чтобы шарды могли откдуато браться после паления серверов значит надо  задать 2хреплики.

получается 60 шардов(по 12 на сервер) + 2 реплики.

теперь вопрос сколько же в итоге подов делать на сервер.
ясно ведь что неодин. 

если сделать по 4 пода на сервер.
	тогда пока все 5 сереверов живы то на кажом поде первого серевера
	будет 3 шарда. 
	кодга упадет пятный север то прилетит 3 шара они распределсятся
	по 4 подам певрого севрера и на трех подах будет по +1 а на четвертом +0 подов. то есть будет
	(3+1) (3+1) (3+1) (3). 
	немного реравномерно но нормально. на каждый секрвер суммарно будет 
	одинаковй ну а на чуток подов будет чуть больше поток.
 
	когда упадет второй сервер то прилетит 5 шардов. и они распределеятся по 4 подам.
	(3+1+1) (3+1+1) (3+1+1) (3+1+1)
	
	тоесть в итоге поток станет супер одинаковый на каждый под на каждом
	сервере и это более важно чем равнромерность на каждый под на предыдущем шаге. 
	
	тогда схема выглядит в  итоге такой:
		5хсерверов
		4хпода per server
		60хшардов ( 12хшардов per server, 3xшарда per pod)
		2хреплики
		
получается у нас будет 20PVx100GB. = 2000 GB из них полезная нагрузка 1\3
значит где то индекс макс будет 650GB. это 32.5ГБ per shard когда индекс достигнет предельного значения. 


		
     	
	




>>
поток который пуолчается сейчас
индекс 100ГБ.5 шардов. =1 копия. 
10 нод. по 5 цпу на ноду.

как я примерно представляю рабтает чтение. если на шарды сейчас идет
запись но наверно они используются только на запись а чтение идет 
только с реплик. а чтение с шардов идет только тогда когда на них не
идет запись.

итого наноду приходится. 1 шард или реплика 25ГБ. 5 цпу. 1\10 потока.
	получаем 1\10 потока на 5 цпу = 0.02 потока\цпу
	получаем 5цпу на 25ГБ = 0.2  цпу\ГБ.
	
	данные со сферы -загрузка цпу в выходные 0.53 цпу на ноду.(тоесть 10% на графике сферы для сервера).


если разбить индекс на 8 шардов. будет 12 нод.
	тогда на ноду придется. 2 шарда (25ГБвсумме)+ копия(25ГБ).
	4 цпу и 1\12 потока. 
		получаем 1\12 потока на 4 цпу = 0.021 потока\цпу
		получаем 4цпу на 25ГБ = 0.16 цпу\ГБ
		
получаем поток > на 5% 
цпу per ГБ < на 25% 
вариант похуже но вроде несильно.

		
>>
постулаты

надо на эластике установить дефолтовое занчение шардов 
и реплик

два пода на 100% грузяищих проц = как бутто две вируалки с половиной процессоров

сокрость отдачи пода пропорциоанльана потоку и числу цпу на шард

эластик несмаштабируешь дианаически поэтму выход несколько физ сереров
на которых он круится с запасом

отталкиваемся что размер шарда по рекомнадции = 10=30ГБ

откаухзосуточйивость любого приложения миниум на уровне выхода из строя
сервера целиком, значит на кжадом сервере в сумме полная реплика индекса

на сервере можно иметь 1 под а можно несколько. зависит только от стораджа
мы со стороны стораджа определяем макс размер PV который нам комфортно
выделять чтобы со стороны стораджа было удобно там с ним работы проводить.
тоесть мы нехотим для виртуалок динамически менять размер PV.хотми
его иметь простоянным. тогда число виртуалок на физ сервере зависит от того сколько нужно PV штук чтобы в него целиком влез индекс.
скажем макс PV = 100GB. индекс = 100ГБ ну значит две виртуалки на сервер.
потмоу что с точки зрения самого эластика (см постулат один+два) эластик
будет рабоатать одинаково что на сервере 1 под что 2 пода.

из прердыдущего получается что если сумма шардов на поде выросла за
границу PV то мы pv неменяем мы на сервер разворачиваем +1под.
тогда на предыдущих подах место автоматом освобождается.

также число подов зависит от размера RAM на хосте. 
1 под под эластик жрет ~ 27GB плюс надо сколько то выделить под его 
PV на RAM кэш файлов. скаежм 10GB на под итого 40GB. также куб начинает переживать
когда на хосте занято 80% памяти и он начинает выдавливать поды.
значит. пусть сервер имеет 192GB ram. можно юзать 75% = 144GB
тогда на этот сервер влезет 3-4 пода. пусть 3 пода на сервере макс.
тогда макс размер индекса будет 300ГБ. 1 сервер. на нем 300ГБ.на 3 подах.
число шардов 3. если нам по трупуту запросов этого сервера нехватает
то мы добавляем точно такой же сервер а на эластике увеличиваем реплику на +1. кстати время репликации индекса с сервера на сервер зависит от размера индекса вцелом а не от числа щардов и размера одного шарда.

на эластике включаем и на кубе прописываем эластк рэк аварнесс.

на эластике влючаем что при падении виртуалок подов другого сервера
чтобы на других серверах он не создавал никаких реплик. нам это нахер 
ненужно. это даст что пдаения другого срвера никак не насрет в конфиг 
другого в плане увеличения занятого места на его PV и не будет 
мега сетевой передачи. умер так умер сервер. и нахер 

при падении виртуалки на впределах того же сервера мы на виртуалках
этого сервера вкючаем что он НЕСОЗДАВАЛ реплики на оставшихся. он 
должен четко ждать пока умершая виртуалка воскреснет и пропавий шард
долэен возвратиться ровно в ту ноды с которой он выпал. это даст 
то что на PV этого серверра место будет отсаваться контрлируемо.
единственное что при падении даже 1 виртуалки на сервере индекс
на ней становится недоступен ибо в нем нехватает куска. 
зато полностью контиролируем забор места на PV. нет массированного
дурацкого движения шардов на сторадже с одного PV на другой PV

еще одно правило число шардов на подах одинаковое. всегда.

число виртуалок\подов на сервере невлияет на скорость отдачи эластика
на этом сервере (если индекс лежит там целиком). или одна или 100.
число виртуалок надо менятьт в большую сторону когда мы хотим на сервере
размещать все больший и больший индекс. чем больше индекс тем больше число
подов надо разворачивать на сервере. но число подов сверху ограничено
размером памяти сервера. при роста индекса мы увеличиваем поды на 
сервере потому что мы нехотим изменять размер PV для пода. вместо этого 
мы хотим просто добавлять очередной PV.
если сервер загибается по процессору то значит на сервер большой поток
запросов. надо эти запросы разделить  направив половину запросов  на другой сервер. тоесть надо сервер добавлять. а не на этом чтото мудить.

минусы иметь на физ сервере 1 большую виртуалку вместо кучки
маленьких. 
		вообще скорость отдачи ноды зависит от 
						обьема потока запросов
						количество цпу на шард
						обьем шардов на ноде
		и еще есть один параметр это фиксированность jvm max

если  забудем что jvm max фиксирован то при одинаковом входящем
потоке запросов на физ сервер без разницы что у нас одна большая
виртуалка что она подроблена на мелкие виртуалки 
								на которых лежат не весь индекс
								а только часть индекса.
например одна виртуалка и полный индекс или две виртуалки в два 
раза меньше по цпу и половинка индекса на каждой. в эбоих случаях
соотношение входяшего потока запросов к числу цпу к обьему индекса
отсается одним и тем же. тогда очевидно что надо использовать одну
большую виртулку так как управлять одной или ворохом конечно накладнее.
но теперь переходим к моментам почему одна большая виртулка это плохо
		вспоминаем про jvm max. непонятно как он там наполняется 
		но логично предположить что он набивается в зависимости от
		обьема потока запросов и от обьема нижележащего индекса.
		и дело в том что и на большой виртуалке и на двух половинчатых
		мы можем прописать один  и тот jvm max. тогда что получается
		получается что на половинчатой виртуалке jvm max обслуживает 
		половину потока запросов и половину индекса поэтому лучше 
		иметь мелкие виртуалки. чтобы jvm max не переоплнился.
		
		следущий момент. я хочу использовать pv фиксированного размера.
		скажем 100ГБ. тоогда на одну большую виртуалку надо будет 
		прикреплят в конфиге несколько таких pv. с точки зрения эластика
		надо чтобы он умел работать с индексом не водной папке а в несколь
		ких. может он умеет а может нет. с мелкиим виртуалками проблем
		нет. одна виртуалка один PV.
		
		следущий момент. если я хочу обслужить виртуалку на физ сервере
		и она одна то получается что я ее гашу и на другие виртуалки
		на других серверах нагрузка по потоку запросов подприыгивыает 
		ровно на тот обьем котоырй мы щас необслуживаем. получаем 
		скачок нагрузки.если виртуалк на физ сервере хотя бы две. то
		поток на этот физ серер будет продолжать идти. и другие серверы
		никак непочустсвуют. поток запросов на них неуменьшится.
		таким образом регламентные работы с виуалокй эластика не ставят
		кластер в полуаварийное положение не вызвыает скачок нагрузки
		на других членов. (единственное что это будет так работат 
		если в пределах физ сервера мы зададим +1 реплику чтобы при 
		остановке одной вируатауалки на другой были все шарды которые 
		были на отсановленной). более того так как мы работае с подами\процесами а не виртуалками работающая виртуалка будет 
		иметь в наличии и свойи процессоры и процессоры виртуалки что
		была оставновлена (чего нет при esxi и вртуалках). таким образом
		оставшаяся виртуалка как бы мгновееннно получит удвоенную мощность
		по цпу и сможет абсолютно отлично обслуживать удвоенный поток
		запросов на нее и свои и от той виртуалки что остановлена. таким
		образом регалментаня работа с виртуалкой неприводмт даже к полу
		аварийнмому состояние по кластеру по откликам.
		
		следущий момент. положим нам неочень хватает мощности с куба.
		и мы хотим развернуть еще скажем +1 виртуалку уже на сфере.
		то есть мы хотим смаштабироваться но не целиком на уровне +1 сервер а частично на уровен + виртуалка. если у нас крупная вируталка размером с сервер то есть скажем 16 цпу итп то мы неможем смашатабироваться. а если у нас мелкая виртуалка скажем на
		5 цпу (треть сервера) то такую дырку в ресурсах мы можем найти 
		на сторонних серверах.

в итоге что мы имеем когда мы имеем мелкие виртуалки по нескольуо штук
на сервер а не одну большую на сервер.
		мы легче можем масштабироваться горизонтально.можем увелчить
		число нод эластика отгрызив часть на стороннем сервере. чем меньше виртуклка тем легче найти кусок на сервере чтобы там создать ее копию. опять же в плане более легкой горизонтальной масштабирвемости. когда у нас одна болшая виртуалка то у нее весь больштй индекс целиком. и нам нужно на стороне искать чтобы там тоже было столько же много много дискогов места. когда виртулка малентькая то у нее и часть индекса на ней тоже маленькая. так что маленькуая виртуалка она маленькая и по цпу и по памяти и по диску что отлчно обегчает масшттабиврание.
		
		регламетная работа (отключение) одной виртуалки на физ сервере не приводит кластер даже к полуаварийной состоянию = то есть на других физ серверах неменятся поток запросов. нет этого скачка.
		на нашем сервере сумматрный поток зпросов тоже неменяется. на оставшихся на нашем сервере виртуалках поток на кжду увеличивается
		но и число цпу приходдящщееся на виртуалку увеличиваектся так что
		удельная нагрузка неменяется. а кмофртные регламетные работы значит что мы можем поменят насройки всего кластера всех нод
		заменяя их по одной штуке и при этом все рабоатет.
		
		мы небоимся ни увеличнеия потока зарпосов ни увеличнеия обьема индекса что это привдеет к переоплнения ограниченного jvm max.
		потому что мы всегда можем виртуалки подробиь еще на более мелкие
		и на каждой из них использвоаться jvm max таким образом у нас увеличиваектся удельный обьем  сколько jvm приходится на один запрос в секунду и на один ГБ индекса.
		
		
надо будет на виртуалках одного физ сервера держать исходную копию +1 реплику. тогда отклюбчение одной виртуалки на физ сервере нестрашно 
в том плане что весь индекс будет доступен на этом физ сервере.

добавление +1 физ сервера надо делать когда у мы хотим отвести чатсь 
потока зарпосов от текущего(их) серверов. чтобы уменьшит на него нагрузку
по цпу. добавление физ сервера для увеличения места под индекс это 
я считаю неправильно. надо добавлять новые сервреа только чтобы смаштабироваться по входяшему потоку. а индекс полностью целиком
дрожен лежать на каждом физ сервере.  итак масштабивраонеи по потоку
запросов понятное как делать.

масштабирвание по увеличнеи обьема индекса. сейчас я вижу это так.
есть сервер с обьемом RAM. на нем мы можем завести солько то макс подов.
скажем 3 пода. сразу их заводим. имеем фиксированный базовый размер PV.
скажем 100ГБ. вот и лепим один PV к поду получаем что макс можем 
хранить на этом физ сервере 300ГБ. так и делать. если нужно еще больше
увелчтить место под индекс то 
		либо придется либо на всех физ серверах добавить RAM 
				ипосле этого запустить доп поды вместе с доп PV    
		либо для всех подов изменить базовый PV
		либо расширять место вот таким способом - мы вводим доп сервера либо выкраиваем на серверах других куски на которых мы запускаем доп поды\виртуалки. таким образом мы получили доп поды и доп PV, далее мы увеличиваем число шардов на индексе. шард становится меньше по обьему мы распределяем эти шарды по бОльшему числу подов. например в моей исходной модели физ сервер хранить две копии данных. можно сделать чтобы физ сервер хранил скажем 0.5 копии данных а полная копия хранилась на двух серверах кпримеру.
		тогда у нас место под индекс увеличилось. но реданданси уменьшилась.
		неполучается увеличить место под индекс неменяя размер PV по всему
		кластеру или не меняя размер RAM по всему кластеру. единственное
		самое безболезненное в плане чтоб ненужна была перепубликация подов это хранить индекс с меньшим коэфццииентом редунданси.скжаем
		чтоб индекс хранился в сумме на двух серверах. тогда можно увеличитьместо под индекс только докупив доп сервера и развернув
		там доп поды той же конфигурации.
		а так получатся что увеличение размера индекса медленно или 
		быстро но увеличивает загрузку на цпу поэтому по мне при увеличении индекса надо увелчивать базовый PV но вместе с этим 
		идокупать доп сервера чтобы снять с каждого физ сервера часть потока таким оброазом удельная нагрузка на физ сервер по потоку
		цислу цпу на обьем информации останется прежним.
		
Еще важный момент то что при поломке пода надо запретить генерировать новые реплики взамен исчезнувших. это даст то что у нас дисковое пронстранство на нодах будем всегда контрлируемо его расход.
а доступность индекса при поломке заранее гарантировать нужным число реплик. никаких генераций шардов после поломки !

если у нас много сервров с той же мощность одного ядра но число ядер
заметно меньше чем у других то можно смаштабировать размер шардов и подов
так чтобы в итоге под поместился на том сервере. 
скажем два сервера с 16 цпу и пять серверов с 5 цпу. можно смасштабировать
так поды чтобы в итоге его размер жрал 5 цпу и засеилть все сервера.
тогда на них на всех будет одинаковая нагрузка. и получим  в итоге разделим поток запросов по всем физ серверам неодинаковым. главное чтобы
на сервере где 5 цпу было достатчно памяти под под. 


		


>>>
пример расчета конфигурации.

отталкиваемся вот от чего,
	задано:
		сервер вмещает 4 пода.
		размер PV = 100GB
    
	значит на этот сервер максимально влезет 4x100 = 400GB индекс.
	но вспоминаем что на сервере мы его храним в виде индекс+1копия
	на случай остановки однй вирртуалки на сервере и чтбы индексы все
	были доступны при этом.
		итак на такой сервер влезет 200ГБ индекса.
	
	теперь считаем как его надо разбить:
	дано: макс рекомендоанный размер индекса = 40ГБ
	200:40 = 5 шардов. нечетное число щардов мы сразу откидываем.
	ближайшее четное 6 шардов. итак будет 6 шардов но 6 шардов на 4 пода 
	никак нацело небьется. так что ищем ближайшее. это 8 шардов.
	8 шардов делится по 2 на 4 пода. итого будет 8 шардов + 1 копия.
	
    Итого:
	на данном сервере можно хранить максимум индекс 200ГБ
	 его нужно разбить на 8 шардов + 1 копия.


получается примерно такая схема. все оттакливется от размера RAM
на сервере и размера PV.


число цпу для сервера выбирается исходя из потока нагрузки и запаса
прочности по нагрузке. скажем если 8 ядер на сервер дают 40% загрузки
цпу а мы хотим 4 кратный запас по нагрузке то надо либо увеличивать
число цпу per server либо увеличивать число серверов

про 80% куба. надо чтобы сумма макс лимита по памяти подов + 20ГБ на
диск кэш = 75% от памяти. получается макс число подов на хосте = (75%RAM-20GB)/40GB 
для 192Gb число подов 3.1 пода нуу можно до 4 подов попробовать
для 256Gb число подов 4.3 пода тут можно до 5 подов попробавть
40GB на под это 27Gb на процесс +13ГБ на файловый кэш его PV


>> про эластик
если малеькое количество шардов то размер их будет очень большой.
больше рекомендованного. и копироваться они будут супер медленно с ноды на ноду  если одна из нод упала. и если шардов мало то и дата нод
будет мало и они будут требовать много цпу. но чем больше шардов
тем нужно болльше дата нод чтбы их равномерно распределить по нодам
также - чем меньше размер шарда и число шардов+реплик на ноде
тем быстрее она отработате и меньше будет цпу. то есть чем меньше
по размеру шард тем можно мельче в плане цпу делать тогда ноды.

по факту есть обьем данных суммарный обьем инедекс по которому надо пробежаться в ходе 1 запроса. и есть суммарый поток запросов.
чем больше суммарный обьем индекса  тем больше будет со вренеем цпу
сумманая загрузка сервера. чем больше сам поток запраосов тем больщше
цпу загрузка. поэтому неважно насклко кусочков резать индекс.

это только влияет чтобы его куски - шарды копировать додлго или недолго
по сети.

и то что размер памяти jvm 20GB.

а так видно чем больше кусков на сервере тем больше его цпу загрузка.

если куски распределны неаврномерно по физ серверам то и нагрузка
будет пропорциоанльная.

надо бы чтобы размер шарда был неболее 30ГБ.

почему бы не сделать индекс одним куском на 100ГБ. и засунуть его в 1
большую вирт машину

ответ на то почему индекс нехранят одним куском -> большой кусок нереально копировать по сети . очень плохо влияет 
на резилиенс.
окей разбили индекс на куски размером 10-30ГБ.
таки куски уже нормально копировать по сети.

ответ на вопрос почему этот индекс неважно одним куском или нарезанный назасунуть в одну большую виртуалку или физ сервер. потому что эластик работает на jvm. а его max heap size ~ 20GB.
 
 а что дает память jvm ее размер. это на самом деле вопрос. что оно дает.
 но будем наивно полагать что в ней например лежат какито горячие данные
 этого индекса.проблемы бы небыло если бы могли дать jvm всю физ память сервера. но нет. только 20GB. 
 
 тогда мы делим физ сервер на несколько виртуалок. вопрос сколько 
 ей давать памяти. неочень понятно от чего зависит необходимый обьем jvm от размера индекса или от потока запросов 
 или и от того и другого сразу.
 но вот  у меня обычно принято что если делать виртуалку то делать ее 
 на макс память на 20ГБ.
 на каждой даем 20ГБ
 
 в jvm и логично что чем меньше кусок данных от индекса на нашей виртуалке
 тем лучше его будет кэшировать этот 20ГБ кусок. поэтому мы на каждой виртуалке храним не полный индекс а его часть.
 
 значит исходя из размера индекса поделили его на кусочки размром 10-30ГБ.
 получили сколько то кусков. далее логично нарезать столько виртулок 
 чтобы кусочки внутри них лежали одинаковым числом. тогда загрзука
 каждой виртуалки будет одна и таже. раз куски одинаковые то и виртуалки делаем одинаковые и число кусков в виртуалках должно быть одинаковое.
 
  
 получается мы отолкнулись от размера индекса, перешли к тому
 что шард должен иметь желательный размер.
 получили число шардов.
 
 отсюда получили число виртуалок. а виртуалки хотят памяти. 
 значит в зависимости от исходного размера индекса на сервере должно быть
 столько то памяти.
 
 и все это на одном физ сервере.
 
 пример . индекс 100ГБ. делим на 5 частей по 20ГБ штука.
 размер норм.
 получаем что надо на сервере имеь 5 виртуалок. чтобы на каждой было 
 одинаовое число шардов. 
 
 
 при каком то внешнем потоке запросов одного сервера начнет нехватать.
 
 тогда мы переходим на конфиг из нескольких серверов. 
 
 
и вот тут момент то что лучше иметь четное число шардов чем нечетное.

возьмем 5 щардов. на два физ сервера надо иметь либо минимум 10 виртуалок
чтобы этои индекс равномерно грузил оба сервера.

если бы шардов было 6. то можно было бы по двум серверм разделить 
по три вртуалки. либо по две. если три сервера то тоже можно раздедлить 
по две или три на сервер.  

а при 5 шардов удобных вариантов ни на 2 физ сервера ни на три физ сервера нет. для двух это 10 для трех это 15. 

очень много вирт машин тоже плохо замучаешься их облсуживать

поэтому суммарно число шардов должно быть четное. винтервале 10-30ГБ.
от этого пляшем. от этого пляшет число ВМ. на один сервер.


ну а сколько физ серверов пдонимать больше зависит от потока запросов.

как еще раз делаем. берем число шардов. под него формирует симметричное
число вирт машин. на них равномерно раскадываются шарды. сточки зрения
внетрнней архитеткуры эластика все краисво. ему ненужно таскать шарды туда оратно по нодам стараясь уравнять нагрузу.а потом берем эти машины
и расклыаем по физ серверам. лучше иметь равномерно наполенные машины 
шардами и потом неодинаковым числом разложить ВМ по физ серверам
пример. 5 шардов. пять вирт машин. разлложили как 2+3 по двум физ серверам. нежели иметь 6 вирт машин. наних положить 5 шардов. и положить
по три ВМ на серверы. снаружи все красиво а внутри все нехорошо.

при 5 шардах надо иметь 5, 10,15 ,20 итд виртулок. и потом раскладвыать
их по физ серверам как получится равномернее.

но еще раз - сбольшой долей правда все в большей степени зависит
от мощности цпу на сервере и размера индекса. а того будет на сервере
крутится 100 виртуалок \подов или 5 неособо играет роль.


еще такой аспект рассмотрю. есть одна вирт машина на ней два шарда на физ сервере. имеем входящий поток.
мы делаем из нее две вирт машины , каждая в два раза меньше цпу 
и на каждой один шард. 
получается поток на каждую машину снижается в два раза. 
также раз данных в два раза меньше то пробегать их нужнов два раза меньше.
цпу не ней в два раза меньше и шардов в два раза меньше. получается
что каждая машина будет иметь нагрузку в два раза меньше чем исходная машина. так что получим в сумме одно и тоже. по скорости обработки.
ну только что памяти стало больше. но думаю это неиграет большую роль.
если забыть про поток то так как даных стало меньше в два раза то 
пробегать их нужно в два раза меньше. значит в итоге запрос обработка
займет в два раза меньше времени. но так как цпу мощность упала в два 
раза то итговый время пробега будет тоже самое как у исходной машины. 
так что в итоге реквест будет обрабоатан за тоже время.
условно говоря раньше на исхродной машине один проц бежал по одному
шарду а второй проц по второму в итоге реквест занимал 1с.
на умельченной машине один проц бежит по одному шарду. в итоге реквест
обрабаытвается за ту же 1секунду.

если уменьшаешь виртуалку по цпу в 2 раза то нужно в два раза наверно
уменьшать индекс шарды по обьему которые на нем лежат.

налчиие интерфесных выделенных дает то что можно спокойно добавлять
менять все что хочешгь с дата нодами и при этом на вэбах вобще ненужно ничего менять перепубликовыать. это круто. интерфейсные ноды это как
бы фртнтенд. а дата ноды это бекенд который никого некасается. в общем
нтерфсеные ноды позволяют гораздо с кластером делать инфраструктурных работ незаметно.прозрачно для остаольной структуры


с другой стороны вопрос - от чего больше зависит нагрухка на вируталку
от потока запросов величины или от обьема индекса.

наверно все так в первую очередь от потока запросов.
тогда получается чем больше суммарно по всем хостам мы подключим цпу
тем лучше будет для нагрузки.

как я понимаюб если на хосте есть два пода которые оба одинаковые
и оба равномерно конкрриуируют за цпу то примерно верно то что их скорость  будет примерно такая как если бы у пода была выделенная половина всех цпу.

тоесть если взять две ноды эластика на физ сервер поместить и они грузят
проц на 50% и мы добавим еще две поды эластика и входязий поток 
тот же самый то в итоге у каждого пода поток станет в два раза меньше 
входящий но и цисло цпу как бы уменшьшится в два раза.  поэтому 
смысла вводить четыре пода вместо двух для одного входящего потока 
вроде как нет.

процессы которые загружены оба постоянно распиливают машину пополам.
как бутто каждый рабоатт на половине цпу.
тоесть по мне два пода работающие на всю катушку на физ сервере
это тоже самое как две вирт машины работающие на половинках цпу физ сервера.

в эластике определяющим является цпу мощность и число, потом размер индекса и в конец память. 
на сервере достаточно иметь 80-100ГБ памяти. и этого хватит за глаза.
20ГБ на дисковый кэш и три виртуалки по 30ГБ. главное что расходуется
это процессор. от того что эти три виртуалки разбить на 6 мелких
и получается будет больше памяи  у сервера эластика на этом физ сервере
это ничего недаст.

чем меньше дата нод в кластре тем чусвительнее они 
к неравмномерности распределние шардов по ним.


>>
как узнать\проверить transport address на всех нодах сразу

$ curl 10.104.104.167:9200/_nodes/settings/?pretty | grep "transport_address"

      "transport_address" : "192.168.7.242:30896",
      "transport_address" : "192.168.7.243:30892",
      "transport_address" : "192.168.7.224:9300",
      "transport_address" : "192.168.7.220:9300",
      "transport_address" : "192.168.7.227:9300",
      "transport_address" : "192.168.7.225:9300",
      "transport_address" : "192.168.7.243:30894",
      "transport_address" : "192.168.7.234:9300",
      "transport_address" : "192.168.7.242:30897",
      "transport_address" : "192.168.7.223:9300",
      "transport_address" : "192.168.7.243:30893",
      "transport_address" : "192.168.7.221:9300",
      "transport_address" : "192.168.7.242:30895",
      "transport_address" : "192.168.7.233:9300",




>> про конфиг прояснилось

вот есть у нас классичесткая запись в кофиге

xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.231:9200"]

оказыется эквивалентная ей

xpack.monitoring.exporters.id1.type: http
xpack.monitoring.exporters.id1.host: "192.168.0.231:9200"

это очень важно чтобы вбивать это в кубернетес




>> discovery.seed_hosts
значит что крутого я обнаружил про эластик  , в его конфиге переменная
discovery.seed_hosts в ней указываются IP адреса мастеров.
именно ip адреса нужны в конечном итоге эластику.  
так вот если там указать dns имя то это имя будет зарезовлено в
конечном итоге , так вот круто то что если при резолвинге имя возвращает
несколько IP адресов то эластик  засасывает их все! 
это дает очень крутую штуку. мы можем указать в конфиге эластика всего
одно dns имя

discovery.seed_hosts [ vasya ]

а далее на dns серверер вбить несоклько A записей для vasya
192.168.1.1 A vasya
192.168.1.10 A vasya
...
192.168.1.20 A vasya
и получается что потом при добавлении мастеров нам на конфигах нод
менять ничего ненадо. нам только надо всего навсег менять записи
на DNS сервреу. это мега круто.

в кубе такую вещь предоставляет headless service.
его DNS имя резволисят сразу во все IP адреса подов



>>
в конфиге можно указыать не IP адреса для транспорта TCP и 
клиенских запросов HTTP сокетов а названия сетевых карт
и более того даже можно не конкретные сетевые карты
а например класс IP чтоли указать а она сама найдет под него карту

пример

http.host: [ _local_ ]

_local_ означает = Any loopback addresses on the system

это избавляет в конфиге прописывать лишнюю конкретику.

 

>> хорошая статья
https://thoughts.t37.net/how-we-reindexed-36-billions-documents-in-5-days-within-the-same-elasticsearch-cluster-cd9c054d1db8
но я ее еще непрочитал

>> reindex
на рабочем кластере 
на гибридной (дата+интерфейс) ноде по карте которая обсдуживает клиентские реквесты течет 2 МБ\с а на карте которая обслуживает 
кластерный тарфик течет 6 МБ\с


>> reindex
насколько я понял из эксперимента - если мы накатвыаем индекс
через реиндекс и указали в параметрах чтобы мы коопировали только отсутствующие документы то скорость прохода по сущетвующим документам
она ровно такая же самая как если бы мы их накатывали с нуля. то есть
если мы с нуля накатывали reindex затратили 10 часов. потом запустим
еще раз с просьбой накатить толлько новые документы то процесс 
реиндекса займет опять же все теже самые 10 часов.
это аутаально если мы запустили реиндекс работающей записи на
исхдоном кластере а потом хотим выключить запись и побыстрому 
накатить только новые документы. так вот по быстрому неполучится
процесс займет все теже 10 часов. получается что выгоднее 
изначально отсанвоить запист на исходном кластере и запустить
реиндекс. это реально поразительно что время одно и тоже что
индекс полностью с нуля засасывать что засасыавть только новые
документы. время понадобится одно и тоже.

при "size" = 100 имел я скорость засоса индекса 0.34ГБ\минуту
при "size" = 500  упало с ошибкой 

{"type":"circuit_breaking_exception","reason":"[parent] Data too large, data for [<transport_request>] 

так что size=500 это много.

при дефолтовом "size" = 1000 скорсть засоса 0.46ГБ\минуту.
при такой size отработало на другом индексе.


при "size" = 500 между хостом с которого мы сосем индекс
и хостам на который приходит индекс ( а уменя это интерфейсная нода с
обоих сторон) по сетевой карте котррая обрабатывает клиентские запросы
течет 20МБ\с.

при "size" = 200 скорость по сетевой карте принимающего хоста 14-15МБ\с
, скорость засоса индекса 0.43ГБ\с на диске индекса прибавляется



когда мы запускаем реиндекс то по окончании всего процесса в 
логах у нас там где мы запускали команду на хосте появится запись

[2020-10-21T15:00:51,308][INFO ][o.e.t.LoggingTaskListener] [interface01-233] 991934 finished with response BulkByScrollResponse[took=16.8m,timed_out=false,sliceId=null,updated=0,created=0,deleted=0,batches=22037,versionConflicts=2203700,noops=0,retries=0,canceled=by user request,throttledUntil=0s,bulk_failures=[],search_failures=[]]

соотвтенно ненужно пугаться что took=16.8m, это размер полностью 
всего процесса засоса индекса. то есть все полностью окей.про время.




>>> при создании индекса аналогичного какому то 
надо с исходного индекса получить его settings и mappings

тогда наш новый индекс по своим настройкам будет аналогичен 
исходному

запросы

GET имя-индекса/_settings
GET имя-индекса/_mapping

тогда при создании нового индекса через 
PUT имя-нового-индекса

надо будет в теле запроса вставит полученные портянки settings
и mappings

>> reindex
на отправляющей интерфейс ноде этот процесс жрет 8% cpu 2x e5-2680-v2 2.8GHz
на принимающей интерфейс ноде этот процесс жрет 15% cpu 2x e5-2620-v4 2.1GHz

дата нода принимающая жрет по цпу 5%-20% cpu 7x e5-2620-v4
про отправляющую дата ноду скзаать немогу ее цпу загрузку ибо
она находится вообще под нагрузкой

индекс с которого данные забираются нагружается както по реквестам
но ничего жуткого нет.одназначно.

я бы сказал что реиндекс не является тяжелой операцией
ни для отпрвляющей стороны ни для принимающей 


>> reindex
как мягко его проводить


в свойствах нового индекса надо на время импорта его 
надо указать

"refresh_interval": "-1"
это дает то что эластик небудет делать на новом индексе телодвижений
чтобы накаченные данные были доступны для поиска.
тоесть он небудет ворочать новый индекс лишними операциями.
этот параметр связан с тем что либо каждый "refresh_interval" эластик
обращается к индексу чтоб сделать новопоступившие за какойто срок
данные доступными для чтения, то ли эластк делает доступными на чтение
данные которые поступили за "refresh_interval" секунд за последнее время.
в любом случае нам при накатыании нового индекса ненадо чтобы с него
ктото читал

вот так запускетс сама команда реиндкса

команда: POST _reindex?wait_for_completion=false

тело запроса:
{
  "source": {
    "remote": {
      "host": "http://192.168.0.189:9200/"
    },
    "index": "mk-estates-17-9-2020",
    "size": 10,
    "query": {
      "match_all": {}
    }
  },
  "dest": {
    "index": "temp-index"
  }
}


где
http://192.168.0.189:9200/ = удаленный хост чужого кластера
"mk-estates-17-9-2020" = исходный индекс удаленного кластера
"temp-index" = наш новый индекс, который нужно предварительно создать
со всеми settings и mapping

при этом
"size": 10,
это очень существенный параметр.он указвыаем размер batch 
запроса к исходному эластику.
по умолчанию он равен 1000. и прооблема в том что на исходном кластере
для процесса reindex выделен некий обьем  в памяти. и если запрос 
превысит этот обьем мы получим оттуда на нашем хосте где мы запустили
запрос в логах ошибку 

"circuit_breaking_exception",
"reason": "[parent] Data too large,

это на той стороне эластик понял что запрос превысит этот буфер и 
прекратил операция вернув нам ошибку.

вот чтобы там буфер непереполнялся надо играться с этим
парамтером "size"

а превышение размера происходит тогда когда попадаются какието отдельные
особо большие по размеру документы на той стороне.пока они мелкие
batch проскакивает успешно, как попадется крупный то получим отлуп.

после того как мы запустили реиндекс как я указал выше 
эластик создаст task. и покажет нам его ID
посмотреть статус таска можно так

команда: GET _tasks/номер-таска
тело:{}

остановить таск

команда: POST _tasks/номер-таска/_cancel
тело: {}

еще покажу как делать реиндеккс и засасыывать только документы
которых нет в нашем dest индексе

команда: POST _reindex?wait_for_completion=false    

тело:
{
  "conflicts": "proceed",
  "source": {
    "remote": {
      "host": "http://192.168.0.189:9200/"
    },
    "index": "mk-estates-17-9-2020",
    "size": 100,
    "query": {
      "match_all": {}
    }
  },
  "dest": {
    "index": "temp-index",
     "op_type": "create"

  }
}

как видно пояивлось две новые строчки
"conflicts": "proceed",
"op_type": "create"




>> reindex
reindex. 
имеет ряд своих очень дебилных правил
1) прописываем 
reindex.remote.whitelist: "192.168.0.189:9200"
на НОВОМ кластере. 
2) прописываем именно на той ноде на которой будем запускать команду реиндекса. так неполучится  что на мастерах прописали и потом с любой 
ноды этого нового клстера можно запустить реиндеккс. это не так ! так неработает. 
так как операция реиндекса тяжелая и неочень понятно что она тяжелее
напрягает нашу ноду или ту ноду поэтому на новом кластере прописываем
строчку НА ИНТЕРФЕЙСНОЙ НОДЕ. и в самой строчке где мы указываем конкретную ноду старого кластера прописываем ИНТЕРФЕЙСНУЮ НОДУ СТАРОГО КЛАСТЕРА.

итак еще раз 
сама строчка reindex.remote.whitelist: "192.168.0.189:9200"
192.168.0.189 = интерфейсная нода СТАРОГО КЛАСТЕРА

и пихаем мы эту строчку в конфиг ИНТЕРФЕЙСНОЙ НОДЫ НОВОГО КЛАСТЕРА.

таким образом у нас с обоих сторон процесса реиндекса находятся интфрейсные ноды. и это будет максимлаьно комфортно и для старого 
и для ноовго кластера

интерфесная нода нового кластера <----> интерфейсная нода старого 
кластера.

еще раз строчку вбиваем на новом кластере.  на который мы будем засасывать
индекс.

новый кластер = на который мы засасываем индекс
старый кластер = с которого будем забирать индекс

а если ноды выбирать от балды
то в процессе импорта индекса будут лезить ошибки вида

"circuit_breaking_exception",
"reason": "[parent] Data too large,

"reason": "[parent] Data too large, data for [<transport_request>] would be [20727707052/19.3gb], which is larger than the limit of [20401094656/19gb], real usage: [20725096752/19.3gb], new bytes reserved: [2610300/2.4mb], usages [request=0/0b, fielddata=0/0b, in_flight_requests=2610300/2.4mb, accounting=1961428/1.8mb]",










>> у эластика есть такое дефолтовая настройка что если диск
использован на 80% то тогда эластик больше небудет разрешать 
размещать на диске новые шарды.
как я понял это сделано из расчета то что для сущесвующих 
шардов надо оставить место под расширение.

>> просмотреть причину почему после каких то проблем эластик отказывается
подхватывать старые шарды ( которые висят в статусе unassigned )
GET /_cluster/allocation/explain

# curl -X GET "127.0.0.1:9200/_cluster/allocation/explain/?pretty"

если мы считаем что мы причину устранили то 
команда запустить повторную попытку подхватит шарды
POST /_cluster/reroute?retry_failed=true



>> версия эластика которая неразваливается
 elasticsearch                      7.7.1 
 

>>команда чтобы вставлять в эластик через bulk
~# for i in `seq 1 10000000`; do  curl -XPOST  -H "Content-Type: application/json" 192.168.0.233:9200/test/typename/_bulk --data-binary  @/home/mkadm/temp/data2.json > /dev/null; done
при этом файл data2.json представляет из себя
# head data2.json
{"index":{}}
{"Amount": "480", "Quantity": "2", "Id": "975463711", "Client_Store_sk": "1109"}
{"index":{}}
{"Amount": "2105", "Quantity": "2", "Id": "975463943", "Client_Store_sk": "1109"}
{"index":{}}
{"Amount": "2107", "Quantity": "3", "Id": "974920111", "Client_Store_sk": "1109"}
{"index":{}}
{"Amount": "480", "Quantity": "2", "Id": "975463711", "Client_Store_sk": "1109"}
{"index":{}}
{"Amount": "2105", "Quantity": "2", "Id": "975463943", "Client_Store_sk": "1109"}


эластик 7.9 
выяснилось что теперь просто так комфортно число реплик
ни по умолчанию ни для индекса динамически хер 
изменишь. для индекса на ходу для каких то срабатыает а для каких
то пишет окей но число реплик неменяется.
по умолчанию вообще жопа. то что у них в доке неработает.
точнее работает но все ломает.
дело в том что они суки сделали что по дефолту 
настройки индексов меняются через шаблоны в которых указываешь
имя будущего индекса по маске и потом прописываешь дефолтные
настройки. так вот они начиная с 7.8 сделали каскадные шаблоны
то есть один шаблон может ссылатся на другой.
теперь  если я создаю шаблон для индекса монтиринга то 
он срабатывает на индексе монтироинга но не срабатывают 
какието другие шаблоны для него и в итоге кибана неможет
считать такой индекс. то есть мне надо теперь знать 
какие шаблоы срабатывают на мониториноговом индексе 
и в своем шаблоне прописать их как вложенные для моего.
но я незнаю какие шаблоны ебаный монтироинговый индекс исопльзует.
оставил пока все по дефолту.



!!! выяснилась очень важная вещь.
про кибану.
у кибаны нет никаких явных настроек чтобы ей указать из какого индекса в ее локальном эластике
читать индекс который отвечает за монтироинг перфоманса удаленного эластика.
она просто ищет индекс вида .monitoring-es-*
но это только  присказка.
у меня этот индекс был но кибана упорно говорила что данных для монтироинга
невидит в вэб морде.
ОКАЗЫВАЕТСЯ!!!  то ли сама кибана толи эластик локальный с которого кибана
читает индекс монтироинга но ктото из них точно ОБРАЩАЕТСЯ К МАСТЕРАМ
ОСНОВНОГО ЭЛАСТИКА чтобы считать с них какую то там доп статистику.
но об этом нигде не написано. написано что мол просто надо чтоб кибана
видела индекс перфоманса. так вот кибана или локальный эластик обращается
к основному кластеру эластика точнее именно к ее мастреам по HTTP\9200
и запрашивает какую то доп статистику. ну и куда то там ее сохраняет себе
а я в настройках мастеров отключил доступ к ним по 9200 из внешнего 
мира оставил доступ по 9200 только с localhost. И ВСЕ! кибана 
обращается и не получает доступ и в итоге говоит - мол нет данных 
по монтироингу. все это обнаружилось методом тыка.
и я проверил два раза.
вывод: чтобы мониторинг через кибану работал обязательно нужно 
чтобы на мастерах был доступ из вне на HTTTP\9200 к мастерам.
как только я это сделал монтироинг заработал мгновенно.
на форуме эластика девелоперы пишут что неработает изза того что
в индексе перфоманса нет документа "cluster_stats" 
это все вранье полное. сейчас у меня кибана работает
а этого документа как небыло так и нет в перфоманс индексе.
итак - нельзя отключать доступ по http к мастерам основного 
эластика. это тупо конечно. все данные должны быть 
в перфоманс индексе.но.




эластик - если отвалились шарды в индексах помогает остановить ВСЕ мастеры
и запустить заново. важно что остановить нужно все мастеры.
и только потом их стартовать.

)))) у 7.7.1 эластика вылезда дичайшая ошибка. по непонятной причине
кибана пишет что мол к ней никакой кластер неподключен. и кибана
не показывает графики перфоманса кластера. хотя в кибану с 
основного кластера льется индекс перфоманса. в итоге - нахуй 
эластик версии 7.7.1  и кибану этой версии. апгрейд 
эластика и кибан до 7.9.1 решает проблему при тех же конфигах


))))в 7 эластике они убрали настройку discovery.zen.minimum_master_nodes
и теперь все они сами решают.
теперь кластер доступен только когда больше чем половина мастеров доступна.
и без вариантов.
если мастера три. то онлайн должно быть два ( половина это полтора а надо
чтобы было больше половины. значит два)
если мастера четыре то должно быть онлайн три.
если мастеров шесть то онлнай должно быть четыре
если мастера два то онлайн должно быть два.
вбщем настроек по кворуму никаких. но зато типа теперь
защита от сплит брейна если мастеров четыре.
однако там же ниже они пишут что если у нас четное число мастеров
то якобы эластик отбрасывает один мастер чтобф число мастеров 
было нечетное.
в итоге - документация гавно. проверять ее нет желания.
самое выгодное - это держать нечетное число мастеров.
и считать что более половины этих мастеров должна быть онлнайн.



)))) в 7 эластике все ноды ялвяются интерфейсными нодами. 
и поэтому если мы нехотим на какойто ноде чтобы на нее
дураки бросли http реквесты то выход только один
вот через такую настройку снаружи убрать просто сокет
http.port: 9200
http.host: [ "127.0.0.1" ]
тоесть только изнутри хоста его можно нагрузить http реквестом

7.7 эластик
конфиг мастера


path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch

cluster.name: mk-es-cluster-03
node.name: mk-es7-master01
cluster.initial_master_nodes: ["mk-es7-master01", "mk-es7-master02", "mk-es7-master03"]

transport.host: [ "192.168.7.190" ]
transport.port: 9300
http.port: 9200
http.host: [ "127.0.0.1" ]

discovery.seed_hosts: ["192.168.7.190", "192.168.7.191", "192.168.7.192"]

node.master: true
node.voting_only: false
node.data: false
node.ingest: false
node.ml: false

xpack.monitoring.enabled: true
xpack.monitoring.collection.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.201:9200", "192.168.0.202:9200", "192.168.0.203:9200" ]

xpack.security.enabled: false


даю комент.
cluster.initial_master_nodes: ["mk-es7-master01", "mk-es7-master02", "mk-es7-master03"]
эта строка должна быть но только на мастерах.
в ней указвыаются все мастера.

transport.host: [ "192.168.7.190" ]
transport.port: 9300
http.port: 9200
http.host: [ "127.0.0.1" ]

транспорт это TCP сокет ноды который выставлен для того чтобы другие ноды
общались с этой нодой через него то есть для служебного трафика кластера

http это сокет через который нода принимает клинетские http 
запросы

discovery.seed_hosts: ["192.168.7.190", "192.168.7.191", "192.168.7.192"]
эта строка должна быть на всех нодах кластера.
в ней указано где искать мастеров кластера. (дебилизм я знаю но так 
надо)



конфиг дата ноды

# cat /etc/elasticsearch/elasticsearch.yml
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch

cluster.name: mk-es-cluster-03
node.name: mk-es7-data01

transport.host: [ "192.168.7.193" ]
transport.port: 9300
http.port: 9200
http.host: [ "192.168.0.193", "127.0.0.1" ]

discovery.seed_hosts: ["192.168.7.190", "192.168.7.191", "192.168.7.192"]

node.master: false
node.voting_only: false
node.data: true
node.ingest: true
node.ml: false

xpack.monitoring.enabled: true
xpack.monitoring.collection.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.201:9200", "192.168.0.202:9200", "192.168.0.203:9200" ]



xpack.security.enabled: false
root@mk-es7-data01:~#


конфиг эластика на хосте с кибаной
root@mk-es7-kibana01:~# cat /etc/elasticsearch/elasticsearch.yml
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch

cluster.name: mk-es7-kibana01
node.name: mk-es7-kibana01-data01
cluster.initial_master_nodes: ["mk-es7-kibana-data01"]
http.host: ["192.168.0.201", "127.0.0.1"]
http.port: 9200
transport.port: 9300
transport.host: [ 127.0.0.1 ]
discovery.seed_hosts: ["127.0.0.1"]
node.master: true
node.voting_only: false
node.data: true
node.ingest: true
node.ml: false

xpack.monitoring.enabled: false
#xpack.monitoring.exporters.id1:
#   type: http
#   host: ["localhost:9200"]

xpack.security.enabled: false

конфиг кибаны на хосте с кибаной

server.port: 5601
server.host: "192.168.0.201"
logging.quiet: true
xpack.security.enabled: false

как только на эластик для кибаны с основго эластика прилетает индекс
с именем .monitoring* то кибана понимает что это индекс перфоманса
и в своей вэб роже начинает рсиовать графики.
специально както в кибане прописыать название перфоманс индекса ненужно.
просто невозможно.
если кибана ничего не рисует значит в ней какойто баг. и надо 
попробвать другую версию эластика+кибана.


как я понял из путанного описания . 
на обычных нодах мы должны указать адреса мастеров через настроку
discovery.seed_hosts: ["192.168.7.190", "192.168.7.191", "192.168.7.192"]
причем это IP адреса транспортных сокетов а не сокетов которые http 
реквесты обрабатывают.

на мастерах помимо discovery.seed_hosts: дополнительно нужно указать тожесамое то есть список
всех мастеров кластера но через доп настройку
cluster.initial_master_nodes: ["mk-es7-master01", "mk-es7-master02", "mk-es7-master03"]

зачем дублировать непонятно. но такой дебилизм у них.


===============


узнать какие версии elasticsearch доступны


# apt-cache madison elasticsearch


установить версию определенную пакета

# apt-get install elasticsearch=5.6.3

cтавим elasticsearch

# apt-get install elasticsearch=5.6.3

при установке оно напишет что невидит JAVA_HOME

замечу что JAVA_HOME это не путь к java бинарнику - хер там.
это путь к /bin/java.

то есть если путь к бинарнику java  = /usr/lib/jvm/java-8-openjdk-amd64/bin/java
то JAVA_HOME надо указать = /usr/lib/jvm/java-8-openjdk-amd64

это особенность именно этой переменной. имено в таком формате эластик будет искать в ней инфо.
поначалу я указал полный путь к java бинарнику /usr/lib/jvm/java-8-openjdk-amd64/bin 
и эластик меня посылал нахер он писал что неможет найти java.

ее нужно прописать в 

/etc/default/elasticsearch
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin

в
/etc/profile
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin

это все поможет только не руту. ему тоже надо персонально.

/etc/bash.bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

все теперть все юзеры будут иметь прописанный JAVA_HOME


при это я уже непомню как ставить java.а ставить ее надо

также сразу нужно в 
/etc/elasticsearch/jvm.options

и поправить память в зависимости от размеру ОЗУ 

-Xms6g
-Xmx6g

дока рекомендует ставить размер 50% от размера ОЗУ.

ставим x-pack на elasticsearch
-->(в эластике 6.8.9 x-pack ставить отдельно ненужно он ставится по дефолту сразу кодга мы 
поставили сам эластик. более того x-pack походу интегрирован в сам бинарник эластика


в 6.8.9 он уже интегрирован в бинарник эластик поэтому получим вот что
# /usr/share/elasticsearch/bin/elasticsearch-plugin list

# /usr/share/elasticsearch/bin/elasticsearch-plugin install x-pack
ERROR: this distribution of Elasticsearch contains X-Pack by default  )<--

если версия эластика более старая чем 6.8.9 то x-pack ставить отдельно нужно.
 
сразу важно обьяснить что же эта за хрень x-pack

это плагин.

он добавляет некий функционал.

этот плагин ставится на кучу программ.
он ставится на эластиксерч, на кибану, на логстэш и еще на всяко разно.
причем в кажду программу его нужно ставить ОТДЕЛЬНО.
отдельно накатывать на эластиксерч, отдельно накатывать на кибану итд.

захуя он нужен. 
если мы ставим его на эластиксерч то он навскидку создает спец индексы в эластике и начинает туда лить специнформацию. 
например  он начинаем собирать и лить информацию по статистике работы кластера. это впервую очередь и есть ради чего я его ставлю. 
чтоб снимать статистику по перфомансу кластера. 
также он добавляет в эластик возможность работать с юзерами ролями итп. но этот функционал платный. если у нас лицензия бейсик
то в ней эта фича недоступна. какие еще есть фичи в x-pack буду освещать походу их использования.

причем чтобы восолтзоваться записанной x-pack статистикой работы кластера мало только поставить x-pack на эластик.
также надо поставить кибану да еще и накатить x-pack на кибану. потом увязать кибану и эластик. и только тогда мы 
сможем вкусить саттистику работы эластика в кибане. но все по порядку.

ставим x-pack на эластика. ( еще раз скажу что дока у эластик гавно.. пока я это все понял..... эх.. )

плагины  в эластике ставятся через спец утилиту эластика

# /usr/share/elasticsearch/bin/elasticsearch-plugin install x-pac


при этом получим ошибку

[=================================================] 100%  
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.io.FilePermission \\.\pipe\* read,write
* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.RuntimePermission setFactory
* java.security.SecurityPermission createPolicy.JavaPolicy
* java.security.SecurityPermission getPolicy
* java.security.SecurityPermission putProviderProperty.BC
* java.security.SecurityPermission setPolicy
* java.util.PropertyPermission * read,write
* java.util.PropertyPermission sun.nio.ch.bugLevel write
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.


я просмотрел кучу доков. и все кладут болт на эти настройки.
итак просто игнорим и ставим.

проверяем что x-pack поставился.

# /usr/share/elasticsearch/bin/elasticsearch-plugin list
x-pack

x-pack он платный. они дают 30 дней на зценить.
потом у него часть функционала отваливается.


после того как мы установили x-pack нам его даже ненужно активировать в конфиге эластике.
плагин уже работатает.

и как первое следствие теперь просто так доступ к кластеру неработает. спрашивает логин и пароль.
поэтому отключаем аутентификацию при доступе к кластеру.

# cat elasticsearch.yml
...
xpack.security.enabled: false

вообще вот сразу нужно отредактировать конфиг.
ибо к примеру под дефолту эластик будет висеть только на 127.0.0.1 интерфейсе
и не будет висеть на IP сетевой карты.
поэтому сразу конфингурируем конфиг (его опции что они значат обсудим потом ниже).

# grep -v '#' /etc/elasticsearch/elasticsearch.yml

path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
cluster.name: escluster-6
node.name: data01-14
network.host: ["172.16.102.14", "127.0.0.1"]
http.port: 9200
transport.tcp.port: 9300
http.enabled: true
discovery.zen.ping.unicast.hosts: ["127.0.0.1"]
discovery.zen.minimum_master_nodes: 1
node.master: true
node.data: true
node.ingest: true
xpack.security.enabled: false
xpack.monitoring.enabled: false

где 172.16.102.14 - IP карточки

запускаем эластик

# service elasticsearch restart

готово.

-->(для версии эластика 7.7 есть отличия в конфиге

http.enabled: true  = этой опции  больше нет
cluster.initial_master_nodes: ["data01-14"] нужна эта доп опция чтобы кластер стартовал
иначе будет писать что нет мастер нод ни одной

итого для версии 7.7. конфиг будет такой

path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
cluster.name: escluster-6
node.name: data01-14
cluster.initial_master_nodes: ["data01-14"]
network.host: ["172.16.102.14", "127.0.0.1"]
http.port: 9200
transport.tcp.port: 9300
discovery.zen.ping.unicast.hosts: ["127.0.0.1"]
discovery.zen.minimum_master_nodes: 1
node.master: true
node.data: true
node.ingest: true
xpack.security.enabled: false
xpack.monitoring.enabled: false

)<--


! также сразу про (elastic 7) я говорю что настройка 
discovery.zen.ping.unicast.hosts 
в эластике 7 она уже устарела. она старая. они ее пока 
сохранили но вместо нее по хорошему нужно теперь уже 
использовать другую аналогичную новую 
discovery.seed_hosts


поговоим про роли

в эластике 7.7  роли обозначают вот так в конфиге ноды
node.master: true 
node.voting_only: false 
node.data: false 
node.ingest: false
node.ml: false
 

а в эластике 7.10 уже вот так 
node.roles: [ blablabla ]


какие бывают роли у нод: master-eligible, data, ingest, and (if available) machine learning. All data nodes are also transform nodes.

сразу возникает вопрос а где же интерфейсная роль? об эттом ниже

разберем каждую роль

Master-eligible node и dedicated master-eligible node
разница меджу первой и второй ролью в том что первая имеет роль
мастера и еще всяких других ролей например
Master-eligible node = node.roles: [ 'master', 'ingest']
а вторая нода это только мастер и больше никаких ролей (за исключением +координационная роль)
dedicated master-eligible node = node.roles: [ master]
нафига вот эти добавки типа элиджибл. суть в том что в данный 
момент только одна нода является мастером. остальные "мастера" они 
запасные потенциальные.

ха! есть еще подвиг мастер роли это voting_only master role.
это такая нода которая участвует в выборах мастера но нестанет им никогда.
ее можно использовать для того чтобы небыло сплит-брейна при выборах.
суть в том что ее можно сделать мега слабой по железу  и вся ее суть 
это чтоб был +1 участик выборов и небыло сплит-брейна.
обозначется она
node.roles: [ master, voting_only ]
опять же если нода имеет только вот такие роли [master, voting_only]
она назвыается dedicated voting_only. а если на этой ноде 
есть еще друнгие доп роли то она называется voting_only eligible.



node.roles: [ data ]
это только дата нода

ingest роль
node.roles: [ ingest ]
как я понял перед индексированием документа с ним может происходит
доп обрабоатка. она и называется ингест. и бывает полезно выделить
отдельные ноды только под ингест роль. а по дефолту каждая дата
нода также является и ингест нодой.

node.roles: [ ml ]
это загадочный машин лернинг. пока небудем углубляться.

node.roles: [ transform ]
это типа дефолтная роль на дата ноде. 
и она позволяет превратить обычный эластик индекс
в какойто там особый индекс для каких то спец целей.
пока в это небудет углубляться. также в кластере
должна быть хотя бы одна нода с трансформ ролью.

в эластике 7 они называют интерфейсную ноду координационной нодой теперь
насколько я понял теперь эту роль нельзя отключить на ноде никак.
даже если мы укажем что node.roles: [ пусто ] то нода все равно будет
иметь роль координационной ноды. так как эту роль никак нельзя выключить
то они вообще убрали эту роль из ролей. сразу возникает вопрос а мастер
нода она что тоже еще и координационная нода? пока незнаю 

dedicated coordinating node обозначается
node.roles: [ ]

!



позже. когда будет готов другой дополнительный эластик в который мы будем лить 
перфоманс лог от данного кластера конфиг с активированным мониторингом будет выглядеть для 7.7.1 как

path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch


cluster.name: escluster-2
node.name: data01-230
cluster.initial_master_nodes: ["master01"]
network.host: ["192.168.0.230", "127.0.0.1"]
http.port: 9200
transport.tcp.port: 9300
discovery.zen.ping.unicast.hosts: ["127.0.0.1"]
discovery.zen.minimum_master_nodes: 1
node.master: true
node.data: true
node.ingest: true

xpack.monitoring.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.231:9200"]

xpack.security.enabled: false

xpack.monitoring.collection.enabled: true


в этом конфиге появилась дополнитеьная опция которой 
небыло раньше (https://www.elastic.co/guide/en/elasticsearch/reference/current/monitoring-settings.html)

xpack.monitoring.collection.enabled: true

она отвечает за то чтобы перфоманс лог начал был создан и данные в него начали литься.
без этой опции перфоманс лог небудет создан. и данные в него небудут записываться.
и соотвесвтенно в кибане небудет видна статистика по перфомансу работы эластика.

кстати говоря в кибане 7.7.1 монтироинг перфоманса теперь нужно смотреть в другой вкладе = "Stack Monitoring"

также в эластике 7 появилась еще одна новая опция
        cluster.initial_master_nodes: ["master01", "master02", "master03"]
как я понял из доки когда создается новый кластер с нуля эту опцию
надо прописать на всех мастер нодах (на остальных ненадо).
тогда при старте все мастера найдут друг друга и образуют кластер.
и эта опция нужна им только для первого старта кластера.
после этого они эту информацию записывают себе в data папку и этой
опцией из конфига больше непользуются.
итак эту опцию надо прописывать только на мастер нодах.

transport.host: ["192.168.0.230"]
transport.port: [ 9300 ]
эти опции прописывают через какой(ие) сокеты другие ноды кластера
по TCP могут общаться с этой нодой
 


далее.
по умолчанию у нас 30 дневная триальная лицензия на эластик ( у эластика 6.8.9 лицензия
сразу уже нетриальная. она базовая и делать нихера ненадо если эластик более старый то читаем
дальше как получить базовую лицензию).  проверяем статус лицензии

~# curl http://test-es3:9200/_license/
{
  "license" : {
    "status" : "active",
    "uid" : "73473f97-9efe-489b-9ad0-71b77b09c1df",
    "type" : "trial",
    "issue_date" : "2019-10-04T21:02:27.361Z",
    "issue_date_in_millis" : 1570222947361,
    "expiry_date" : "2019-11-03T21:02:27.361Z",
    "expiry_date_in_millis" : 1572814947361,
    "max_nodes" : 1000,
    "issued_to" : "elasticsearch",
    "issuer" : "elasticsearch",
    "start_date_in_millis" : -1
  }
}


видно что лицензия триальная.

заходим на сайт эластика регаемся и заказыаем basic беплатную лицензию. и на почту приходит ссылка на файл с лицензией.
ее нужно впихнуть в эластик.

через запрос

POST /_license
{
  "licenses": [
    {
      "uid":"893361dc-9749-4997-93cb-802e3d7fa4xx",
      "type":"basic",
      "issue_date_in_millis":1411948800000,
      "expiry_date_in_millis":1914278399999,
      "max_nodes":1,
      "issued_to":"issuedTo",
      "issuer":"issuer",
      "signature":"xx"
    }
    ]
}


при попытке впихнуть базовую лицензию в эластик он пишет


{
"acknowledged": false,
"license_status": "valid",
"acknowledge": {
"message": "This license update requires acknowledgement. To acknowledge the license, please read the following messages and update the license again, this time with the "acknowledge=true" parameter:",
"watcher": [
"Watcher will be disabled"
],
"security": [
"The following X-Pack security functionality will be disabled: authentication, authorization, ip filtering, and auditing. Please restart your node after applying the license."
,
"Field and document level access control will be disabled."
,
"Custom realms will be ignored."
],
"monitoring": [
"Multi-cluster support is disabled for clusters with [BASIC] license. If you are running multiple clusters, users won't be able to access the clusters with [BASIC] licenses from within a single X-Pack Kibana instance. You will have to deploy a separate and dedicated X-pack Kibana instance for each [BASIC] cluster you wish to monitor."
,
"Automatic index cleanup is locked to 7 days for clusters with [BASIC] license."
],
"graph": [
"Graph will be disabled"
],
"ml": [
"Machine learning will be disabled"
]
}
}


то есть часть функционгала будет вырублена нахер. так происходит когда мы впихиываем лицензию которая более урезающая чем была. 
у нас была полная но на 30 дней а мы пихаем базовую.

чтобы впихнуть базовую надо  добавить параметр acknowlege=true

POST /_license?acknowledge=true
{
  "licenses": [
    {
      "uid":"893361dc-9749-4997-93cb-802e3d7fa4xx",
      "type":"basic",
      "issue_date_in_millis":1411948800000,
      "expiry_date_in_millis":1914278399999,
      "max_nodes":1,
      "issued_to":"issuedTo",
      "issuer":"issuer",
      "signature":"xx"
    }
    ]
}


успех. проверяем текущую лицензию

# curl http://test-es3:9200/_license
{
  "license" : {
    "status" : "active",
    "uid" : "210f3f42-52e4-4edb-9429-f6758fb47061",
    "type" : "basic",
    "issue_date" : "2019-09-05T00:00:00.000Z",
    "issue_date_in_millis" : 1567641600000,
    "expiry_date" : "2020-09-05T23:59:59.999Z",
    "expiry_date_in_millis" : 1599350399999,
    "max_nodes" : 100,
    "issued_to" : "Al",
    "issuer" : "Web Form",
    "start_date_in_millis" : 1567641600000
  }
}

видно что лицкнзия активно неистекла и что она бейсик

а вот еще как можно увидеть инфо по лицензии и установлен и по плагинам какие грузит эластик

# tail -f /var/log/elasticsearch/elasticsearch.log
(имя лог файла зависит от названия кластера на самом деле )
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-expression]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-groovy]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-mustache]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-painless]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [parent-join]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [percolator]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [reindex]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [transport-netty3]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [transport-netty4]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded plugin [x-pack]
[2019-10-13T16:15:08,926][INFO ][o.e.l.LicenseService     ] [tbgAYqa] license [210f3f42-52e4-4edb-9429-f6758fb47061] mode [basic] - valid



как видно из того что выше при бейсик лицензии security отключается. нет аутентификации нет юзеров итп.

закончили мы на rbac активации при бейсик лицензии.

как я понял из дурацких перекрестноссылочной говно доки от эластик -  авторизацию аутентификацию 
обеспечивает именно плагин x-pack соотвесвтенно если в его бесплатной версии этот функционал отключен.
значит либо плати либо иди нахуй.

когда мы поставили x-pack то в нем уже есть несолько предустановленных юзеров.

эти юзеры принадлежат к ролям ( аналог групп в виндовсе). таковые юзеры

elastic принадлежит роли superuser другими словами полный доступ ко всему и вся

kibana этот юзер используется кибаной для того чтобы под этим юзером иметь доступ к эластику.

logstash_system бла бла.


что я понял - если у нас лицензия непозволяет работать с какойто фичей эластика например у нас базовая лицензия 
которая непозволяет работать с фичей security от плагина x-pack то : буквально почти никакие команды от данной фичи работать не
будут . например посмотреть список пользователей будет посмотреть нельзя. список ролей посмотреть будет нельзя . 
и тому подобное. от фичи security будет работать некий убогий супер минимум.да. в этом тоже прикол. 
фича по лицензии недоступна ( тут тоже полная хуйня  и неразбериха. у них на сайте сказано что лицензия бейсик она
дает овзможность работаь с роолями и юзерами. а при установке лицензии x-pack сразу говорит что security фича не будет работать).
тем не менее после установки x-pack и перезапуска эластика при попытке обраться к эластику по 9200 выплывет окно которое спросит
логин и пароль. типа чего блядь?  фича же неработает .однако как показала практика всеже убого эта фича работает
на том уровне что можно залогинться на кластер под акаунтом уже встроенным при установке x-pack

l:elastic
p:elastic

это креды типа суперюзера в эластике.

итак. чтобы нееобаться с этой фичей и юзерами - идем в конфиг и этот функционал нахуй дизейблим.


# cat elasticsearch.yml
...
xpack.security.enabled: false

как тока мы это сделали. то доступ ко всем ресурами эластика без всяких ограничений.
без всякий аутентификаций и юзеров.

далее вся тема о юзерах ролях итп это все сразу уже просто недоступно отваливается и об этом
можно забыть в эластике

индексы вида 

.watcher*
.monitoring*

это индексы которые автоматом заводит x-pack


итак эластик поставили.
бейсик лицензию вбили
на эластик x-pack накатили.
функционал security нахер выключили.

* Ставим кибану.

кибану можно ставить и на туже виртуалку где эластик если чисто 
для тестов.
а так лучше кибану ставить на отделную виртуалку.

как я понял. с бейсик лицензией для каждого кластера эластик надо ставить свою кибану. 
то есть кибана несможет обслужвать несколько эластиков. это недаст лицензия.


еще сразу замечу что щас четко девеплоеры сказали что все версии должны одинаковы у компоненетов - у кибаны эластика логстэша и прочих.
у всех должна быть едтиная версия.

выясняем установленную версию эластика к которому мы будем привязывать кибану

# dpkg -l | grep elas
ii  elasticsearch                      5.6.3

смотрим доступна ли эта же версия кибаны

~# apt-cache madison kibana

ставим версию 5.6.3

~# apt-get install kibana=5.6.3

далее я столкнулся с тем что кибана выводит ошибку в логах

permission denied, unlink '/usr/share/kibana/optimize/bundles/timelion.style.css

и она сука никак неможет стартануть. все крутит икрутит и крутит процессор
и никак не может завестись

в инете нашел что надо сделать

# chown -R kibana:kibana /opt/kibana/optimize/bundles
# chown -R kibana:kibana /usr/share/kibana/optimize/bundles



ставим x-pack на кибану
(для эластика 6.8.9 ставим x-pack ненужно он уже интегрирован в бинарник кибаны)

# /usr/share/kibana/bin/kibana-plugin install x-pack


насоклько я понял в кибану лицензии вбивать никакие ненужно



заходим в конфиг кибаны

там ставим вот такие настройки

root@mk-kibana-01:/etc/kibana# grep -v '#' kibana.yml

elasticsearch.hosts: ["http://localhost:9200"]
port: 5601
server.host: "192.168.0.121"
logging.quiet: true
xpack.security.enabled: false


где elasticsearch.hosts: ["http://localhost:9200"] это адрес эластика в котором кибана будет хранить свой индекс.
да да. кибане нетолько читает данные от эластика-А о его перфомансе но и самой кибане
нужно хранить гдето свои внутренние данные и хранит кибана их тоже в форме эластикового индекса.
кибана будет искать на http://localhost:9200 сервере специальный индекс .
индекс перфоманса производительности эластика. какого эластика кибане пофиг.
если кибана найдет на http://localhost:9200 специальный индекс спецаиально оформленный. то она будет из него
данные чиатть и рисовать графики в своей закладке мониторинг.
прикол в том что эластик может хранить индекс статистики своего перфоманса как на самом себе
так и на отдельном другом внешнем эластике.  завиисит это от настройки в эластике

настройка server.host: "192.168.0.121"  прописывает адрес веб морды кибаны.


# grep -v '#' /etc/elasticsearch/elasticsearch.yml
xpack.monitoring.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["server-IP:9200"]
xpack.monitoring.collection.enabled: true

еще раз подчеркну что в 7.7.1 появилась доп опция xpack.monitoring.collection.enabled: true
без нее перфоманс индекс небудет создан. 



еслимы укажем server-IP=localhost то эластик свой индекс перфоманса будет писать на самого себя.
если мы укажем server-IP=8.8.8.8 то эластик будет посылать свой индекс перфоманса на удаленный 8.8.8.8 эластик.

так вот кибане в настройке  elasticsearch.hosts: ["http://localhost:9200"] мы указываем 
где ей искать индекс перфоманса.

в кибане настройка server.host: "192.168.0.121" прописывает сокет по которому 
можно подключиться к веб морде кибаны.



еще раз нарисую схему

эластика -А  ---------------> эластик -Б <------------ кибана

эластик А посылает свой индекс перфоманса в эластик Б.
кибана читает данные из эластика Б о том как быстро работает эластик А.

настройки эластика-А

# grep -v '#' /etc/elasticsearch/elasticsearch.yml
xpack.monitoring.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["192.168.0.121:9200"]        - ( адрес другого эластика-Б куда исходный эластик-А пишет свой перфоманс индекс)

настройки кибаны
# grep -v '#' kibana.yml
elasticsearch.hosts: ["http://localhost:9200"]      - (еще раз это адрес эластика из которого эластик читает данные)
port: 5601
server.host: "192.168.0.121"  - (это адрес веб морды кибаны)
logging.quiet: true
xpack.security.enabled: false

где 192.168.0.121 это адрес эластика -Б.


если мы хотим чтобы перфоманс индекс хранился на самом эластике А то host: ["localhost:9200"]

тогда в кибане нужно указать elasticsearch.hosts: ["http://адрес-эластика-А:9200"] 

если кибана сидит на тойже виртуалке что и эластик-А и эластик-А хранитс свой перфоманс индекс в себе 
тов кибане указыаем elasticsearch.hosts: ["http://localhost:9200"]

коментрую оставшиеся настройкив к конфиге кибаны

logging.quiet: true   = это чтобы кибана в логи несрала горы

xpack.security.enabled: false = это чтобы кибана при входе в ее вэб морду не спрашивала логин и пароль.


ффффуууууууууууууууууууууух.

стартуем кибану.


# systemctl enable kibana
# systemctl start kibana

вход в вэб морду по порту 

:5601


и тут мы видим что кибана никак нехочет вставать. (для эластика кибаны 6.8.9 этого бага нет)

порт 5601 неначинает слушаться.

лезем в  syslog и видим


 Started Kibana.
Oct  6 22:57:39 test-es3 kibana[9218]: {"type":"log","@timestamp":"2019-10-06T19:57:39Z","tags":["fatal"],"pid":9218,"level":"fatal","message":"EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'","error":{"message":"EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'","name":"Error","stack":"Error: EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'\n    at Error (native)","code":"EACCES"}}
Oct  6 22:57:39 test-es3 kibana[9218]: FATAL { Error: EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'
Oct  6 22:57:39 test-es3 kibana[9218]:     at Error (native)
Oct  6 22:57:39 test-es3 kibana[9218]:   cause:
Oct  6 22:57:39 test-es3 kibana[9218]:    { Error: EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'
Oct  6 22:57:39 test-es3 kibana[9218]:        at Error (native)
Oct  6 22:57:39 test-es3 kibana[9218]:      errno: -13,
Oct  6 22:57:39 test-es3 kibana[9218]:      code: 'EACCES',
Oct  6 22:57:39 test-es3 kibana[9218]:      syscall: 'open',
Oct  6 22:57:39 test-es3 kibana[9218]:      path: '/usr/share/kibana/optimize/bundles/graph.entry.js' },
Oct  6 22:57:39 test-es3 kibana[9218]:   isOperational: true,
Oct  6 22:57:39 test-es3 kibana[9218]:   errno: -13,
Oct  6 22:57:39 test-es3 kibana[9218]:   code: 'EACCES',
Oct  6 22:57:39 test-es3 kibana[9218]:   syscall: 'open',
Oct  6 22:57:39 test-es3 kibana[9218]:   path: '/usr/share/kibana/optimize/bundles/graph.entry.js' }


я залез в гугл и в форуме эластика я нашел что это типа известный баг. и есть воркэранд.

# chown -R kibana.kibana /usr/share/kibana/

после этого еще раз перезапускаем кибану

и видим в логах инфо

 Started Kibana.
Oct  6 23:00:06 test-es3 kibana[9642]: {"type":"log","@timestamp":"2019-10-06T20:00:06Z","tags":["info","optimize"],"pid":9642,"message":"Optimizing and caching bundles for graph, monitoring, ml, kibana, stateSessionStorageRedirect, timelion and status_page. This may take a few minutes"}


итак ждем несколько минут и кибана встает. и мы успешно в  нее входим.  
при этом видно в перфоманс мониторе что кибана она непралалелится. она однопроцессорная.


итак вошли в кибану


тут я наткнулся еще на один момент. кибана написала что неможет подключииться к http://localhost:9200

ну это ясно. так как в эластике в конфиге стояло чтобы он слушал входящие на IP:9200
заменил на  0.0.0.0:9200

перезашел в кибану. все окей.


ура.



и вот мы зашли на кибану. и он спрашивает типа про какие индесы вы хотите иметь инфо.
пока можно на это забить. 
эта херня никак несвязана с закладкой мониторинг которая нам собственно и нужна закладка 
где можно смотреть перфоманс мониторинг.

нам надо чтобы в заладке мониторинг начали рисовать графики.
как я уже сказал для этого нам надо чтобы на эластике-А 
настройить чтобы эластик-А начал лить свой перфоманс индекс в эластик-Б их которго читает кибана.



возвращаемся  в эластик-А в конфиг 
и сделать там вот такие настройки
# grep -v '#' /etc/elasticsearch/elasticsearch.yml

xpack.monitoring.enabled: true
xpack.monitoring.exporters.id1:
   type: http
   host: ["Server-IP:9200"]

где  Server-IP0 это адрес эластика-Б в который наш эластик-А посылает данные о статистике
перфоманса своей работы.

перезапускаем эластик-А.
и он должен создать новый индекс с точкой в имени. и должен начать в него лить
данные о статисткике перфоманса своей работы.

возвращемся в кибану.

и вот в кибане в закладке мониторинг  мы наконец увидим графическую статистику работы кластера эластика.
собственно ради чего мы и ставим кибану - смотреть статистику перфоманса эластика.

надеюсь стало понятно от чего зависит чтобы кибана начала рисовать графиики в закладке 
мониторинг.


то чего хотели добились


далее. еще надо зайти в конфиг

/etc/kibana/kibana.yml
и активировать найстроку

logging.quiet: true

она даст то что кибана перестанет каждые 10 секунд срать в syslog
а будет только писать туда если ошибка (об этом уже написал и выше)




значит далее я изучал раздел 

https://www.elastic.co/guide/en/kibana/5.6/getting-started.html

он на примерах показывает как использовать визуализацию в кибане

часть разделов в кибане она появляется только если на кибану установлен x-pack.
если мы откроем документацию кибаны - https://www.elastic.co/guide/en/kibana/5.6/xpack-monitoring.html
и откроем какойто раздел. то если рядом с названием раздела написано x-pack
значит этот раздел появлется только при устанолвке x-pack на кибану

значит основной момент - в конфиге кибаны мы указыаем из какого кластера эластика мы будем
засасмывать данные в кибану

elasticsearch.url: "http://localhost:9200"

( об этом написано здесь - https://www.elastic.co/guide/en/kibana/5.6/settings.html )

так вот. указали в конфиге кибаны где ему искать эластик с которого мы хотим данные.

далее открываем саму веб морду кибаны http://test-es3:5601

идем в  management - index patterns - create index pattern - index pattern

указываем имя индекса 

logstash-*


и только после этого уже можно будет в кибане с этого индекса засасывать реквестами данные.

так вот . получается адрес кластера эластика мы указываем в конфиге кибаны. а индексы 
этого кластера уже указываются в веб морде кибаны.

так вот ! 
возникает вопрос, а как же сделать так чтобы в кибане можно было видеть неодин кластер эластика а
несколько. 
ответ - пока непонятно.

скрипт. 
как для всех индексов на кластере изменить число реплик

===
#!/bin/bash


for value in $(curl -s http://localhost:9200/_cat/indices | awk '{print $3}')

do

    curl -XPUT -H 'Content-Type: application/json' "localhost:9200/$value/_settings" -d '
{
    "index" : {
        "number_of_replicas" : 0
    }
}'

done

===


как получить список индексов 

# curl -s http://localhost:9200/_cat/indices?v

==

запрос на выдачу всех документов из локального индекса

curl -XGET "http://test-es3:9200/.kibana/_search" -d '
{
  "query": {
    "match_all": {}
  }
}'
 
локальный индекс значит что для test-es3 он является родным.

так вот можно сделать так чтобы мы обращались к кластеру А чтобы он 
поискал нам документы в кластере Б.

фишка в том что мы можем искать хоть в десяти кластерах эластика 
и для этого нам ненужно обращаться к 10 разным кластерам а делать 
это из одной точки из кластера А.

для  того чтобы в кластер test-es3 добавить удаленный кластер test-es2 
нужно в конфиге кластера test-es3 добавить

test-es3# cat /etc/elasticsearch/elasticsearch.yml

...

search:
    remote:
        test-es2:
            seeds: test-es2:9300


теперь мы можем искать в test-es2 обращаясь из test-es3

вот как это делается


curl -XGET "http://test-es3:9200/test-es2:.kibana/_search?pretty" -d '
{
  "query": {
    "match_all": {}
  }
}'


индекс на удаленном кластере мы задаем как 

test-es2:.kibana

таким макаром эластик понимает ищем ли мы в локальном индексе или удаленном.


такая фича в эластика назвыается cross-cluster search

как видно что чтобы фича работала нужно делать изменения только на одном кластере.   похожий функционал дает другая фича эластика - tribe node. но она 
deprecateed поэтому мы ее не рассамтриваем.

зачем нам вообще нужна щас эта фича cross-cluster search

нужна она затем чтобы в кибане иметь возможность работать
с индексами из разных кластеров.

сама кибана умеет подключитаться только к одному кластеру эластика.

заюзав на этом эластике cross-cluster search 
мы можем в кибане работаь с несколькими кластерами эластика

после того как в конфиге эластика мы указали remote кластеры
мы идем в кибану в management и там указываем новый index pattern в полном виде

test-es2:someindex*

готов. 

итак в management у нас появился новый index pattern.
но! что это нам дает?
это нам нифига недает возможность смотреть мониторинговые состояние по данному индексу удаленному
в закладке мониторинг!

это нам только дает что мы можем делать запросы к документам в закладке discover.
это конечно неплохо. 

но важно что смотреть состояние перфоманса в заклкдке monitoring мы сможем.

по крайней мере это недоступно в кибане для лицензии эластика basic.

судя по странице с лицензиями - https://www.elastic.co/subscriptions
Multi-stack monitoring недоступна в basic лицензии.
а это походу и есть фишка что мы из одной кибаны можем 
смотреть на здоровье нескольких кластеров эластика


установка x-pack дает в кибане следующие закладки

* Machine Learning
* Graph
* Monitoring

насколько я понимаю --- лицензия вбивается в эластик.  а потом другие компоненты
эластик стека ( кибана, логстэш ) обращаются к эластику и читают лицензию. 
и исходя из этого компоненты ( кибана логстэш) у себя отключают или включают их функционал.

в нашем конкретном случае мы в конфиге кибаны ууказываем к какому кластеру 
эластика кибане коннектится , какой эластик будет для нее первичным основным локальным.
и кибана читает из этого эластика лицензию и на основе ее отключает функционал у себя.

лиценизия вибивает только в эластик. в другие комопненты стека ее вбивать ненужно.

исхлодя из того что у нас эластик который указан в кибане имеет бейсик лицензию ,
то в кибане недоустпен мониторинг нескольких эластиков.


итак cross-cluster search в эластике нам никак не поможет мониторить несоклько 
эластиков в кибане если мы имеет только basic лицензию. нужна более продвинутая лицензия
которая поддерживает Multi-stack monitoring

итак с этим  разобрались. с бесплатной лицензией - нельзя иметь мониторинг несокльких
кластеров эластика в одной кибане.
максимум что можно.  это делать запросы к индексам в разных кластерам.
но это к мониторингу отрошения не имеет.


мониторинг это чисто опция x-pack
без нее даже закладки такой в кибане небудет


вот мы имеем толлько поставленный эластик.
и мы хотим чтобы логи мониторринга лились на другой хост.

1. ставим x-pack	( смотри выше)
2. ставим  лицензию	( смотри выше )
3. вносим изменения в конфиг

xpack.security.enabled: false

xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://test-es3:9200"]

где 
test-es3 - это хост на который мы хотим лить логи мониторинга перфоманса.


так значит что я выяснил  что  мониторинг перфоманса кластера эластика никак несвязан с 
закладкой management и какие там индекс паттерны заведены. поэтому cross cluster search нахуй никак
непомогает в вопросе привязать мониторинг второго кластера эластика в одной кибане.

щас я раскажу как мониторинг в кибане работает.

прежде всего как работает мониторинг в эластике.

вот есть эластик. вначале у него нет никакой статистики скорости работы.
мы ставм на эластик x-pack и он мониторит скорость работы эластика и результаты 
логирует в спец индексы которые он создает . они имеют '.' точку в названии.

далее мы этот эластик указываем как основной для кибаны.
кибана смотрит какие индексы есть в эластике . видит специндексы  созданные 
плагином x-pack и понимает что эти спецындексы содержат инфо 
о перфомансе этого кластера. 
и он их читает и отображает на графиках в закладке monitoring.
( в кибане тоже должен быть установлен x-pack).


отсюда и вытекает что индекс паттерны от удаленных кластеров в закладке management
не имеет никакого смысла с точки зрения монитторинга скрорости этого удаленного кластера.

второй кластер может появиться в закладке мониторинг
если заставить удаленный кластер писать свои перфоманс логи в наш эластик.

и это можно сделать.

на удаленном кластере нужно сделать вставку в конфиг

(удаленный эластик)# cat /etc/elasticsearch/elasticsearch.yml
...
xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://test-es3:9200"]

test-es3 - наш локальный эластик.


так что я выяснил. значит индекс в который x-pack пишет инфо о перфомансе кластера имеет 
имя вида 


.monitoring-es-6-2019.10.13

а вот как еще раз выглядит конфиг эластика чтобы писать перфоманс логи
на удаленный эластик

xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://test-es3:9200"]
    index.name.time_format: YYYY.MM.dd.MM.ww.YYYY
  id2:
    type: local


этот конфиг пишет пермофманс логи нетолько на удаленный кластер но и локально.

полезная опция 

index.name.time_format: YYYY.MM.dd.MM.ww.YYYY 

она задает формат для имени индекса в который перфоманс кластера пишется.

на выходе будет такое имя

.monitoring-es-6-2019.10.13.10.41.2019

значит что я могу сказать.

вот мы пишем перфоманс логи в наш кластер А с удаленного кластера Б  и пишеи логи с локального А кластера. 

так вот несмотря на то что у нас в кластер пишутся логи с двух кластеров - кибана не будет показывать
в закладке monitoring статистику по двум кластерам.

я вначале думал что это изза того что кластеры имеют одно имя. или что  лог имеет одно имя.
все поменял. каждый кластер имеет свое имя. и перфоманс логи пишутся в разыне файлы.
однако в кибане будет попеременно показыаться то статтистика одного кластера
то другого. типа как бутто они друг друга затирают. 

есть подозрение что помимо лога перфоманса вида .monitoring-* еще доп индекс X создается
в котором написано с какого лога читать инфо. 

и вот в нем то одна инфо то другая. так как оба кластера пишут в этот индекс X.

еще я думаю так происходит потому что в базовой лицензии указано 
что отображение с несолкьких кластеров перфоманс не поддерживается.

далее. вот мы имеем кластер А и кластер Б.
и настроили чтобы из кластера Б лились перфоманс логи в кластер А.
тогда чтобы в кластер А НЕлились логи кластера А который как мы уже выяснили все равно
в кибане вызовут путаницу конфликт - надо отключить на кластере А запись перфоманс логов.

для этого на кластере А надо в конфиге указать

xpack.monitoring.enabled: false

опять же замечу что в кибане ненужно ничего указать в разделе management 
для того чтобы в разделе monitoring шла статистика по перфомансу кластера.


будем считать что с кибаной более менее разобрались

как стало ясно нам ненужен ни логстэш ни beam ни все остальное чтобы в кибана
видеть статистику по пеорфомансу кластера эластика

--
значит вернемся к эластику.

во первых что интересно.
в systemd конфиге в /etc/systemd/system/multi-user.target.wants/elasticsearch.service
указан в качестве бинарника который мы запускаем

ExecStart=/usr/share/elasticsearch/bin/elasticsearch ...

а в ps aux такого бинарника вы не увидите. там будет /usr/bin/java

хахаха...


далее значит эластик крутится на JVM.
опции запуска JVM положено конфигурировать прежде всего через 
/etc/elasticsearch/jvm.options

в конфиге линии которые начинаются с минуса "-" относятся к JVM любой версии

-Xmx2g

а линии с числами

8:-Xmx2g

отвечают за опции для JVM уже определенной версии

а если вот такая опция

8-:-Xmx2g

то опция подходит для JVM >= версии 8

а вот эта опция

8-9:-Xmx2g

то версия JVM между 8 и 9

рассмотрим конкретно как выглядит живой jvm.options


#  cat jvm.options | grep -v '#'

-Xms1g
-Xmx1g
-XX:+UseConcMarkSweepGC
-XX:CMSInitiatingOccupancyFraction=75
-XX:+UseCMSInitiatingOccupancyOnly
-XX:+AlwaysPreTouch
-server
-Xss1m
-Djava.awt.headless=true
-Dfile.encoding=UTF-8
-Djna.nosys=true
-Djdk.io.permissionsUseCanonicalPath=true
-Dio.netty.noUnsafe=true
-Dio.netty.noKeySetOptimization=true
-Dio.netty.recycler.maxCapacityPerThread=0
-Dlog4j.shutdownHookEnabled=false
-Dlog4j2.disable.jmx=true
-Dlog4j.skipJansi=true
-XX:+HeapDumpOnOutOfMemoryError

как видно все просто. то есть все опции валидны для любой версии JVM и без всякой
мудоты с версиями JVM

посмотрим есть ли эти опции уже на живом запущенном эластике

# ps aux | grep elas
elastic+ 16697  2.2 21.2 4350128 1517204 ?     SLsl Nov04  64:26 /usr/bin/java 

-Xms1g 
-Xmx1g
-XX:+UseConcMarkSweepGC
-XX:CMSInitiatingOccupancyFraction=75
-XX:+UseCMSInitiatingOccupancyOnly 
-XX:+AlwaysPreTouch
-server
-Xss1m
-Djava.awt.headless=true
-Dfile.encoding=UTF-8
-Djna.nosys=true
-Djdk.io.permissionsUseCanonicalPath=true 
-Dio.netty.noUnsafe=true
-Dio.netty.noKeySetOptimization=true
-Dio.netty.recycler.maxCapacityPerThread=0
-Dlog4j.shutdownHookEnabled=false
-Dlog4j2.disable.jmx=true
-Dlog4j.skipJansi=true
-XX:+HeapDumpOnOutOfMemoryError

итак вот эти настройки что выше они из jvm.options
а те что ниже их там нет.хм.. давайте искать откуда они взяты,

хм.. вот эти настройки непонятно из какого конфига 
-Des.path.home=/usr/share/elasticsearch 
-cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch 


а настройки что ниже они из /etc/systemd/system/multi-user.target.wants/elasticsearch.service

-p /var/run/elasticsearch/elasticsearch.pid 
--quiet 
-Edefault.path.logs=/var/log/elasticsearch 
-Edefault.path.data=/var/lib/elasticsearch 
-Edefault.path.conf=/etc/elasticsearch


рассмотрим еще раз две настройки которые непонятно из какого файла читаются
-Des.path.home=/usr/share/elasticsearch 
-cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch 

мне кажется что первая настройка
-Des.path.home=/usr/share/elasticsearch 

она прописывается в /etc/systemd/system/multi-user.target.wants/elasticsearch.service
через

Environment=ES_HOME=/usr/share/elasticsearch

видно что не совсем совпадает. но я думаю это оно.

про вторую настройку
-cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch

незнаю где она прописывается.


итого. настройки сервиса эластик ( судя по ps aux ) идут из двух файлов

/etc/elasticsearch/jvm.options
/etc/systemd/system/multi-user.target.wants/elasticsearch.service

это существенно.
также существенно что по факту настройки сервиса эластик это по факту 
настройки для запуска java.

ну а теперь соберем суммарно вообще в каких файлах вообще делаются 
всевозможные настройки эластика

/etc/elasticsearch/jvm.options
/etc/systemd/system/multi-user.target.wants/elasticsearch.service
/etc/elasticsearch/elasticsearch.yml
/etc/default/elasticsearch

далее. 
насколко я понимаю /etc/default/elasticsearch используется для указания настроек
которые должны override настройки в файле /etc/systemd/system/multi-user.target.wants/elasticsearch.service

еще немного скажу про конфигурирование имеенно параметров запуска java
можно делать через параметры вида
-блаблабла

а можно через опцию

ES_JAVA_OPTS="$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir"
./bin/elasticsearch

нахуя она нужна если честно непонятно. ибо исходный способ итак отличный.
в любом случае данная опция в ubuntu указывается в 

/etc/default/elasticsearch


еще замечу такую фишку. что настройки эластика имеют две природы.
первый тип настроек это настройки самой java которая лежит в underline природе эластика
а второй тип настроек это настройки уже самого эластика как проги.

далее. в доке написано что java когда стартует она встроенно имеет такую фичу
что она также ищет опции старта через переменную JAVA_TOOL_OPTIONS.
но! эластик конкретно эту переменную неисполтзует. так как ее юзают 
другие проги на джаве и мы нехотим чтобы настройки старта эластика
пересекались с настройками других джава программ.

так ( 11/6/2019) закончил на этом - https://www.elastic.co/guide/en/elasticsearch/reference/6.5/secure-settings.html


итак судя по живому запущенному сервису эластика

есть весьма важные параметры которые надо бы прописать в конфиге эластика

1)
path:
  logs: /var/log/elasticsearch
  data: /var/lib/elasticsearch


что прикольно что можно указать несколько папок для сохраннеия индексов эластика

path:
  data:
    - /mnt/elasticsearch_1
    - /mnt/elasticsearch_2
    - /mnt/elasticsearch_3

предположим у нас закончилось место

но также важно что отдельный индекс будет храниться в одной папкой а не разобросан по
нескольким


2) имя кластера

cluster.name: logging-prod

нода может присоединиться к кластеру если ее имя совпадает с именем кластера 
на других нодах.

тут я сразу скажу о такой щтуке как - какие ноды достаточно указать в конфиге ноды 
чтобы в итоге все ноды были видны для кластера.

на мастер нодах достаточно указать других мастеров и все.

на нена мастер нодах досттано указать мастеров. итого - на всех нодах 
достаточно указывать только мастеров. тогда кластер успешно срастется.

3) имя ноды

node.name: prod-data-2

или так

node.name: ${HOSTNAME}

важно понимать что имя ноды это чисто для нас юзеров. 
для самого кластера важна node id.

его он хранит где то внутри /var/lib/elasticsarch.

поэтому если мы склонировали ноду. то надо обязательно эту всю папку удалить
иначе при запуске эалсасктика он напишет что есть две ноды с одним node id

4)  очень важно чтобы эластик никоогда  из памяти небыл помещен в свап.
для этого в документации сказано надо в /etc/fstab задизейблить swap

типа нет свапа нет проблем.

это делается чтобы эластик лежащий в памяти не попал в свап
иначе гарьаж коллектор охуенно затормозит свою работу.

(вообще в целом это колхозная
на мой счет насйтрока)

далее дока зачем то говорит что дополнительно надо

 заюзай фичу


elasticsearch.yml

bootstrap.memory_lock: true

тут я сразу говорю что не в доке а в конференции я нашел разьясннеие 
от разрабов эластика что если свап задизейблен на виртуалке
то юзать фичу bootstrap.memory_lock ненужно. в ней уже нет смысла.

далее про подробности фичи.

по факту эта настройка юзер линуксовую функцию mlockall
эта фича линукса заставляет его непомещать процесс в свап. это становится
запрещено для конкретного процесса.

если мы не можем почему то задизейблить  свап - вот 
только тогда надо пользовать фичу bootstrap.memory_lock: true
а если свап мы задизейблили то использовать эту фичу ненужно!

тем неменее

я заюзал фичу и после этого эластик не может стартунть.
выдает ошибку

memory locking requested for elasticsearch process but memory is not locked

оказалось чтобы это вылечить нужно
в 

/etc/systemd/system/elasticsearch.service.d/override.conf

добавить

[Service]
LimitMEMLOCK=infinity

замечу сразу что наебался с неправильным путем.
соблюди правильно путь
/etc/systemd/system/elasticsearch.service.d/override.conf
а я сделал вначале неправильный путь
/etc/systemd/system/multi-user.target.wants/elasticsearch.service.d/override.conf





обмусолим обсудим подоробнее что это как это итп.
значит мы  активировали опцию bootstrap.memory_lock: true 
она пытается заюзать опцию линукса mlockall которая запустит процесс и недаст
ему никогда попасть в свап. но чтобы эта mlackall сработал нужно чтобы для процесса 
котоырый мы запускаем чтобы был задан правильный лимит на количество оперативной
памяти которую для данного процесса можно защищать от попадания в свап.
дада. в линуксе по дефолту для каждого запускаемого процесса есть масса ограничений
которые на него накладываются чтобы типа этот процесс немог убить нахрен всю систему.
в частности для процесса  ограничено количество памяти которые у этого процесса 
неможет попасть в свап. стоп ! я сразу тут поправлюсь. не для процесса 
а для юзера. лимит в линуксе накладыватся на юзера а не на процесс.
 мы запускаем наш сервис под юзером elasticsearch.
так вот лимиты в линуксе указываются не для процессов на самом деле
а для юзеров. мы запускаем эластик под юзером elasticsearch 
мы можем узнать какие лимиты имеет данный юзер если под ним выполним команду


# sudo su elasticsearch -s /bin/bash
$ ulimit -a

core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 27745
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 27745
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

параметр каотоый нас интересует выглядит вот так

max locked memory       (kbytes, -l) unlimited

щас он говорит что количество памяти для юзера elasticsarch которую нельзя помещать в
свап неограничено

ну хорошо лимит посмотрели. а как его задать.
задать через файл /etc/security/limits.conf
elasticsearch    -       memlock         unlimited

либо 

elasticsearch    soft       memlock         unlimited
elasticsearch    hard       memlock         unlimited

- означает soft + hard

что такое soft и hard.  soft это некое дефолтное значение котороые мы задаем. но его 
при желании пользователь можем переопределить ( через командную строку и команду ulimit.
так как с помощью нее нетолько просматривать можно лимиты но и менять на лету).

а hard это ограничение сверху свыше которого юзер неможем зайти даже если захочет.


хорошо. лимит установили. лимит проверили через ulimit -a. но это мы проверили
для юзера. а как нам проверить для процесса лимиты.

1 способ

# cat /proc/13898/limits

2 способ
если мы сервис запустили через systemd то
# systemctl show elasticsearch | grep -i limitmemlock
LimitMEMLOCK=65536
LimitMEMLOCKSoft=65536

и тут мы подходим к самой большой наебке  в линукс с systemd.!!!!!!!
а именно - она игнориует настройки /etc/security/limits.conf

она кладет на них полный стог гумуса ! именно поэтому у нас небудет запускаться
эластик через systemd с опцией bootstrap.memory_lock: true хотя ulimit будет
показывать что лимит стоит бесконечный !!! вот так фигня...

поэтому если мы стартуем эластик через systemd то можно вообще забыть 
парится о насйтроках /etc/securyt/limits.conf ( aka ulimit )

для systemd лимиты надо настраивать в конфигах самой systemd.
тут и вылезает пресловутая настройка

[Service]
LimitMEMLOCK=infinity


а если к примеру мы запускаем эластик через init.d то тогда 
все нормально. 	init.d использует лимиты прописанные в /etc/security/limits.conf

что также странно.
что мы сделали настройку systemd через файл 

/etc/systemd/system/elasticsearch.service.d/override.conf

кстати замечу что при обнолвении версии эластика настройка не будет затерта.

но! если мы попробуем вместо этого вставить настройку в /etc/default/elasticsearch
то это почемуто непрокатывает. почему непонятно.

и вот наконец кстати подтверждение что мы все сдедали правильно из документации
https://www.elastic.co/guide/en/elasticsearch/reference/master/setting-system-settings.html

кстати там написано что если у нас ubuntu  + init.d то тогда 
у нас опять же игнорируются при запуске сервиса лимиты ulimit и нужно делать 
доп манипуляции с /etc/pam.d/su

пиздец... как все запутанно по дебильному. но у нас  не init.d

у нас ubuntu 16 + systemd

что еще прикольно в /etc/default/elasticsearch 
есть такой текст про нашу больную тему

# The maximum number of bytes of memory that may be locked into RAM
# Set to "unlimited" if you use the 'bootstrap.memory_lock: true' option
# in elasticsearch.yml.
# When using Systemd, the LimitMEMLOCK property must be set
# in /usr/lib/systemd/system/elasticsearch.service
#MAX_LOCKED_MEMORY=unlimited

то есть как ни крути. memlock настроаивается только в systemd конфиге. увы..




///////////////////////////////////////////////
отхожу немного в сторону

значит что я еще с удивелением одбнаружил из форума гитхаба эластика.
что если ты поставил эластик из пакета то ( внимание) запускать эластик 
из командной строки уже неподразумевается. ты несможешь.
только через сервис.
а если тебе хочется запускать эластик из командной строки то будь
добр тогда ставить эластикиз tar.gz. тогда можно.
пиздец.

но! потом я сам раскопал как же все таки стартаунуть эластик руками
при условии что мы его поставили из пакета.

для этого надо просто посмотреть как же он запускается в конфиге systemd

# cat /etc/systemd/system/multi-user.target.wants/elasticsearch.service

и мы увидим чтото типа того когда подставим переменные 

$ /usr/share/elasticsearch/bin/elasticsearch \
                                                -p /var/run/elasticsearch/elasticsearch.pid \
                                                -Edefault.path.logs=/var/log/elasticsearch \
                                                -Edefault.path.data=/var/log/elasticsearch \
                                                -Edefault.path.conf=/etc/elasticsearch

ура мы научились запукать эластик из комадной строки !

---

curator

эта такая официальная прога от команды эластика
прежде всего она мне понадоьилась чтбы с помощью нее 
удалять в эластике устаревшие индексы


ставим куратор , 
адрес .deb пакета для эластика версии 5.6.3 можно взять отсюда 
https://www.elastic.co/guide/en/elasticsearch/client/curator/5.6/apt-repository.html

# wget https://packages.elastic.co/curator/5/debian/pool/main/e/elasticsearch-curator/elasticsearch-curator_5.6.0_amd64.deb

# dpkg -i elasticsearch-curator_5.6.0_amd64.deb


создаем конфиг для куратора в

/etc/curator/curator.yml

пример этого файла можно посмотреть здесть 

https://www.elastic.co/guide/en/elasticsearch/client/curator/5.6/configfile.html


я же даю готовый конфиг.

---
# Remember, leave a key empty if there is no value.  None will be a string,
# not a Python "NoneType"
client:
  hosts:
    - 172.16.102.14
  port: 9200
  url_prefix:
  use_ssl: False
  certificate:
  client_cert:
  client_key:
  ssl_no_validate: False
  http_auth:
  timeout: 30
  master_only: False

logging:
  loglevel: INFO
  logfile: /var/log/syslog
  logformat: default
  blacklist: ['elasticsearch', 'urllib3']

как видно логи льются в syslog 
так сделано чтобы дополнительно log-rotate ненастраивать. у сислога то он уже настроен.

конфиг файл нужен чтобы куратор мог успешно подконекотиться к эластику.
тестируем что он это может

# curator_cli --config /etc/curator/curator.yml show_indices

если куратор непоказывает индексы то скорей всего как  у меня было неуказан 
путь к конфигу и куратор просто никуда не конектится.

если все правильно то мы должны увидеть список индексов эластика.


с конектом все в порядке.
теперь еще нужнен один файл. 

/etc/curator/action.yml

который говорит куратору что конкретно мы хотим сделать в с индексами в элатике
пример этого файла можно псмотреть здесь

https://www.elastic.co/guide/en/elasticsearch/client/curator/current/ex_delete_indices.html


даю уже мой файл

---
# Remember, leave a key empty if there is no value.  None will be a string,
# not a Python "NoneType"
#
# Also remember that all examples have 'disable_action' set to True.  If you
# want to use this action as a template, be sure to set this to False after
# copying it.
actions:
  1:
    action: delete_indices
    description: >-
      Delete indices older than 45 days (based on index name), for logstash-
      prefixed indices. Ignore the error if the filter does not result in an
      actionable list of indices (ignore_empty_list) and exit cleanly.
    options:
      ignore_empty_list: True
      disable_action: False
    filters:
    - filtertype: pattern
      kind: prefix
      value: .monitoring-
    - filtertype: age
      source: name
      direction: older
      timestring: '%Y.%m.%d'
      unit: days
      unit_count: 2

он удаляет все индексы которые имеют имя по маске 

.monitoring-*

 и имеют в имени дату старше 2 дня.


вначале запустим куратор в холостом режиме с опцией --dry-run

# curator --config /etc/curator/curator.yml /etc/curator/action.yml --dry-run

тогда в /var/log/syslog
будет написано какие индексы в эластике он собиарется удалить.

теперь можем уже запустить на удаление

# curator --config /etc/curator/curator.yml /etc/curator/action.yml

теперь надо добавить запуск куратора в линукс таск щедулер.

# crontab -e

вставляем

53 1 * * * /usr/bin/curator /etc/curator/action.yml --config /etc/curator/curator.yml >> /var/log/syslog 2>&1


так как мы крон настраивам от имени рута то и куратор будет стартовать от имени рута.
интересно что в самом файле ненужно указывать юзера под которым он стартанет
иначе любой бы лошара мог создать под собой крон задание которые бы запускалось 
от имени рута

каждый день в 01:53 он будет заупуксать куратор и вывод этой проги пихать в 
syslog (и ошибки если они будут)

---
//////////////////////////////////////////
возвращаемся обратно

отключать полностью свап вообще это ебанутая политика.
лучше всеже свап неотключать.
а юзать опцию для эластика

bootstrap.memory_lock: true 

также стаонвоится понятно что лучше при отключенном свапе на ноде кибану и эластик 
вместе назепускать. может нехватить памяти.


далее.что я для себя открыл про /etc/deafult
/etc/default - это доп файлы конфигурации для sysV (init.d) которые служат
в основном для того чтобы задать кастомные параметры запуска сервиса который запускается через 
init.d обычно туда пихаются параметры которые мы хотим чтобы остались в силе
даже после обновления пакета сервиса. то есть скрипт в init.d обновился но нам пофиг
ибо наша кастомная переменная запуска этого сервиса осталась незатронута.
также это значит  что к systemd каталог /etc/default
не имеют никакой силы никакого отношения для конфигурирования!

что я узнал про systemd.
есть дефолтная папка /lib/systemd в  которой лежат конфиги сервисов.
сейчас чаще всего она не используется в вместо нее используется /usr/lib/systemd.
итак либо в одну либо в другую папку падают конфиги при установке из пакета.
пример для пакета эластика

# dpkg-query -L elasticsearch | grep systemd
/usr/lib/systemd/system/elasticsearch.service

важно то что конфиги в этих папках менять нельзя. не положено.
а если мы хотим отредактировать сервис на наш лад для этого есть два пути.
путь первый. скопировать конфиг 
из /usr/lib/systemd/system/elasticsearch.service
в  /etc/systemd/system/multi-user.target.wants/elasticsearch.service

и там менять как нам угодно.  при наличии файла во второй папке файл из первой
папки нечитается. плюс состоит в том что обновление пакета никак неповлияет 
на наш кастомный конфиг. его настройки небудут перетерты. ура!

но  у этого способа есть минус. во первых неочевидно чтоже мы там наменяли 
по сравнению с исходным файлом. во вторых, когда мы обновим пакет 
то новые фичи прописанные в обновленном /usr/lib/systemd/system/elasticsearch.service
небудут задействованы.

поэтому есть еще второй способ отредактировать конфиг сервиса.
создать файл в папке 
/etc/systemd/system/elasticsearch.service.d/vasya.conf

и в нем мы указываем ТОЛЬКО те параметры конфига которые хотим поменять или добавить.
то есть всю портянку конфига переписывать ненужно.

пример
[Service]
LimitMEMLOCK=infinity

у этого способа есть два плюса.
во первых четко видно что конкретно мы внесли нового.
во вторых при обновлении пакета эластика его обновленный конфиг /usr/lib/systemd/system/elasticsearch.service с новыми модными фичами будет задействован.
очено хорошо !

из этого обьяснения становится очень хорошо понятно 
почему настройка LimitMEMLOCK=infinity не имела никакой силы когда 
мы ее вносили в /etc/default/elasticsearch - потому что этот конфиг никакого отношения к
systemd неимеет!

также становится хорошо понятно что внесение данной настройки в файл 
/etc/systemd/system/multi-user.target.wants/elasticsearch.service
это мы используем первый способ
а внесение насторойки в файл /etc/systemd/system/elasticsearch.service.d/vasya.conf
это мы вносим настройку вторым способом.
очень хорошо теперь понятно!


далее. в systemd есть параметры которые могут иметь сразу несколько значений (хз что это значит)
но важно тут другое. что чтобы его переписать надо его вначале занулить. 
пример. меняем параметр ExecStart.

если написать в /etc/systemd/system/elasticsearch.service.d/vasya.conf
[Service]
ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}

то systemd выдаст ошибку при рестарте сервиса.
а правильно будет вот так :

если написать в /etc/systemd/system/elasticsearch.service.d/vasya.conf
[Service]
ExecStart=
ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}


то есть его вначале занулить а потом уже указать новое значение!

далее. я узнал что systemd имеет свой лог файл. как я понял под логом 
имеется ввиду что сиранул сервис при старте в стандартный output. то есть
грубо говоря чтобы мы увидели на экране если бы стартовали сервис руками 
из командной строки. это значит что  в данном логе применительно к эластику
небудет логов которые эластик пишет в /var/log/elasticsearch/name.log
они как были так там и будут. здесь имеется еще раз подчеркну в виду 
только та информация которую сервис кидает на стандартный output
то бишь грубо говоря то что выдает сервис на экран при старте.
неболее того.
далее лог этот он нетекстовый он бинарный.
чтобы его посмотреть надо юзать команду

#journalctl

а чтобы в этом журнале посмотреть логи от конкретного юнита ( сервиса ) то надо юзать

# journalctl --unit elasticsearch

в доке от эластика сказано что по дефолту эластик молчит на экран при старте
(https://www.elastic.co/guide/en/elasticsearch/reference/6.5/deb.html)
а если мы хотим чтобы он не молчал то надо убрать опцию --quiet
она прописана в конфиге systemd в опции ExecStart

# cat /usr/lib/systemd/system/elasticsearch.service
...
[Service]

ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                --quiet
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}

а соотвесвтенно чтобы --quiet отключить надо написать конфиг

# cat /etc/systemd/system/elasticsearch.service.d/override.conf
[Service]
ExecStart=
ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}

я это сделал. но ничего существенного в логирование старта это недало.
вот что я вижу при старте эластика в systemd журнале

# journalctl --unit elasticsearch
Nov 04 00:49:05 test-es2 systemd[1]: Starting Elasticsearch...
Nov 04 00:49:05 test-es2 systemd[1]: Started Elasticsearch.


то есть буквально нихрена полезного.
двигаем дальше.


закончил доку примерно на этом

https://www.elastic.co/guide/en/elasticsearch/reference/6.5/deb.html

а в итоге надо дойти до этого
https://www.elastic.co/guide/en/elasticsearch/reference/master/setup-configuration-memory.html


===
очень важная полезная информация.

в эластике  невсе настройки настраиваются в 
elasticsearch.yml
jvm.options

часть настроек этот дебил хранит в 

/var/lib/elasticsearch/nodes/0/_state

на мастер нодах.
там лежат файлы в бинарном формате.


там хранятся настрройки которые типа должны для кластера выжиывать при перезагрузке.
чем при этом плохи настройки в elasticsearch.yml jbm.options которые тоже выжиывают в процессе перезагрузки
непонятно.

посмотреть что там лежит можно командой

GET /_cluster/settings

то что будет указано в секции persistent 
вот оно там и лежит.

поэтому очень важно если мы хотим вычистить всю скверну после клонирования виртулкм с эластиком это  удалить
нетолько elasticsearch.yml
jvm.options

но и удаить все в 

/var/lib/elasticsearch/

просто начитсто и полностью.

иначе когда мы индексы накатим то у нас вылезут старые настройки кластера котоыре могут нам реаьно портить кровь.


 ====
приколы эластика версии 7.

теперь вот так просто соединить мастера и дата ноды неполучится.

типичная ситуация.
вначале создаем кластер на котором все роли сидят на одной виртуалке-А. окей . подняли заработало.
потом хотим роли разделить. на первом шаге на отдельную виртулаку выносим мастера.

склонировали с первой виртуалки на вторую виртуалку-Б. поменяли конфиг. 
и тут оно напишет что хотя роль мастера но обнаружен индекс. а значит иди нафиг.
итак. если роль мастер то надо в /var/lib/elasticsearh стереть все индексы.

далее. когда мы на виртуалке-Б стерли индексы то стерся и UUID кластера.
и был создан заново.

какая нам от этого разница?
а такая что когда мы попробуем виртуалку-А подсоединить  к мастеру на виртуалке-Б
то мастер в логах напишет что дескать к нему ломиться в кластер дата нода виртуалки-А но у нее UUID отличается
от UUID который у мастера прописан. и поэтому иди нафиг.


вот эта ошибка в логах

Caused by: org.elasticsearch.cluster.coordination.CoordinationStateRejectedException: join validation on cluster state with a different cluster uuid YHNUQ5SET9uJgbCESwVEGg than local cluster uuid E5-ttBEHRwqet43tD2wdww, rejecting

поэтому присоединить дата ноду с индексами к мастеру НЕПОЛУЧИТСЯ! 
придется на дата ноде виртуалки-А удалить все индексы в /var/lib/elasticsearch

и только после этого виртуалку-Б получиться подсоеднить к мастеру виртуалки-А

---------

наверно если эластик стопишь целый кластер
то лучше всего прежде всего затушить именно мастеров.
тогда при гашении дата нод у кластера небудет ехать крыша

а если кластер с нуля стартовать то прежде всего нужнро дата ноды
запустить а мастера последними. чтобы кластер немог никакой движухи устроить дебильной

моя теория подтвердилась на практике, отключаем движуху шардов на кластере.
потом тушим мастеров. потом можно тушить все остальное.

включаем кластер в обратном порядке.
вначале запускаем все кроме мастеров. мастеров самыми последними.

также вот эта вот настройка 
discovery.zen.minimum_master_nodes
она ненужна на дата нодах . она нужна только на мастер нодах.
на всех других нодах кроме мастеров ее можно из конфига убрать.



---

с эластика 6 версии через curl команды отсылать в кластер изменилось..

добавилась фигня  -H 'Content-Type: application/json'

БЫЛО

curl -XPUT 127.0.0.1:9200/_cluster/settings -H 'Content-Type: application/json' -d '{
                "transient" : {
                    "cluster.routing.allocation.enable" : "all"
                }
        }'


СТАЛО

curl -XPUT 127.0.0.1:9200/_cluster/settings -H 'Content-Type: application/json' -d '{ 
  "transient" : {
    "cluster.routing.rebalance.enable" : "all"
  }
}'
---------------
И ЕЩЕ ЗАМЕТЬ.

В ОДНОЙ ВЕРСИИ ЭЛАСТИКА ОПЦИЯ

cluster.routing.allocation.enable" : "all"

В ДРУГОЙ

"cluster.routing.rebalance.enable" : "all"



----

я проверенно научился без потери индексов тушить и запускать кластер.
при выключении нужно первым делом погасить мастера. 
после этого спокойно гасить дата ноды и остальные ноды.
при запуске кластера нужно первым делом запустить все ноды кроме мастеров.
самыми последними нужно запускать мастеров.

итак еще раз если гасим кластер то первым делом гасим мастеров.
если включаем кластер то мастеров включаем самыми последними.
если нет мастеров то никаких процессов на дата нодах невозможно.
в том числе херня с индексами.

на практике я даже нетушил мастера а отключал на них сетевые карты.
когда я потом включил карты то мастера несмогли подключиться в логах
у эластика на мастере было написано о фатальной ошибке. и мне пришлось
перезапустить службу эластика на мастере
---

ка отклюбчит shard reallocation

PUT _cluster/settings

{
  "transient": {
    "cluster.routing.allocation.enable": "none"
  }
}

---

curator

как сделать так чтобы автоматом удалялись индексы.
актуально для индексов монтиронинга перформанса для кибаны

делаем как здесь

https://discuss.elastic.co/t/automatically-removing-index/106866/3

как запусить задание из крона вручную

run-parts -v /etc/cron.weekly

---

внос мастеров в куб.
	создать на ноде ip=192.168.0.x
	опубликовать на ней одиночный мастер

когда все ноды эластика будут внутри куба поменять 
	по одному у них publish_host на внутрикубовский
	потом поудалять службы котороые их пробрасвыают наружу
	
elastic master нода сколко ей нужно ресурсов
	cpu
		1 ядро E5-2620 v4
		min 3% m
		max 25% m
	ram
		min 1975 Mi
		max 2.3 Gi
	hdd
		200 KB
	QOS
		spec:
		  containers:
			....
		status:
		  qosClass: Guaranteed
		

	
	в yaml это выглядит вот так

		 status:
			  qosClass: Guaranteed
			  containers:
		 limits:
              cpu: 3m
              memory: 1975Mi
            requests:
              cpu: 25m
              memory: 2.3Gi
		 storageClassName:  pv-localdisks-1g
			  resources:
				requests:
				  storage: 1Gi

 
мы можем перенести мастера  в куб при налчии нод которые останутся вне куба только при условии что тогда на нодах которые вне куба мы пропишем в конфиг чтобы они discovery_seed_host искали в этом файле.
а в файле мы пропишем IP адреса вместе с портами. только благодрнаря этой фиче мы можем перенести мастеров в куб непоследними.

 
 прикол с сервисом NodePort что нельзя на хосте на внешнем IP выбрать любой порт для проброса - нет. только в диапазоне 30 000 - 32 тыщи с копейками.
 NodePort в чем еще  его прикол. все порты которые ты в нем укажешь внутьренние он обязательно их всех пробросит наружу хоста.даже если неуказывать какой порт снаружи ты хочешт иметь так он автоматом наружний порт выберет.
 
 когда у нас часть нод вне куба и когда часть внутри. как перенести
 мастер ноду снаружи вовнутрь.
	как будут его исктаь наружные ноды:
	через файл.
	в конфиге внешней ноды пропишу
		elasticsearch.yml
			discovery.seed_providers: masters-list.txt
		
		masters-list.txt
			masters.local:30897
		
		нода с ним свяжется и мастер ей через настройку сообщит
		 - name: "transport.publish_host"
            value: "192.168.7.34"
          - name: "transport.publish_port"
            value: "30907"
	
        где 192.168.7.43 это ip хоста куба на котором сидит мастер
	таким образом наружняя нода теперь имеет связь с мастером
	
	как будут его исктаь внутренние ноды:
		в конфиге внутренней ноды я укажу чтобы discovery.seed.hosts она в брала из конфиг мапа
		
		конфиг_ноды.yaml
          - name: "discovery.seed_hosts"
            valueFrom:
              configMapKeyRef:
                name: confmap
                key: "discovery_seed_hosts"
  			
        и в конфигмапе я пропишу что мастера надо искать по имени его сервиса
		$ cat confMap.yaml
		...
		  discovery_seed_hosts: "master5-internal:9300"

		внутреняя нода через сервис связыается с мастером и он ей сообщает  что связь с ним надо держать через 192.168.7.43:30907
		
		таким образом и внешние и внутренние ноды смогли найти и наладить связь с мастером который сидит внутри куба.
		

		 


