узнать какие версии elasticsearch доступны


# apt-cache madison elasticsearch


установить версию определенную пакета

# apt-get install elasticsearch=5.6.3

cтавим elasticsearch

# apt-get install elasticsearch=5.6.3

ставим x-pack на elasticsearch
сразу важно обьяснить что же эта за хрень x-pack

это плагин.

он добавляет некий функционал.

этот плагин ставится на кучу программ.
он ставится на эластиксерч, на кибану, на логстэш и еще на всяко разно.
причем в кажду программу его нужно ставить ОТДЕЛЬНО.
отдельно накатывать на эластиксерч, отдельно накатывать на кибану итд.

захуя он нужен. 
если мы ставим его на эластиксерч то он навскидку создает спец индексы в эластике и начинает туда лить специнформацию. 
например  он начинаем собирать и лить информацию по статистике работы кластера. это впервую очередь и есть ради чего я его ставлю. 
чтоб снимать статистику по перфомансу кластера. 
также он добавляет в эластик возможность работать с юзерами ролями итп. но этот функционал платный. если у нас лицензия бейсик
то в ней эта фича недоступна. какие еще есть фичи в x-pack буду освещать походу их использования.

причем чтобы восолтзоваться записанной x-pack статистикой работы кластера мало только поставить x-pack на эластик.
также надо поставить кибану да еще и накатить x-pack на кибану. потом увязать кибану и эластик. и только тогда мы 
сможем вкусить саттистику работы эластика в кибане. но все по порядку.

ставим x-pack на эластика. ( еще раз скажу что дока у эластик гавно.. пока я это все понял..... эх.. )

плагины  в эластике ставятся через спец утилиту эластика

# /usr/share/elasticsearch/bin/elasticsearch-plugin install x-pac


при этом получим ошибку

[=================================================] 100%  
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.io.FilePermission \\.\pipe\* read,write
* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries
* java.lang.RuntimePermission getClassLoader
* java.lang.RuntimePermission setContextClassLoader
* java.lang.RuntimePermission setFactory
* java.security.SecurityPermission createPolicy.JavaPolicy
* java.security.SecurityPermission getPolicy
* java.security.SecurityPermission putProviderProperty.BC
* java.security.SecurityPermission setPolicy
* java.util.PropertyPermission * read,write
* java.util.PropertyPermission sun.nio.ch.bugLevel write
* javax.net.ssl.SSLPermission setHostnameVerifier
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.


я просмотрел кучу доков. и все кладут болт на эти настройки.
итак просто игнорим.

перезапускаем эластик

# # service elasticsearch restart

проверяем что x-pack установлен на хосте

# /usr/share/elasticsearch/bin/elasticsearch-plugin list
x-pack

x-pack он платный. они дают 30 дней на зценить.
потом у него часть функционала отваливается.

остаестся только 
мониторинг
search profiler
debugger

якобы вот эти джаава пермишнс нужна для двух вещей. для алертинга по почте и чегото там про машинлернинг.


после того как мы установили x-pack нам его даже ненужно активировать в конфиге эластике.
плагин уже работатает.

и как первое следствие теперь просто так доступ к кластеру неработает. спрашивает логин и пароль.
поэтому отключаем аутентификацию при доступе к кластеру.

# cat elasticsearch.yml
...
xpack.security.enabled: false


перезапускаем эластик.  готово.

далее.
по умолчанию у нас 30 дневная триальная лицензия на эластик. проверяем статус лицензии

~# curl http://test-es3:9200/_license/
{
  "license" : {
    "status" : "active",
    "uid" : "73473f97-9efe-489b-9ad0-71b77b09c1df",
    "type" : "trial",
    "issue_date" : "2019-10-04T21:02:27.361Z",
    "issue_date_in_millis" : 1570222947361,
    "expiry_date" : "2019-11-03T21:02:27.361Z",
    "expiry_date_in_millis" : 1572814947361,
    "max_nodes" : 1000,
    "issued_to" : "elasticsearch",
    "issuer" : "elasticsearch",
    "start_date_in_millis" : -1
  }
}


видно что лицензия триальная.

заходим на сайт эластика регаемся и заказыаем basic беплатную лицензию. и на почту приходит ссылка на файл с лицензией.
ее нужно впихнуть в эластик.

через запрос

POST /_license
{
  "licenses": [
    {
      "uid":"893361dc-9749-4997-93cb-802e3d7fa4xx",
      "type":"basic",
      "issue_date_in_millis":1411948800000,
      "expiry_date_in_millis":1914278399999,
      "max_nodes":1,
      "issued_to":"issuedTo",
      "issuer":"issuer",
      "signature":"xx"
    }
    ]
}


при попытке впихнуть базовую лицензию в эластик он пишет


{
"acknowledged": false,
"license_status": "valid",
"acknowledge": {
"message": "This license update requires acknowledgement. To acknowledge the license, please read the following messages and update the license again, this time with the "acknowledge=true" parameter:",
"watcher": [
"Watcher will be disabled"
],
"security": [
"The following X-Pack security functionality will be disabled: authentication, authorization, ip filtering, and auditing. Please restart your node after applying the license."
,
"Field and document level access control will be disabled."
,
"Custom realms will be ignored."
],
"monitoring": [
"Multi-cluster support is disabled for clusters with [BASIC] license. If you are running multiple clusters, users won't be able to access the clusters with [BASIC] licenses from within a single X-Pack Kibana instance. You will have to deploy a separate and dedicated X-pack Kibana instance for each [BASIC] cluster you wish to monitor."
,
"Automatic index cleanup is locked to 7 days for clusters with [BASIC] license."
],
"graph": [
"Graph will be disabled"
],
"ml": [
"Machine learning will be disabled"
]
}
}


то есть часть функционгала будет вырублена нахер. так происходит когда мы впихиываем лицензию которая более урезающая чем была. 
у нас была полная но на 30 дней а мы пихаем базовую.

чтобы впихнуть базовую надо  добавить параметр acknowlege=true

POST /_license?acknowledge=true
{
  "licenses": [
    {
      "uid":"893361dc-9749-4997-93cb-802e3d7fa4xx",
      "type":"basic",
      "issue_date_in_millis":1411948800000,
      "expiry_date_in_millis":1914278399999,
      "max_nodes":1,
      "issued_to":"issuedTo",
      "issuer":"issuer",
      "signature":"xx"
    }
    ]
}


успех. проверяем текущую лицензию

# curl http://test-es3:9200/_license
{
  "license" : {
    "status" : "active",
    "uid" : "210f3f42-52e4-4edb-9429-f6758fb47061",
    "type" : "basic",
    "issue_date" : "2019-09-05T00:00:00.000Z",
    "issue_date_in_millis" : 1567641600000,
    "expiry_date" : "2020-09-05T23:59:59.999Z",
    "expiry_date_in_millis" : 1599350399999,
    "max_nodes" : 100,
    "issued_to" : "Al",
    "issuer" : "Web Form",
    "start_date_in_millis" : 1567641600000
  }
}

видно что лицкнзия активно неистекла и что она бейсик

а вот еще как можно увидеть инфо по лицензии и установлен и по плагинам какие грузит эластик

# tail -f /var/log/elasticsearch/elasticsearch.log
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-expression]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-groovy]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-mustache]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [lang-painless]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [parent-join]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [percolator]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [reindex]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [transport-netty3]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded module [transport-netty4]
[2019-10-13T16:15:02,458][INFO ][o.e.p.PluginsService     ] [tbgAYqa] loaded plugin [x-pack]
[2019-10-13T16:15:08,926][INFO ][o.e.l.LicenseService     ] [tbgAYqa] license [210f3f42-52e4-4edb-9429-f6758fb47061] mode [basic] - valid



как видно из того что выше при бейсик лицензии security отключается. нет аутентификации нет юзеров итп.

закончили мы на rbac активации при бейсик лицензии.

как я понял из дурацких перекрестноссылочной говно доки от эластик -  авторизацию аутентификацию 
обеспечивает именно плагин x-pack соотвесвтенно если в его бесплатной версии этот функционал отключен.
значит либо плати либо иди нахуй.

когда мы поставили x-pack то в нем уже есть несолько предустановленных юзеров.

эти юзеры принадлежат к ролям ( аналог групп в виндовсе). таковые юзеры

elastic принадлежит роли superuser другими словами полный доступ ко всему и вся

kibana этот юзер используется кибаной для того чтобы под этим юзером иметь доступ к эластику.

logstash_system бла бла.


что я понял - если у нас лицензия непозволяет работать с какойто фичей эластика например у нас базовая лицензия 
которая непозволяет работать с фичей security от плагина x-pack то : буквально почти никакие команды от данной фичи работать не
будут . например посмотреть список пользователей будет посмотреть нельзя. список ролей посмотреть будет нельзя . 
и тому подобное. от фичи security будет работать некий убогий супер минимум.да. в этом тоже прикол. 
фича по лицензии недоступна ( тут тоже полная хуйня  и неразбериха. у них на сайте сказано что лицензия бейсик она
дает овзможность работаь с роолями и юзерами. а при установке лицензии x-pack сразу говорит что security фича не будет работать).
тем не менее после установки x-pack и перезапуска эластика при попытке обраться к эластику по 9200 выплывет окно которое спросит
логин и пароль. типа чего блядь?  фича же неработает .однако как показала практика всеже убого эта фича работает
на том уровне что можно залогинться на кластер под акаунтом уже встроенным при установке x-pack

l:elastic
p:elastic

это креды типа суперюзера в эластике.

итак. чтобы нееобаться с этой фичей и юзерами - идем в конфиг и этот функционал нахуй дизейблим.


# cat elasticsearch.yml
...
xpack.security.enabled: false

как тока мы это сделали. то доступ ко всем ресурами эластика без всяких ограничений.
без всякий аутентификаций и юзеров.

далее вся тема о юзерах ролях итп это все сразу уже просто недоступно отваливается и об этом
можно забыть в эластике

индексы вида 

.watcher*
.monitoring*

это индексы которые автоматом заводит x-pack


итак эластик поставили.
бейсик лицензию вбили
на эластик x-pack накатили.
функционал security нахер выключили.

ставим кибану.

как я понял. с бейсик лицензией для каждого кластера эластик надо ставить свою кибану. 
то есть кибана несможет обслужвать несколько эластиков. это недаст лицензия.


еще сразу замечу что щас четко девеплоеры сказали что все версии должны одинаковы у компоненетов - у кибаны эластика логстэша и прочих.
у всех должна быть едтиная версия.

выясняем установленную версию эластика к которому мы будем привязывать кибану

# dpkg -l | grep elas
ii  elasticsearch                      5.6.3

смотрим доступна ли эта же версия кибаны

~# apt-cache madison kibana

ставим версию 5.6.3

~# apt-get install kibana=5.6.3

ставим x-pack на кибану

# /usr/share/kibana/bin/kibana-plugin install x-pack


насоклько я понял в кибану лицензии вбивать никакие ненужно

# systemctl enable kibana
# systemctl start kibana

доступ к кибане по порту :5601


замечу по конфигу кибаны

# cat /etc/kibana/kibana.yml | grep -v '#'

server.host: "test-es3"


зачем эта опция нужна.

по дефолту кибана открывает сокет на 127.0.0.1:5601

поэтому чтобы зайти на кибану надо ей настрить чтобы она открыла сокет для внешних соединений на некий IP.

так вот можно там  прописать напрямую IP хоста. но! если потом мы его поменяе то придется еще раз менять в конфиге кибаны.
так вот более круто добавить hostname вместо IP . тогда поменяв ip на хосте впоследствии никак не сломает кибану.


далее. поставил я все это. пытаюсь зайти на кибану через браузер

host:5601

а оно мне выводит окно для логина и пароля. и пишет что мол есть проблема с лицензией на эластике поэтому проверяйте на эластике.

так нупонятно что раз на эластике мы отключили seciruty то есть юзеров то на кибане тоже нужно отключить.

итак заносим в конфиг эластика фичу


# cat /etc/kibana/kibana.yml  | grep -v '#'

server.host: "test-es3"

xpack.security.enabled: false

перезапускаем кибану.

и тут мы видим что она никак нехочет вставать.

порт 5601 неначинает слушаться.

лезем в  syslog и видим


 Started Kibana.
Oct  6 22:57:39 test-es3 kibana[9218]: {"type":"log","@timestamp":"2019-10-06T19:57:39Z","tags":["fatal"],"pid":9218,"level":"fatal","message":"EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'","error":{"message":"EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'","name":"Error","stack":"Error: EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'\n    at Error (native)","code":"EACCES"}}
Oct  6 22:57:39 test-es3 kibana[9218]: FATAL { Error: EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'
Oct  6 22:57:39 test-es3 kibana[9218]:     at Error (native)
Oct  6 22:57:39 test-es3 kibana[9218]:   cause:
Oct  6 22:57:39 test-es3 kibana[9218]:    { Error: EACCES: permission denied, open '/usr/share/kibana/optimize/bundles/graph.entry.js'
Oct  6 22:57:39 test-es3 kibana[9218]:        at Error (native)
Oct  6 22:57:39 test-es3 kibana[9218]:      errno: -13,
Oct  6 22:57:39 test-es3 kibana[9218]:      code: 'EACCES',
Oct  6 22:57:39 test-es3 kibana[9218]:      syscall: 'open',
Oct  6 22:57:39 test-es3 kibana[9218]:      path: '/usr/share/kibana/optimize/bundles/graph.entry.js' },
Oct  6 22:57:39 test-es3 kibana[9218]:   isOperational: true,
Oct  6 22:57:39 test-es3 kibana[9218]:   errno: -13,
Oct  6 22:57:39 test-es3 kibana[9218]:   code: 'EACCES',
Oct  6 22:57:39 test-es3 kibana[9218]:   syscall: 'open',
Oct  6 22:57:39 test-es3 kibana[9218]:   path: '/usr/share/kibana/optimize/bundles/graph.entry.js' }


я залез в гугл и в форуме эластика я нашел что это типа известный баг. и есть воркэранд.

# chown -R kibana.kibana /usr/share/kibana/

после этого еще раз перезапускаем кибану

и видим в логах инфо

 Started Kibana.
Oct  6 23:00:06 test-es3 kibana[9642]: {"type":"log","@timestamp":"2019-10-06T20:00:06Z","tags":["info","optimize"],"pid":9642,"message":"Optimizing and caching bundles for graph, monitoring, ml, kibana, stateSessionStorageRedirect, timelion and status_page. This may take a few minutes"}


итак ждем несколько минут и кибана встает. и мы успешно в  нее входим.  
при этом видно в перфоманс мониторе что кибана она непралалелится. она однопроцессорная.


итак вошли в кибану


тут я наткнулся еще на один момент. кибана написала что неможет подключииться к http://localhost:9200

ну это ясно. так как в эластике в конфиге стояло чтобы он слушал входящие на IP:9200
заменил на  0.0.0.0:9200

перезашел в кибану. все окей.


ура.


и вот мы зашли на кибану. и он спрашивает типа про какие индесы вы хотите иметь инфо.
пока можно на это забить. 

и вот в кибане в закладке мониторинг  мы наконец увидим графическую статистику работы кластера эластика.

то чего хотели добились


далее. еще надо зайти в конфиг

/etc/kibana/kibana.yml
и активировать найстроку

logging.quiet: true

она даст то что кибана перестанет каждые 10 секунд срать в syslog
а будет только писать туда если ошибка




значит далее я изучал раздел 

https://www.elastic.co/guide/en/kibana/5.6/getting-started.html

он на примерах показывает как использовать визуализацию в кибане

часть разделов в кибане она появляется только если на кибану установлен x-pack.
если мы откроем документацию кибаны - https://www.elastic.co/guide/en/kibana/5.6/xpack-monitoring.html
и откроем какойто раздел. то если рядом с названием раздела написано x-pack
значит этот раздел появлется только при устанолвке x-pack на кибану

значит основной момент - в конфиге кибаны мы указыаем из какого кластера эластика мы будем
засасмывать данные в кибану

elasticsearch.url: "http://localhost:9200"

( об этом написано здесь - https://www.elastic.co/guide/en/kibana/5.6/settings.html )

так вот. указали в конфиге кибаны где ему искать эластик с которого мы хотим данные.

далее открываем саму веб морду кибаны http://test-es3:5601

идем в  management - index patterns - create index pattern - index pattern

указываем имя индекса 

logstash-*


и только после этого уже можно будет в кибане с этого индекса засасывать реквестами данные.

так вот . получается адрес кластера эластика мы указываем в конфиге кибаны. а индексы 
этого кластера уже указываются в веб морде кибаны.

так вот ! 
возникает вопрос, а как же сделать так чтобы в кибане можно было видеть неодин кластер эластика а
несколько. 
ответ - пока непонятно.

скрипт. 
как для всех индексов на кластере изменить число реплик

===
#!/bin/bash


for value in $(curl -s http://localhost:9200/_cat/indices | awk '{print $3}')

do

    curl -XPUT "localhost:9200/$value/_settings" -d '
{
    "index" : {
        "number_of_replicas" : 0
    }
}'

done

===


как получить список индексов 

# curl -s http://localhost:9200/_cat/indices?v

==

запрос на выдачу всех документов из локального индекса

curl -XGET "http://test-es3:9200/.kibana/_search" -d '
{
  "query": {
    "match_all": {}
  }
}'
 
локальный индекс значит что для test-es3 он является родным.

так вот можно сделать так чтобы мы обращались к кластеру А чтобы он 
поискал нам документы в кластере Б.

фишка в том что мы можем искать хоть в десяти кластерах эластика 
и для этого нам ненужно обращаться к 10 разным кластерам а делать 
это из одной точки из кластера А.

для  того чтобы в кластер test-es3 добавить удаленный кластер test-es2 
нужно в конфиге кластера test-es3 добавить

test-es3# cat /etc/elasticsearch/elasticsearch.yml

...

search:
    remote:
        test-es2:
            seeds: test-es2:9300


теперь мы можем искать в test-es2 обращаясь из test-es3

вот как это делается


curl -XGET "http://test-es3:9200/test-es2:.kibana/_search?pretty" -d '
{
  "query": {
    "match_all": {}
  }
}'


индекс на удаленном кластере мы задаем как 

test-es2:.kibana

таким макаром эластик понимает ищем ли мы в локальном индексе или удаленном.


такая фича в эластика назвыается cross-cluster search

как видно что чтобы фича работала нужно делать изменения только на одном кластере.   похожий функционал дает другая фича эластика - tribe node. но она 
deprecateed поэтому мы ее не рассамтриваем.

зачем нам вообще нужна щас эта фича cross-cluster search

нужна она затем чтобы в кибане иметь возможность работать
с индексами из разных кластеров.

сама кибана умеет подключитаться только к одному кластеру эластика.

заюзав на этом эластике cross-cluster search 
мы можем в кибане работаь с несколькими кластерами эластика

после того как в конфиге эластика мы указали remote кластеры
мы идем в кибану в management и там указываем новый index pattern в полном виде

test-es2:someindex*

готов. 

итак в management у нас появился новый index pattern.
но! что это нам дает?
это нам нифига недает возможность смотреть мониторинговые состояние по данному индексу удаленному
в закладке мониторинг!

это нам только дает что мы можем делать запросы к документам в закладке discover.
это конечно неплохо. 

но важно что смотреть состояние перфоманса в заклкдке monitoring мы сможем.

по крайней мере это недоступно в кибане для лицензии эластика basic.

судя по странице с лицензиями - https://www.elastic.co/subscriptions
Multi-stack monitoring недоступна в basic лицензии.
а это походу и есть фишка что мы из одной кибаны можем 
смотреть на здоровье нескольких кластеров эластика


установка x-pack дает в кибане следующие закладки

* Machine Learning
* Graph
* Monitoring

насколько я понимаю --- лицензия вбивается в эластик.  а потом другие компоненты
эластик стека ( кибана, логстэш ) обращаются к эластику и читают лицензию. 
и исходя из этого компоненты ( кибана логстэш) у себя отключают или включают их функционал.

в нашем конкретном случае мы в конфиге кибаны ууказываем к какому кластеру 
эластика кибане коннектится , какой эластик будет для нее первичным основным локальным.
и кибана читает из этого эластика лицензию и на основе ее отключает функционал у себя.

лиценизия вибивает только в эластик. в другие комопненты стека ее вбивать ненужно.

исхлодя из того что у нас эластик который указан в кибане имеет бейсик лицензию ,
то в кибане недоустпен мониторинг нескольких эластиков.


итак cross-cluster search в эластике нам никак не поможет мониторить несоклько 
эластиков в кибане если мы имеет только basic лицензию. нужна более продвинутая лицензия
которая поддерживает Multi-stack monitoring

итак с этим  разобрались. с бесплатной лицензией - нельзя иметь мониторинг несокльких
кластеров эластика в одной кибане.
максимум что можно.  это делать запросы к индексам в разных кластерам.
но это к мониторингу отрошения не имеет.


мониторинг это чисто опция x-pack
без нее даже закладки такой в кибане небудет


вот мы имеем толлько поставленный эластик.
и мы хотим чтобы логи мониторринга лились на другой хост.

1. ставим x-pack	( смотри выше)
2. ставим  лицензию	( смотри выше )
3. вносим изменения в конфиг

xpack.security.enabled: false

xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://test-es3:9200"]

где 
test-es3 - это хост на который мы хотим лить логи мониторинга перфоманса.


так значит что я выяснил  что  мониторинг перфоманса кластера эластика никак несвязан с 
закладкой management и какие там индекс паттерны заведены. поэтому cross cluster search нахуй никак
непомогает в вопросе привязать мониторинг второго кластера эластика в одной кибане.

щас я раскажу как мониторинг в кибане работает.

прежде всего как работает мониторинг в эластике.

вот есть эластик. вначале у него нет никакой статистики скорости работы.
мы ставм на эластик x-pack и он мониторит скорость работы эластика и результаты 
логирует в спец индексы которые он создает . они имеют '.' точку в названии.

далее мы этот эластик указываем как основной для кибаны.
кибана смотрит какие индексы есть в эластике . видит специндексы  созданные 
плагином x-pack и понимает что эти спецындексы содержат инфо 
о перфомансе этого кластера. 
и он их читает и отображает на графиках в закладке monitoring.
( в кибане тоже должен быть установлен x-pack).


отсюда и вытекает что индекс паттерны от удаленных кластеров в закладке management
не имеет никакого смысла с точки зрения монитторинга скрорости этого удаленного кластера.

второй кластер может появиться в закладке мониторинг
если заставить удаленный кластер писать свои перфоманс логи в наш эластик.

и это можно сделать.

на удаленном кластере нужно сделать вставку в конфиг

(удаленный эластик)# cat /etc/elasticsearch/elasticsearch.yml
...
xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://test-es3:9200"]

test-es3 - наш локальный эластик.


так что я выяснил. значит индекс в который x-pack пишет инфо о перфомансе кластера имеет 
имя вида 


.monitoring-es-6-2019.10.13

а вот как еще раз выглядит конфиг эластика чтобы писать перфоманс логи
на удаленный эластик

xpack.monitoring.exporters:
  id1:
    type: http
    host: ["http://test-es3:9200"]
    index.name.time_format: YYYY.MM.dd.MM.ww.YYYY
  id2:
    type: local


этот конфиг пишет пермофманс логи нетолько на удаленный кластер но и локально.

полезная опция 

index.name.time_format: YYYY.MM.dd.MM.ww.YYYY 

она задает формат для имени индекса в который перфоманс кластера пишется.

на выходе будет такое имя

.monitoring-es-6-2019.10.13.10.41.2019

значит что я могу сказать.

вот мы пишем перфоманс логи в наш кластер А с удаленного кластера Б  и пишеи логи с локального А кластера. 

так вот несмотря на то что у нас в кластер пишутся логи с двух кластеров - кибана не будет показывать
в закладке monitoring статистику по двум кластерам.

я вначале думал что это изза того что кластеры имеют одно имя. или что  лог имеет одно имя.
все поменял. каждый кластер имеет свое имя. и перфоманс логи пишутся в разыне файлы.
однако в кибане будет попеременно показыаться то статтистика одного кластера
то другого. типа как бутто они друг друга затирают. 

есть подозрение что помимо лога перфоманса вида .monitoring-* еще доп индекс X создается
в котором написано с какого лога читать инфо. 

и вот в нем то одна инфо то другая. так как оба кластера пишут в этот индекс X.

еще я думаю так происходит потому что в базовой лицензии указано 
что отображение с несолкьких кластеров перфоманс не поддерживается.

далее. вот мы имеем кластер А и кластер Б.
и настроили чтобы из кластера Б лились перфоманс логи в кластер А.
тогда чтобы в кластер А НЕлились логи кластера А который как мы уже выяснили все равно
в кибане вызовут путаницу конфликт - надо отключить на кластере А запись перфоманс логов.

для этого на кластере А надо в конфиге указать

xpack.monitoring.enabled: false

опять же замечу что в кибане ненужно ничего указать в разделе management 
для того чтобы в разделе monitoring шла статистика по перфомансу кластера.


будем считать что с кибаной более менее разобрались

как стало ясно нам ненужен ни логстэш ни beam ни все остальное чтобы в кибана
видеть статистику по пеорфомансу кластера эластика

--
значит вернемся к эластику.

во первых что интересно.
в systemd конфиге в /etc/systemd/system/multi-user.target.wants/elasticsearch.service
указан в качестве бинарника который мы запускаем

ExecStart=/usr/share/elasticsearch/bin/elasticsearch ...

а в ps aux такого бинарника вы не увидите. там будет /usr/bin/java

хахаха...


далее значит эластик крутится на JVM.
опции запуска JVM положено конфигурировать прежде всего через 
/etc/elasticsearch/jvm.options

в конфиге линии которые начинаются с минуса "-" относятся к JVM любой версии

-Xmx2g

а линии с числами

8:-Xmx2g

отвечают за опции для JVM уже определенной версии

а если вот такая опция

8-:-Xmx2g

то опция подходит для JVM >= версии 8

а вот эта опция

8-9:-Xmx2g

то версия JVM между 8 и 9

рассмотрим конкретно как выглядит живой jvm.options


#  cat jvm.options | grep -v '#'

-Xms1g
-Xmx1g
-XX:+UseConcMarkSweepGC
-XX:CMSInitiatingOccupancyFraction=75
-XX:+UseCMSInitiatingOccupancyOnly
-XX:+AlwaysPreTouch
-server
-Xss1m
-Djava.awt.headless=true
-Dfile.encoding=UTF-8
-Djna.nosys=true
-Djdk.io.permissionsUseCanonicalPath=true
-Dio.netty.noUnsafe=true
-Dio.netty.noKeySetOptimization=true
-Dio.netty.recycler.maxCapacityPerThread=0
-Dlog4j.shutdownHookEnabled=false
-Dlog4j2.disable.jmx=true
-Dlog4j.skipJansi=true
-XX:+HeapDumpOnOutOfMemoryError

как видно все просто. то есть все опции валидны для любой версии JVM и без всякой
мудоты с версиями JVM

посмотрим есть ли эти опции уже на живом запущенном эластике

# ps aux | grep elas
elastic+ 16697  2.2 21.2 4350128 1517204 ?     SLsl Nov04  64:26 /usr/bin/java 

-Xms1g 
-Xmx1g
-XX:+UseConcMarkSweepGC
-XX:CMSInitiatingOccupancyFraction=75
-XX:+UseCMSInitiatingOccupancyOnly 
-XX:+AlwaysPreTouch
-server
-Xss1m
-Djava.awt.headless=true
-Dfile.encoding=UTF-8
-Djna.nosys=true
-Djdk.io.permissionsUseCanonicalPath=true 
-Dio.netty.noUnsafe=true
-Dio.netty.noKeySetOptimization=true
-Dio.netty.recycler.maxCapacityPerThread=0
-Dlog4j.shutdownHookEnabled=false
-Dlog4j2.disable.jmx=true
-Dlog4j.skipJansi=true
-XX:+HeapDumpOnOutOfMemoryError

итак вот эти настройки что выше они из jvm.options
а те что ниже их там нет.хм.. давайте искать откуда они взяты,

хм.. вот эти настройки непонятно из какого конфига 
-Des.path.home=/usr/share/elasticsearch 
-cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch 


а настройки что ниже они из /etc/systemd/system/multi-user.target.wants/elasticsearch.service

-p /var/run/elasticsearch/elasticsearch.pid 
--quiet 
-Edefault.path.logs=/var/log/elasticsearch 
-Edefault.path.data=/var/lib/elasticsearch 
-Edefault.path.conf=/etc/elasticsearch


рассмотрим еще раз две настройки которые непонятно из какого файла читаются
-Des.path.home=/usr/share/elasticsearch 
-cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch 

мне кажется что первая настройка
-Des.path.home=/usr/share/elasticsearch 

она прописывается в /etc/systemd/system/multi-user.target.wants/elasticsearch.service
через

Environment=ES_HOME=/usr/share/elasticsearch

видно что не совсем совпадает. но я думаю это оно.

про вторую настройку
-cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch

незнаю где она прописывается.


итого. настройки сервиса эластик ( судя по ps aux ) идут из двух файлов

/etc/elasticsearch/jvm.options
/etc/systemd/system/multi-user.target.wants/elasticsearch.service

это существенно.
также существенно что по факту настройки сервиса эластик это по факту 
настройки для запуска java.

ну а теперь соберем суммарно вообще в каких файлах вообще делаются 
всевозможные настройки эластика

/etc/elasticsearch/jvm.options
/etc/systemd/system/multi-user.target.wants/elasticsearch.service
/etc/elasticsearch/elasticsearch.yml
/etc/default/elasticsearch

далее. 
насколко я понимаю /etc/default/elasticsearch используется для указания настроек
которые должны override настройки в файле /etc/systemd/system/multi-user.target.wants/elasticsearch.service

еще немного скажу про конфигурирование имеенно параметров запуска java
можно делать через параметры вида
-блаблабла

а можно через опцию

ES_JAVA_OPTS="$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir"
./bin/elasticsearch

нахуя она нужна если честно непонятно. ибо исходный способ итак отличный.
в любом случае данная опция в ubuntu указывается в 

/etc/default/elasticsearch


еще замечу такую фишку. что настройки эластика имеют две природы.
первый тип настроек это настройки самой java которая лежит в underline природе эластика
а второй тип настроек это настройки уже самого эластика как проги.

далее. в доке написано что java когда стартует она встроенно имеет такую фичу
что она также ищет опции старта через переменную JAVA_TOOL_OPTIONS.
но! эластик конкретно эту переменную неисполтзует. так как ее юзают 
другие проги на джаве и мы нехотим чтобы настройки старта эластика
пересекались с настройками других джава программ.

так ( 11/6/2019) закончил на этом - https://www.elastic.co/guide/en/elasticsearch/reference/6.5/secure-settings.html


итак судя по живому запущенному сервису эластика

есть весьма важные параметры которые надо бы прописать в конфиге эластика

1)
path:
  logs: /var/log/elasticsearch
  data: /var/lib/elasticsearch


что прикольно что можно указать несколько папок для сохраннеия индексов эластика

path:
  data:
    - /mnt/elasticsearch_1
    - /mnt/elasticsearch_2
    - /mnt/elasticsearch_3

предположим у нас закончилось место

но также важно что отдельный индекс будет храниться в одной папкой а не разобросан по
нескольким


2) имя кластера

cluster.name: logging-prod

нода может присоединиться к кластеру если ее имя совпадает с именем кластера 
на других нодах.

тут я сразу скажу о такой щтуке как - какие ноды достаточно указать в конфиге ноды 
чтобы в итоге все ноды были видны для кластера.

на мастер нодах достаточно указать других мастеров и все.

на нена мастер нодах досттано указать мастеров. итого - на всех нодах 
достаточно указывать только мастеров. тогда кластер успешно срастется.

3) имя ноды

node.name: prod-data-2

или так

node.name: ${HOSTNAME}

важно понимать что имя ноды это чисто для нас юзеров. 
для самого кластера важна node id.

его он хранит где то внутри /var/lib/elasticsarch.

поэтому если мы склонировали ноду. то надо обязательно эту всю папку удалить
иначе при запуске эалсасктика он напишет что есть две ноды с одним node id

4)  очень важно чтобы эластик никоогда  из памяти небыл помещен в свап.
для этого в документации сказано надо в /etc/fstab задизейблить swap

типа нет свапа нет проблем.

это делается чтобы эластик лежащий в памяти не попал в свап
иначе гарьаж коллектор охуенно затормозит свою работу.

(вообще в целом это колхозная
на мой счет насйтрока)

далее дока зачем то говорит что дополнительно надо

 заюзай фичу


elasticsearch.yml

bootstrap.memory_lock: true

тут я сразу говорю что не в доке а в конференции я нашел разьясннеие 
от разрабов эластика что если свап задизейблен на виртуалке
то юзать фичу bootstrap.memory_lock ненужно. в ней уже нет смысла.

далее про подробности фичи.

по факту эта настройка юзер линуксовую функцию mlockall
эта фича линукса заставляет его непомещать процесс в свап. это становится
запрещено для конкретного процесса.

если мы не можем почему то задизейблить  свап - вот 
только тогда надо пользовать фичу bootstrap.memory_lock: true
а если свап мы задизейблили то использовать эту фичу ненужно!

тем неменее

я заюзал фичу и после этого эластик не может стартунть.
выдает ошибку

memory locking requested for elasticsearch process but memory is not locked

оказалось чтобы это вылечить нужно
в 

/etc/systemd/system/elasticsearch.service.d/override.conf

добавить

[Service]
LimitMEMLOCK=infinity

замечу сразу что наебался с неправильным путем.
соблюди правильно путь
/etc/systemd/system/elasticsearch.service.d/override.conf
а я сделал вначале неправильный путь
/etc/systemd/system/multi-user.target.wants/elasticsearch.service.d/override.conf





обмусолим обсудим подоробнее что это как это итп.
значит мы  активировали опцию bootstrap.memory_lock: true 
она пытается заюзать опцию линукса mlockall которая запустит процесс и недаст
ему никогда попасть в свап. но чтобы эта mlackall сработал нужно чтобы для процесса 
котоырый мы запускаем чтобы был задан правильный лимит на количество оперативной
памяти которую для данного процесса можно защищать от попадания в свап.
дада. в линуксе по дефолту для каждого запускаемого процесса есть масса ограничений
которые на него накладываются чтобы типа этот процесс немог убить нахрен всю систему.
в частности для процесса  ограничено количество памяти которые у этого процесса 
неможет попасть в свап. стоп ! я сразу тут поправлюсь. не для процесса 
а для юзера. лимит в линуксе накладыватся на юзера а не на процесс.
 мы запускаем наш сервис под юзером elasticsearch.
так вот лимиты в линуксе указываются не для процессов на самом деле
а для юзеров. мы запускаем эластик под юзером elasticsearch 
мы можем узнать какие лимиты имеет данный юзер если под ним выполним команду


# sudo su elasticsearch -s /bin/bash
$ ulimit -a

core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 27745
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 27745
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

параметр каотоый нас интересует выглядит вот так

max locked memory       (kbytes, -l) unlimited

щас он говорит что количество памяти для юзера elasticsarch которую нельзя помещать в
свап неограничено

ну хорошо лимит посмотрели. а как его задать.
задать через файл /etc/security/limits.conf
elasticsearch    -       memlock         unlimited

либо 

elasticsearch    soft       memlock         unlimited
elasticsearch    hard       memlock         unlimited

- означает soft + hard

что такое soft и hard.  soft это некое дефолтное значение котороые мы задаем. но его 
при желании пользователь можем переопределить ( через командную строку и команду ulimit.
так как с помощью нее нетолько просматривать можно лимиты но и менять на лету).

а hard это ограничение сверху свыше которого юзер неможем зайти даже если захочет.


хорошо. лимит установили. лимит проверили через ulimit -a. но это мы проверили
для юзера. а как нам проверить для процесса лимиты.

1 способ

# cat /proc/13898/limits

2 способ
если мы сервис запустили через systemd то
# systemctl show elasticsearch | grep -i limitmemlock
LimitMEMLOCK=65536
LimitMEMLOCKSoft=65536

и тут мы подходим к самой большой наебке  в линукс с systemd.!!!!!!!
а именно - она игнориует настройки /etc/security/limits.conf

она кладет на них полный стог гумуса ! именно поэтому у нас небудет запускаться
эластик через systemd с опцией bootstrap.memory_lock: true хотя ulimit будет
показывать что лимит стоит бесконечный !!! вот так фигня...

поэтому если мы стартуем эластик через systemd то можно вообще забыть 
парится о насйтроках /etc/securyt/limits.conf ( aka ulimit )

для systemd лимиты надо настраивать в конфигах самой systemd.
тут и вылезает пресловутая настройка

[Service]
LimitMEMLOCK=infinity


а если к примеру мы запускаем эластик через init.d то тогда 
все нормально. 	init.d использует лимиты прописанные в /etc/security/limits.conf

что также странно.
что мы сделали настройку systemd через файл 

/etc/systemd/system/elasticsearch.service.d/override.conf

кстати замечу что при обнолвении версии эластика настройка не будет затерта.

но! если мы попробуем вместо этого вставить настройку в /etc/default/elasticsearch
то это почемуто непрокатывает. почему непонятно.

и вот наконец кстати подтверждение что мы все сдедали правильно из документации
https://www.elastic.co/guide/en/elasticsearch/reference/master/setting-system-settings.html

кстати там написано что если у нас ubuntu  + init.d то тогда 
у нас опять же игнорируются при запуске сервиса лимиты ulimit и нужно делать 
доп манипуляции с /etc/pam.d/su

пиздец... как все запутанно по дебильному. но у нас  не init.d

у нас ubuntu 16 + systemd

что еще прикольно в /etc/default/elasticsearch 
есть такой текст про нашу больную тему

# The maximum number of bytes of memory that may be locked into RAM
# Set to "unlimited" if you use the 'bootstrap.memory_lock: true' option
# in elasticsearch.yml.
# When using Systemd, the LimitMEMLOCK property must be set
# in /usr/lib/systemd/system/elasticsearch.service
#MAX_LOCKED_MEMORY=unlimited

то есть как ни крути. memlock настроаивается только в systemd конфиге. увы..




///////////////////////////////////////////////
отхожу немного в сторону

значит что я еще с удивелением одбнаружил из форума гитхаба эластика.
что если ты поставил эластик из пакета то ( внимание) запускать эластик 
из командной строки уже неподразумевается. ты несможешь.
только через сервис.
а если тебе хочется запускать эластик из командной строки то будь
добр тогда ставить эластикиз tar.gz. тогда можно.
пиздец.

но! потом я сам раскопал как же все таки стартаунуть эластик руками
при условии что мы его поставили из пакета.

для этого надо просто посмотреть как же он запускается в конфиге systemd

# cat /etc/systemd/system/multi-user.target.wants/elasticsearch.service

и мы увидим чтото типа того когда подставим переменные 

$ /usr/share/elasticsearch/bin/elasticsearch \
                                                -p /var/run/elasticsearch/elasticsearch.pid \
                                                -Edefault.path.logs=/var/log/elasticsearch \
                                                -Edefault.path.data=/var/log/elasticsearch \
                                                -Edefault.path.conf=/etc/elasticsearch

ура мы научились запукать эластик из комадной строки !

---

curator

эта такая официальная прога от команды эластика
прежде всего она мне понадоьилась чтбы с помощью нее 
удалять в эластике устаревшие индексы


ставим куратор , 
адрес .deb пакета для эластика версии 5.6.3 можно взять отсюда 
https://www.elastic.co/guide/en/elasticsearch/client/curator/5.6/apt-repository.html

# wget https://packages.elastic.co/curator/5/debian/pool/main/e/elasticsearch-curator/elasticsearch-curator_5.6.0_amd64.deb

# dpkg -i elasticsearch-curator_5.6.0_amd64.deb


создаем конфиг для куратора в

/etc/curator/curator.yml

пример этого файла можно посмотреть здесть 

https://www.elastic.co/guide/en/elasticsearch/client/curator/5.6/configfile.html


я же даю готовый конфиг.

---
# Remember, leave a key empty if there is no value.  None will be a string,
# not a Python "NoneType"
client:
  hosts:
    - 172.16.102.14
  port: 9200
  url_prefix:
  use_ssl: False
  certificate:
  client_cert:
  client_key:
  ssl_no_validate: False
  http_auth:
  timeout: 30
  master_only: False

logging:
  loglevel: INFO
  logfile: /var/log/syslog
  logformat: default
  blacklist: ['elasticsearch', 'urllib3']

как видно логи льются в syslog 
так сделано чтобы дополнительно log-rotate ненастраивать. у сислога то он уже настроен.

конфиг файл нужен чтобы куратор мог успешно подконекотиться к эластику.
тестируем что он это может

# curator_cli --config /etc/curator/curator.yml show_indices

если куратор непоказывает индексы то скорей всего как  у меня было неуказан 
путь к конфигу и куратор просто никуда не конектится.

если все правильно то мы должны увидеть список индексов эластика.


с конектом все в порядке.
теперь еще нужнен один файл. 

/etc/curator/action.yml

который говорит куратору что конкретно мы хотим сделать в с индексами в элатике
пример этого файла можно псмотреть здесь

https://www.elastic.co/guide/en/elasticsearch/client/curator/current/ex_delete_indices.html


даю уже мой файл

---
# Remember, leave a key empty if there is no value.  None will be a string,
# not a Python "NoneType"
#
# Also remember that all examples have 'disable_action' set to True.  If you
# want to use this action as a template, be sure to set this to False after
# copying it.
actions:
  1:
    action: delete_indices
    description: >-
      Delete indices older than 45 days (based on index name), for logstash-
      prefixed indices. Ignore the error if the filter does not result in an
      actionable list of indices (ignore_empty_list) and exit cleanly.
    options:
      ignore_empty_list: True
      disable_action: False
    filters:
    - filtertype: pattern
      kind: prefix
      value: .monitoring-
    - filtertype: age
      source: name
      direction: older
      timestring: '%Y.%m.%d'
      unit: days
      unit_count: 2

он удаляет все индексы которые имеют имя по маске 

.monitoring-*

 и имеют в имени дату старше 2 дня.


вначале запустим куратор в холостом режиме с опцией --dry-run

# curator --config /etc/curator/curator.yml /etc/curator/action.yml --dry-run

тогда в /var/log/syslog
будет написано какие индексы в эластике он собиарется удалить.

теперь можем уже запустить на удаление

# curator --config /etc/curator/curator.yml /etc/curator/action.yml

теперь надо добавить запуск куратора в линукс таск щедулер.

# crontab -e

вставляем

53 1 * * * /usr/bin/curator /etc/curator/action.yml --config /etc/curator/curator.yml >> /var/log/syslog 2>&1


так как мы крон настраивам от имени рута то и куратор будет стартовать от имени рута.
интересно что в самом файле ненужно указывать юзера под которым он стартанет
иначе любой бы лошара мог создать под собой крон задание которые бы запускалось 
от имени рута

каждый день в 01:53 он будет заупуксать куратор и вывод этой проги пихать в 
syslog (и ошибки если они будут)

---
//////////////////////////////////////////
возвращаемся обратно

отключать полностью свап вообще это ебанутая политика.
лучше всеже свап неотключать.
а юзать опцию для эластика

bootstrap.memory_lock: true 

также стаонвоится понятно что лучше при отключенном свапе на ноде кибану и эластик 
вместе назепускать. может нехватить памяти.


далее.что я для себя открыл про /etc/deafult
/etc/default - это доп файлы конфигурации для sysV (init.d) которые служат
в основном для того чтобы задать кастомные параметры запуска сервиса который запускается через 
init.d обычно туда пихаются параметры которые мы хотим чтобы остались в силе
даже после обновления пакета сервиса. то есть скрипт в init.d обновился но нам пофиг
ибо наша кастомная переменная запуска этого сервиса осталась незатронута.
также это значит  что к systemd каталог /etc/default
не имеют никакой силы никакого отношения для конфигурирования!

что я узнал про systemd.
есть дефолтная папка /lib/systemd в  которой лежат конфиги сервисов.
сейчас чаще всего она не используется в вместо нее используется /usr/lib/systemd.
итак либо в одну либо в другую папку падают конфиги при установке из пакета.
пример для пакета эластика

# dpkg-query -L elasticsearch | grep systemd
/usr/lib/systemd/system/elasticsearch.service

важно то что конфиги в этих папках менять нельзя. не положено.
а если мы хотим отредактировать сервис на наш лад для этого есть два пути.
путь первый. скопировать конфиг 
из /usr/lib/systemd/system/elasticsearch.service
в  /etc/systemd/system/multi-user.target.wants/elasticsearch.service

и там менять как нам угодно.  при наличии файла во второй папке файл из первой
папки нечитается. плюс состоит в том что обновление пакета никак неповлияет 
на наш кастомный конфиг. его настройки небудут перетерты. ура!

но  у этого способа есть минус. во первых неочевидно чтоже мы там наменяли 
по сравнению с исходным файлом. во вторых, когда мы обновим пакет 
то новые фичи прописанные в обновленном /usr/lib/systemd/system/elasticsearch.service
небудут задействованы.

поэтому есть еще второй способ отредактировать конфиг сервиса.
создать файл в папке 
/etc/systemd/system/elasticsearch.service.d/vasya.conf

и в нем мы указываем ТОЛЬКО те параметры конфига которые хотим поменять или добавить.
то есть всю портянку конфига переписывать ненужно.

пример
[Service]
LimitMEMLOCK=infinity

у этого способа есть два плюса.
во первых четко видно что конкретно мы внесли нового.
во вторых при обновлении пакета эластика его обновленный конфиг /usr/lib/systemd/system/elasticsearch.service с новыми модными фичами будет задействован.
очено хорошо !

из этого обьяснения становится очень хорошо понятно 
почему настройка LimitMEMLOCK=infinity не имела никакой силы когда 
мы ее вносили в /etc/default/elasticsearch - потому что этот конфиг никакого отношения к
systemd неимеет!

также становится хорошо понятно что внесение данной настройки в файл 
/etc/systemd/system/multi-user.target.wants/elasticsearch.service
это мы используем первый способ
а внесение насторойки в файл /etc/systemd/system/elasticsearch.service.d/vasya.conf
это мы вносим настройку вторым способом.
очень хорошо теперь понятно!


далее. в systemd есть параметры которые могут иметь сразу несколько значений (хз что это значит)
но важно тут другое. что чтобы его переписать надо его вначале занулить. 
пример. меняем параметр ExecStart.

если написать в /etc/systemd/system/elasticsearch.service.d/vasya.conf
[Service]
ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}

то systemd выдаст ошибку при рестарте сервиса.
а правильно будет вот так :

если написать в /etc/systemd/system/elasticsearch.service.d/vasya.conf
[Service]
ExecStart=
ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}


то есть его вначале занулить а потом уже указать новое значение!

далее. я узнал что systemd имеет свой лог файл. как я понял под логом 
имеется ввиду что сиранул сервис при старте в стандартный output. то есть
грубо говоря чтобы мы увидели на экране если бы стартовали сервис руками 
из командной строки. это значит что  в данном логе применительно к эластику
небудет логов которые эластик пишет в /var/log/elasticsearch/name.log
они как были так там и будут. здесь имеется еще раз подчеркну в виду 
только та информация которую сервис кидает на стандартный output
то бишь грубо говоря то что выдает сервис на экран при старте.
неболее того.
далее лог этот он нетекстовый он бинарный.
чтобы его посмотреть надо юзать команду

#journalctl

а чтобы в этом журнале посмотреть логи от конкретного юнита ( сервиса ) то надо юзать

# journalctl --unit elasticsearch

в доке от эластика сказано что по дефолту эластик молчит на экран при старте
(https://www.elastic.co/guide/en/elasticsearch/reference/6.5/deb.html)
а если мы хотим чтобы он не молчал то надо убрать опцию --quiet
она прописана в конфиге systemd в опции ExecStart

# cat /usr/lib/systemd/system/elasticsearch.service
...
[Service]

ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                --quiet
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}

а соотвесвтенно чтобы --quiet отключить надо написать конфиг

# cat /etc/systemd/system/elasticsearch.service.d/override.conf
[Service]
ExecStart=
ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                -p ${PID_DIR}/elasticsearch.pid \
                                                -Edefault.path.logs=${LOG_DIR} \
                                                -Edefault.path.data=${DATA_DIR} \
                                                -Edefault.path.conf=${CONF_DIR}

я это сделал. но ничего существенного в логирование старта это недало.
вот что я вижу при старте эластика в systemd журнале

# journalctl --unit elasticsearch
Nov 04 00:49:05 test-es2 systemd[1]: Starting Elasticsearch...
Nov 04 00:49:05 test-es2 systemd[1]: Started Elasticsearch.


то есть буквально нихрена полезного.
двигаем дальше.


закончил доку примерно на этом

https://www.elastic.co/guide/en/elasticsearch/reference/6.5/deb.html

а в итоге надо дойти до этого
https://www.elastic.co/guide/en/elasticsearch/reference/master/setup-configuration-memory.html



