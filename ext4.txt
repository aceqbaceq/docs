ext4
https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout

создадим файл и сделаем из него блочное устройство

первый шаг
$ dd if=/dev/zero of=./01.block bs=2k count=100000


на следущем шаге используем losetup
прогармма позвяолет взять файл и организовать к нему доступ как к блочному утсройству
через /dev/loopX файл.

поговорим о флагах команды ибо они описаны дебильно.




losetup без флагов или с флагом -a показывает инфо о том какие /dev/looop заняты
щас и на какие файлы они привязаны. 


$ sudo losetup
NAME        SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop9          0      0         1  1 /var/lib/snapd/snaps/gnome-characters_741.snap
/dev/loop7          0      0         0  0 /root/block-devices/01.block

siezelimit и offset это хрень какая то потому что они задают то что: 
sizelimit ограничивает доступ внутри файла до какогто предела. зачем это надо?
если мы завели файл то хотим там лазить везде внутри
offset задает что доступ начинается не с нулевого байта файла а с какогото 
отступа. тоже нахер это надо?
RO - рид онли доступ. ну понятяно

вобщем в итоге нас интерусет только NAME и BACK-FILE
$ sudo losetup
NAME         RO BACK-FILE
/dev/loop9   1 /var/lib/snapd/snaps/gnome-characters_741.snap
/dev/loop7   0  0 /root/block-devices/01.block

итак losetup позволяет иметь доступ к бекенд файлу как блочному устройству через
/dev/loop9 спецфайл. тоесть имели файл получили блочное устройство в котором файл 
это бекенд.






ключ -f (--find) = описание у ключа полностью дебильное. если других ключей в добавок к ключу -f неиспользуется в команде то ключ -f показывает свободный /dev/loopX на данный момент

$ losetup  -f
/dev/loop15

но более того (и об этом несказано в мануале) это то что если свободного loop нет то в ответ
на эту команду система создает +1 новый свободный loop. правда и тут не без ососбенностей
это произодйет олько если мы запусккаем losetup -f с правами рута. показываю:

$ losetup -f
losetup: cannot find an unused loop device: Permission denied
$ sudo losetup -f
/dev/loop28

тоесть в системе небыло свободных loop. я заустил losetup -f без прав рута и он об этом 
просто написал. а вот когда запустил под рутом то система в ответ создала новый свобдрный loop

у этого ключа дебильная расшифровка я продолжаю об этом,  по своему смыслу этот ключ совсем не "find" как написано в мануале а более похож по смыслу на "loop file"
потому что когда мы создаем связь между бекенд файлом и /dev/loopX то ключ -f используется
для того чтобы указать какой /dev/loopX мы хотим использовать. так что -f обозначает /dev/loopX файл. если мы неукажем конкетный /dev/loopX файл то система автоматом подставит первый свободный. тоесть

$ sudo losetup -f  ~/01.img   
эта команда найдет свободный /dev/loopX и привяжет его к файлу 01.img

хотя можно как я сказал указать /dev/loopX руками
$ sudo losetup -f /dev/loop12  ~/01.img

таким образом видно что -f указывает имя /dev/loop файла. так что какой нахер find ? -f по своему смыслу это никакой нахер не --find а  file по своему смыслу.

еще раз скажу что указывать руками -f /dev/loop12 нет смысла. во первых потому что этот /dev/loop12 уже 
должен быть в системе более того  надо проверять свободен ли он.

также еще важный момент скажу. если у нас в системе исчерапались все доступные loop устройства
то при вызове losetup -f с правами рута система создаст +1 свободный loop. отсюда ответ 
на вопрос откуда беретуся новые свободные loop-ы.
пример я щас заюзаю последний свободный loop27: 

$ losetup -f
/dev/loop27

создаем бекенд файл 
$ touch ~/block-devices/01.img

привязываем 01.img к блочному loop27
$ sudo losetup /home/vasya/block-devices/01.img  -f /dev/loop27

таикм образом теперь доступ к 01.img можно делать как к блочному устройству чеерез /dev/loop27
В данном случае я указал loop руками но можно было сделать и по другому без ручного указания:

$ sudo losetup /home/vasya/block-devices/01.img  -f

далее я проверяю появился ли новый loop.
$ losetup -f
losetup: cannot find an unused loop device: Permission denied

как видно непоявился.


но  далее я запуска команду  под рутом и система создает новый!
$ sudo losetup -f
/dev/loop28

таким обоазом я резюмирую что ключ -f дает и делает.
1. он показывает какой свободный loop есть (при запуске не под рутом)
$ losetup -f
/dev/loop28

2. если свободного loop нет но мы запустили под рутом то система создаст новый свободный 
loop
$ sudo losetup -f
/dev/loop28


3. указввает loop который использовать при создании связи между бекенд файлом и loop файлом
	$ sudo losetup /home/vasya/block-devices/01.img  -f /dev/loop27
если после -f неуказан loop то система сама найдет свободный и подставит.  я даже
поозреваю что если свободного неокажется то система создаст новый loop автоматом
тоесть вот такая команда тоже работает
	$ sudo losetup /home/vasya/block-devices/01.img  -f
причем я считаю что это и есть более практически удобная команда


Итак создаем блочное устройство из файла 01.block
# losetup --sector-size 4k /home/vasya/block-devices/01.block -f

проверяем:
$ losetup | grep -E "OFFSET|01.block"
NAME         RO BACK-FILE                                          DIO LOG-SEC
/dev/loop28   0 /home/vasya/block-devices/01.block                   0    4096


LOG-SEC это logical sector size это сектор 4к который мы выставили ранее.
что такое logical sector и чем он отличается от physical sector мы поговорим ниже.



создаем файловую систему ext4 с дефолтовыми настройками:

$ sudo mkfs.ext4 /dev/loop28
mke2fs 1.44.1 (24-Mar-2018)
Discarding device blocks: done                            
Creating filesystem with 50000 4k blocks and 50048 inodes
Filesystem UUID: d70e8847-c2a9-4ca5-9d03-a42fb6c454a9
Superblock backups stored on blocks: 
	32768

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done


теперь начинаем изучать ext4.

теперь посморим что про фс напишет утилита:
$ sudo tune2fs -l /dev/loop28
tune2fs 1.44.1 (24-Mar-2018)
Filesystem volume name:   <none>
Last mounted on:          <not available>
Filesystem UUID:          d70e8847-c2a9-4ca5-9d03-a42fb6c454a9
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file dir_nlink extra_isize metadata_csum
Filesystem flags:         signed_directory_hash 
Default mount options:    user_xattr acl
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              50048
Block count:              50000
Reserved block count:     2500
Free blocks:              44278
Free inodes:              50037
First block:              0
Block size:               4096
Fragment size:            4096
Group descriptor size:    64
Reserved GDT blocks:      24
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         25024
Inode blocks per group:   782
Flex block group size:    16
Filesystem created:       Thu Jul 14 20:17:44 2022
Last mount time:          n/a
Last write time:          Thu Jul 14 20:17:44 2022
Mount count:              0
Maximum mount count:      -1
Last checked:             Thu Jul 14 20:17:44 2022
Check interval:           0 (<none>)
Lifetime writes:          16 MB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:	          128
Journal inode:            8
Default directory hash:   half_md4
Directory Hash Seed:      785a7c2b-e142-432b-b4d9-7ae4b3399411
Journal backup:           inode blocks
Checksum type:            crc32c
Checksum:                 0x97a880be

пока двигаем дальше.

ФС ее драйвер делит диск на blocks. это не сектора диска нет. это логическая структура 
с точки зрения драйвера фс. насколко я понимаю это минимальная стркуткура которую можно
прочитать и записать на фс через драйвер. но я пока непонимаю это внутренняя кухня
только для драйвера фс или эти блоки они както наружу выставляются для сторонних внешних
потребителей. пока я этого не понимаю. пока двигаем дальше.

блоки обьединяются в block group. ее размер опредеяется через 
	sb.s_blocks_per_group
либо как 
	8 * block_size_in_bytes

All fields in ext4 are written to disk in little-endian order. HOWEVER, all fields in jbd2 (the journal) are written to disk in big-endian order.

немного поговорим про рейды. что такое stripe size и chunk size.
у IBM нашел такое определение - A stripe, which can also be referred to as a redundancy unit, is the smallest amount of data that can be addressed. сокращаю их выражение до:
 A stripe is the smallest amount of data that can be addressed


что такое chunk size. важно отметить что этот термин
имеет смысл ТОЛЬКО ДЛЯ ОПРЕДЕЛЕННЫХ ТИПОВ RAID А НЕ ДЛЯ ВСЕХ ПОДРЯД.
из man mdadm я читаю:
 -c, --chunk=
              Specify  chunk  size of kilobytes.  The default when creating an array is 512KB.  To ensure compatibility with earlier versions, the default when building an array with no persistent metadata is 64KB.  This is only meaningful for RAID0, RAID4, RAID5, RAID6, and RAID10.
              RAID4, RAID5, RAID6, and RAID10 require the chunk size to be a power of 2.  In any case it must be a multiple of 4KB.
по своей сути chunk означает сколько байт будет записано на отдельный диск если мы на весь 
массив послали X KB данных за 1 IO. читаю у орейли - An array's chunk-size defines the smallest amount of data per write operation that should be written to each individual disk.



- о чем это ?
cat /sys/block/loop0/queue/physical_block_size 
cat /sys/block/loop0/queue/logical_block_size

во первых эти хрени это результат того что firmware диска доакладывает драйверу.
тоесть это незадается из линукса это типа как бы свойства самой железки 
которые она сообщает о себе. якобы hdparm может просить firmware железки чтобы
она поменяла свой logical_block_size и иногда она меняет.

тееперь о значении этих хреней
physical_block_size = походу это типа каким блоками информация сохраняется
	внутри железки по факту.

logical_block_size = это какими минимальными блоками железка принимает от драйвера информацию на запись.(вроде бы оттуда же LBA - logical block addressing произошло название). тоесть 
я так понимаю что вот для драйвера железка сообщает что у нее есть 1 000 000 LBA блоков размером по 512 байт. соотвесвтенно драйвер может подать на запись команду вида - запиши 
ка в 15-ый блок LBA 512 байт такого то вида. тоесть идея в чем что для драйвера диск 
выглядит как коробка у которой два параметра - число блоков (LBA) и размер блока. 
и драйвер может либо читать из блока либо писать в блок. соотвественно можно либо 
целиком прочитать блок либо целиком записать в блок. частичная операция диском не поддерживатеся поэтому драйверу надо знать размер блока обязательно. 
 иттогда операция чтения со стороны драйвера выглядит как:
 READ блок 15 размер 512 байт
 WRITE блок 17 размер 512 байт.

 если же ядро хочет записать 1 байт то ему нужно с помощью драйвера 
 1. считать блок LBA 12 размером 512 байт с помозью драйвера
 2. поменять один байт в RAM в этом куске 512байтном 
 3. записать обартно 512 байт (модифицированных ) с помозью драйвера в LBA 12


 так вот 512 байт это logical_block_size.

 еще раз драйверу кгда он хочет считать инфо с диска ему нужно передать какой номер блока
 драйвер хочет чтобы диск считал. я незнаю передает ли драйвер в диск размер сектора
 или просто драйвер вслпую получает байты от диска пока он незактнется  и тогда важно
 чтобы размер буфера чтения оказался ни больше ни меньше. но я думаю что точно что при записи на диск нужно указать номер блока и нужно чтобы размер данных которые мы пихаем на диск 
 был в точности совпадал с размером сектора потому что если мы попробуем сунуть в сектор инфомрации болше чем сектор по размеру то диск выдаст ошибку. вот почеу важен размер сектора.
 и вот этот вот размер сектора который мы получаем при чтении сдиска указав его номер 
 и размер который можно запихать в диск указав номер блока как раз равен logical_block_size
 который диск сообзает драйверу. тоесть если мы заказли чтение с диска lBA блока 12 то 
 мы можем смоело ожидаь что диск на передаст ровно logical_block_size байт.
 а если мы хотим запистьа на диск в блок 12 LBA то диск ждет от нас logical_block_size байт.
 вот каков физический смысл logical_block_size.

 а смысл physical_block_size состоит в том что таков размер блока записи на диске внутри 
 по факту. он может неосовпата с logical_block_size. к чему это приводит. если унас 
 physcal_block_sie=4K а logical_block_size=512байт то когда мы пишем на диск 512 байт
 то по факту диск внутри себя считывает 4к модифицирует его и записывает обратно модифицироыванный 4к.  что конечно же ведет к сумашедшему потере скорости. 
 логично предполжить что если мы заказали запись восьми блоков LBA по 512 байт то 
 диск понимает что весь его физичесмы кий блок 4к нужно  переписать поэтому читать уже его ненужно его можно сразу перезапиывать. и потери скрорости нет вообще!
 поэтмоу что наам дает знание  physical_block_size. поменять мы его неомжем но мы можем
 на файловой системе сделать размер блока равынй или кратный physical_block_size.
 что это дает - мы в программе в линуксе даем команду 

  write(1, const void *buf, 12КБ);

запускается ядро и сисколл берет наши 12КБ данных и передает драйверу ФС. 
драйвер ФС разбивает нащи 12Кб на куски по 4КБ(размер блока на ФС) насколько я пнинимаю
далее дайрвер фс делает вот что. он переводит каждый кусок 4КБ в блоки LBA и передает
драйвер диска типа такой команды - мол блоки LBA c 1 по 8 (суммарно 4КБ) надо записать кучкой
далее мол блоки с 9 по 16(4КБ) тоже надо записать кучкой. драйвер диска его специифика то что
он знает какие конктенотно сигналы надо послать в диск чтобы передать ему что  LBA 1-8 надо
записать кучкой. это приводит  к тому что диск видит что надо записать 8 втртуальных блоков LBA фактически которые укладываются в один физический блок и диск понимаает что это перепывает
весь физ блок целиком и он это делает и сокрость нетеряется. вместо того чтобы прочитать блок 
поменять на наем первый байт записать обратно . потом считать заново поменять второй баайт
записать заноово. имеем  непотерю скоорости. 

в чем разница если бы на ФС были бы блоки по 512байт ( тоесть мы есл бы незнали что физ блок 4К а думали что он 512байт). было бы то что мы дали команду write(1, const void *buf, 12КБ);
она долетела до ФС. она взяла и 12КБ разделила по 512байтовым LBA кусочкам. и далее фс
будет требовать от драйвера диска чтобы он просто записал LBA блоки в любом порядке.
и  в этом то жопа потому что тогда как раз может получатьс ситцаия на диске что он 
читает 4к сектор. меняет в нем один или два байта. пишет обратно. потому опять читает
меняет один два байта и пишет обратно. огромная потеря сокрости.  вслучае е когда  у нас на фс 4Кб блоки это приводит к тому что у нас на ываходе тоже куча полукиолбтаный LBA блоков но
фм будет требовать от дравера диска чтобы группа LBA от 1 по 8 была записана в одном порыве.
или друг за другом. и значит диск небует читать 4к блок а сразу будет его переписывать. в этом 
и есть смысл нам знать о внутрнеейней размере физ диска на диске и на оснвое этого задавать 
размер блока на фс. иначе я не вижу смысла в блоках ФС.

насколько я понимаю про страйп на рейде. это минимальная логическая структура размером которой
он читает и пишет. если она скажем 64КБ это значит что если мы задаем запись 1КБ то он прочитает с жиска кусок 64Кб изменит его и потом запишет обратно. тоест идет чтение+запись 
вмест записи. 


- у нас рейд и страйп size =64k значит ли этом что при запитсь 1к у нас всегда будет
вначале чтние и только потом запись. то есть бещеное падание скоокрсти.

на сайте редхата читаю: 
	Storage vendors can also supply "I/O hints" about a device's preferred
minimum unit for random I/O ('minimum_io_size') and streaming I/O
('optimal_io_size').  For example, these hints may correspond to a RAID
device's chunk size and stripe size respectively.


что такое direct i\o = Direct I/O is a feature of the file system whereby file reads and writes go directly from the applications to the storage device, bypassing the operating system read and write caches. Direct I/O is used only by applications (such as databases) that manage their own caches.
An application invokes direct I/O by opening a file with the O_DIRECT flag.
дествииельно читаю man 2 open:
O_DIRECT (since Linux 2.4.10)
              Try to minimize cache effects of the I/O to and from this file.  In general this will degrade performance, but it is useful in special situations, such  as  when  applica‐
              tions  do  their own caching.  File I/O is done directly to/from user-space buffers.  The O_DIRECT flag on its own makes an effort to transfer data synchronously, but does
              not give the guarantees of the O_SYNC flag that data and necessary metadata are transferred.  To guarantee synchronous I/O, O_SYNC must be used in  addition  to  O_DIRECT.
              See NOTES below for further discussion.
еще таеаы добавка идет:
When a file is opened with O_DIRECT, or when a GFS direct I/O attribute is attached to a file, all I/O operations must be done in block-size multiples of 512 bytes. The memory being read from or written to must also be 512-byte aligned.

кстати посмотреть список scsi устрйоств:
$ apt-get install lsscsi

пакет чтобы посылать команы на scsi устройства:
$ sudo apt-get -y install sg3-utils

здесь нащел https://people.redhat.com/msnitzer/docs/io-limits.txt :
Linux filesystems are not allowed to be
formatted to use a block size that is smaller than the underlying
storage's 'logical_block_size'.

в доке от ibm нашел:
logical_block_size Smallest possible unit in bytes that is addressable in a request.
physical_block_size Smallest unit in bytes handled without read-modify-write.

тоесть logical_block_device это с каким размером сектора диск себя презентует в user space.
как он видится для юзер программ.

physuica_block_size это его размер сектора изнутри внутри. если совать в диск данные размером
меньше чем physuica_block_size то диску придется вначале считать physuica_block_size изменить его а потом только заисать обратно. что ведет к беешеной перфоманс деградации.

Тут я напоролся на тему linux kernel i\o scheduler:
Simply sending out requests to the block devices in the order that the kernel issues them, as soon as it issues them, results in awful performance. One of the slowest operations in a modern computer is disk seeks. Each seekpositioning the hard disk's head at the location of a specific blocktakes many milliseconds. Minimizing seeks is absolutely crucial to the system's performance.

Therefore, the kernel does not issue block I/O requests to the disk in the order they are received or as soon as they are received. Instead, it performs operations called merging and sorting to greatly improve the performance of the system as a whole[2]. The subsystem of the kernel that performs these operations is called the I/O scheduler.

io scheduler называт еще elevator.
потмоу что его задача ( по крайней мере для шппненльных дисков) в тмо чтобы 
головка шла ровно и нескакала постоянно.

на праатике есть дефолтовый шедулер в системе. как его поменять.
надо в grub примненмть опцию elevator

/boot/grub/menu.lst :

title CentOS (2.6.18-128.4.1.el5)
root (hd0,0)
kernel /vmlinuz-2.6.18-128.4.1.el5 ro root=/dev/VolGroup00/LogVol00 elevator=noop
initrd /initrd-2.6.18-128.4.1.el5.img

это по деолфту выставит для всех блочных устройств io шедулер = noop

провеоить прям щас какой шедулер работает на кокнктеном блочном устрйостве можно так:
$ cat /sys/block/nvme0n1/queue/scheduler 
[none] mq-deadline 
 

его на лету можно переклчить. через 
echo mq-deadline > /sys/block/disk/queue/scheduler

если хотим для оотдельного диска кастомный и чтобы после перегазуруки осталось 
перманентно то надо заюзать правила для udev.

vmware пишет что для линуксов втруталок надо бы ставить none шедулер потому что 
в конечном итоге каждля виртуалка незнает о других вируалках поэтому надо оставит 
задачу реэрейнждмента порядка счтывания LBA блоков гипервизору. тоесть надо на виртуалках
проставить шедулер = noop

раньше до появления nvme дисков да и вообще в эру шпиндельных дисков
были такие шедулеры:
noop
Completely Fair Queuing (cfq)
Anticipatory
Deadline

потом появился еще bfq

основная  цель оптимизации у шедулеров была уменьшить прыганье головки на шпинеделях.
а когда пошли ssd то это все отпало. 
на данный момент для nvme считается самым отличным шедуелером это none шедулер.  тоесть
в устроство подается вобщмто просто поток запросов а вся оптиамизация переклдывадется на 
сам nvme диск.

- закончил на том что есть старый io шедулер. у него там разные есть элеавторы.
но их общая фигня в том что в конечном итоге реквесты от всех процессов к блочному устройству
как я понял обрабатываться в рамках одной очереди. и эта одна очередь обрабабтыватеся
всего одиним ядром цпу. поэтому скорость одного ядра цпу это ровно то место узкое
которое мещает выжмать иопсы. поэтому был придуман новый шедулер механизм. 
в котором во первых очередь разбивается на две части. в первой части находтся софт очереди
во вторйо части хард очереди. софт очередй много столько скольк ядер. в этих очередях
происходит перегруппирова запросов и так далее.  уаждого ядра есть своя софт очередь доя блочного устрйоства. тоеть если 10 ядер то для конкретногого блочного /dev/nvme у нас будет 10 софт очередей. если процесс сидит на ядре 1 и он на этом ядре вызывал read() и далее 
цпу перешел в режим ядра то этот реквест будет попмщен в софт очередь этого цпу. и ответ
от железкт придет именно на это цпу. окей получаетя запросы к /dev/sda от процессовна разных
цпу начали обрабаывться на разных цпу (распараллелились). эта вся хрень с очередями 
работает на уровне block layer в ядре. ина данном этапе мы никак не связаны с конкретикой
драйвера /dev/sda. вся мудота с группировой перемшиванием ркевестов в очереди идет именно
в софт очереди. я неочень понимаю как долго работает block layer по времени. пока я вижу это так. процесс на цпу запустил read() если еще тайм слайст незакончился то цпу переходит в режим 
ядра и запускает сисколл то выполняет код ядра с очередями. и в какойто момент происхоит
таймер интеррапт запускается процесс шедулер. и он выдавливает block layer код с цпу и запускае другой процесс. потом опять доходит очередь до этого проецесса и код ядра block layer наконец заканчивает там манипулиции с софт очередью и далее видимо первый с начала запрос 
из софт очереи передается в хард очередь если в ней нет места то там есть промежуточная 
хрень для храннеия. хард очередь это очереь в которой уже непроисходит никаких перестановок
запросов  в ней. она чисто я так понял FIFO. и из нее запросы уже передаются в драйвер /dev/sda
на исполнение. число хард очередей равно числу очередей которые поддеживает железка.
и тут мне непонятно то что какой цпу обратвает хард очередь конкретную. если софт очередл 
каждя четко привязана к кокнкретному цпу то как выбирается цпу для обработки для продивджения реквестов по хард очереди. ведь очереь это просто кусок памяти. а чтобы она работал нужен цпу
которй будет по ней двигать реквесты. вот это непооятно. а так запрос из хард очереди передается в low level драйвер /dev/sda и тот уже как то там конкретно пихает запрос в железку
все это время тайм слайс может закончится и шедулеор выдавить этот код с цпу. потом обратно 
засунет. а ответ в систему прилетает уже вот как - когда железка прочитала сектора то она
гененирует интеррапт и доставку от железки до ядра куска инфомрации уже идет по интерапту.через оарботчик интерапта. щас для меня все вынгляди так что если процесс 
однотредовый то скорость иопсов для него все равно зависит от скорости одного ядра цпу.
хотя конечно процесс шедулер может случайно перемещать процес между цпу и он успееет насовать
свои рекветы во всех софт очереди на всех цпу. тогда да мы неупелис в одно ядро. но 
по прежнему неонятнен вопрос какие цпу обслуживают хард очереди. 

двигаем дальше. я понял что при запросе на чтение\запись read(), write() можно исполовать
флаu O_DIRECT (называется Direct I\O операция). он дает то что мы говорим ядру что мы нехотим использовать page cache а мы 
хотим читать и писать сразу на железку.

вот картинка про i\o систему в ядре. = https://www.thomas-krenn.com/de/wikiDE/images/e/e0/Linux-storage-stack-diagram_v4.10.png

ищем там I\O scheduler и blmk.
I\O shceduler это старая версия i\o шедулера а blmk это новая.
на схеме мало что понятно но видно что у старого шедулера была одна штука hardware 
dispatch queue а у нового blmk их несколько. hardware dispatch queue это очередь 
запросов на чтение запиьс которая идет от block layer(общий абстрактный уровень работающий с очердями на запросы ) к драйверу кокретного блочного утсройства /dev/sda.
так вот эту очередь  hardware dispatch queue в итоге обслуживает некоторый кокретный цпу
так вот когда раньше она была одна ( персональная для кажого блочного устрйоства)
то как я понимаю поскоьку продивжением запросов по ней и передачей запросов из нее в 
драйвер блочного устройства занимался один цпу и поэтому скорость работы на блочном устройстве
упиралась в скорость одного ядра цпу (при этом непонятно а что сам драйвер блочного устройства
он что неупирался в ядро одного цпу?) а с новым blmk таких очередей hardware 
dispatch queue уже несколько (их число зависит от числа очередей поддерживаемых железкой)
и поэтому как бы теперь число iops которые мы снимаем с железки неупирается в скокрость одного
ядра цпу.(при этом непонятно а что сам драйвер блочного устройства
он что неупирался в ядро одного цпу?)
том кренн пишет - Blk-mq allows for over 15 million IOPS with high-performance flash devices (e.g. PCIe SSDs) on 8-socket servers
Вот еще картинка про blmq - https://www.thomas-krenn.com/en/wiki/File:Blkmq-two-level-Linux-block-layer-design.png

насколко я примерно понял запрос от проги read() поступает в ядро в vfs. 
она обращается  к драйверу конкретной File system оно делает обращение в block layer коду
а тот к драйверу конкртеного блочного устройства /dev/sda (ествственно это все внутри ядра
и там нет никакого /dev/sda ведь это точка входа к блочному утсройству для юзерских программ.)

тут я еще чтото хочу сказать. есть две проблемы. одна это собрать все запросы на чтение\запись 
от всех процессов перегруппироват их и сунуть в блочное устройство. большая работа и большая 
мудота  при этом в том чтобы все это правильно перегруппировать. 
вторая часть работы это обратно доставить к процессу данные которые прилетели наконец
от диска. это две разные проблемы. обратная доставка вроде проще. надо просто взять то
что поступило и сунуть в процесс. значит про первую про первую часть проблемы. как я понял 
со старым планировщиком было так что была одна софт очередь и одна хард очередь на весь комп ( для одного блочного устройства. у каждого блочного устройства своя инивиудальная 
софт+хард очередь. мы щас рассматриваем отдельное блочное устройство и проблему высокосростного доступа к нему). так вот если на пяти цпу крутится пять процессов и каджый из них щас хочет прочитать из одного /dev/sda то каждый из них переходит в режим ядра и пять экземпляров кусков ядра хотят получит доступ к одной и тойже стркутуре в памяти ядра софт очереди и хард очереди. эта структура защищена спинлоком то есть в однин момент времени 
только с одного цпу один кусок кода ядра может с ней рабоатть. незнаю насколко это проблема большая но это проблема. в новом планирщвике каждый код ядра на каждом цпу имеет свою индфивидуаьную софт очередь. ну хорошо. и есть несколько ( в зависимости сколько прддеживает железка /de/sda) хард очередей.это безусловно позволяет более бодро принимать от процессов
запросы IO к /dev/sda. ну хорошо. а драйвер /dev/sda он позвояляет запускаться в несколльких 
экземплярах чтобы бодро принимать увеличивешееся количество запросов с нескольких хард очередей? ведь как это походу работает по мне: процесс на цпу юзерский запускает read (/dev/sda) цпу переклюается в режим ядра. там код ядра пихает запрос в софт очередь ( тут проблем и пересечений с другими процессами и ядрами цпу нет). далее код ядра беррет чтото из софт очереди
и пытается впихнуть в хард очередь . и тут уже есть проелма так как число хард очередей может 
быьт меньше чем число цпу. тоесть там должен стоять спин лок. итак мы тут уже ждем. потом 
наконец мы получили доступ к хард очереди . пихнули туда запрос. далее как я понимаю мы запускаем уже драйвер /dev/sda и из хард очереди он забирает кусок и пихает на железку.
так вот в этот момент надругом цпу может быть ровно такая же ситуация и вопрос когда на первом
цпу запущен драйвер /dev/sda то на дургом цпу может ли быть запущена +1 копия драйвера 
/dev/sda или надо ждать?  если надо ждлат то тогда эта новая модель шедулера всего навсего
нам дала доп буфер в виде софт очередей ( по одному на цпу) и только то а в конечной точке
у нас попрежнему узкое место. непонятно. ну и опять же на старой модели шедулера. ну была у нас одна софт и хард очередь на весь комп ну и что. пока на одном цпу пихает запросы в очередь
на других цпу код ядра неможет этого сделать. типа он ждет ну и что. дело в том что 
даже если код ядра весело запихал на всех цпу запросы в очередь все равно юзерское приложение
спит пока неполучит ответ с диска. непонятно. непонятно также то что очереди конечно 
же предназначены чтобы двигать запросы от приложений к диску. а как доставляются ответы от диска к процессам ? совершенно понятно что когда диск нашел ответ  на запрос он запускает 
интеррапт. выывается интеррапт обработчик. он берет этот ответ от диска и тут вопрос
как обрботчик знает какие куски ответа к какому процессу надо доставить? это вобще непонятно.
также еще вот что непонятно. вот нащ интерапт хендлер забрал ответ от диска и сунул пока в
 память ядра и начал разбраться какому процессу какие байты сунуть. пока он это делает 
 может ли диск выставиь еще интеррапт чтобы его на другом цпу обработали или нет? если нет 
 то получается что ответы от диска завязаны на скости работы одного ядра!
 так что одни вопросы. ответов в инете нет. 
но так в целом они прям усираются о том что раньше мол была одна софт очередь и одна хард 
очередь а теперь их несколько и это мол чудо. мол это снимает ботлнек как бутто там других
ботлнеков нет в этой цепи от read() до запроса в диск.
так вот непонятно вот когда мы в iometer задавали скажем  число потоков в 10.
то ведь они же все равно все уходили в одну очередь на старом шедулере поэтому непонятно
как же эти потоки в юзерской программе трансформировались в потоки к диску ведь в диск
уходил все равно один поток который сумма просто напросто этих десяти потоков.
непонятно.
но они щас на что напирают мол раньше были шпиндели поэтому мол посылать на диск
следущий реквест когда он невернул ответ по текущему реквесту типа небыло смысла ( не знаю
неуверен в верности этого заявления) поэтому старый шедулер имел только один поток ведущий
в драйвер. а щас мол так как у нас ssd\nvme то они могу спокойно искать одновременно 
неоколько независимых реквестов.

очереди зовутся как software queues и hardware queues( синонимы hardware contexts queues\hardware dispatch queues).

далее я щас пытаюсь выяснить сколько hardware queues поддерживает мой текущий 
nvme диск.

$ sudo nvme smart-log  /dev/nvme0n1 
Smart Log for NVME device:nvme0n1 namespace-id:ffffffff
critical_warning                    : 0
temperature                         : 33 C
available_spare                     : 100%
available_spare_threshold           : 10%
percentage_used                     : 1%
data_units_read                     : 10 169 142
data_units_written                  : 14 999 997
host_read_commands                  : 134 639 775
host_write_commands                 : 340 324 085
controller_busy_time                : 399
power_cycles                        : 1 609
power_on_hours                      : 2 318
unsafe_shutdowns                    : 31
media_errors                        : 0
num_err_log_entries                 : 0
Warning Temperature Time            : 0
Critical Composite Temperature Time : 0
Thermal Management T1 Trans Count   : 0
Thermal Management T2 Trans Count   : 0
Thermal Management T1 Total Time    : 0
Thermal Management T2 Total Time    : 0

посмотреть какие nvme неймспейсы есть на диске (это не линукс неймпесейсы а чисто хрень nvme)
$ sudo nvme list-ns  /dev/nvme0n1 
[   0]:0x1

видим что есть один неймспейс.

а теперь более интересная хрень.
выяснить какой размер LBA сектора nvme диск экспортирует наружу в ядро:

$ sudo nvme id-ns  /dev/nvme0n1  -n 1 -H  | grep "LBA Format" | grep Metadata
LBA Format  0 : Metadata Size: 0   bytes - Data Size: 512 bytes - Relative Performance: 0x2 Good (in use)
LBA Format  1 : Metadata Size: 0   bytes - Data Size: 4096 bytes - Relative Performance: 0x1 Better 

отсюда видно что для линукса диск сообщает что у него как бы размер сектора LBA равен 512 байт.
но лучше бы нам ппреключить железку (правда при этом все данные сдохнут) чтобы она стала 
для внешнего мира как устрйоства с размером LBA сектора равным 4KB.
меньше будет запиливаться и больше будет перформанс.

также можно без ключа -n 1 чтобы получить эту инфо
$ sudo nvme id-ns  /dev/nvme0n1   -H  | grep "LBA Format" | grep Metadata

чтобы перкключить диск в режим LBA 4K надо его отформатировать
$ nvme format --lbaf=1 /dev/nvme0n1   
ключ --lbaf=1 его цифра 1 означает то что выше написано тоесть 
0 = LBA Format  0 = 512 байт
1 = LBA Format  1 = 4K 

==========================================================================
- далее я пытас выяснить сколько hardware queues поддерживает мой диск
$ sudo nvme get-feature /dev/nvme0n1 --feature-id=7 -H
get-feature:0x7 (Number of Queues), Current value:0x0f000f
	Number of IO Completion Queues Allocated (NCQA): 16
	Number of IO Submission Queues Allocated (NSQA): 16


pci vs pci-e vs sata vs ahci
что такое шина. шина это набор проводов, разьем и стандарт передачи по проводам.
шина имеет скорость. 
шина содеиняет чтото слева с чем то справа.

pci: это шина и разьем на матплате. одним концом на мат плате она представлена разьемом(куда мы втыкаем внешнее устройство контроллер чегото) другим концом шина уходит в южный мост например ICH9. шина многопроводковая тоесть паралельная. паралельность еще заключается и в том 
что все устройства все разьемы на плате в итоге разделяют одну шину. поэтому они все конкурируют и делят пропускную способность шины. для параеллельных шин обычно
указывают частоту. у pci это 33 Mhz , размерность шины 32 бита. скорость при этом 132МБ\с(33Mhz*4байта=132МБ\с). 
разьемов на плате можте быть несколько скажем пять но все эти разьемы сидят на общей шине
так что либо одна плата будет выжимать 133МБ\с а остальные ноль. Либо все пять будут делить 
133МБ\с.( а не так что каждый разьем будет иметь 133МБ\с). это был основной стандарт.
где то там в конце жизни pci была модификация 66MHZ и 64бита и она выдавала 532МБ\с.
(оно и логично в 2 раза боьше разрядность и в 2 раза болбше частота итого 133МБ\с *2 *2 = 532МБ\ч). итак если мы рассмотрим все же обычную шину PCI 33Mhz,32bit то скорость суммарная
на все устрйоства которые к ней поключены 132МБ\с, они все ее делят между собой.
32битная шина имеет 124 провода (62 по одну сторону от разьема и 62 по другую) в архитектуре. из них данные и адрес используется 64 провода(32 используются под сигнал а 32 для второго конца провода. условно говоря 32 провода это фаза и 32 провода это ноль).
тоесть 4 байта можно передать за раз.  поскольку под передачу данных используется 32 пары
проводов ( остальные под какито доп обеспечительные нужды) то это означает что 4 байта можно 
передавать либо в одну сторону либо в другую , либо туда либо обратно. поэтому шина pci она 
half-duplex. для того чтобы она была полностью дуплексная нужно было добавлять еще 32 пары проводов. для сравннеия ethernet 100. там 2 пары проводов тоест 4 штуки в итоге. по одной паре сигнал идет на отправку по второй на принятие. поэтому там полный дуплекс. зато за один раз можно передать только 1 бит. тоеть еще раз у pci у нее 32 пары проводов и все пары предназначены для отправки только в одну сторону либо туда либо обратно. а у ethernet 100 две пары прием одна строго для отправки а другая строго на прием.  так вот оказалось что 
когда у нас много пар используется для отправки в одну сторону то это типа очень плохо потому что по одной паре сигнал придет чуть позже по другой чуть раньше а поскольку мы дожны принять 
по всем парам в итоге то нам приходится ждать и там наверное еще какието проблемы с влиянием
сигнаа  в однйо паре на другие пары короче оказалось что наращивать частоту сигнала когда у нас
много пар работает в одну сторону очень хреновая перспектива , оказалось что горазло более проще взять вместо 32 пар всего одну пару и по ней охиренно поднять частоту и на выходе мы сможем получить гораздо более высокую скорость пропускную. поэтому отказались от дальнейшего развития многожильной pci шины в сторону маложильных шин в которых скажем 2 или 4 провода. 
прием если 4 то это два в одну сторону и два в другую тоест они друг от друга независят. 
Забегу впреед pci-e шина. у нее есть понятие линия. линия это 4 провода . где одна пара идет
на отправку вторая на принятие. поэтому она фулл дуплекс. и скорость у нее на  данный момент чтото типа 15GB\s ( по сравнению с 133МБ\с у обычной pci). я продолжаю сранивать pci и pci-e.
у pci на ее 32 парах линий сидели сразу все устройства  и делили ее межу собой. у pci-e 
выгляди по другому. условно говоря на мат плате разведено 50 пар линий pci-e. и скажем там первые четыре линии выходят на разьем. потом следущие 4 линии на матплате выходят на разьем.
потом еще 8 линий выходят на разьем. и фишка в том что если мы воткнули карточки в первый и второй разьем то так как линии независимые от устройства невлияют друг на друга. это как 
если бы у pci первые 4 линии выходили на разьем и вторые 4 линии  выходили на свой разьем. 
тогда бы два устройства невлияли друг на друга в плане конкуренции за пропусную способность по через шину. если сказано что разем pci-e x16 это значит что 16 линий выведены на данный разьем.
совестенно если 1 линия выжиаем 15GB то суммарно 16 линий могут выжать 16*15GB\s.
Возвращаемся к PCI. значит из ich9 выходило 62 пары проводов и они уходили на мат плату на разьемы. эти разьемы все сидели на этих 62 парах делили их. получаетс что в каждй момент времени из pci в ich9 прилетало 4 байта инфо только от одного устройства. остальные сидели и ждали когда им дадут тайм слот. но в ICH9 приходили линии нетолько с pci шины но и из других
шин. это и usb контроллер и ethernet контроллер и звуковая карта они тоже приходят 
с мат платы на ich9. тоесть к примеру говоря на плате есть разьем sata и он 
по отдельно распаянной sata шине на мат плане входит  в ICH9. значит ICH9 далее соединяется
с северным мостом по шине DMI. для ICH10 эта шина была по скорости 1GB\s и полный дуплекс.
северный мост уже соединяется с цпу через FSB шину. вот наконец картинка как это выглядело
https://en.wikipedia.org/wiki/File:Motherboard_diagram.svg
на ней видно что все разьемы pci делят между собой одну шину. про FSB шину ну скажем что 
она имела скорость 3.2GB\s. возникает вопрос а  как цпу  обращался к pci шине. 
нужны либо отдельные на цпу адресные линии которые можно выставлять которые должны вести 
в pci линии. либо нужен memory map chip который будет делать то что при выставлении н
на стадартной шине адреса для памяти на цпу некоторого адреса чтобы этот чип переключал
на адресные линии pci. ответа незнаю как это было сделано. потому что в целом варианта у цпу
еще раз повторю два. либо у него должные быть отдельные ножки адреса\данных для ообращения
к шине либо он должен юзать ножки предназначеные для памяти а спец мемори чип должен 
в это время перехватывать и мапить на шину. также видно что dmi шина точно неузкое
место тоесть всем периферийным устройствам хватит ее ширины для обращения наверх.
но возникает интересный вопрос - из картинки видно что слева от цпу видеокарта
справа память снизу южный мост с периферичнымт устройствами так вот фишка втом насколько я понимаю что в каждый момент времени цпу может обращаться тольк к одному устройству. 
тоесть либо к памяти. либо в к видеокарте либо к pci устройству. тоест неможет одновременно
читаь и из видеокрта и из памяти и из pci шины поэтому неочень понятно скажем зачем DMI размером 1GB\s если в любом случае ... хотя на картинке от ICH9 изображен 1GB сеетвой контроллер поэтому да чтобы его сокрость раскрыть нужен наверх шина 1GB\s . я к тому что 
на DMI нужна скорость несуммарная от всех периферийных устройств сидящих на ICH9 а только 
скорость самого быстрого. потому что цпу неможет обращаться более чем к 1 устройству на текущий  момент. едиснвтенное что возможно еще играет роль это DMA. тоесть 
DMA контроллер может качать данные из периферийного устройства в память через ich9 и северный мост и однрвемнно цпу в это время может качать данные скажем с сетевого контроллера. тогда 
да получается что через DMI в момент времени будет протекать потоки от двух периферийных устройств одноврменно и тогда DMI должен быть шире чем пропускная споосбность этих двух 
и более устройств. вот еще картинка для схемы северный мост-южный мост 
https://en.wikipedia.org/wiki/File:Intel_4_Series_arch.png
кстати когда бала схема севеный+южный мост то контроллер памяти сидел в северном мосту.
далее вместо и южного появилась схема одного моста PCH (platform controller hub)
который соединяется с цпу по DMI. а контроллер памяти сидит в цпу. память уже содеиняется
напрямую с цпу без участия мостов. схема - https://en.wikipedia.org/wiki/File:Intel_5_Series_architecture.png

Возврашаемся к PCI.
итак байти от нее летел в ICH мост южный. далее через DMI летет в северный мост и далее
через FSB летел в цпу. вот стока переходов. 
это нам пригодится когда щас будет рассматривать PCI-e. насколько там сократился этот путь.

а пока рассматриваем PCI и ее связь с дисками. диск в pci напрямую не воткнуть.
в PCI втыкался контроллер дисковый . на нем находился PATA контроллер.одним концом
эта плата воткнут в pci шину. на контрллере находится разьем PATA. воткнут кабель PATA
и он уже уходит в диск с разьемом PATA. либо контроллер распаян на плате и тут я незнаю
он был подключен к шине PCI либо он был подлкючен к ICH мосту напрямую минуя PCI. 
скорость PATA шины это 133МБ\с.
PATA шина она тоже как и PCI много проводковая. она также полудуплекс. 
шина имела 40 либо 80 проводков. ширина 16бит то есть 2 байта. значит 16 проводков под дата
3 под адрес ( на щине могло сидеть максимум 2 устройства). с обратный проводками там как то более хитро чем у PCI. но главное что 40 жильный разьем и 80 жильный отличались тем что 
доп 40 жил это все типа ЗЕМЛЯ предназначенная для того чтобы можно было повысить частоту 
шины до 66MHz. тоесть доп 40 жил они были не для того чтобы добавить полный дуплекс. 
они были служебные чтобы можно было добавить частоту. да и смысла небыло в полном дуплексе
ибо крутящиеся диски могли либо читать либо писать. одновременно там бы упала скорость сильно.
получается схема выглядела в лучем случае байтик который летел от диска его путь 
выглядел так:

диск - PATA шина - (PATA контроллер в ICH9) - DMI щина - north bridge - FSB шина - цпу

или еще хуже


диск - PATA шина - PATA контроллер - PCI шина - ICH9 - DMI щина - north bridge - FSB шина - цпу

так значит с PCI и PATA разобрались.

переходим к SATA:
Sata это шина и разьем. SATA в отличие от PATA или PCI неимеет множества проводов.
в этом плане sata шина она очень узкая. в плане скорости выдает SATA 3 выдает 6Gbit/s или еще пишут так что выдает 6GT/s это одно и тоже. но фишка в чем, нахера тексели ввели, фишка втом что на каждые 8 переданных бит добавляется 2 служебных бита(обозначается 8b/10b) поэтому 6Gbit\s это полный поток
а поток полезной информации это 6(Gbit\s) * 8\10 = 4.8 Gbit\s или ~600MB/s 
Далее важно сказат что sata это полудуплексный шина тоесть данные в момент времени
по шине летят либо только туда либо только обратно но не одновременно. 600MB\s это скорость
либо туда либо обратно. 
теперь возвращемся к очень интересному моменту. в проводе SATA между контроллером и диском
4 провода а наконцах этого провода 7 пинов. И это для меня загадка. Единственное что 
4 пина это для передачи данных , тоесть эти пины это как провода что мы видим в проводе
а 3 пина это GROUND и я непонимаю а что ground нетребует наличия проводов в проводе?
вот это мне непонятно. переходим к тому почему 4 провода в кабеле. как я понял смысл такой - 
эти четыре провода это две пары обычных контуров электрических. тоесть в одной паре ток входит
в один провод и возвращается через другой и во второй паре тоже самое. возникает идея что 
по одной паре передается инфо туда а по другой обратно. тоесть один бит пераедается по одной паре туда а другой бит передатеся по другой паре обратно. типа мы имеем полнодуплексную шину шириной 1 бит. ну или это шина шириной 2 бита в одну сторону полудуплексная. однако все совсем
нихера нетак все устроено у SATA. значит у SATA применяеся differential signaling. что это значит. для него нужно 4 провода (например). далее я пока непонимаю как физически передается сигнал и как он фиксируется но примерно так: у нас есть первая пара проводов которая является
классической электрической парой (типа как два провода из розетки и лампочка, один провод это фаза второй это ноль). так вот на первой паре на первом проводе на фазе мы дергаем по времени 
Voltage. например было там +0 а мы на короткое время дернули его на +12V. и это возмущение
побежало по проводу. Что ловят на том конце я незнаю. То ли изменнеие волттаджа во врмени
то ли появление тока. я незнаю. но возмущение долетит до второго конца и там словят условно говоря появление +12V. и через ноль эта хрень улетает обратно. Теперь берем вторую пару 
проводов и в фазу подаем в тотже момент времени -12V. и это вомзущение побежало по проводу.
А теперь самая главная хрень зачем мы это мутили - если на провода снаружи идет наводка электромагнитная то она и в первом контуре скажем для примера исказит сигнал и прибавит  к нему условно говоря +1V и во втором контуре она сделает тоже самое. так вот на той стороне
стоит электрический (незнаю как это на практике выглядит) дифференциатор. и тогда вот что 
он там получает 

+12V+1V - ( -12V +1V) = +12V + 1V +12V -1V = 24V

тоесть наводка будет полностью уничтожена этим дифференциатором исходя из того что она дала 
одинаковое возмущение на оба контура. а наш исходный сигнал получен с удвоенной амплитудой.
вот эта вся схема и называется differential signaling. а используется она для защиты от внешних наводок. сотвественно логично что обе фазы обоих контуров надо в кабеле расположить максимально близко чтобы внешнее воздействие на обе фазы было по амплитуде максимаьльно одинаковое. Итак по сути у нас через два канал передается всего один бит информации. но зато мы имеем помехаозащищенность. для сравнения посмотрим что было бы если бы мы использовали 
одну пару проводов и single ended signaling. тоесть у нас есть одна пара проводов и мы
на фазе дергаем и выставляем имупульс +12V.
возмущение летит по проводу и тут снаружи у нас помеха и она дает -13V
тогда на втором конце мы вместо +12V получим -1V тоесть бит информации абсоллютно потерян.
когда же у нас differential signaling то мы будем иметь

+12V-13V - ( -12V -13V) = +12V - 13V +12V +13V = 24V

как видно бит непотерян ( ну тоесть если у нас будет делитель сигнала на два то мы получим
на втором конце теже самые +12V). вот что знчит differential signaling, вот зачем он нужен
и вот почему в sata кабеле четыре провода и при этом по факту шина полудуплексаная и имеет ширину 1 бит. 

Единственное что мне непонятно. для differential signaling надо иметь 4 провода в кабеле
и экранировать вроде их получается ненадо это плюс. но зато они должны идти друг от друга 
на постоянном расстоянии. это минус.  а если мы гвооими про защиту от помех то можно было бы
взять просто два провода с single ended signaling и просто на них наложить железный экран
снаружи и тогда тоже небудут помехти снаружи влиять. так в чем проблема? неужели 
изговтовить кабель  с двумя экранрованными проводами сложнее чем изготоввить кабель с четырьмя 
неэкранированными проводами но идущих на одинаковом расстоянии? непонятно

но зато хотя бы стало понятно почему в sata кабеле 4 провода при том что передается всего 
1 бит и сввязь полудуплексаная.


- компексные числа и сигналы.
компдексное число это (по определению) всего навсего пара чисел. обозначается вот так
(x,y).  и сразу видно что компл число в точности выглядит как координата точки на какойто
плоскости.

еще (по определению ) комплексное число обозначается как z. тоесть
(x,y) и z это одно и тоже.

для комлпесного числа (по определению) определены операции сложения, умножения, равенства.

равенство:
 это когда
z1=(x1,y1)
z2=(x2,y2) 
тогда z1=z2 если x1=x2 и y1=y2

cсложение:
z3=z1+z2
z1=(x1,y1), z2=(x2,y2)
z3=(x1+x2,y1+y2)

умножение:
z3=z1*z2 = (x1*x2-y1*y2, x1*y2+x2*y1)
физичекий смысла неочень понятен.

далее рассматриваются числа вида (x,0)

z1+z2 = (x1,0) + (x2,0) = (x1+x2, 0)
z1*z2 = (x1*x2,0)

ну видно что получаемое компл число тоже имеет вид (x,0)

дальше они пишут стремную вещь что (x,0) поэтому можно обозначать как просто x 
тоесть

(x,0)=x

по мне это равенство это хрень какаято. обьекты справа и слева это разные штуки.
а именно слева находится пара чисел x и 0 а справа одно число x и как 
это можно приравнивать? ведь комплексное число по опредеению это ПАРА чисел! пара!
поэтому пара никак не может быть эквивалентна одному числу!. два числа никак не могут
быть эквивалентны одному числу! поэтому я пока могу эту хрень только вот так 
интерпретировать: если у нас указано x то это типа такое типографское сокращение
для обозначения (x,0). итак еще раз
если указано просто x то мы пониммаем что это сокращение обозначений ( считай что по факту
это некоректное обозначение) и по факту это некоректное обоначение обозначает (x,0).
опять же еще раз - определение комплексного числа это всегда пара чисел. (x,y)
нет никаких комплесных чисел вида x. что еще интеренее что есть определение о том как 
сложить два компелксных числа и как умножить два комплеатных числа но совсем нет 
инфо о том как сложить или умножить комлпексное число и некомплексное число!


(тут еще набор пространных рассуждений. - по мне хрень полная. слева координата а справа число. совершенно разные обьекты.
хотя можно вот так рассматривать что число x оно может быть обозначено на числовой прямой.
если у нас есь числовая прямая то координата числа x это x или вот так (x) просто обычно
никто не использует координаты для числовой прямой. числовая прямая это одна из осей плоскости в которой есть две оси координат. как только мы добавляем вторую ось то все числа
на исходной числовой прямой автоматом приобретают вид (x,0) а если третью ось добавим 
то приобретают вид (x,0,0) таким образом в какойто степени (x,0)=x хотя это как бы и хрень хотя как сказать. что является первичным число или числовая прямая. если числовая прямая то число это всего лишь ее свойство тогда скорее (x,0) явялется коректной записью числа а запись 
вида x это некий колхоз ибо небывает просто чисел числа все сидят на числовой прямой. вобщем хрен знает. ясно по крайней мере то что между числом x и точкой на оси (x,0) можно установить однозначное соотвествие. это точно. тоесть нет такого числа x которого бы небыло на (x,0)
и нет такой точки (x,0) на оси для которого бы несуществовало бы числа x. а так в целом непонятно что первично числа которые уже затем образуют числовую ось или ось внутри которой содержатся числа. поидее ось первична а числа это ее свойство. типа как  космос и пространство первича а атомы это их просто свойства. типа нет ни одного числа которые бы было рождено вне числовой прямой. наверно как то так. так.. хотя... что такое число если взять
напримере натуральных чисел. число это кучка яиц куриных или горка песка морского. хотя опять же это несамо число это его форма визуализации. что такое само число непонятно. чтото 
нематериальное чтото неведомое. типа как тепло. есть некая форма носителя или форма восприятия на теле но что такое тепло само неясно. так и число. если начать выкладывать 
в ряд числа в форме песка то это будет ряд горок от маленьких до больших. это будет 
альтернативный аналог числовой прямой. поэтому все таки вначале числа а числовая ось это форма визализации.. пока про это все.

далее рассматривается 

(x,0)*(1,0)=(x*1,0)=(x,0)
мы видим что умножение дало тот же результат что и исходное число
тоесть 
(x,0)*(1,0)=(x,0)

далее они еще раз пытаются установить связь между обычными числами и комплексными
они говорят что обычное число А и комплексное число (A,0) это одно и тоже ( у меня шок)
с одной стороны как число и пара чисел может быть одним и тем же. вот для примера 
прдеставим обычную трехмерную систему координат ( несвязанную с комплексным числом никак).
далее берем число 1. вопрос  (1,0,0) (0,1,0) (0,0,1) и 1 это одно и тоже число ? или может
вообще нельзя сравнивать обьекты разной природы. что такое (1,0,0) = есть три множества
чисел ( каждая ось это множество) и мы берем по одному числу из каждого множества 
и создаем новое множество из трех чисел 1,0,0 . множество из трех чисел. как можно множество
из трех чисел приравнивать к числу? поэтому я считаю что говорить что число А и множество
из двух чисел А,0 одно и тоже это некоректно. однако далее я принимаю эту их точку зрения
и считаю что если написано А то это типографское сокращение для комплексного числа (А,0)
Так вот далее они говорят что мы щас мол введем операцию умножения обычного реального числа
на комплексное число ибо к данному моменту мы такого рода операцию не определелили.

А- обычное число . но мы себе понимаем что на самом деле это комплекс число (A,0)
z - комплексное число
тогда
A*z это будет умножение не обычного числа на комплексное а комплексного на комплексное (а это мы уже умеем делать) а именно

A*z=(A,0)*(x,y)=(А*х, А*y)

итак еще раз если мы видим в одной строке комлпксное число и обычное число то мы сразу
понимаем что никакого обычного числа нет а это просто типографское сокрашение для комлпексного числа (А,0)



Далее рассмотрим умножение  (x,0)на  (0,1)

(x,0)*(0,1)=(x1*x2-y1*y2, x1*y2+x2*y1)=(0-0,x)=(0,x)
тоесть
(q,0)*(0,1)=(0,q) 
если мы заменим (q,0) типографским сокращением на q то получаем
q*(0,1)=(0,q)
или
(0,y)=y*(0,1)
еще раз подчеркну что y это просто сокращение типографское для комплксного числа (y,0)
потому что у нас операции определены только для комлпксных чисел поэтмоу все числа 
в строке дожны быть строго комлексные.
комплексное число (0,1) называется мнимой единицой(чисто такой у нее позывной). это 
число так называют потому что
(0,1)*(0,1)=(0-1,0)=(-1,0)
если мы обозначим число 0,1 как i то получаем
i*i=(-1,0)
или
i^2=(-1,0)
далее заменяем (-1,0) на типографское сокращение -1
получем
i^2=-1
еще раз хочу подчернуть что -1 в данном случае это не -1 а сокращение для комлпксного числа (-1,0) так что ненадо путать -1 и (-1,0)

двигаем дальше:
z=(x,y)=(x,0)+(0,y)
вышем мы получали что (0,y)=(y,0)*(0,1)
заменяем (0,1) на ее обозначение через i получаем 
(0,y)=(y,0)*i=i*(y,0) подставляем в z

z=(x,y)=(x,0)+(0,y)=(x,0)+i*(y,0)

пока все строго пока все четко.

дальше применяем типографское сокращение что (x,0) это x тогда

z=(x,0)+i*(y,0)=x+iy

z=x+iy 
это типа алгебраическая форма обозначения комлпкексного числа.
еще раз хочу подчеркнуть что эта форма это фикция. на самом деле нет в этой формул никакого
x и y болтающихся свободно в строке. это просто сокращение. реальная формула коректная это

z=(x,0)+i*(y,0)

где опять же i  это (0,1)


еще раз итак мы имеем
z=(x,y)=x+iy
отсюда мы чисто на глаз видим важную полезняшку
когда мы имеем алгабраическую форму комлпексного числа то то что стоит за буквой i
это вторая компоенента а то что стоит без буквы i то это первая компонента.тоест 
пример
z=3-i6 это значит что z=(3,-6)
тоесть мы можем мгновенно переводить одну форму записи коплексного числа в другую





рассмотрим
z1+z2=(по определению)=(x1+x2,y1+y2)
теперь рассмотрим в новой форме z1+z2=(x1+iy1)+(x2+iy2) , теперь приравниваем
(x1+x2,y1+y2) = (x1+iy1)+(x2+iy2)
раскроем правую часть и проверим что получим то что слева
(x1+iy1)+(x2+iy2)=(x1,0)+i*(y1,0)+(x2,0)+i*(y2,0)=(x1+x2,0)+(0,y1)+(0,y2)=
=(x1+x2,0)+(0,y1+y2)=(x1+x2,y1+y2)
сравниваем с тем что слева
(x1+x2,y1+y2)=(x1+x2,y1+y2)
совпало ну а как же оно могло несовпасть.
я думаю что из алгеабраической формы для операции сложения тут важно
заметить что походу мы можем(чисто на пальцах) вынести правило такое что :
 z1+z2=(x1+iy1)+(x2+iy2)=(x1+x2)+i(y1+y2)



ну и для операции умножения: 
z1*z2=(x1,y1)*(x2,y2)
c одной стороны(по определению) 
z1*z2=(x1*x2-y1*y2, x1*y2+x2*y1)
с другой стороны используем новую форму записи комплексного числа 
z1*z2=(x1+iy1)*(x2+iy2)
получеаем:
(x1*x2-y1*y2, x1*y2+x2*y1)=(x1+iy1)*(x2+iy2)
исходя из этой штуки мы можем взять дерзость смелость применяя к правой части
колхозные методы работы с обыкновенными числами получить то что слева.тоесть раскрыть скобки
как мы бы это делали с обыкновенными числами. (хотя такой подход это чисто наш неподекрепленны математически метод). поехали раскрываем скобки:
(x1+iy1)*(x2+iy2)=x1x2+i*x1y2+i*y1x2+i*i*y1*y2
группируем и используем то что i*i=(-1,0)=-1
x1x2+i*x1y2+i*y1x2+i*i*y1*y2=x1x2+i(x1y2+y1x2)-y1y2=(x1x2-y1y2)+i(x1y2+y1x2)
сравниваем левую часть которая точно верна потому что по определению
и правую которую мы вывели колхозным пальцевым некоректным методом
(x1*x2-y1*y2, x1*y2+x2*y1) = (x1x2-y1y2)+i(x1y2+y1x2)
и как бы да все совпало. откуда я чисто из пальца делаю вывод
что в алгебраической форме с компоентами комплексного числа можно работать ровно также
в плане раскртия скобок сложения и умножения так же как мы это делаем для простых чисел.
это удобно. в этом плюс алгабраической формы. хотя строгого доказтельсва у меня нет.

далее.
комплксно сопряженное число.
число (x,-y)=x-iy является комплесно сопряженным числу (x,y)=x+iy

далее.
модуль комплексного числа ( по определению) обозначается как |z| и оно равно sqrt(x^2+y^2)
тоесть
|z|=sqrt(x^2+y^2)
теперь тоже самое в других формах
|z|=|(x,y)|
|z|=|x+iy|

теперь смотрим когда же |z|=0. это будет когда sqrt(x^2+y^2)=0 а это будет когда 
x=0 и y=0.
подставим x=0 и y=0 в алгебраическу форму
|z|=|0+i*0|=|i*0|=|(0,1)*(0,0)|=|(0,0)|=sqrt(0+0)=0

далее расматривается случай коплексного числа (x,0)
|z|=sqrt(x^2+0)=|x|
таким образом |z| для (x,0) совпадаеь с модулем обычного числа |x|

далее
z=x1+iy1
(сопряженное)z=x1-iy1
z*(сопряженное)z=(x1+iy1)(x1-iy1)=(x1,y1)*(x1,-y1)=(x1*x1+y1*y1, 0)=(x1^2+y1^2,0)
получаем(используем алгебраическую форму)
z*(сопряженное)z=x1^2+y1^2
теперь вспоминаем что |z|= sqrt(x^2+y^2).
возведем в квадрат
|z|^2= (sqrt(x^2+y^2))^2=x^2+y^2
теперь сравниваем:
z*(сопряженное)z=x1^2+y1^2
|z|^2= (sqrt(x^2+y^2))^2=x^2+y^2
видим что получаем одно и тоже .значит:
z*(сопряженное)z = |z|^2


далее. 
замечают что модуль |z| и модуль |сопряженный z| совпадают.
тоесть
|z|=sqrt(x^2+y^2)
|сопряженный z| = sqrt(x^2+(-y)^2)

далее они доказывают что
(z1*z2)*z3 = z1*(z2*z3)
и то что
(z1+z2)*z3 = z1*z3 + z2*z3
ну понятно как доказывается. 
и в книжке написано что благодаря этому доказывается что можно выполнять операции
с комплекными числами как бутто это обыкновенные числа и i  в том числе. то чем я занимался
там выше.

далее рассматриваются числа 1=(1,0) и 0=(0,0) и их свойства
z+0=z+(0,0)=z
z*1=(x+iy)*(1+i*0)=x+iy=z


далее.
разность z1-z2 называется такой z3 что z3+z2=z1

найдем 0-z
пусть z=(x,y)
искомый z3=(x3,y3)
0=(0,0)
тогда
(x3,y3)+(x,y)=(x3+x,y3+y)=(0,0)
значит
|x3+x=0
|y3+y=0

значит
|x3=-x
|y3=-y

значит
z3=(-x,-y)

найдем теперь общий случай z1-z2=z3
(x3+x2,y3+y2)=(x1,y1)

значит
|x1=x3+x2
|y1=y3+y2

значит
|x3=x1-x2
|y3=y1-y2

значит общее правило как выполнять отнимание:
z1-z2=(x1-x2, y1-y2)


далее
посмотрим чему равно 
-1*z= (-1+0i)(x+iy)=-x-iy=(-x,-y)
сравниваем и замечаем что 
0-z=-1*z
интересно.


далее
определяем что такое операция деления. z1:z2=z3
так вот z3 по определению это такое число что z3*z2 должно дать z1
итак ищем z3 пользуясь выражением z3*z2=z1
умножаем левую и правую часть на (сопряженное)z2

(сопряженное)z2*z3*z2=(сопряженное)z2*z1
тут мы пользуем формулу что раньше получили (сопряженное)z2*z2=x2^2+y2^2
значит
(x2^2+y2^2)*z3=(x2-iy2)(x1+iy1)
двигаем дальше
работаем с правой частью
(x2-iy2)(x1+iy1)=x2*x1+i(y1*x2)-i(y2*x1)-i^2*y2*y1=x2*x1+i(y1*x2-y2*x1)+y2*y1=
=(x2*x1+y2*y1)+i(y1*x2-y2*x1)=(x2*x1+y2*y1, y1*x2-y2*x1 )
подставляем
(x2^2+y2^2)*z3 = (x2*x1+y2*y1, y1*x2-y2*x1 )
работаем с левой частью
(x2^2+y2^2)*z3=(x2^2+y2^2)*(x3+i*y3)=x3*(x2^2+y2^2)+i*y3*(x2^2+y2^2)=
= ( x3*(x2^2+y2^2),  y3*(x2^2+y2^2)   ) 
подставляем
( x3*(x2^2+y2^2),  y3*(x2^2+y2^2)   ) = (x2*x1+y2*y1, y1*x2-y2*x1 )
значит чтобы было равенство то значит должно выполняться:
| x3*(x2^2+y2^2) = x2*x1+y2*y1
| y3*(x2^2+y2^2) = y1*x2-y2*x1

значит
x3= (x2*x1+y2*y1)
    --------------
    (x2^2+y2^2)

y3= y1*x2-y2*x1
    ------------
    (x2^2+y2^2)



значит
z1     (  x2*x1+y2*y1     y1*x2-y2*x1  )
--- =  ( --------------,  ------------ )
z2     (  (x2^2+y2^2)     (x2^2+y2^2)  )


выполним пример
1+i     2*1+(-3)*1       1*2-(-3)*1     -1       5
---- = ------------ + i*------------ = ---- + i*----
2-3i    4+9               13            13       13


далее смотрим на счет |z1*z2| = |z1|*|z2| проверяем
берем левую часть
z1*z2 = (x1+iy1)*(x2+iy2) = x1*x2+x1*i*y2+i*y1*x2-y1*y2 = (x1*x2-y1*y2)+i(x1*y2+y1*x2)
далее 
|z1*z2|=sqrt[(x1*x2-y1*y2)^2+(x1*y2+y1*x2)^2]
тут давай упростим выражение в скобках
(x1*x2-y1*y2)^2+(x1*y2+y1*x2)^2=(x1*x2)^2 + (y1*y2)^2 - 2*(x1*x2)(y1*y2) +
+ (x1*y2)^2 + (y1*x2)^2 + 2*(x1*y2)(y1*x2) = (x1*x2)^2 + (y1*y2)^2 +  (x1*y2)^2 + 
+ (y1*x2)^2 = x1^2*(x2^2+y2^2)+y1^2*(y2^2+x2^2) = (x2^2+y2^2)*(x1^2+y1^2)
подсталяем 
|z1*z2|=sqrt[ (x2^2+y2^2)*(x1^2+y1^2) ]

теперь работаем с правой стороной
|z1|*|z2|=sqrt(x1^2+y1^2)*sqrt(x2^2+y2^2)=sqrt[ (x1^2+y1^2)*(x2^2+y2^2) ]

теперь сравниваем
|z1*z2|   = sqrt[ (x2^2+y2^2)*(x1^2+y1^2) ]
|z1|*|z2| = sqrt[ (x1^2+y1^2)*(x2^2+y2^2) ]
получили одно и тоже, значит
|z1*z2| = |z1|*|z2| 

теперь проверяем то что:
|z1|   |z1|
|--| = ----
|z2|   |z2|

используем то что (z1\z2)*z2 =z1
действительно z1\z2= (по определению) z3 такой что z3*z2=z1
далее накладываем на них модуль.
|(z1\z2)*z2| = |z1|
теперь используем выше доказанное свойство :
|z1*z2| = |z1|*|z2|
значит
 |(z1\z2)*z2|=|(z1\z2)| * |z2| . подставляем это вверх

|(z1\z2)| * |z2| = |z1|
далее поскольку модуль это уже чисто обыкновенное число то мы можем с ними 
делать классические алгебраические операции , значит:
|(z1\z2)| = |z1| \ |z2|

или более красиво
|z1|   |z1|
|--| = ----
|z2|   |z2|


далее. 
переходим к геометрической интерретации комплексного числа.
берем плоскость вначале классическу прямоугольную систему координат
обе оси действительные. тогда что значит классическая координата (x,y) то это 
вектор в разложении по двум векторам (1,0) и (0,1) тоесть

{x,y}=x*{1,0}+y*{0,1}

теперь посмотрим на алгебрасичкую запись комплекс числа.
(x,y)=x+iy=(x,0)+y*(0,1)=x*(1,0)+y*(0,1)
тоесть
(x,y)=x*(1,0)+y*(0,1)

получается что компл число это разложение по базису двух векторов
(1,0) и (0,1) где гориз ось это реаьльные числа а верт ось это ось ..... надо эту 
главу пеероссмыслить.




- frequency domain representation - амплитудно частотная характеристика.
состоит из набора частот и амплутуд этих частот.
идея такая что сигнал можно разложить в ряд. 


- baseband
что это такое в вики дается очень мутное определение.
у одного индуса я нашел такое определение - A baseband signal is one which has spectrum from 0 Hz to some fc Hz where fc is the cut off frequency (necessary to have spectrum around 0Hz to qualify for baseband signals).

но сразу упомяну такую штуку - Ethernet protocol, which transfers data using the original baseband signal. In fact, the word "BASE" in "10BASE-T," "100BASE-T," and "1000BASE-T" Ethernet refers to baseband transmission. These Ethernet protocols do not require signal modulation. 

Еще полезняшка по этой теме:
So, summing up, the base band signals modulated with higher frequencies have the following benefits:
More number of baseband signals can be accommodated over a single wire or channel
Signals can be transmitted over long distances without amplifier.
You can also encrypt the communication for privacy and security
Costs much less.
Еще полезнящка: телефонный проводной телефон это пример baseband communication.

baseband сигнал через кабель можно пускать а через радиоканал нет потому что 
антенна на передачу радиосигнала должна быть размером примерно с половину волны.
для волны 20KHz размер волны измеряется километрами. а именно вот формула f = 𝜈 / λ
где f это частота в герцах, 𝜈 это скорость волны. для радиоволны 𝜈 = 3*10^8 m\s
подставляем λ = (3*10^8 m\s)/(20 000 Hz)= 15 000m , тоесть 15км. значит антенна
должна быть 7.5км что нереально для размера антенны.

также полезняшка low pass filter это такой фильтр который пропускает чеерз себя
только частоты ниже какойто. high pass filer это фильтр который пропускает частоты
выше заданной а band pass filter это фильтр который пропускает частоты между f1 и f2.

чем выше частота радиосигнала тем меньше антенна на передачу и прием и тем меньше 
по размеру вся требуха в приемнике. 






- coaxial cable означает кабедь в котором провода лежат на одной линии потому что 
axi это типа ось, а coaxi на одной оси. тоесть токи текут в обоих направлениях
вдоль одной оси.





- типа очен полезная книга по связи:
Modern digital and analog communication by B.P. Lathi


- attenuation. что это . а это attenuation (also known as signal loss).


- infrared излучение. что там за фишки

- фундаментальный вопрос ( я там ниже рассмотрел что такое цифровой и аналоговый сигнал).
так вот вопрос как генерируется цифровой сигнал. так как в природе обычно все величины аналоговые (кроме квантового мира где есть четко дифференцированные фискированные квантовые состояния) а будем даже более конкретными в электрическом мире мире напряжения и силы тока
мы неможем сгененировать скажем первую секунду 100% U=5V а потом вторую секунду U=0V.
окей если 0V мы еще можем сделать то мы неможем обеспечить 100% 5V он будет все равно меняться
и плавать в течение этой секунды. тоесть он все равно будет аналоговый так как же генерируется
цифровой сигнал?


- balanced cable

- непонятно вот у нас есть несколько разьемов на плате (пусть будет 4) под SATA провод. каждый разьем ведет к одному диску. вопрос сколько контроллеров SATA на плате четыре или один.
далее было сказано что SATA AHCI имеет одну очередь на 32 запроса. вопрос это 
на все четыре диска или на каждый диск своя очередь.
- надо переходить от SATA к AHCI SATA  а потом к NVME 
- NVME очереди (65 535 очередей с глубиной 65 535 команд)


- analogues signal vs digital signal
analogues означает аналогичный,  чтото аналогичное чемто другому.
что такое сигнал - согласно вики это некая величина чегонибудь (масса, напряжение, величина светового потока,величина тока, положение рук регулировщика) которая для наблюдателя (кудаж без него) меняется во времени и\или в пространстве. итак именно некая величина+время+наблюдатель образуют сигнал.
если честно я чтото непредставляю
что за сигнал который неменяется во времени но меняется в пространстве. если величина
меняется во времени тут вопросов нет что это сигнал. например фонарь был темный стал светлый. 
это сигнал. летающая тарелка ее координата она поменялась во времени это сигнал. светофор
погас зеленый и в другом месте пронстранства загорелся красный - поменялся и во времени в 
пространстве тоже сигнал. непонимаю какой пример сигнала (некоторая величина) которая неменяется во времени но меняется в пространсве. все таки по мне это величина меняющаяся
во времени. 
насколко я понимаю когдамы говорим про сигнал - значит должен быть генератор сигнала
его источник, должен быть носитель сигнала , должна быть некая величина которая переносится в этом носителе, и должен быть приемник сигнала и должен быть наблюдатель сигнала.
переходим к электрическим сигналам. величина наблюдаемая тут это либо ток 
либо напряжение. аналоговый сигнал( как я понял ) что это означает - вот мы начали мерять сигнал и снимаем с прибора величину с некоторым шагом например снимаем виличину напряжения каждую секунду. меряли 10 секунд. получили 10 величин. потом мы повторяем сигнал и меряем
его опять 10 секунд но уже каждые пол секунды получим 20 величин. потом мы повтоярем сигнал
и меряем его каждую треть секунды получили 30 величина. далее мы возьмем полученные 10 величин и выкинем из них все повторяющиеся получим множества A. потом возьмем 20 величин и выкинем из них повторяющиеся получим множество B, 
потом возьмем 30 величин и выкинем повтоярющиеся получим множество C, так вот
если сигнал аналоговый то множество C будет больше чем множество B а оно бльше чем множество A.
и чем чаще мы будем снимать сигнал и выкидываь из полученного множества дубли тем суммарное
множество будет больше ( вштуках) чем прердыдущее множество которое было получено путем фиксации сигнала при большем интервале времени. хотя максимум и минимум сигнала конечные
но на отрезке времени количество велиичин сигнала бесконечно какой бы маленький шаг 
фиксации величины мы бы ни взяли. на него непохож цифровой сигнал - если мы возьмем отрезок времени то мы можем найти такой минимальный шаг фиксации величин сигнала что при уменьшении времени снятия сигнала (другими словами при увечличении частоты фиксации сигнала) мы будем получать одно и тоже множество величина сигнала (после того как мы выкинем из него дубликаты.)
пример аналогового сигнала и цифрого:
положим что вольты в проводе меняются по закону U=t
будем фиксировать вольты первые 2 секунды.
вначале инетрвал фиксации 1 секунда.
U(1)=1
U(2)=2

дублей нету. значит наше множество это {1,2}

далее фикируем каждые 0.5 секунды
U(0.5)=0.5
U(1)=1
U(1.5)=1.5
U(2)=2
дублей нету. конечное множество {0.5, 1, 1.5, 2}
фиксируем сигнал каждые 0.1с множствео будет {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, ... 2}
так вот видно что чем чаще мы снимаем сигнал тем множстве значений растет растет и растет 
и при любой частоте снятия сигнала мы можем взять еще более частое снятие сигнала и множество
будет еще больше. это аналоговый сигнал. множество значений увеличивается до бесконечности
при уменьшении времени снятия сигнала ( даже после выкидывания дублей . хотя в данном случае дублей просто нет).
цифровой сигнал.
положим что сигнал вот по такому закону генерируется
U=1(при t=[0,1) )
U=0(при t=[1,2] )

снимаем сигнал каждые 0.5с
U(0.5)=1
U(1)=0
U(1.5)=0
U(2)=0

выкидываем дубли(это важно выкидывать дубли) получем множество {1,0}

снимаем сигнал каждые 0.25с
U(0.25)=1
U(0.5)=1
U(0.75)=1
U(1)=0
U(1.25)=0
U(1.5)=0
U(1.75)=0
U(2)=0

выкидываем дубли получаем ножество {1,0}

так вот что более часто сигнал снимать нет смысла. какой бы мы ни взяли шаг еще меьше
после выкидыаний дублей мы получим все тоже множество {1,0} 
это и показыает нам что сигнал цифровой.

еще раз аналоговый сигнал - чем чаще мы снимаем его значение (на одном и том же конечном
промежутке времени) и потом выкидываем дубли то полученное множество будет все расти и расти
и расти оно будет бесконечно при уменьшении шага снятия величины.
цифровой сигнал - его множество значений не растет бесконечно (после выкидываений дублей) при уменьшении шага фиксации величины сигнала. начиная с определенного шага  фиксации величины сигнала S0 полученное множество значений (после выыкидываний дублей) будет одно и тоже для любого сколь угодно малого S1<S0.
Обычно в природе величины(сигналы) являются аналоговыми. например температура 
у тела имеет аналоговый характер. цифровой же сигнал обычно искусственным образом создается генерируется людьми. 
амплитудная модуляция я узнал как она выглядит через математику. если 
низкоачастотный сигнал это y1=f(t) и если у нас высокочастотный носитель это y2=A*Sin(bt)
то амплитудная модуляция выглядит как y3=f(t)*Sin(bt).



- а ethernet 100 он тоже исплзует differential signaling ?
из того что я прочитал нет. эзернет НЕиспользует differential signaling.
у них в одну сторону используется пара проводов и в другую сторогу использутеся пара проводов.
итак еще раз в SATA в одну сторону исползуется 4 провода.  а в ethenet 100 в одну сторону
исползуется 2 провода. кручение кабеля испольуется для того чтобы защититться от влияния
одного кабеля на другой. оно тем больше чем длиннее кабели а скрутка это убирает. от внешнего 
влияния из вне скртка никак не помогает. получается что  SATA каелбелть он короткий скажем 1метр. на такой длинне влияние одного провода на другой видимо еще маленькое ( потому что я читал что чем длинее провода тем они в в коце друг на другна сильнее влиятют) а более 
влияет помехи снаружи. поэтому satat не скручивают зато используют differential signaling.
ethernet же прводо длинной 100метров поэтому влияне одного проводоника на другой уже
на такой длинне накапливается и провод крутят . скорость у satat 600MB\s а у эзернет
10MB\s и поэтому видимо у него неиспользуют differential signaling тоеть небоятться внешних 
помех.  ятолько в одной сттье нащел что 10Gb ethernet использует differential signaling
 

- амплитудная модуляция. это когда унас есть какойто высокочастотоный сигнал (радиоволна)
и мы меняем у него амплитуду так что если мы соединим линией все его амплитудные точки
то получим волну низкочастотную котору мы и хотели закодировать через этот выскочастотоный сигнал ( картирнка https://en.wikipedia.org/wiki/File:Amfm3-en-de.gif)
еще  я непонял вот что. если мы возьмем наш низкочастоный сигнал разложим его в ряд фурье
тоесть представим наш сигнал как сумму синусоид с индивидуальными амплитудами то мы получим некоторый спектр. это понятно. ксатии вобще то ряд фурье он для периодических функций
а если непериодическая? ну как я понял тогда берем кусок функции и потом ее как то там продолжаем и вот для нее строится ряд фурье и он на заданном куске совпадает с с функцией.
так вот ( как я понял) если мы возьмем наш промодулированный сигнал (тоесть  выскочастотный несущий синал который по амплитуде промудирован низкочастостным сигналом ) возьмем кусок
этого сигнала ибо он же нерегулярный поэтому как я понял для всего сигнала построить 
ряд фурье вот так единой формулой нельзя. поэтому мы берем кусок сигнала потом хитро его там
продолжаем (как написано в вики) и строим для него ряд фурье который на заданном куске 
совпадает с сигналом так вот утверждается что промудулироваронный сигнал в виде ряда фурье
будет иметь спектр в два раза шире чем спектр исходного низкочастоного сигнала на данном куске. хотя чисто интуитивно мне непонятно то что промодулированный сигнал имеет в себе 
всего одну частоту (ту самую высокочастотную) изменилась всего лишь амплиутда то там то здесь
непонятно как это может влиять на появление новых частот в интеграел фурье. но тем не менее надо двигать дальше.
далее такая хрень еще встречается как baseband bandwidth и passband bandwidth.
и тут я тоже замучался разбираться и неразобрался. 




- почему у коаксиала гораздо выше и экранирование и полоса пропускания чем у витой
пары а скорость передачи данных цифровых гораздо ниже чем у витой пары. я вот думал 
что коаксимал неиспользуют только потмоу что его дороже исзготавливать и он гораздо
хуже монтировать. (типа его нельзя гнуть и так далее). так что если забыть что его метр
стоит дороже и что его монтировать недоубно мне непонятно почему его цифровая скорость 
ниже чем у витой пары что за фигня. 
- baseband bandwidth vs passband bandwidth  vs broadband
я всегда часто думал что эти фразы больше как маркетинговый булшит а оказывается
они имеют типа рельный смысл. 

- alexanderson alternator
- электронные лампы. как работают в чем фишка
- как максимально простыми методами можно генерировать радио волны
- катушка румкорфа она юзается при генерации искры. как она работает
- как передается сигнал в проводах через меняющийся ток или меняющееся напряжение
и как это ловят на том конце
- в чем фишка коаксиального кабеля почему он такой и для чего
- в чем фишка витой пары почему она такая и для чего
- сигнал передается через дергание Вольтов? и это породжает дергающийся ток или что?
- почему в sata разьеме 7 пинов а в проводе всего 4 провода
- рассссмотрим SATA диск ( с AHCI и без) и путь от него байта до цпу










главный вопрос как найти число очередей на своем nvme диске.
как на sysfs посмотреть число hardware queues о которых выше написано
get-feature:0x7 (Number of Queues), Current value:0x0f00g0f
	Number of IO Completion Queues Allocated (NCQA): 16
	Number of IO Submission Queues Allocated (NSQA): 16
что такое AHCI как он отличается от sata
когда идет ahci то как там идет связь между диском и цпу
nvme это через какой интерфейс и какую шину оно втыкается. как идет связь
между цпу и диском.

нашел в pdf от сигейта(у кингстона тоже самое написано) что AHCI(контроллер наверно) имеет только один command queue с глубиной depth на 32 команды. а nvme (диск наверно контроллера то нет ) может иметь до 65 535 command queue с глубиной depth команд до 65 535.
в чем разница между AHCI и NVME интерфейсом (из той же брошюры сигейта) - когда у нас

==================================================================================



- как выглядит прочесть 1 байт из файла на ассемблере? куда ядро сует ответ с диска процессу ?

- block devices vs character devices?
разница вроде бы в том что из character device можно читать и писать но тупо напрямую в 
/dev/name но нельзя задать какойто поиск какойто сдвиг. либо читаешь то что есть 
либо пишешь туда напрямую. а у block  device можно задать некий offset и данные будут
записаны\считаны изнутри кишок где то там начиная с offset.
еще такая добавка что якобы FreeBSD doesn’t use block devices at all.


- bfq, linux i\o scheduler ?

- 165.pdf стр 8 ?


- $ dd if=/dev/zero of=/dev/null bs=514 count=1
вот инеерсено заупускаю я эту команду. диск у нас 512байт сектор.
на стороне юезоер спейса будет написано что была только запись а вот на стороне
ядра и доно быьт и чтение и запись чего конечно небудет видно из юзер спейса через iostat
но должно быть в итоге идно по перфомансу конечному на диске.
как бы проверить?

- 
$ lsblk -o  NAME,ALIGNMENT,MIN-IO,OPT-IO,PHY-SEC,LOG-SEC  /dev/nvme0n1
NAME       1MENT MIN-IO OPT-IO 12КБSEC
запускается ядро и сисколл берет наши 12КБ данных и смотрит а какой размер блока на фс. 
ага 4КБ. он режет наши 12КБ на три куска
nvme0n1             0    512    512     512    
 512
├─nvme0n1p1         0    512    512     512     512
└─nvme0n1p2         0    512    512     512     512



- фс блоки. это внутренняя кухня  внутри драйвера фс или наружу тоже както 
это выставляется для внешних потребителей? тоесть непонятно нам то внешнему потребителю 
какая разница на эти блоки нахер это нам дает? мы хотим записать считать файл и только то.
какое нам дело до этих блоков ?


- https://www.opennet.ru/base/sys/info_diag_tools.txt.html







