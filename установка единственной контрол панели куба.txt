установка единственной контрол панели куба 
(когда не HA кластер)


	

ставим первую контрол панель
будем запускать кубернетес на мастер ноде.
называется инициализируем кластер


# kubeadm init  --pod-network-cidr=10.252.0.0/16 \ --apiserver-advertise-address=172.16.102.31

так что я щас немгу понять 
1) --apiserver-advertise-address=172.16.102.31
что реально значит эта настройка
2)как так получаетс что kube-apiserver слушает на хосте сокет :6443
хотя сидит в другом сетевом неймспейсе
3) почему мы указали --control-plane-endpoint "172.16.102.100:6440" 
но этой опции вообще нет в стартанувшем аписервере в строке запуска
4) наскольк я понял кубелет при запуске контрлол панели сразу же 
исопльзщует для связи с аписервером именнро этот кипэлайвный сокет.
то есть сраз обращается через него.

5) нахосте netatsat -tnlp показвыает что  аписервер слушает сокет 0.0.0.0:6443
а вот эта настройка --apiserver-advertise-address=172.16.102.31 это наверно
настройка для кубелетов на мастер ноде и дата нодах. чтобы кубелеты знали 
на какой сокет ломиться. то есть сам аписервер он будет по любом слушать
сокет 0.0.0.0:6443

6) хапрокси инемжоет сидеть на том же порту что и аписервер ( хапроси 
и аписервер у меня сидят на одном хосте)

7) когдя иницировал куб через балансер kubeadm init --pod-network-cidr=10.253.0.0/16 --control-plane-endpoint "172.16.102.100:6440" --upload-certs
и еще указал --apiserver-advertise-address=172.16.102.31   
у мен ничего не вышло . был таймаут и на этом все закончилось.
почему

8) на стадии запуска kebe init надо иметь чтоы в хапрокси стоял пробррос
на 127.0.0.1Ж6443 то есть на тот сокет на котором апмсервер будет слушать.


а еще есть вариант когда мы хотим чтобы наш кубернетес сидел за балансером
тоесть когда мы поставим несколько кубернес control plane посадим их 
за балансером (который тоже кластеризован) и таким образом получим отказоу
стойчивый куб control plane. так вот вэтом случае куб иницилиазируется вот так
ОЧЕНЬ ВАЖНЫЙ МОМЕНТ - если мы инициализруем чтобы куб работал через балансер
то надо чтобы ДО того как мы инициализируем куб на балансере УЖЕ был правильно
настроен путь куда он проксирует. он должен проксировать на наш будущий аписервер.
пример. хапрокси и куб control panel на одном хосте. тогда хапрокси
должен проксировать на 127.0.0.1:6443 на этом хосте. почему это так важно
потому что когда мы запустим kube init то кубелет будет выполнять запросы
к аписерверу СРАЗУ через балансер. СРАЗУ! ПОЭТОМУ если балансер показвыает нетуда
то куб несможет инициализироваться. я на это попал.
так вот как мы в итгге иницалищицеруем куб:

# kubeadm init --pod-network-cidr=10.253.0.0/16 --apiserver-advertise-address=172.16.102.34 --control-plane-endpoint "172.16.102.100:6440" --upload-certs

 --apiserver-advertise-address=172.16.102.31 --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs

 где 172.16.102.100:6440 = это кластерный ip адрес балансера (keepalived) либо
 dns имя  балансера  на который будут обращаться кубелеты с нод и
 попадать на балансер а он их будет проксировать уже на контрол плейн на 
 апи сервер. подчеркиваю что если указыем ip то указвыаем кластерный ip балансера.
 у меня балансер это keepalived
 
 :6440 = это порт на балансере. так как и балансер и аписервер сидят на одном
 хосте и аписервер использует порт 6443 то для порта балансера входящего
 то нужно выбрать порт друго чем 6443. я выбрал порт 6440.
 
 

тут надо сразу сказать что --pod-network-cidr=10.252.0.0/16  это сеть никак несвязанная с реальными сетями и ip 
адресами которые имеет наша нода. это сеть чисто внутренняя внутри нод кубернетеса которая будет 
использоваться для выдачи внутренних ip адресов для ПОДОВ.
а вот --apiserver-advertise-address=172.16.102.31 это уже реальный ip адрес нашей ноды. по которому к ней
можно убдет с других нод достучаться до api сервера. этот тот ip который уже имеет наша LAN карточка.

[init] Using Kubernetes version: v1.16.0
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.2. Latest validated version: 18.09
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [test-kub-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.102.31]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [test-kub-01 localhost] and IPs [172.16.102.31 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [test-kub-01 localhost] and IPs [172.16.102.31 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 41.501947 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node test-kub-01 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node test-kub-01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: ho9qay.07wqqjjeuypa5i0p
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.102.31:6443 --token ho9qay.07wqqjjeuypa5i0p \
    --discovery-token-ca-cert-hash sha256:95e4e1db5e638587e3cee2d662e1d55ab48a04d9d2564e8e1bad6db6d6c85500


система напишет что она ставит и запускает статические поды.
что это такое будем узанвать потом позже.

кубернетес стартовал.

сразу скажу что чтобы подключиться к кубернетесу 
# kubectl get pods --namespace=kube-system 
и убедиться  что  control plane стартовал нужно сразу же сделать

root@test-kub-01:~#  mkdir -p $HOME/.kube
root@test-kub-01:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@test-kub-01:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config

иначе если мы попробуем команду kubectl get pods то получим ошибку

kubectl The connection to the server localhost:8080 was refused

далее.
сразу проверим какой cgroup driver использует кубелет.
для этого как уже писал выше можно использовать три метода

1 # journalctl -u kubelet | grep cgroup
kubelet[21951]: Setting cgroupDriver to systemd

2 # /var/lib/kubelet/config.yaml
cgroupDriver: systemd

3. # ps aux | grep kubelet
root     21951  2.7  4.8 1190824 99076 ?       Ssl  02:55   0:15 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2
и мы тут видим в каких файлах можно посмотреть параметры с которыми 
стартует кубелет


далее
выполняем вот эти три команды
чтобы мы могли коннектится к кубернетесу через командную строку

root@test-kub-01:~# mkdir -p $HOME/.kube
root@test-kub-01:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@test-kub-01:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config


и надо отдельно про них сказать. в этом файле config находится ключ от кластера. 
если мы удалим кластер командой  kubeadm reset и создадим кластер заново 
то нужно будет стереть в $HOME файл config и скопировать его заново. потому что 
ключ от кластера изменится. и если мы config в $HOME оставим старый то при попытке 
начать работат с кластером нам вылетит ошибка

unable to recognize  x509: certificate signed by unknown authority 
(possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes"


вот она еще раз скажу значит то что у нас config файл от старого кластера со старым уже невалидным ключом 
или сертификатом пофик смысл один.

можно подключаться к кубернетесу к его контрольной панели ( публикация подов итп) из под любого
пользователя линукс так как для этого линукс пользователю нужно всего лишь иметь ключ от кубернетеса 
в своей домешней папке

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

но мы щас этого небудет делать и будем работать из под рута.


root@test-kub-01:~# kubectl get nodes
NAME          STATUS     ROLES    AGE   VERSION
test-kub-01   NotReady   master   87s   v1.16.0

также мы увидим что часть подов никак не может стартануть, находся в 
статусе pending

# kubectl get pods --namespace=kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-96tjb               0/1     Pending   0          2m22s
coredns-f9fd979d6-d4cbk               0/1     Pending   0          2m22s
etcd-test-kub-04                      1/1     Running   0          2m27s
kube-apiserver-test-kub-04            1/1     Running   0          2m27s
kube-controller-manager-test-kub-04   1/1     Running   0          2m27s
kube-proxy-58qr4                      1/1     Running   0          2m22s
kube-scheduler-test-kub-04            1/1     Running   0          2m26s


секрет в том что в все поды контрол плейна они сидят в том же 
сетевом неймспейсе что и хост и поэтому для них сетевой стек
уже готов. а поды coredns они сидят в своем индивидуальном сетевом неймспейсе
и им нужна оверлейная сеть. та которая фланнель. и пока мы фланнель
неустановим поды coredns не смогут стартовать. (фланнель мы поставим чуть ниже)

вот видно что поды coredns потом в итоге заимеют другие ip

NAME     READY   IP
coredns-f9fd979d6-6h9dc          1/1     10.252.0.21
coredns-f9fd979d6-cwn55          1/1     10.252.0.20
kube-flannel-ds-pgqq2    		 1/1     172.16.102.31
kube-proxy-ltz7l     	         1/1     172.16.102.31
kube-proxy-tch8f                 1/1     172.16.102.33


вот эти вот ip = 172.16.102\ это ip сетевеого неймспейа хоста
а ip = 10.252.0.0\ это ip которые предоставить бридж cni0 который установит 
фланнель.




видно что статус мастер ноды notready.
это потому что нам нужно поставить компонент который будет рулить внутренней сетью к8.
опять же ставить его до команды kubadmin init неполучится. пошлет система нахер.
ставим этот сетевой компонент уже после kubeadmin init.

обычно советуют ставить calico


но!!!  calico достаточно глючный продукт . он успешно не стартует когда мы ставим уже третью дата ноду. 
так что нахуй calico

так что в реале ставим flannel. этот продукт вообще работает без проблем

# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml


после этого через пару минут должны автомтатом стратартунть поды coredns

проверяем что теперь все системные поды которые к контрольной панели относятся они все встали .

# kubectl get pods --all-namespaces
   NAME                                  READY   STATUS    
   coredns-f9fd979d6-6h9dc               1/1     Running   
   coredns-f9fd979d6-cwn55               1/1     Running   
   etcd-test-kub-01                      1/1     Running   
   kube-apiserver-test-kub-01            1/1     Running   
   kube-controller-manager-test-kub-01   1/1     Running   
   kube-flannel-ds-pgqq2                 1/1     Running   
   kube-proxy-ltz7l                      1/1     Running   
   kube-scheduler-test-kub-01            1/1     Running   


все окей.
статус правильный.
это мы видим какие поды  составляют струкруру самого кубернетеса.



flannel нужен для того чтобы поды пользуясь лишь маршрутом по умолчанию в своем сетевом неймспейсе могли  пинговать друг друга между
серверами.

далее еще проверка

root@test-kub-01:~# kubectl get nodes
NAME          STATUS   ROLES    AGE   VERSION
test-kub-01   Ready    master   23m   v1.16.0
root@test-kub-01:~#


мастер нода должны быть в статусе=ready

все - кубернетес на мастер ноду установлен успешно.
