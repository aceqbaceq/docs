| nginx
| apache

значит начнем с жинкса. 
но прежде всего нужно прочитать "epoll.txt"
без него ничего не понять будет.

беру жинкс в котором два воркера. тоесть один мастер процесс и два воркера.
список процессов выглядит так

$ ps aux | grep nginx
root         465  0.0  0.2  20848  1288 ?        Ss   09:11   0:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;
www-data     468  0.0  0.6  22424  4196 ?        S    09:11   0:00 nginx: worker process
www-data     469  0.0  0.6  22424  4196 ?        S    09:11   0:00 nginx: worker process



вначале посмотрим открытые файлы на этих процессах. а именно посмотрим для начала юникс 
сокеты. локальные юникс сокеты
значит я внаале их посмтрю на мастер процессе

$ sudo lsof -Pnp 465 | grep  -E "Recv|STREAM"
nginx   465 root    6u  unix 0x00000000d420067f      0t0  16603 type=STREAM (CONNECTED)
nginx   465 root    7u  unix 0x00000000317da459      0t0  16604 type=STREAM (CONNECTED)
nginx   465 root    8u  unix 0x00000000b8629ba6      0t0  16605 type=STREAM (CONNECTED)
nginx   465 root    9u  unix 0x000000002213b227      0t0  16606 type=STREAM (CONNECTED)

итак у нас показано что открыто 4 юникс сокета
что значит type=STREAM. это значит что у нас дескриптор смотрит в локальный юникс сокет
у котрого тип STREAM. такой сокет создется командой

   int s = socket(AF_UNIX, SOCK_STREAM, 0)

а что это за сокет такой.  в отличии от сетеого сокета этот сокет предназначен чтобы 
связать два процесса локально на компе. тоесть один процесс создает себе такой сокет.
второй процесс создает себе такой сокет. там еще идут маниуляции но в итоге через эти
сокеты и ядро они друг с другом имеют двухстороннюю фулл дуплекс связь. и особенность 
связт через такой сокет что какимы бы кусками отправитель не отправлял свои байты 
для принимающей стороны нет никаких кусков и принмиающая сторона может читать байты
из сокета любыми кусками хоть по 1 байту за 1 раз. тоесть если отправитель сделает 
один    write(s, &buff, 10);
второй  write(s, &buff, 5);

то в сокет прилетит 15 байт и примающий процесс может читать эти 15 байто любыми кусками.
хоть по 1 байту за раз хит три раза по 5 байтов.

    read(s, &buff, 5);
    read(s, &buff, 5);
    read(s, &buff, 5);

если бы у нас был другой тип юникс сокета вот такой

   int s = socket(AF_UNIX, SOCK_DGRAM, 0)

то ситуация была бы другая вот в каком смысле. (аналог с udp). если отправитель делает
первую отправку 10 байт а вторую отправку 5 байт. то это считается две независмые хрени.
они в сокете который принимает данные будут лежать в двух отдельных даатаграммах.
и я за одну операция чтения смогу прочитать масксимум столько байтов сколько лежит в теле
этой датаграмы. например . пусть у нас вверху лежит дата грамма 10 байт 
а потом лежит датаграмма 5 байт. тогда две мои команды

    read(s, &buff, 100);
    read(s, &buff, 100);

приведут к тому что первая команда прочитает 10 байт. и не более. вторая прочитает 5 байт
и неболее. для сравнения если бы это был сокет который вышет то моя первая команда прочитала
бы все 15 байт ( если непонятно читай файл tcp2.txt там есть кусок про udp как он работает
чем отличется от tcp)
итак я пояснил в целом что значит тип сокета type=STREAM чем он отличатется от типа
сокета type=DGRAM


теперь я посмотрю информацию не о дескрипторах а об сокетах на которые
указывают декрипторы  через ss

$ sudo ss -f unix  -a -e | grep -E "Recv-Q|16603" 
Netid State         Local Address:Port  Peer Address:Port     Process                           
u_str ESTAB         * 16603             * 16604               <->                              
u_str ESTAB         * 16604             * 16603               <->                              

$ sudo ss -f unix  -a -e | grep -E "Recv-Q|16605" 
Netid State         Local Address:Port  Peer Address:Port     Process                           
u_str ESTAB         * 16606             * 16605               <->                              
u_str ESTAB         * 16605             * 16606               <->          


из чего я делаю вывод что 16603 и 16064 это ну условно говоря это номер портов 
которые используются у сокетов которые принадлежат одному конекту. 
тоесть если я буду из процесса писать в дескриптор который имеет порт 16603 то моя инфо появится
в сокете кооторый имеет порт 16604.
из чего я делаю вывод возвращаясь к мастер процессу жинкса что у него открыто 4 сокета
но пофакту эти 4 сокета обслуживают два конекта. 

          16603 <---> 16604
          16605 <---> 16606

а теперь посмтрим на наличие открытых сокетов во всех уже жинкс процессах

$ sudo lsof -Pnp 465 | grep  -E "Recv|STREAM"
nginx   465 root    6u  unix 0x00000000d420067f      0t0  16603 type=STREAM (CONNECTED)
nginx   465 root    7u  unix 0x00000000317da459      0t0  16604 type=STREAM (CONNECTED)
nginx   465 root    8u  unix 0x00000000b8629ba6      0t0  16605 type=STREAM (CONNECTED)
nginx   465 root    9u  unix 0x000000002213b227      0t0  16606 type=STREAM (CONNECTED)

$ sudo lsof -Pnp 468 | grep  -E "Recv|STREAM"
nginx   468 www-data    6u     unix 0x00000000b8629ba6      0t0  16605 type=STREAM (CONNECTED)
nginx   468 www-data    7u     unix 0x00000000317da459      0t0  16604 type=STREAM (CONNECTED)

$ sudo lsof -Pnp 469 | grep  -E "Recv|STREAM"
nginx   469 www-data    6u     unix 0x00000000d420067f      0t0  16603 type=STREAM (CONNECTED)
nginx   469 www-data    9u     unix 0x000000002213b227      0t0  16606 type=STREAM (CONNECTED)


из чего я делаю вывод что в целом исходя из этих сокетов все три процесса при желании
могут друг с другом обмениваться байтами. тоесть если какойто процесс запишет в  16603
то эти байты могут быть прочитаны если другой процесс при этом прчитает 16604
и так как это связь двусторонняя то если записать в 16604 то можно будет прочитать в 16603.
а так конечно пока непонятно кто с кем связывается через эти сокет. толи мастер процесс
общается с воркерами через эти сокеты то ли воркерв общаются друг с другом через эти сокеты.
то ли все друг с другом общаются. потому что если бы скажем толко воркеры друг сдругом
общались то гда им ненужно иметь каджому по два открытых сокета. им бы было достаточно
по одному открытому сокету для связт друг с другом. но в любом сулчае эта хрень нужна только для
бслуживания своих каких то внетренних интерконектов.

   
теперь посмотрим на счет открытых TCP сокетов

$ sudo lsof -Pnp 465 | grep  -E "Recv|TCP"
nginx   465 root    5u  IPv4                     16591      0t0    TCP 127.0.0.1:8888 (LISTEN)

$ sudo lsof -Pnp 468 | grep  -E "Recv|TCP"
nginx   468 www-data    5u     IPv4              16591      0t0    TCP 127.0.0.1:8888 (LISTEN)

$ sudo lsof -Pnp 469 | grep  -E "Recv|TCP"
nginx   469 www-data    5u     IPv4              16591      0t0    TCP 127.0.0.1:8888 (LISTEN)


получается что фрмально у них у всех открыт один и тот же слушающий сокет.


теппрь посмотрим еще раз. может еще какито интрсненькие файлы у них есть.
у мастер роцесса 465 больше ничего инеерснгого нет.

у 468 воркера есть

$ sudo lsof -Pnp 468
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   468 www-data    5u     IPv4              16591      0t0    TCP 127.0.0.1:8888 (LISTEN)
nginx   468 www-data    6u     unix 0x00000000b8629ba6      0t0  16605 type=STREAM (CONNECTED)
nginx   468 www-data    7u     unix 0x00000000317da459      0t0  16604 type=STREAM (CONNECTED)
nginx   468 www-data    8u  a_inode               0,14        0   9609 [eventpoll:5,7,9]
nginx   468 www-data    9u  a_inode               0,14        0   9609 [eventfd:9]


а именно мы видим декрптор 8 который имеет тип epoll. вот эта хрень в скобочках [eventpoll:5,7,9]
означает что  еполл следит за дескрипторами 5, 7, 9
5 это слушающий TCP сокет. тоест отслеживаются входящие конекты из сети
7 это унас юникс сокет для видим какогто интерконекта со своим процессами колегами
9 это дескриптор тип eventfd , про него я уже гдето писал ну это такой тир буфера в ядре
куда можно писать и читать но только жестко фиксированный размер идет 8 байт. тоесть можно 
либо записать 8 байт либо считать 8 байт. обычно служит для уведомления между процессами
о наступлении какогото события. один процесс открывает этот eventfd. потом он себя клонирует.
в итоге у них у кжадого есть дескриптор на этот обьект. один туда пишет. второй через еполл
замечает что тото пришло и читает. вобщем это тоже такой инстурмент для межпроцессной
синхронизации. 


и смотрим на воркер 469
$ sudo lsof -Pnp 469
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   469 www-data    5u     IPv4              16591      0t0    TCP 127.0.0.1:8888 (LISTEN)
nginx   469 www-data    6u     unix 0x00000000d420067f      0t0  16603 type=STREAM (CONNECTED)
nginx   469 www-data    9u     unix 0x000000002213b227      0t0  16606 type=STREAM (CONNECTED)
nginx   469 www-data   10u  a_inode               0,14        0   9609 [eventpoll:5,9,11]
nginx   469 www-data   11u  a_inode               0,14        0   9609 [eventfd:8]

однако в данном случае eventfd неисползется для межпроецессоной синхроанизации потому
что у каждго воркеа окзывается свой eventfd.
вот смотрим


$ sudo lsof -Pnp 468
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   468 www-data    9u  a_inode               0,14        0   9609 [eventfd:9]


$ sudo lsof -Pnp 469
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   469 www-data   11u  a_inode               0,14        0   9609 [eventfd:8]


возникает вопрос - что это за дурацукая цифра в скобочке?  [eventfd:8] или [eventfd:8]
а это оказывает некий идентификатор этого обьекта вунтри ядра. поэтому если бы у нас это 
был быодинаковый обьект внутри ядоа то цифра была одна и таже у двух процессов.
а так как это сворешенно разные оьькекты то и цфира разная. полчается что эта хрень
исползуется жинксом ну типа как стек какойто самоппльный. чтобы туда можно было полжить 8 
байт. а потом попозже считать. 
вот я взял прогармму 349.c и так это все проверил. вот наример когда у нас вначале создан
eventfd а потом я сделал клон процесса то воткак это выглядиь

 R 눞  $ cat /proc/2323571/fdinfo/3
pos:  0
flags:   02
mnt_id:  17
ino:  1090
eventfd-count:                0
eventfd-id: 279 <==== ****
eventfd-semaphore: 0
 5 ᇪ  $ 
 / 惻  $ cat /proc/2323522/fdinfo/3
pos:  0
flags:   02
mnt_id:  17
ino:  1090
eventfd-count:                0
eventfd-id: 279  <==== ****
eventfd-semaphore: 0

тоесть видно что у них это одно и тоже число. 

а потмо  я взял вначале клонировал процесс а потом уже вгутои кажодого индиудально созвадвал
этот обьект и вот как он овыгляди тв этом случае

  $ cat /proc/2323213/fdinfo/3
pos:  0
flags:   02
mnt_id:  17
ino:  1090
eventfd-count:                0
eventfd-id: 279 <==== ***
eventfd-semaphore: 0

$ cat /proc/2323214/fdinfo/3
pos:  0
flags:   02
mnt_id:  17
ino:  1090
eventfd-count: 1020304050607080
eventfd-id: 281  <==== ***
eventfd-semaphore: 0


тоесть тут уже разные числа. потому что эта два разных обьекта. 

ксстати вот это поле 
eventfd-count: 1020304050607080
оно покзывает какие 8 байт в hex виде сйчас лежат вэтом обьекте.


таким оразом я возвращаюсь к этой штуке

$ sudo lsof -Pnp 468
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   468 www-data    9u  a_inode               0,14        0   9609 [eventfd:9]


$ sudo lsof -Pnp 469
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   469 www-data   11u  a_inode               0,14        0   9609 [eventfd:8]


и так как 8 и 9 это разные числа я делаю вывод что это два разных оьекта. и поэтому 
эти файлы не служат для синхронзации между воркерами.


кстати еше я скажу такую вещь я об этом говрю в epoll.txt о том что 

$ sudo lsof -Pnp 468
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   468 www-data    8u  a_inode               0,14        0   9609 [eventpoll:5,7,9]


$ sudo lsof -Pnp 469
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   469 www-data   10u  a_inode               0,14        0   9609 [eventpoll:5,9,11]


если мы посмотрим на номем NODE в обоих воркерах для epoll то увидим что он один и  тотже
NODE=9509 из за этого может возникнуть идлллюзия что якобы оба воркера смотрят на один 
и тот же инстанс еполл в ядре. так вот нет. эт хуйня поланя. на саоммо дело каджй из них
смотрит нА СВОЙ ИНДИВДУЛАЬНЫЙ ЕПОЛЛ ИСНТАНС. просто у ядра какая бага и хуйня что совереенно
выглядит одинаково если у нас два прцоесса смотрят на один еполл или на два разных то все равно
всегда NODE при этом один и тот же так что непонять. но  я реально ганаортирую что это два
разных еполл инстанса.  это даже можно дооказать тем что если бы это был один и тот же еполл
инстанс то у него тогда в обоих воркерах список дексриторов за котоырм он следит был бы один 
и тотже. а унас разные.

если мы занялнем в fdinfo для этих дескрипров
то здесь к соаждению тоже мы ненайдем каких то отличий.

# cat  /proc/468/fdinfo/8
pos:  0
flags:   02
mnt_id:  15
ino:  9609
tfd:        9 events: 80000019 data:     55f7c5ce81a0  pos:0 ino:2589 sdev:e
tfd:        7 events:     2019 data:     7f18fbd12100  pos:0 ino:40dc sdev:8
tfd:        5 events: 10000019 data:     7f18fbd12010  pos:0 ino:40cf sdev:8

# cat  /proc/469/fdinfo/10
pos:  0
flags:   02
mnt_id:  15
ino:  9609
tfd:        9 events:     2019 data:     7f18fbd12100  pos:0 ino:40de sdev:8
tfd:        5 events: 10000019 data:     7f18fbd12010  pos:0 ino:40cf sdev:8
tfd:       11 events: 80000019 data:     55f7c5ce81a0  pos:0 ino:2589 sdev:e

так что доказательнство того что эт два разных инстанса еполл в том что мы видим
что у низ разные списки дсерпиторов за котоыим они следят.




c открытыми файлами разобрались. теперь я запускаб стрейс для всех трех 
процессов. и запускаю http реквест. 

значит вот я запустлв вот такой реквест

   $ curl http://localhost:8888/index.html

вот трейс мейн процесса 465
# strace -p 465
strace: Process 465 attached
rt_sigsuspend([], 8

тоесть он в обработке поткоа не принмиает нкиакого участия. насколько я понима он 
только играет роль если прилетит какойто сигнал в процесс. тогда он будет его обрабаываеть
а к обработке конектов из сети он неимет никакого отношения.

# strace -p 468
strace: Process 468 attached
epoll_wait(8, 

тоесть один из воркеров он тоже нихрена ничего не сделал.

и вот наконец воркер который и сделал всю работу

# strace -p 469
epoll_wait(10, [{events=EPOLLIN, data={u32=4224786448, u64=139745280729104}}], 512, -1) = 1
accept4(5, {sa_family=AF_INET, sin_port=htons(46518), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 7
epoll_ctl(10, EPOLL_CTL_ADD, 7, {events=EPOLLIN|EPOLLRDHUP|EPOLLET, data={u32=4224786928, u64=139745280729584}}) = 0
epoll_wait(10, [{events=EPOLLIN, data={u32=4224786928, u64=139745280729584}}], 512, 60000) = 1
recvfrom(7, "GET /index.html HTTP/1.1\r\nHost: "..., 1024, 0, NULL, NULL) = 88
newfstatat(AT_FDCWD, "/var/www/html/index.html", {st_mode=S_IFREG|0644, st_size=615, ...}, 0) = 0
openat(AT_FDCWD, "/var/www/html/index.html", O_RDONLY|O_NONBLOCK) = 8
newfstatat(8, "", {st_mode=S_IFREG|0644, st_size=615, ...}, AT_EMPTY_PATH) = 0
setsockopt(7, SOL_TCP, TCP_CORK, [1], 4) = 0
writev(7, [{iov_base="HTTP/1.1 200 OK\r\nServer: nginx/1"..., iov_len=327}], 1) = 327
sendfile(7, 8, [0] => [615], 615)       = 615
write(4, "127.0.0.1 - - [08/Jan/2025:20:43"..., 96) = 96
close(8)                                = 0
setsockopt(7, SOL_TCP, TCP_CORK, [0], 4) = 0
epoll_wait(10, [{events=EPOLLIN|EPOLLRDHUP, data={u32=4224786928, u64=139745280729584}}], 512, 75000) = 1
recvfrom(7, "", 1024, 0, NULL, NULL)    = 0
close(7)                                = 0
epoll_wait(10, 


значит берем первую строчку
epoll_wait(10, [{events=EPOLLIN, data={u32=4224786448, u64=139745280729104}}], 512, -1) = 1

у нас сработал epoll_wait тоест он вернлся потому что произошел ивент на которы мы были подписаны.
из того что мы видим четко ясно что произошло событие EPOLLIN токсть на каойто сокет приелетли
байты. но к сожалению из вот этой штуки data={u32=4224786448, u64=139745280729104}} не понятно
на каком десрипторе это произошло. вот эта концовка "= 1" гвоорит о том что событие произошло
толко на одном дескрипторе.  дело в том что походу это показано второе поле в структуре 
epoll_event которое ядро возвратило нам на событие по этому дескриптору. дело  в том что второе
поле это union поэтому оно может хранить до 8 байт данных причем совершенно разного типа.
щас покажу

вот у нас сам epoll_event

 struct epoll_event {
           uint32_t      events;  /* Epoll events */
           epoll_data_t  data;    /* User data variable */
       };

а его второе поле это union 

       union epoll_data {
           void     *ptr;
           int       fd;
           uint32_t  u32;
           uint64_t  u64;
       };

и внутри него может лежать либо поинтер 8 байт. либо это 4 байтовая хрень которая как бы сообой
представляет целое число но со знаком.(int) , либо это 4 байтовое число котоое собой как 
бы предсталет целое без знака (uint32_t), либо это 8 байтво число кторое как бы кодирует
целое число без ззнака (uint64_t ) 
поэтому что там внутри на саоммо деле незивсетно. 
и сам strace тжое непонимает. он просто берет 8 байт этой субстанции и нам на экране
это показывает в интерпетации как если бы это было целое 4 байтое число без знака
или 8 байтовое целое число без знака
   {u32=4224786448, u64=139745280729104}

вот у меня прогармма ест 377.exe и 378.exe так вот там я пишу в это второе поле 4 байтовое
целое число со знаком а по факту я туда пишут номер деискриптора. и тогда у меня на экране
в стрейс реально вылезает интпретаация в виде числа дескриптора

epoll_wait(4, [{events=EPOLLRDHUP, data={u32=5, u64=132005819842565}}], 3, -1) = 1

тоесть u32=5
ну потмоу что int 5 и uint32_t 5 выглядят в боайтовом виде одинаково (да я считаю что u32 это
uint32_t )

так как в нашем примере 
epoll_wait(10, [{events=EPOLLIN, data={u32=4224786448, u64=139745280729104}}], 512, -1) = 1

мы видим что u32=4224786448 это какое то огомроне число это значит что жинкс во второе поле
структуры event_poll запиывает чтото другое. точно не номер дескриптора. наприме он туда 
запмвает (и имеет на это право) поинтер. (чтоб понять о чем я говорю прочиатй статью про
union "с-union.txt" и про epoll  "epoll.txt")

но последующей сторчке можно догатдаться на каком дескрипроторе произошел ивент

accept4(5, {sa_family=AF_INET, sin_port=htons(46518), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 7

очевидно произошел на десрктторе 5 
а что это за дескриптор 5

$ sudo lsof -Pnp 469
COMMAND PID     USER   FD      TYPE             DEVICE SIZE/OFF   NODE NAME
nginx   469 www-data    5u     IPv4              16591      0t0    TCP 127.0.0.1:8888 (LISTEN)
nginx   469 www-data    6u     unix 0x00000000d420067f      0t0  16603 type=STREAM (CONNECTED)
nginx   469 www-data    9u     unix 0x000000002213b227      0t0  16606 type=STREAM (CONNECTED)
nginx   469 www-data   10u  a_inode               0,14        0   9609 [eventpoll:5,9,11]
nginx   469 www-data   11u  a_inode               0,14        0   9609 [eventfd:8]

а это дескриптор слушающего сокета. тоесть на него пртелет новый конект. и еполл
это дело словил.

еще раз смотрим на это выржание
accept4(5, {sa_family=AF_INET, sin_port=htons(46518), sin_addr=inet_addr("127.0.0.1")}, [112 => 16], SOCK_NONBLOCK) = 7

из чего видно что мы принмиаем этот конект. создаем через ядро новый сокет под этот конект.
номер его дескриптора 7.  видим IP адрес и порт удаленного ремот пира 
sin_port=htons(46518), sin_addr=inet_addr("127.0.0.1") ( поповоду аргументов у accept
читай в c-accept.txt)

по поводу  SOCK_NONBLOCK прочти "sock_nonblock.txt"

<<<<<<< ПРОДОЛЖЕНИЕ СЛЕДУЕТ >>>>>>




