| etcd

опишу как его ставить и что значат его ебанутые ключи


идем на гитхаб и качаем бинарники

  https://github.com/etcd-io/etcd/releases/


раскопокываем в /usr/local/bin
добавляем этот путь в $PATH

получеаем

# ls -1 /usr/local/bin
etcd
etcdctl
etcdutl



далее etcd демон он на 2379 порту принимает запросы от клиентов 
а на 2380 порту он разговривает с другими etcd нодами (пирами peer)

2379  клиент порт
2380  peer   порт 



мы хотим создать кластер из нескольких нод.
я покажу на примере когда мы создаем кластер из ДВУХ нод
а потом я его расширю налету до ТРЕХ нод.

на первой ноде я имею локальный ip

 ip = 172.16.80.20

и запускаю


etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \
  \
  --listen-peer-urls http://172.16.80.20:2380 \
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.80.20:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state new \
  \
  --data-dir  /root/ETCD/infra0


сразу замечу что в /etc/hosts ничего прописывать ненужно   (никаких infra0 прописывать ненужно),
также замечу что в etcd кластер неимеет никакого имени как это обычно бывает в других
кластерных программах,
теперь поясняю за ключи

  --listen-peer-urls http://172.16.80.20:2380 \

вот эта настройка она говорит процессу etcd на какую сет карту повесить сетевой сокет
на котором она будет слушать порт 2380 по которому другие ноды могу связываться с этой 
нодоы (внутри кластерный служебный трафик) , тоесть в данном случае мы говорим чтобы 
ectcd слушать 2380 порт на сет карте где сидит ip=172.16.80.20 
более того. эта настока также указыват какой протокол L7 должны использововать удаенный ноды
при обращении на этот сокет. тоесть в даннмо случае это HTTP
еще раз 
эта настройка она указывает что etcd доолжен создать
слушающий сокет на tcp порту 2380  на той карте где сидит ип=172.16.80.20
и то что другие ноды будут звонить по протоколу http

итак после этой настройке у нас будет вот такое

# ss -tnlp | grep -E "State|2380"
State  Recv-Q Send-Q Local Address:Port Peer Address:PortProcess                          
LISTEN 0      4096    172.16.80.20:2380      0.0.0.0:*    users:(("etcd",pid=804,fd=3))   

тоесть etcd создал слушающий сокет TCP на порту 2380 и слушает его. (плюс как  я сказал
также эта настройка задает протокол уровня L7 на котором они должны общаться в данном
случае это HTTP)


итак я возвращаюсь к команде


etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \
  \
  --listen-peer-urls http://172.16.80.20:2380 \
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.80.20:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state new \
  \
  --data-dir  /root/ETCD/infra0






следущий важный ключ это 

  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 

он описывает вот что. он опиывает какие ноды входят в состав кластера. это нужно один 
раз на начальном этапе создания кластера когда ноды должны свзяться друг с другом чтобы оброзаться кластер и записать настрйки на свои локальные диски. так вот наша нода должна знать какие
ноды входят в состав нового кластера и как до тех других нод достучаться и это
все описано в этой опции,
так вот здесь указыается список нод
и их параметры

 1) имя ноды infra0
 2) на какой IP и TCP порт нужно звонить чтобы достучаться до этой удаленной ноды 
    в данном случае это TCP 2380 и 172.16.80.20 и что нужно юзать L7 HTTP

 3) имя следующей ноды infra1
 4) ip и порт этой удаленной ноды чтобы знать куда звонить TCP 172.16.80.21:2380
    и L7 протокол HTTP юзать

и в этой опции важно чтобы для каждой ноды 

  infra0=http://172.16.80.20:2380

это значение совпадало по значению с двумя другими параметрами

  --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \


почему так. потому что  на второй ноде этот параметр --initial-cluster 
будет тоже прописан точно также

  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 

и что сделает нода infra1 при запуске. она заглянет в этот параметр. найдет там


   infra0=http://172.16.80.20:2380

пропарсит его и позвонит на   http://172.16.80.20:2380 и наша нода infra0 она в ответ
отправит два параметра 


  --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \


и нода infra1 она начнет сверять совпдают ли эти предоставленые параметры
с тем что указано в ее конфиге в ключе --initial-cluster

  --initial-cluster infra0=http://172.16.80.20:2380,...


если не совдаадает то infra1 пошлет эту удаленный ноду нахер и в состав кластера ее не 
включит. тоесть в параметре


  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 


прописано и куда звонить на другие ноды и прописано ЧТО другие ноды должны ответить
в качестве своей самоидентификации. а наша нода должна это сверить с тем что
указано в этом --initial-cluster
тоесть еще раз. у нас есть нода infra1 в ней указано

  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 


тогда наша нода звонить на http://172.16.80.20:2380 и ожидает что в ответ 
в качесвет иднетикации удаленная нода вернет вот такие поля


  --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \


тогда она их сверить с тем что у нас прописано в 


  --initial-cluster infra0=http://172.16.80.20:2380


тоесть увидит что 

 
  --name infra0  =   --initial-cluster infra0

и что 

  --initial-advertise-peer-urls http://172.16.80.20:2380  =   --initial-cluster ...http://172.16.80.20:2380


и тогда наша нода понимает что она достучалась до нужной ноды а не просто какогото мусорогого
хоста



таким образом параметр 


  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 


указывает ноде КУДА надо звонить чтобы достучаться до других нод кластера 
и ЧТО эти ноды должны вернуть обратно в качестве самоидентикиации перед нашей нашей
нодой.


а параметры

  --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \


указывают ноде что ей нужно слать в ответ если снаружи от другой ноды прилетит
запрос чтобы наша нода представилась. тоесть это параметры самоидентификации которые
наша нода отправляет по запросу во внешний мир.

также подчеркну что параметр 

  --initial-advertise-peer-urls http://172.16.80.20:2380 \


он соврешенно не заставляет etcd на этой ноде создавать данный слушающий сокет. НЕТ.
это лишь параметр ДЛЯ ОТВЕТА на внешний запрос.
а параметр который заставляет etcd создать слушающий сокет это 

  --listen-peer-urls http://172.16.80.20:2380 \

как говортся почуствуй разницу предназначения этих двух параметров.

это значит то что в этих двух параметрах 

  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 

  --initial-advertise-peer-urls http://172.16.80.20:2380 \


вот эта хрень  http://172.16.80.20:2380 должна совпадать
но при этом она совершенно не обязана совдпатаь с тем что прописано вот тут 

  --listen-peer-urls http://172.16.80.20:2380 \

она может совпдать но это не обязательно. 

дело вот в чем. наш etcd может сидеть на NAT-ом. тоесть скажем наша вируталка
имеет ip=172.16.80.20 но для других нод она будет видна как ip=1.1.1.2
тогда мы должны сделать вот так


  --initial-cluster infra0=http://1.1.1.1:2380,infra1=http://172.16.80.21:2380 

  --initial-advertise-peer-urls http://1.1.1.1:2380 \

  --listen-peer-urls http://172.16.80.20:2380 \


тоесть на infra1 ноде будет стоять строка


  --initial-cluster infra0=http://1.1.1.1:2380,infra1=http://172.16.80.21:2380 


это заставить infra1 искать ноду infra0 через  http://1.1.1.1:2380 
при этом будет срабатыать между ними NAT на промежуточной железке
и превращать 1.1.1.1 в 172.16.80.20 
а на infra0 наш etcd создал слушающий сокет на 172.16.80.20:2380 

итак в этих двух параметрах

  --initial-cluster infra0=http://1.1.1.1:2380,infra1=http://172.16.80.21:2380 

  --initial-advertise-peer-urls http://1.1.1.1:2380 \


мы указыаем тот IP под которым другие ноды "видят" нашу ноду, тоесть под каким ip
внешний мир (другие ноды) видят нашу ноду


а в этом параметре


  --listen-peer-urls http://172.16.80.20:2380 \


мы указываем тот ip которы у нас фактчиески есть на нашей виртальной машине
на ее сетевой карте на которой etcd может создать слушающий сокет для междунодоового
взаимодействия


итак я возвращаюсь к команде которую мы запустили на первой ноде


etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \  ****
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \  ****
  \
  --listen-peer-urls http://172.16.80.20:2380 \  ****
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.80.20:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state new \
  \
  --data-dir  /root/ETCD/infra0



при этом звездочками **** я обозначил те параметры котоыре уже разьяснил,

следущий параметр

  --data-dir  /root/ETCD/infra0

он укзаыавет папку на диске где наш демон будет хранить базу данных и настройки

следущий параметр

  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \


он указывает демону на какой сет карте и какой порту создать слушающий сокет
на котором демон будет принимать запросы ОТ КЛИЕНТОВ. ранее мы обсуждали сокет
преданазначеный для трафика служебного от пиров. а этот сокет предназанчен для
принятия запросов от клиентов.
в данном случае для этой цели будет создано два сокета на тех картах где 
сидят ip   
    172.16.80.20
    127.0.0.1

на TCP порту 2379
и то что ожидается что клиенты будут обраатся через этот сокет через L7 HTTP протокол


теперь рассморим следущий параметр

  --advertise-client-urls http://172.16.80.20:2379 \

он недолжен совпадать с параметром   --listen-client-urls
он используется вот когда. наша нода сообщает другим нодам о себе этот параметр 

  --advertise-client-urls http://172.16.80.20:2379 \

и другие ноды фиксируют у себя этот момент. и также он прописыаывается в базе на диске.
и если клиент сделает ОСОБЫЙ ЗАПРОС например  etcdctl member list
то тогда этот параметр будет извлечен из базы и сообщен клиенту.
по факту в этом парамтере нужно укзаать тот IP и порт под которым можно от клиентов
достучаться до этой ноды. тоесть нужно учитывать если есть NAT на какойто промежуточной
железке между виртуалкой с etcd и клиентом. в конечном итоге нужно чтобы при 
обращении чреез ип и порт указанный в   

  --advertise-client-urls http://172.16.80.20:2379 \


мы попали на сокет указанный в параметре

  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \

в данном случае между клиентом и нодой нет NAT поэтому 

  172.16.80.20:2379

совпадает в обоих параметрах


итак еще раз возвращаюсь к команде которую мы ввели на первой ноде


etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \  ****
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \  ****
  \
  --listen-peer-urls http://172.16.80.20:2380 \  ****
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \  ****
  --advertise-client-urls http://172.16.80.20:2379 \  ****
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state new \
  \
  --data-dir  /root/ETCD/infra0   ****


звездами я выделил те параметры которые я уже обьяснил.

следущий параметр


  --initial-cluster-token etcd-cluster-1 


он используется как shered key между нодами когда они связыаются друг с другом.
это сделано как доп защита от того чтобы ноды разных кластеров случайно не связались 
друг с другом

следущий параметр 

  --initial-cluster-state new 

он указыает нашей ноде при старте о том что либо 
  
   1) наша нода участвует в создании кластера с нуля. тоесть его еще нет. мы его
      вот щас как раз создаем. 

   2) либо наша нода присоединяется к кластеру который уже существует. наща нода
      при этом новая пустая. а кластер уже существует. для этого этот параметр 
      должен быть выставлен как 

      --initial-cluster-state existing



дело в том что у нас при старте ноды может быть три ситуации. 
нода стартует и видит что у нее на диске пусто. значит она еше не входит в состав
ни одного кластера. тогда возможно два субварианта. либо те другие ноды тоже еще пустые
и мы создаем кластер поность с нуля. либо только наша нода пустая и она подкюается к 
нодам которые уже образовали кластер. 
вот поэтому этот парамтер имеет два возможных значения

  --initial-cluster-state new 
  --initial-cluster-state existing


если на диске уже есть база данных то этот парамтер при старте игнорируется 




etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \  ****
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \  ****
  \
  --listen-peer-urls http://172.16.80.20:2380 \  ****
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \  ****
  --advertise-client-urls http://172.16.80.20:2379 \  ****
  \
  --initial-cluster-token etcd-cluster-1 \  ****
  --initial-cluster-state new \  ****
  \
  --data-dir  /root/ETCD/infra0   ****


в целом получается я рассмотрел все параметры.
если на диске база уже есть то как я понял etcd при старте игнориурет вот 
эти параметры


etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \   <<<<---
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \   <<<<---
  \
  --listen-peer-urls http://172.16.80.20:2380 \  
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \  
  --advertise-client-urls http://172.16.80.20:2379 \   <<<<---
  \
  --initial-cluster-token etcd-cluster-1 \    <<<<---
  --initial-cluster-state new \    <<<<---
  \
  --data-dir  /root/ETCD/infra0   



потому что все они хранятся в базе на локальном диске.
возможно вообще все эти параметры кроме --data-dir  /root/ETCD/infra0   
харнятся в базе на диске после того как кластер образован и игнорируются
при старте сервиса

еще раз хочу заметить что в /etc/hosts ничего вносить ненадо чтобы все заработало.
все хранится либо в параметрах запуска либо в базе на диске



итак 
на первой ноде я имею локальный ip

 ip = 172.16.80.20

и запускаю


etcd --name infra0 \
  --initial-advertise-peer-urls http://172.16.80.20:2380 \
  \
  --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \
  \
  --listen-peer-urls http://172.16.80.20:2380 \
  \
  \
  --listen-client-urls http://172.16.80.20:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://172.16.80.20:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state new \
  \
  --data-dir  /root/ETCD/infra0



при этом у нас стартует программа. но кластер еще не образован.
тогда
на второй ноде я имею локальный ip

 ip = 172.16.80.21

и запускаю


 etcd --name infra1   \
 --initial-advertise-peer-urls http://172.16.80.21:2380  \    
 --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \
 --listen-peer-urls http://172.16.80.21:2380  \     
 --listen-client-urls http://172.16.80.21:2379,http://127.0.0.1:2379 \   
 --advertise-client-urls http://172.16.80.21:2379  \   
 --initial-cluster-token etcd-cluster-1   \
 --initial-cluster-state new  \   
 --data-dir  /root/ETCD/infra1


теперь у нас кластер должен был образоваться.
проверяем это.

проверяем какие мемберы входят в состав


# etcdctl member list --endpoints http://127.0.0.1:2379
47a835a883b86bfe, started, infra1, http://172.16.80.21:2380, http://172.16.80.21:2379, false
5a1e3e74478b7608, started, infra0, http://172.16.80.20:2380, http://172.16.80.20:2379, false

в параметре --endpoints указываю одну из хреней указанных на любой одной ноде
кластера в ее параметре

 --listen-client-urls http://172.16.80.21:2379,http://127.0.0.1:2379 \   

соовсввенно если я сижу прям на ноде то я могу заюзать 127.0.0.1
а если я сижу вне ноды то тогда надо юзать


# etcdctl member list --endpoints http://172.16.80.21:2379


в любом случае достоочной указать один ендпоинт одной ноды , все ендпоинты все 
нод укаыать ненужно. и мы в ответе мы уувидим список нод входящих в кластер.
кстати нода не ответит если кластер не сформировался или развалился. 

также можно сделать вывод в форме более информативной таблицы добавив ключ --write-out=table


# etcdctl member list --endpoints http://127.0.0.1:2379  --write-out=table
+------------------+---------+--------+--------------------------+--------------------------+------------+
|        ID        | STATUS  |  NAME  |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |
+------------------+---------+--------+--------------------------+--------------------------+------------+
| 47a835a883b86bfe | started | infra1 | http://172.16.80.21:2380 | http://172.16.80.21:2379 |      false |
| 5a1e3e74478b7608 | started | infra0 | http://172.16.80.20:2380 | http://172.16.80.20:2379 |      false |
+------------------+---------+--------+--------------------------+--------------------------+------------+

итак мы обращаемся только к одной ноде а видим инфо по всем членам кластера.
вроде как логично.
но здесь почемуто эти дебилы не показывают какая из нод являтся мастером. 
для этого нужна другая команда


#  etcdctl endpoint status --endpoints=http://172.16.80.20:2379  --endpoints=http://172.16.80.21:2379 --write-out=table
+--------------------------+------------------+---------+-----------------+---------+--------+-----------------------+-------+-----------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
|         ENDPOINT         |        ID        | VERSION | STORAGE VERSION | DB SIZE | IN USE | PERCENTAGE NOT IN USE | QUOTA | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | DOWNGRADE TARGET VERSION | DOWNGRADE ENABLED |
+--------------------------+------------------+---------+-----------------+---------+--------+-----------------------+-------+-----------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| http://172.16.80.20:2379 | 5a1e3e74478b7608 |   3.6.0 |           3.6.0 |   20 kB |  20 kB |                    0% |   0 B |      true |      false |         3 |          8 |                  8 |        |                          |             false |
| http://172.16.80.21:2379 | 47a835a883b86bfe |   3.6.0 |           3.6.0 |   20 kB |  16 kB |                   20% |   0 B |     false |      false |         3 |          8 |                  8 |        |                          |             false |
+--------------------------+------------------+---------+-----------------+---------+--------+-----------------------+-------+-----------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+

вот мы видим полезную колонку

+-----------+
| IS LEADER | 
+-----------+
|      true |
|     false |
+-----------+

если все верно работает то в кластере должен быть мастер и должен 
он быть ровно один.


дебилизм этой команды состоит в том что мы обязаны в --endpoints указать ВСЕ ноды
которые входят в кластер. тоесть это нетак что мы указываем одну ноду и получаем
инфо по всем нодам нет. вот сколько нод мы укажем столько строк в таблице 
мы и получим. 

еще одна дебильная проблема в том что у нас может быть несколько кластеров
и мы в параметре --endpoints можем указать одну ноду из одного кластера
а вторую из второго и из таблицы мы никак не увидим входят ли эти ноды в один 
кластер или в разные.
пиздец как тупо они сделали.


ТЕПЕРЬ еще раз подчеркну важный момент. если мы остановим сервис на ноде
и запустим заново

 etcd --name infra1   \
 --initial-advertise-peer-urls http://172.16.80.21:2380  \
 --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380 \
 --listen-peer-urls http://172.16.80.21:2380  \
 --listen-client-urls http://172.16.80.21:2379,http://127.0.0.1:2379 \
 --advertise-client-urls http://172.16.80.21:2379  \
 --initial-cluster-token etcd-cluster-1   \
 --initial-cluster-state new  \
 --data-dir  /root/ETCD/infra1


то так как на диске уже есть файлы уже записана база то сервис почти все параметры
указныне здесь ИГНОРИРИУЕТ. так что при повторном запуске etcd никакого нового кластера
образоываться НЕБУДЕТ.



ДАЛЕЕ.
теперь я добавлю налету динамически еще одну ноду в этот кластер.
перед тем как запустить сервис на третьей ноде нужно внести изменения 
в базу. для этого вначале нужно сделать запрос к кластеру и сказать ему о том что мы 
собираемся добавить +1 ноду.  тоесть нам надо как бы налету изменить вот этот параметр
прописаный уже в базе --initial-cluster

# etcdctl member add infra2 --peer-urls=http://172.16.80.22:2380 --endpoints http://172.16.80.20:2379
Member c9414b238017190b added to cluster 2904ef1066e801d0


здесь я указаал

  infra2 --peer-urls=http://172.16.80.22:2380

тоесть я указал то что было бы указано в параметре

  --initial-cluster    infra2=http://172.16.80.22:2380

если бы я добавлял новую ноду с самого начала создания кластера
теперь другие ноды знают минимально необходимый набор параметров как будет 
выглядеть новая нода

но пока что сервис на третьей ноде я незапустил. посмотрим что покажет мемберлист


# etcdctl member list --endpoints http://127.0.0.1:2379  -w=table
+------------------+-----------+--------+--------------------------+--------------------------+------------+
|        ID        |  STATUS   |  NAME  |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |
+------------------+-----------+--------+--------------------------+--------------------------+------------+
| 47a835a883b86bfe |   started | infra1 | http://172.16.80.21:2380 | http://172.16.80.21:2379 |      false |
| 5a1e3e74478b7608 |   started | infra0 | http://172.16.80.20:2380 | http://172.16.80.20:2379 |      false |
| c9414b238017190b | unstarted |        | http://172.16.80.22:2380 |                          |      false |
+------------------+-----------+--------+--------------------------+--------------------------+------------+

вот мы видим что третья нода показан как unstarted

команда endpoint status нам ничего нового не покажет потому что третья нода пока неподнята


теперь запускаем сервис на третьей ноде



 etcd --name infra2   \
 --initial-advertise-peer-urls http://172.16.80.22:2380  \
 --initial-cluster infra0=http://172.16.80.20:2380,infra1=http://172.16.80.21:2380,infra2=http://172.16.80.22:2380 \
 --listen-peer-urls http://172.16.80.22:2380  \
 --listen-client-urls http://172.16.80.22:2379,http://127.0.0.1:2379 \
 --advertise-client-urls http://172.16.80.22:2379  \
 --initial-cluster-token etcd-cluster-1   \
 --initial-cluster-state existing  \
 --data-dir  /root/ETCD/infra2



запускаем теперь мемберлист

# etcdctl member list --endpoints http://127.0.0.1:2379  -w=table
+------------------+---------+--------+--------------------------+--------------------------+------------+
|        ID        | STATUS  |  NAME  |        PEER ADDRS        |       CLIENT ADDRS       | IS LEARNER |
+------------------+---------+--------+--------------------------+--------------------------+------------+
| 47a835a883b86bfe | started | infra1 | http://172.16.80.21:2380 | http://172.16.80.21:2379 |      false |
| 5a1e3e74478b7608 | started | infra0 | http://172.16.80.20:2380 | http://172.16.80.20:2379 |      false |
| c9414b238017190b | started | infra2 | http://172.16.80.22:2380 | http://172.16.80.22:2379 |      false |
+------------------+---------+--------+--------------------------+--------------------------+------------+


теперь все впорядке

теперь посмотрим endpoint status

#  etcdctl endpoint status --endpoints=http://172.16.80.20:2379  --endpoints=http://172.16.80.21:2379 --write-out=table --endpoints=http://172.16.80.22:2379
+--------------------------+------------------+---------+-----------------+---------+--------+-----------------------+-------+-----------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
|         ENDPOINT         |        ID        | VERSION | STORAGE VERSION | DB SIZE | IN USE | PERCENTAGE NOT IN USE | QUOTA | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | DOWNGRADE TARGET VERSION | DOWNGRADE ENABLED |
+--------------------------+------------------+---------+-----------------+---------+--------+-----------------------+-------+-----------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+
| http://172.16.80.20:2379 | 5a1e3e74478b7608 |   3.6.0 |           3.6.0 |   20 kB |  16 kB |                   20% |   0 B |      true |      false |         3 |         10 |                 10 |        |                          |             false |
| http://172.16.80.21:2379 | 47a835a883b86bfe |   3.6.0 |           3.6.0 |   20 kB |  16 kB |                   20% |   0 B |     false |      false |         3 |         10 |                 10 |        |                          |             false |
| http://172.16.80.22:2379 | c9414b238017190b |   3.6.0 |           3.6.0 |   20 kB |  16 kB |                   20% |   0 B |     false |      false |         3 |         10 |                 10 |        |                          |             false |
+--------------------------+------------------+---------+-----------------+---------+--------+-----------------------+-------+-----------+------------+-----------+------------+--------------------+--------+--------------------------+-------------------+

как видно в эндпоинтах я указал все три ноды.
также видно что одна нода это мастер а остальные эти фоловеры


ТАКИМ ОБРАЗОМ ЗАДАЧА КАК РАЗВЕРНУТЬ КЛАСТЕР  С НУЛЯ И КАК В НЕГО НАЛЕТУ
ДОБАВИТЬ НОДУ И КАК ПРОСМОТРЕТЬ СТАТУС РАССМОТРЕНА



ДАЛЕЕ. в etcd есть встроенный перфоманс тест. выгдяит это вот так

#  etcdctl check perf --endpoints=http://172.16.80.20:2379   --write-out=table
60 / 60 [---------------------------------------------------------------------------------] 100.00% 1 p/s
PASS: Throughput is 150 writes/s
PASS: Slowest request took 0.075453s
PASS: Stddev is 0.006815s
PASS

как видно указано что данный кластер позволяет писать на скорости 150 событий 
в секунду. 

как сделать запись в базу

# etcdctl put key1 vasya  --endpoints=http://172.16.80.20:2379
OK

как прочитать из базы

# etcdctl get key1   --endpoints=http://172.16.80.20:2379
key1
vasya

поэтому как я понимаю вот эта строчка

PASS: Throughput is 150 writes/s

она говрит о том что можно 150 операций put провести в секунду.



Теперь. 
как посмотреть текущий перформанс кластера


# curl -#   http://172.16.80.20:2379/metrics | grep -E "etcd_disk_wal_fsync_duration_seconds|etcd_disk_backend_commit_duration_seconds" | grep -v '#'

etcd_disk_backend_commit_duration_seconds_bucket{le="0.001"} 0
etcd_disk_backend_commit_duration_seconds_bucket{le="0.002"} 2
etcd_disk_backend_commit_duration_seconds_bucket{le="0.004"} 64
etcd_disk_backend_commit_duration_seconds_bucket{le="0.008"} 489
etcd_disk_backend_commit_duration_seconds_bucket{le="0.016"} 555
etcd_disk_backend_commit_duration_seconds_bucket{le="0.032"} 573
etcd_disk_backend_commit_duration_seconds_bucket{le="0.064"} 573
etcd_disk_backend_commit_duration_seconds_bucket{le="0.128"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="0.256"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="0.512"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="1.024"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="2.048"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="4.096"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="8.192"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="+Inf"} 574
etcd_disk_backend_commit_duration_seconds_sum 3.6654326769999988
etcd_disk_backend_commit_duration_seconds_count 574
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.001"} 872
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.002"} 4948
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.004"} 8095
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.008"} 8794
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.016"} 8903
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.032"} 8906
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.064"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.128"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.256"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.512"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="1.024"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="2.048"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="4.096"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="8.192"} 8907
etcd_disk_wal_fsync_duration_seconds_bucket{le="+Inf"} 8907
etcd_disk_wal_fsync_duration_seconds_sum 20.516985662000017
etcd_disk_wal_fsync_duration_seconds_count 8907



обьясняю что вот этот кусок значит

etcd_disk_backend_commit_duration_seconds_bucket{le="0.001"} 0
etcd_disk_backend_commit_duration_seconds_bucket{le="0.002"} 2
etcd_disk_backend_commit_duration_seconds_bucket{le="0.004"} 64
etcd_disk_backend_commit_duration_seconds_bucket{le="0.008"} 489
etcd_disk_backend_commit_duration_seconds_bucket{le="0.016"} 555
etcd_disk_backend_commit_duration_seconds_bucket{le="0.032"} 573
etcd_disk_backend_commit_duration_seconds_bucket{le="0.064"} 573
etcd_disk_backend_commit_duration_seconds_bucket{le="0.128"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="0.256"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="0.512"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="1.024"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="2.048"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="4.096"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="8.192"} 574
etcd_disk_backend_commit_duration_seconds_bucket{le="+Inf"} 574


вот эта строка означает 

	etcd_disk_backend_commit_duration_seconds_bucket{le="0.004"} 64

что 64 операции ЗАПИСИ в базу были выполенны на скорости 4мс или даже быстрее
тоесть <=4ms

тоесть 
0 операций записи было сделано на скорости <=1ms
2 операции записи было сделано на скорости <=2ms
64 операции записи было сдлано на скорости <=4ms
555 операций записи было сделано на скорости <=16ms

соотсвтенно каждая следующая строчка она включает в себя предыдущие операции 
записи. 
вот эта строчка

  etcd_disk_backend_commit_duration_seconds_bucket{le="+Inf"} 574

она означает что всего было на данный момент с момента старта кластера выполенено 574
операций записи.  а далее уже идет более точная разбаловка.

тоесть из этой строчки мы видим 

    etcd_disk_backend_commit_duration_seconds_bucket{le="0.128"} 574

что все эти 574 операции прошли точно как максимум на скрости 128ms
из этих 574 операций записи мы видим что 64 операции прошли как максимум на
сокрости 4ms

   etcd_disk_backend_commit_duration_seconds_bucket{le="0.004"} 64



из этих 64 операций две операции прошли на скорости как максимум 2мс

  etcd_disk_backend_commit_duration_seconds_bucket{le="0.002"} 2


а вот эта разбаловка

etcd_disk_wal_fsync_duration_seconds_bucket{le="0.001"} 872
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.002"} 4948
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.004"} 8095
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.008"} 8794
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.016"} 8903
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.032"} 8906

показывает что например 4948 операций запись на диск в WAL (write ahead log)
прошли на скорости как максимум 2ms


прямой связи между число записей в базу и число записей на диск в WAL нет.
их число необязано совпадать. 

по физике новые значения вначале пишутся в WAL а уже только потом позже 
пишутся в базу . причем база она тоже на диске.
при старте сервиса можно указат чтобы WAL хранится в другой папке

etcd --data-dir /var/lib/etcd/data --wal-dir /var/lib/etcd/wal

таким образом можно под WAL выделить мелкий но быстрый диск NVME а под 
базу как я понимаю более медленный SATA SSD


СЛЕДУЩИЙ МОМЕНТ КОТОРЫЙ НУЖНО РАССМОТРЕТЬ ЭТО ДОБАВЛЕНИЕ TLS ХРЕНИ к кластеру

быстрое черновое описание про TLS

нужно сгенеириоовать сертифкаты. 
для этого вначале устанавливаем cfssl

https://github.com/cloudflare/cfssl

и ставим go  и make


потом качаем вот это 

https://github.com/etcd-io/etcd/tree/main/hack/tls-setup

заходим  этот файл req-csr.json и правим его


{
  "CN": "etcd",
  "hosts": [
    "localhost",
    "127.0.0.1",
    "172.16.10.25",
    "172.16.10.26",
    "172.16.10.27"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "O": "autogenerated",
      "OU": "etcd cluster",
      "L": "the internet"
    }
  ]
}



потом еще делаем в баше экспорт

export infra0=172.16.10.25
export infra1=172.16.10.26
export infra2=172.16.10.27


запускаем make

получаем кучу сертфикатов. только я не понял в чем разница между ними 
по мне это все одинаковый сертифкат толко имена разные

172.16.10.25-key.pem
172.16.10.25.pem
172.16.10.26-key.pem
172.16.10.26.pem
172.16.10.27-key.pem
172.16.10.27.pem
ca.pem
peer-172.16.10.25-key.pem
peer-172.16.10.25.pem
peer-172.16.10.26-key.pem
peer-172.16.10.26.pem
peer-172.16.10.27-key.pem
peer-172.16.10.27.pem


копруем их на вртуалки с etcd в папку /etc/ssl/etcd
удляем лишние сертфикаты
и переимноваыем вот так


ca.pem
client.key
client.pem
peer.key
peer.pem


теперь на каждой ноде запускаем вот так






etcd --name infra0 \
  --initial-advertise-peer-urls https://172.16.10.25:2380 \
  \
  --initial-cluster infra0=https://172.16.10.25:2380,infra1=https://172.16.10.26:2380,infra2=https://172.16.10.27:2380 \
  \
  --listen-peer-urls https://172.16.10.25:2380 \
  \
  \
  --listen-client-urls https://172.16.10.25:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.10.25:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state existing \
  \
  --data-dir  /root/ETCD/infra0 \
  \
  --client-cert-auth \
  --trusted-ca-file=/etc/ssl/etcd/ca.pem \
  --cert-file=/etc/ssl/etcd/client.pem --key-file=/etc/ssl/etcd/client.key \
  \
  --peer-client-cert-auth \
  --peer-trusted-ca-file=/etc/ssl/etcd/ca.pem \
  --peer-cert-file=/etc/ssl/etcd/peer.pem  --peer-key-file=/etc/ssl/etcd/peer.key







  etcd --name infra1 \
  --initial-advertise-peer-urls https://172.16.10.26:2380 \
  \
  --initial-cluster infra0=https://172.16.10.25:2380,infra1=https://172.16.10.26:2380,infra2=https://172.16.10.27:2380 \
  \
  --listen-peer-urls https://172.16.10.26:2380 \
  \
  --listen-client-urls https://172.16.10.26:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.10.26:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state existing \
  \
  --data-dir  /root/ETCD/infra1 \
  \
  --client-cert-auth \
  --trusted-ca-file=/etc/ssl/etcd/ca.pem \
  --cert-file=/etc/ssl/etcd/client.pem --key-file=/etc/ssl/etcd/client.key \
  \
  --peer-client-cert-auth \
  --peer-trusted-ca-file=/etc/ssl/etcd/ca.pem \
  --peer-cert-file=/etc/ssl/etcd/peer.pem  --peer-key-file=/etc/ssl/etcd/peer.key






  etcd --name infra2 \
  --initial-advertise-peer-urls https://172.16.10.27:2380 \
  \
  --initial-cluster infra0=https://172.16.10.25:2380,infra1=https://172.16.10.26:2380,infra2=https://172.16.10.27:2380 \
  \
  --listen-peer-urls https://172.16.10.27:2380 \
  \
  --listen-client-urls https://172.16.10.27:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.16.10.27:2379 \
  \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster-state new \
  \
  --data-dir  /root/ETCD/infra2 \
  \
  --client-cert-auth \
  --trusted-ca-file=/etc/ssl/etcd/ca.pem \
  --cert-file=/etc/ssl/etcd/client.pem --key-file=/etc/ssl/etcd/client.key \
  \
  --peer-client-cert-auth \
  --peer-trusted-ca-file=/etc/ssl/etcd/ca.pem \
  --peer-cert-file=/etc/ssl/etcd/peer.pem  --peer-key-file=/etc/ssl/etcd/peer.key





поидее кластер стартанул.


заходим на одну из нод еще раз и запускаем уже клиентский запрос


etcdctl --endpoints=https://172.16.10.25:2379 \
  --cacert=/etc/ssl/etcd/ca.pem \
  --cert=/etc/ssl/etcd/client.pem \
  --key=/etc/ssl/etcd/client.key \
  member list


если все окей то он сработает

83d91f7dcbb49093, started, infra0, https://172.16.10.25:2380, https://172.16.10.25:2379, false
afdd28cddf39f46b, started, infra1, https://172.16.10.26:2380, https://172.16.10.26:2379, false
e76182116c6204fd, started, infra2, https://172.16.10.27:2380, https://172.16.10.27:2379, false




следущий этап поставить к8. 
влезть  в него и добавить руками +1 ноду в кластер




