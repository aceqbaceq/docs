WDC WD10JPVX-08JC3T5 05.01A05

=== START OF INFORMATION SECTION ===
Model Family:     Western Digital Blue Mobile
Device Model:     WDC WD10JPVX-08JC3T5
Serial Number:    WD-WXG1E6498ZYE
LU WWN Device Id: 5 0014ee 6af883e8d
Firmware Version: 05.01A05
User Capacity:    1,000,204,886,016 bytes [1.00 TB]
Sector Sizes:     512 bytes logical, 4096 bytes physical     <====(!! это 512E диск !!!!! )
Rotation Rate:    5400 rpm
Device is:        In smartctl database [for details use: -P show]
ATA Version is:   ACS-2 (minor revision not indicated)
SATA Version is:  SATA 3.0, 6.0 Gb/s (current: 3.0 Gb/s)     <=====
Local Time is:    Sat Dec  6 07:07:03 2025 PST
SMART support is: Available - device has SMART capability.
SMART support is: Enabled
AAM feature is:   Unavailable
APM feature is:   Disabled
Rd look-ahead is: Enabled
Write cache is:   Enabled
DSN feature is:   Unavailable
ATA Security is:  Disabled, frozen [SEC2]
Wt Cache Reorder: Enabled

якобы 8MB Buffer Cache (оперативка)




вот этот ноутбучный диск 
=== START OF INFORMATION SECTION ===
Model Family:     Western Digital Blue Mobile
Device Model:     WDC WD10JPVX-08JC3T5
Serial Number:    WD-WXG1E6498ZYE
LU WWN Device Id: 5 0014ee 6af883e8d
Firmware Version: 05.01A05
User Capacity:    1,000,204,886,016 bytes [1.00 TB]
Sector Sizes:     512 bytes logical, 4096 bytes physical
Rotation Rate:    5400 rpm
Device is:        In smartctl database [for details use: -P show]
ATA Version is:   ACS-2 (minor revision not indicated)
SATA Version is:  SATA 3.0, 6.0 Gb/s (current: 3.0 Gb/s)
Local Time is:    Wed Dec 10 14:04:48 2025 PST
SMART support is: Available - device has SMART capability.
SMART support is: Enabled
AAM feature is:   Unavailable
APM feature is:   Disabled
Rd look-ahead is: Enabled
Write cache is:   Enabled
DSN feature is:   Unavailable
ATA Security is:  Disabled, frozen [SEC2]
Wt Cache Reorder: Enabled


его скорость линейного чтения на голом блочном устройстве   106 MB/s



диск имеет две настройки.
look ahead
write cache

# smartctl -g all /dev/da0
Rd look-ahead is: Enabled
Write cache is:   Enabled


лук ахед влияет вот на что.
отключаю лук ахед. читаю на iodepth=1


# smartctl --set=lookahead,off /dev/ada1
Read look-ahead disabled


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$(( 1024*1024)) --iodepth=1  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
Jobs: 1 (f=1): [R(1)][0.3%][r=46.0MiB/s][r=46 IOPS][eta 01h:52m:28s]
test: (groupid=0, jobs=1): err= 0: pid=1559: Mon Jan  5 20:39:06 2026
  read: IOPS=45, BW=45.7MiB/s (47.9MB/s)(958MiB/20975msec)


получаем некую базовую скорость лин чтения этого диска


теперь включаю лук ахед

# smartctl --set=lookahead,on /dev/ada1
Read look-ahead enabled


повторяю тест с iodepth=1

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$(( 1024*1024)) --iodepth=1  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=94, BW=94.5MiB/s (99.1MB/s)(1918MiB/20296msec)

и мы видим двукратную прибвку скорости. тоесть мы говорим прочти 1МБ а диск читает еще
1 МБ и кладет его в буфер. 



данный рараметр никак не влияет на тесты по иопсам я проверил.
чтоиз этой опции можно выжать. 
можно выключить  лук-ахед. заппутсить тест на лин чтение на iodepth=1
получить базовое значние лин скорости чтения диска.
потом включаем лук-ахед. и запускаем тот же самый тест. если скость прибавилась 
значит опция работает. 


диск вращается 5400 рпм тоесть в минуту. это 11мс на один оборот.
скорость чтения без лук ахед 45 МБ\с при запросе 1МБ.  значит один запрос отарабывает за 22мс.
тоесть за 2 оборота диска. 


какой еще можно сделать тест. 
можно проверить раотает ли NCQ очередь для этого диска. 
отключаем лук-ахед. 
в запросе выставляем iodepth=2
если видим скорость 90МБ\с значит на диск реально долетает 2 запроса сразу и 
он их обаратывает за 1 проход.


итак лук ахед нам попожмет понять долетают ли до диска очереди команд. 
или они до диска летят по одной штуке




врайт кеш работает вот как:
на диске ест две хрени. одна хрень это очередь команд (NCQ)
а есть место куда склаыатся данные которые нужно записать на пластины.
если кеш включен то когда с компа данные попадают  в этот кеш то диск сразу 
рапортует обратно что данные уже на пластинах. 
а если кеш выклюен то данные все равно попадают в кеш  
а дальше вот что - диск ссмотрит какие команды у него в NCQ и какие данные от этих команд
сидят в кеше на запись. далее диск пытается за один проход записать неколкьо команд с их 
данными на пластины. и когда он это сделает то он обратно сообщает в комп что данные 
записаны на пластины. тоесть еще раз. в первом сулучае как только данные с компа упали в
кеш то диск сразу сообает на комп что данные уже якобы на пластинах. во вторм случае
диск сообщит что данные на платине толко кога они реально  окажбутся на пластине.
при этом он не поонднму иопсу пишет на пластину за раз а пытается их агрегировать в макро
записи. 
и вот это на пратикке

вырубаю кеш
  # smartctl -s wcache,off /dev/ada1
Write cache disabled


пишу линуейно по 1МБ кусками и iodepth=1
#   fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((1024 *1024)) --iodepth=1 --runtime=10  --readwrite=write --numjobs=1  --group_reporting 
  write: IOPS=44, BW=44.8MiB/s (46.9MB/s)(510MiB/11391msec); 0 zone resets



далее пишк кусками по 1МБ но сразу пачкой таких запрсов.
# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((1024 *1024)) --iodepth=32 --runtime=10  --readwrite=write --numjobs=1  --group_reporting 
  write: IOPS=95, BW=96.0MiB/s (101MB/s)(1022MiB/10647msec); 0 zone resets

как видно скростт возрослла потому что диск за один прход пишет на пластины сразу
по два запроса!


тут я пишу запросами по 4КБ кусками. в первом сулчае по одному
а во втоом целыми гроздями. 

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((4 *1024)) --iodepth=1 --runtime=10  --readwrite=write --numjobs=1  --group_reporting 
  write: IOPS=86, BW=346KiB/s (354kB/s)(3576KiB/10344msec); 0 zone resets


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((4 *1024)) --iodepth=32 --runtime=10  --readwrite=write --numjobs=1  --group_reporting 
  write: IOPS=4382, BW=17.1MiB/s (18.0MB/s)(171MiB/10002msec); 0 zone resets


во втором случае так как у нас грозь то диск оьединяет эти грозди в макрозаписи
поэтмоу запись возсрола


а теперь я активрую кеш

# smartctl -s wcache,on /dev/ada1
Write cache enabled


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((4 *1024)) --iodepth=1 --runtime=10  --readwrite=write --numjobs=1  --group_reporting 
  write: IOPS=12.6k, BW=49.1MiB/s (51.5MB/s)(491MiB/10003msec); 0 zone resets


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((4 *1024)) --iodepth=32 --runtime=10  --readwrite=write --numjobs=1  --group_reporting 
  write: IOPS=20.8k, BW=81.4MiB/s (85.3MB/s)(814MiB/10004msec); 0 zone resets


и видно что запись кусков по 4КБ возросла драматически
почему. потому что как толко куски ппадают в кеш то диск сразу обратно сообщает 
что даные уже якобы на шпинделях.



важно заметить что даже при "выключенном кеше" у нас все равно есть ускорение
записи если прилетела пачка запросов. по факту кеш на запись никоогда не выключается.
изменется лишь алгоритм в какой момент диск обратно сообщает что якобы данные 
записаны на пластины.


исходя из этого тоже можно подумать как протестировать провал записи на полке Altos 200F

вначале нужно разобраться как рабоатет полка с голыми диском.
а уже потом поняв это переходить к понимаю как зфс раобтает в связке с этой
полкой.


далее я открыл вот что.
вот я сунул запрос в зфс. а зфс сует запросы в vdev пула.
так вот в зфс есть органичение сколько запосов зфс можно засунуть запросов 
на один вдев. напрмиер за чтение определяет вот это 

	# sysctl vfs.zfs.vdev.sync_read_min_active=2
	vfs.zfs.vdev.sync_read_min_active: 2 -> 2

	# sysctl vfs.zfs.vdev.sync_read_max_active=30
	vfs.zfs.vdev.sync_read_max_active: 3 -> 30


синк риды с точки зрения зфс шедулера это запросы на чтение от приложений юзера.
асинк риды это риды котоыре пуляет на диски сама зфс по своей иницативе.

по дефлту эти штуки равны 10

и вот как можно проверить эти штуки
запускаю тест

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=32  --runtime=20  --readwrite=randread --numjobs=1  --group_reporting


 	# iostat -d -x ada1  1
                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada1         187       0  23901.0      0.0   173     0     0   173   29 100 


и тут видно что qlen=29 а ada1 это vdev



такиеже перменные 
есть для иопсов на запись

	# sysctl vfs.zfs.vdev.sync_read_min_active=2
	vfs.zfs.vdev.sync_read_min_active: 2 -> 2

	# sysctl vfs.zfs.vdev.sync_read_max_active=30
	vfs.zfs.vdev.sync_read_max_active: 3 -> 30

чем длинее очередь на диске шпиндельном тем диск может их взять кучкой и за один 
проход обрабоать. с другой стороны каждй реквест может там "застрять".


запись в зфс раоатет так. прога шлет сколько то запросов ио 
тоесть у них есть размер запроса и число заопросвов в штуках

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$(( 8 *1024)) --iodepth=32  --runtime=10  --readwrite=write --numjobs=1  --group_reporting


здесь в зфс улаетает от проги 32 запроса размером 8КБ каждый.
зфс это все заглатывает и обратно наше проге сразу шлет что данные засписсаны.
далее зфс из этих данных фориртует макрогруппу. и далее зфс разбивает эту группу
на новые иопсы и рассылает их по vdev причем зфс учитывает что ей нельзя на 1 вдев 
слать больше чем 

[root@fr ~]# sysctl vfs.zfs.vdev.sync_write_min_active
vfs.zfs.vdev.sync_write_min_active: 10
[root@fr ~]# sysctl vfs.zfs.vdev.sync_write_max_active
vfs.zfs.vdev.sync_write_max_active: 10


покыаываю на прмиере.
у меня пул из одного вдев . один диск шпиндеьдель

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds-128K/fio.dat  --bs=$(( 128 *1024)) --iodepth=32  --runtime=10  --readwrite=write --numjobs=1  --group_reporting

прога шлет 32 запроса 128К размером.
зфс это все перехуяривает на вдев она шлет куски размером 1MB

^C                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada1           0     112      0.0 115126.0     0    88     0    88   10  99 

и также зфс шлет за раз неболее чем 10 таких запросов.

зфс выбирает каким кусками преобоарзыватть ихсодные запросы при записи на вдев
на основе

# sysctl -a | grep zfs | grep aggreg
vfs.zfs.vdev.aggregation_limit_non_rotating: 131072
vfs.zfs.vdev.aggregation_limit: 1048576

вот тут видно что зфс разреешено писать на шпиндель диски агрегироваными кусками до 1МБ
макс. из тестов я точно вижу что если у датасета рекордсайз 128К то на диски идет
загрегроанная запись кусками по 1МБ.

так как в конечно итоге запись идет на диск и как выяснислось кусками по 1МБ
то вот мой тест о том какая длина очереди нужна на диск при записи кусками по 1МБ
чтобы достичь макс скоорсти записи диска
как показал тест даже при iodeth=1 уже диск достигает макс скрости записи
#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((1024 *1024)) --iodepth=1  --runtime=20  --readwrite=write --numjobs=1  --group_reporting

и вот докатеьсво что ос на диск будет пулять нагрузку с qlen=1
# iostat -d -x ada1 1
                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada1           0     105      0.0 107091.8     0     9     0     9    1  99 


тоесть уже если я пишу на детесет 128К кусками по 128К и иодепф даже 1
то мы уже должны полностью выирать лин скрсть записи на диск по макумуму


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds-128K/fio.dat  --bs=$(( 128 *1024)) --iodepth=1  --runtime=10  --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=1054, BW=132MiB/s (138MB/s)(1328MiB/10074msec); 0 zone resets

                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada1           0     120      0.0 104955.7     0    72    30    72   10  97 

видно что один ио реквест(который релаьный тоесть 1МБ) при этом обслуживается на скорости 72мс



вообще почти что макс скоось записи достичгается если я пишу на диск даже кускми по 8КБ
и и иодепф=1

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1p2  --bs=$((8 *1024)) --iodepth=1  --runtime=20  --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=11.3k, BW=88.1MiB/s (92.4MB/s)(680MiB/7720msec); 0 zone resets



еще раз сехма такая . прога кидает сколькото иопсов (iodepth) и bs (арзмер запроса)
кидает в зфс. та принимает сразу отвечает что запиано. потом агрегирует эти запросы
и с новым bs и qlen кидает на vdev пула. когда запрос прилетает на диск то он 
попдает в кеш на запись и диск сразу сообщает что все записано. 

если рекорсдайз 128К то мешать запиать на диск с макс скорсью может только 
если на диск запросы летят строго по одному , плюс выключен кеш у диска, и потомждем
что диск пришет подтврежение записи.









НИЖЕ тесты зфс датасетов:


1M
лин запись
            ada1              da0 
KB/t   tps  MB/s KB/t   tps  MB/s 
1024   103 102.99  0.0     0  0.00 
1024   105 105.19  0.0     0  0.00 
 786   132 101.30  7.7    75  0.56 
1024   102 102.01  0.0     0  0.00 



                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada0           0       0      0.0      0.0     0     0     0     0    0   0 
ada1           0      99      0.0 101042.2     0   102     0   102   10 100 
da0            0       6      0.0    180.4     0     3     0     3    0   1 


ранд врайт

# iostat -d da0 ada1  1
            ada1              da0 
KB/t   tps  MB/s  KB/t   tps  MB/s 
78.4   178 13.63  8.7     1   0.00 
1018   101 100.28 0.0     0   0.00 
1024    99 98.97  32.0    30  0.94 
1024   108 108.00 32.0    22  0.69 
 818   116 92.70  9.3     76  0.69 
1024    95 94.61  34.1    15  0.50 
1024   101 101.03 32.0    18  0.56 
1024   102 102.22 32.0    16  0.50 







128К
лин запись

[root@fr ~]# iostat -d  1
            ada0             ada1              da0            pass0 
KB/t   tps  MB/s KB/t   tps  MB/s    KB/t  tps  MB/s KB/t   tps  MB/s 
97.0    25  2.34 72.3   262 18.47    51.7   54  2.72  0.0     0  0.00 
 0.0     0  0.00  726   132 93.54    6.7    79  0.52  0.0     0  0.00 
 0.0     0  0.00 1024   113 113.11   32.0    2  0.06  0.0     0  0.00 
 0.0     0  0.00 1024    98 97.79    32.0    2  0.06  0.0     0  0.00 
 0.0     0  0.00 1024   100 100.18   12.9   13  0.16  0.0     0  0.00 
 0.0     0  0.00  870   111 94.27    6.7    67  0.44  0.0     0  0.00 
 0.0     0  0.00 1024   103 102.89   0.0     0  0.00  0.0     0  0.00 
 0.0     0  0.00 1024   105 105.08   32.0    2  0.06  0.0     0  0.00 
 0.0     0  0.00  877   116 99.37    6.9    80  0.54  0.0     0  0.00 
 0.0     0  0.00 1024   102 101.57   32.0    2  0.06  0.0     0  0.00 
 0.0     0  0.00 1024    98 98.39    32.0    2  0.06  0.0     0  0.00 
 0.0     0  0.00 1024   102 101.89   9.1    14  0.12  0.0     0  0.00 
 0.0     0  0.00  728   107 76.26    6.8   122  0.81  0.0     0  0.00 



                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada0           0       0      0.0      0.0     0     0     0     0    0   0 
ada1           0     113      0.0 115716.9     0    89     0    89   10 100 
da0            0       5      0.0    147.6     0     3     0     3    0   1 


на ада1 скорость 103МБ\с , тназакций 100\с   , размер агрег записи 1М на диск
как видно на спешл тоже идет работа. транзации олько записи. сурпут мелкий. ипосы 13-122



пишем на зфс рандомно



# iostat -d  1
            ada0             ada1              da0            pass0 
KB/t   tps  MB/s KB/t   tps  MB/s KB/t    tps  MB/s KB/t   tps  MB/s 
96.8    22  2.09 87.7   238 20.39 50.8     49  2.45  0.0     0  0.00 
 0.0     0  0.00 1024   105 104.95 32.0    18  0.56  0.0     0  0.00 
 0.0     0  0.00  872   102 86.80 13.2    110  1.42  0.0     0  0.00 
 0.0     0  0.00 1008   109 107.25 32.0    22  0.69  0.0     0  0.00 
 0.0     0  0.00 1021   113 112.71 32.0    20  0.63  0.0     0  0.00 
 0.0     0  0.00 1024   102 102.38 32.0    20  0.62  0.0     0  0.00 
 0.0     0  0.00  871   111 94.08 10.6     88  0.91  0.0     0  0.00 
 0.0     0  0.00 1024   114 114.06 32.0    22  0.69  0.0     0  0.00 
 0.0     0  0.00 1024   104 104.02 32.0    17  0.54  0.0     0  0.00 



                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada0           0       0      0.0      0.0     0     0     0     0    0   0 
ada1           0     114      0.0  74778.9     0    51    35    50   10  90 
da0            0     189      0.0   1740.1     0     4     0     4    0  10 


паттерн такойже остался. и на основной диск и на спешл диск
срупут остался 108 МБ\с




по этим двум тестам видно вот что про зфс.
если рекордсайз 128К или 1М то в обоих случаях зфс пишет на диски кусками по 1МБ.  
чисто физически рисунок нагрузки на диски один и тотже. число иопсов тоже самое и всетакое.







теперь лин запись на рекрорсдет 4К


            ada0             ada1              da0            pass0 
KB/t   tps  MB/s KB/t   tps  MB/s KB/t   tps  MB/s KB/t   tps  MB/s 
 0.0     0  0.00 45.9  1965 88.14 14.9    121  1.76  0.0     0  0.00 
 0.0     0  0.00 47.9  2365 110.59 28.0    54  1.48  0.0     0  0.00 
 0.0     0  0.00 47.9  2020 94.55 28.0     48  1.31  0.0     0  0.00 
 0.0     0  0.00 46.5  1740 78.98 14.7    117  1.68  0.0     0  0.00 
 0.0     0  0.00 47.9  2223 103.91 28.0    52  1.42  0.0     0  0.00 
 0.0     0  0.00 47.1  1702 78.31 14.5    113  1.60  0.0     0  0.00 
 0.0     0  0.00 47.8  2180 101.78 28.0    52  1.42  0.0     0  0.00 
 0.0     0  0.00 47.9  2290 107.14 17.4   124  2.10  0.0     0  0.00 



                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada0           0       0      0.0      0.0     0     0     0     0    0   0 
ada1           0    2161      0.0 103525.9     0     5     0     5   10 100 
da0            0      51      0.0   1429.8     0     1     0     1    0   5 


срупут 93 МБ\с. тоесть почти неимзенися. выросло число траназкций на основной диск
было 100 ипос а стало 2000 иопс.  размер агрегированного размера записи на диск 46КБ\с

нагрузка на спешл неизменилась


теперь пишу рандом врайт на этот же датасет


            ada0             ada1              da0            pass0 
KB/t   tps  MB/s KB/t   tps  MB/s KB/t   tps  MB/s KB/t   tps  MB/s 
 0.0     0  0.00 47.5  2215 102.74 56.0   610 33.39  0.0     0  0.00 
 0.0     0  0.00 39.5   289 11.15 49.0    540 25.85  0.0     0  0.00 
 0.0     0  0.00 47.6  2152 100.03 57.6   600 33.74  0.0     0  0.00 
 0.0     0  0.00 42.0   600 24.62 47.0    603 27.69  0.0     0  0.00 
 0.0     0  0.00 47.0  1884 86.45 58.3    589 33.51  0.0     0  0.00 


                        extended device statistics  
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
ada0           0       0      0.0      0.0     0     0     0     0    0   0 
ada1           0       0      0.0      0.0     0     0     0     0    0   0 
da0            0     604      0.0  34952.8     0    77     0    77   28 100 
pass0          0       0      0.0      0.0     0     0     0     0    0   0 


срупут упал до 57.6 МБ\с. рамер агрегированный записи остался такойже 46КБ\с
зато значительно вырос срупут на спешл с коппеек до 34 МБ\с, хараетер нагрузки это запись,
средний размер транзации 56КБ\с








ДАЛЕЕ. старый материал

скорость линейного чтения из файла который записан на zfs dataset
с разными рекордсайзами. плюс каждый файл был максимально разорван на куски через cow
путем рандомврайта по всему телу.  чтения шло размером 1МБ (заказ в фио) в независимости 
от размера рекордсайза

fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/boot-pool/ds2-16K/fio.dat  --bs=$(( 1024 *1024)) --iodepth=1  --runtime=45  --readwrite=read --numjobs=1  --group_reporting


меняяем рекордсайз

i=1024
  BW=44.2MiB/s 


i=512
  BW=39.1MiB/s 


i=256
  BW=22.6MiB/s 


i=128
  BW=18.9MiB/s 


i=64
  BW=14.3MiB/s



i=32
  BW=7.6MiB/s 

i=16
  BW=4.2MiB/s 


i=8
  BW=2.4MiB/s


i=4
  BW=1.5MiB/s



итого видно что на 128К у нас остается 18% от голой скорости блочного устройства.
а если мы хотим в 2 раза болше то нужно увеличичвать рекордсайз до 512К



далее продолжение 


хочу скказат вот что - чем больше размер файла мы создаем. его размер его тела.
и чем меньше у нас блокФС тем больше нужно вложенность в b-tree метаданных дереве
в котором записано где искать на диске очередной блокФС. поэтому можно заметить такой момент
что для файла 10ГБ и блокаФС(рекордсайз) 4К чтобы прочесть 1 блокФС полезных данных то 
нужносчитать с диска 3 или 4 блока (по 4К) метаданных. поэтому можно заметить такой прикол
если мы читаем с диска блоками 4К с датасета с рекордсайзом 4К то у нас в фио трафик
срупута равен N а реально с диска считывается 3*N или 4*N. вот это в этом дело. если 
в арк нет всех метаданных то он будет читаь их с диска.  поэтому чем меньше у нас рекордсайз
у датасета и чем больше по размеру файл тем больше по факту места на диске будет занимать
не сам файл а его метаданные! тоесть чтобы хранить на диске файл 10ГБ по факту на диске
будет занято 30-40ГБ места! вот такое огромная потеря полезного места на диске!

ровно и из за этого падает срений размер блока чтения с диска делая его меньше
чем рекордсайз. это значит что у зфс в арке нет пока что метаданных от тех блоков которые
мы считываем. поэтому он их читает с диска. блоки метаданных имеют размер 4К (размер ашифта)
и это круто уменьшает средний размер иопса. вот в чем секрет






вначале про iodepth
что это такое.
это когда прога какимто макаром запульнула к ядру а оно в свою очеред к диску сразу 
несколько запросов. а прога после этого сидит и ждет возврата хотя бы одого ответа и 
доо тех пор новые запросы не шлет.
что такое  в жизни прижоение которое формирует на диск iodepth>1. значит прога открывает 
файл в неблокирующем режиме и делает запрос на прочтение блока. в пейдж кеще ядра этого
блока нет. так как файл открыт не блокирующе то ядро сразу вовзраещает управление программе
и она деает запрос еще к одному блоку. его тоже нет в пейж кеш ядра и ядро сразу возварщает
уравление прогармме. программа понимает что ни одного блока нет и больше спрашивать не собиарется
а скажем тогда даелает какито дургие дела или просит ее усыпить.. а ядро посылает два ио запроса на диск. с точки зрения програмиста логика такая.
я делаю запрос к блоку1. если он есь хорошо я его брабатываю. а если его нет то я делаю
запрос к еще одному блоку. если хотя бы один блок прилетит уже будет чем занться. а если
ни одного блока не будет то либо заснуть либо чтото другое делать. 

далеко не все engine движки fio позволяют делать iodepth>1
в линуксе такой это libaio с вклюенным флагом direct=1
в фрибсл это posixaio
если мы поставим iodepth=5 в связке с движклм psync то ошибка не вылезет но 
в выводе программы молча будем указано что iodepth  был все время равен 1.
щас покажу

# export BS_SIZE_KB=32; fio --randrepeat=1  --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1    --runtime=20  --readwrite=read   --group_reporting --loop=1    --ioengine=psync --iodepth=10 --bs=$(( "$BS_SIZE_KB" *1024)) 
test: (g=0): rw=read, bs=(R) 32.0KiB-32.0KiB, (W) 32.0KiB-32.0KiB, (T) 32.0KiB-32.0KiB, ioengine=psync, iodepth=10
fio-3.28
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=115MiB/s][r=3679 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=5142: Sat Dec  6 10:48:04 2025
  read: IOPS=3431, BW=107MiB/s (112MB/s)(2146MiB/20010msec)
   bw (  KiB/s): min=102934, max=118035, per=99.95%, avg=109765.56, stdev=5165.81, samples=39
   iops        : min= 3216, max= 3688, avg=3429.74, stdev=161.45, samples=39
  cpu          : usr=2.04%, sys=4.55%, ctx=68706, majf=0, minf=8
>>  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%  <<<
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=68670,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=10

Run status group 0 (all jobs):
   READ: bw=107MiB/s (112MB/s), 107MiB/s-107MiB/s (112MB/s-112MB/s), io=2146MiB (2250MB), run=20010-20010msec


вот эта строка

>>  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%  <<<

в ней видно что все время iodeth был равен 1.
также это можно увидеть в живую в gstat столбик  L(q)


dT: 1.002s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    0      0      0      0    0.0      0      0    0.0    0.0| ada0
    1   3648   3648 116725    0.3      0      0    0.0   93.8| ada1
    0      0      0      0    0.0      0      0    0.0    0.0| ada0p1
    0      0      0      0    0.0      0      0    0.0    0.0| ada0p2
    0      0      0      0    0.0      0      0    0.0    0.0| gptid/dd1693ec-d2b2-11f0-94



так что можно легко наебаться. нужно за этим следить. 
итак  в нашем случае юзаем posixaio iodetph >1


# export BS_SIZE_KB=4; fio --randrepeat=1  --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1    --runtime=20  --readwrite=read   --group_reporting --loop=1    --ioengine=posixaio  --bs=$(( "$BS_SIZE_KB" *1024)) --iodepth=1
test: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=posixaio, iodepth=1
fio-3.28
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=57.6MiB/s][r=14.8k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=5184: Sat Dec  6 10:51:24 2025
  read: IOPS=14.7k, BW=57.5MiB/s (60.2MB/s)(1149MiB/20003msec)
   bw (  KiB/s): min=55514, max=59320, per=100.00%, avg=58893.92, stdev=754.22, samples=39
   iops        : min=13878, max=14830, avg=14723.08, stdev=188.53, samples=39
  cpu          : usr=12.89%, sys=22.86%, ctx=294634, majf=0, minf=2
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=294206,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=57.5MiB/s (60.2MB/s), 57.5MiB/s-57.5MiB/s (60.2MB/s-60.2MB/s), io=1149MiB (1205MB), run=20003-20003msec



флаг --direct=1 сообщает ядру чтобы оно нам при выаче контенте выдалвало инфо с диска
 а не из пейджкеша. 


публикую команду котоая будет юзаться для теста LINEAR READ


#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 1024 *1024 ))  --iodepth=1 --offset=0  --runtime=30  --readwrite=read --numjobs=1  --group_reporting --loop=1  


#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 1024 *1024 ))  --iodepth=32 --offset=0  --runtime=30  --readwrite=read --numjobs=1  --group_reporting --loop=1  






LINEAR READ 

	1 iodepth
		4k :  57.2 MB\s
		8k :  94.5 MB\s
		16k:  105  MB\s
		32k:  107 MB\s
		128k: 107 MB\s
		1M:   107 MB\s


	bs 16K
		iodepth 1 : 105 MB\s
		iodepth 2:  107 MB\s  
		iodepth 4:  107 MB\s
		iodepth 32: 107 MB\s  


	bs 1M
		iodepth 1 : 107 MB\s
		iodepth 32: 107 MB\s  


видно что прошивка на диске тупая. она вобще походу неумеет эфективно анализировать
запросы в своей NCQ очереди. поэтому никакой разницы один запрос прилетел  в очредь на диск
или 32 да еще и линейных для прошивки диска нет. так что этот конкретно диск
можно в плане линейнного чтения споеойно брать bs=8K. 
что иодпепф равен 1 что 32 диску похеру. походу он каждый запрс из своей очереди 
обрабатывает тупо один за одним. 


можно запустить много воркеров. которые будут запрашивать блоки которые уже запросил
другой воркер. и тогда диск будет находитть этот блок своем буфере и отдавать его.
так можно по приколу получить из диска поток 200МБ\с. 

# export BS_SIZE_KB=128; fio --randrepeat=1  --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1    --runtim=20  --readwrite=read   --group_reporting --loop=1    --ioengine=posixaio --iodepth=1 --bs=$(( "$BS_SIZE_KB" *1024))   --offset=0   --offset_increment=$(( "$BS_SIZE_KB" *1024))  --numjobs=32

  BW=236MiB/s

но еще раз подчекрун что часть байтов диск отдает не читая с шнпинделей а забирая с своего кеша.
поэтому 236.
при том что у нас порт 3Gb\s. у него макс скорость полезной нагрузки 240-270МБ\с



шаблон команды для теста LINEAR WRITE
# export BS_SIZE_KB=1024; fio --randrepeat=1  --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1    --runtime=20  --readwrite=write   --group_reporting --loop=1    --ioengine=posixaio  --bs=$(( "$BS_SIZE_KB" *1024)) --iodepth=32






LINEAR WRITE
	1 iodepth
		4k :  46.5 MB\s
		16k:  75.8 MB\s
		24k:  107 MB\s
		32k:  107 MB\s
		128k: 107 MB\s
		1M:   107 MB\s


	bs 16K
		iodepth 1 :  75.8 MB\s
		iodepth 2:   MB\s  
		iodepth 4:   MB\s
		iodepth 32: 107 MB\s  


	bs 32K
		iodepth 1 : 107 MB\s
		iodepth 2:   MB\s  
		iodepth 4:   MB\s
		iodepth 32: 107 MB\s  


	bs 128K
		iodepth 1 :  MB\s
		iodepth 2:   MB\s  
		iodepth 4:   MB\s
		iodepth 32:  MB\s  


	bs 1M
		iodepth 1 : 107 MB\s
		iodepth 2:   MB\s  
		iodepth 4:   MB\s
		iodepth 32: 107 MB\s  



кстати лин запись такая высокая условно потому что запись идет в RAM диска. 
и оттуда собщает что иопс записан. если вырубить этот кеш. то сурупут записи станет
драматически низким. хе-хе.


ИТОГО: можно брать блок 16К и будет нормал

СУММАРНО:
Итого с точки зрения лин чтение можно брть 8К . сточки зерняи лин записи 16К.
суммарно можно брать 16К.






ОТОЙДЕМ В СТОРОНУ.
малек отойем в сторону. проестируем read-modify-write
тоесть потрею скорости на этом диске за счет того что он вытсавляет в ОС что у него
якобы физ блок 512 байт хотя по факту у него физ блок 4096 байт.
я сделал вот что я выключил кеш буфер на диске.

		# smartctl --set=wcache,off /dev/ada1

теперь тест
#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=0  --runtime=20  --readwrite=write --numjobs=1  --group_reporting --loop=1  
Jobs: 1 (f=1): [W(1)][0.0%][w=358KiB/s][w=89 IOPS][eta 32d:20h:17m:13s]


#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=2048  --runtime=20  --readwrite=write --numjobs=1  --group_reporting --loop=1  
Jobs: 1 (f=1): [W(1)][0.0%][w=90KiB/s][w=22 IOPS][eta 121d:14h:56m:23s]

видим что срупут упал в х4 раза когда алинмент нарушился!

а вот в случае когда пишем 1МБ. падение не такое крутое.
япочемуто думал что он будет по одному блоку читать . потом модиифиовать. и будет мегапровал.
но походу он умный. он читает сразе все блоки. потом в памяти модиифире. и потом сразу запиывает.
поэтому провал всего в 2 раза.




#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 1024 *1024 ))  --iodepth=1 --offset=0  --runtime=30  --readwrite=write --numjobs=1  --group_reporting --loop=1  
Jobs: 1 (f=1): [W(1)][100.0%][w=49.1MiB/s][w=49 IOPS][eta 00m:00s]


#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 1024 *1024 ))  --iodepth=1 --offset=2048  --runtime=30  --readwrite=write --numjobs=1  --group_reporting --loop=1  
Jobs: 1 (f=1): [W(1)][0.1%][w=22.7MiB/s][w=22 IOPS][eta 12h:02m:06s]



потеря перфоманса однозначно есть и видна.
прикол в том что если включить буфер кеш то потеря хаха невидна вобще нихера . а нас все
время пугали.


   # smartctl --set=wcache,on /dev/ada1



]#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=0  --runtime=20  --readwrite=write --numjobs=1  --group_reporting --loop=1  
  write: IOPS=12.7k, BW=49.8MiB/s (52.2MB/s)(399MiB/8012msec); 0 zone resets

#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=2048  --runtime=20  --readwrite=write --numjobs=1  --group_reporting --loop=1  
  write: IOPS=12.7k, BW=49.5MiB/s (51.9MB/s)(649MiB/13106msec); 0 zone resets


как видно никакой потери перфоманса!


тогда  я делаю другой тест и буфер включен
я буду и писать и читать. посколку у нас врайты порожают дополтнительные риды то  у нас
если врайты не вырровнены у нас риды должны упасть.


#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=0  --runtime=60  --readwrite=write --numjobs=1  --group_reporting --loop=1  
  write: IOPS=6153, BW=24.0MiB/s (25.2MB/s)(1442MiB/60002msec); 0 zone resets



#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=0  --runtime=60  --readwrite=read --numjobs=1  --group_reporting --loop=1  
  read: IOPS=4691, BW=18.3MiB/s (19.2MB/s)(1099MiB/60001msec)

тоест пока у нас все окей с алинментом то 

      writes BW=24.0MiB/s  +  reads  BW=18.3MiB/s


теперь нарушаем элинмент


#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=2048  --runtime=60  --readwrite=write --numjobs=1  --group_reporting --loop=1  
  write: IOPS=4440, BW=17.3MiB/s (18.2MB/s)(1041MiB/60001msec); 0 zone resets



#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=0  --runtime=60  --readwrite=read --numjobs=1  --group_reporting --loop=1
  read: IOPS=3236, BW=12.6MiB/s (13.3MB/s)(758MiB/60004msec)



      writes BW=17.3MiB/s  +  reads  BW=BW=12.6MiB/s


видно что общий пермфосмнас упал в х1.4-х1.5 раза!
тоесть тут буфер уже не спассает.




провести эксеримент с рандом ридами и врайтами не получилось. почемуто ругается.
#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1  --bs=$(( 4 *1024 ))  --iodepth=1 --offset=1200  --runtime=60  --readwrite=randwrite --numjobs=1  --group_reporting --loop=1  
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
fio: io_u error on file /dev/ada1: Invalid argument: write offset=60325639344, buflen=4096
fio: first direct IO errored. File system may not support direct IO, or iomem_align= is bad, or invalid block size. Try setting direct=0.
fio: pid=4708, err=22/file:io_u.c:1845, func=io_u error, error=Invalid argument






ВОЗВРАЩАЕМСЯ К ОСНОВНОМУ ТЕСТУ:


шаблон команды для RANDOM READ
# export BS_SIZE_KB=16; fio --randrepeat=1  --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1    --runtime=20  --readwrite=randread   --group_reporting --loop=1    --ioengine=posixaio  --bs=$(( "$BS_SIZE_KB" *1024)) --iodepth=32



RANDON READ 
	1 iodepth
		4k :  61 IOPS
		16k:  60  IOPS
		32k:  60  IOPS
		128k: 55 IOPS
		1M:   35 IOPS



	bs 4K
		iodepth 1: 60 IOPS
		iodepth 2:  IOPS
		iodepth 4:  IOPS
		iodepth 32: 131 IOPS




	bs 16K
		iodepth 1: 60 IOPS
		iodepth 2: 69 IOPS
		iodepth 4: 81 IOPS
		iodepth 32: 139  IOPS


	bs 32K
		iodepth 1: 60  IOPS
		iodepth 2: 69  IOPS
		iodepth 4: 79  IOPS
		iodepth 32: 125 IOPS


	bs 128K
		iodepth 1: 55 IOPS
		iodepth 2:   IOPS
		iodepth 4:   IOPS
		iodepth 32: 110 IOPS


	bs 1M
		iodepth 1: 35 IOPS
		iodepth 2:   IOPS
		iodepth 4:   IOPS
		iodepth 32: 50 IOPS


здесь задача такая. нам нужно взять наиоболее вынгодный BS из тестов выше. 
и потом от него идти в сторону еще меньшего блока чтобы приблизится к той сокрости ИОПС 
которую дает скорость на блоке 4К.
выше мы постванили что нам нормал брать 16К
видно что перфманс 16К ничего нехуже чем преоманс 4К
значит можно в итоге брать 16К.



шаблон команды для теста RANDOM WRITE
# export BS_SIZE_KB=4; fio --randrepeat=1  --direct=1 --gtod_reduce=1 --name=test --filename=/dev/ada1    --runtime=20  --readwrite=randread   --group_reporting --loop=1    --ioengine=posixaio  --bs=$(( "$BS_SIZE_KB" *1024)) --iodepth=32




RANDON WRITE
	1 iodepth
		4k :  61 IOPS
		16k:  60  IOPS
		32k:  60  IOPS
		128k: 55 IOPS
		1M:   35 IOPS



	bs 4K
		iodepth 1: 60 IOPS
		iodepth 2:  IOPS
		iodepth 4:  IOPS
		iodepth 32: 131 IOPS




	bs 16K
		iodepth 1: 60 IOPS
		iodepth 2: 69 IOPS
		iodepth 4: 81 IOPS
		iodepth 32: 139  IOPS


	bs 32K
		iodepth 1: 60  IOPS
		iodepth 2: 69  IOPS
		iodepth 4: 79  IOPS
		iodepth 32: 125 IOPS


	bs 128K
		iodepth 1: 55 IOPS
		iodepth 2:   IOPS
		iodepth 4:   IOPS
		iodepth 32: 110 IOPS


	bs 1M
		iodepth 1: 35 IOPS
		iodepth 2:   IOPS
		iodepth 4:   IOPS
		iodepth 32: 50 IOPS




c точки рандом врайт мы сравнивая 16К и 4К можем спокойно брать 16К


ИТОГО :
суммарно для этого диска можно брать 16К. исходя из всех тестов.



суммарно скорости которые можно ожидать



LINEAR READ 


	bs 16K
		iodepth 1 : 105 MB\s
		iodepth 2:  107 MB\s  
		iodepth 4:  107 MB\s
		iodepth 32: 107 MB\s  




LINEAR WRITE

	bs 16K
		iodepth 1 :  75.8 MB\s
		iodepth 2:   MB\s  
		iodepth 4:   MB\s
		iodepth 32: 107 MB\s  




RANDON READ 



	bs 16K
		iodepth 1: 60 IOPS
		iodepth 2: 69 IOPS
		iodepth 4: 81 IOPS
		iodepth 32: 139  IOPS



RANDON WRITE


	bs 16K
		iodepth 1: 60 IOPS
		iodepth 2: 69 IOPS
		iodepth 4: 81 IOPS
		iodepth 32: 139  IOPS



ИТОГО: лин чтение запись ~ 100 MB\s
      рандом чтение запись  ~ 60-139 IOPS


---


я собрал zpool на базе этого диска.
и создал датасет с рекордсайз 128К.
записал на нем файл fio.dat 10G размером.

zfs create  -o compression=off -o  primarycache=metadata  -o secondarycache=metadata -o recordsize=128KiB  POOL-02/ds2-128K


далее я в него записал в рандомные блоки. чтобы файл разорвал как с динамита. чтоб блоки
рандомно ралзлетелись по диску.



# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/ds2-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1  --size=10G  --readwrite=randwrite --numjobs=1  --group_reporting --loop=5




далее я померил скорост линейного чтения


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-128K/fio.dat  --bs=$(( 1024 *1024 ))  --iodepth=1 --offset=0  --runtime=30  --readwrite=read --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1

  read: IOPS=18, BW=18.5MiB/s (19.3MB/s)(574MiB/31110msec)



тоесть за счет разорванности файла по диску лин скорость чтения оффсетов из файла
кусками  128К  на датасете с рекрордсайз 128К упала 
  с 105МБ\с (как эта есть сктрость у голого блочного устройтва) 
  до 18.5MiB/s  тоесть в 5.6 раза.


итак LINEAR READ из файла  на датасете с рекорд сайз 128К крадет скорость в 5.6 раза по сравнению
с линейным чтением с голого блочного устройства ( за счет того что блочное устрйоства 
читется реально линейно. а файл читается линейно только с точки зрения номеров офсетов а блоки
на диске читаются очень даже рандомно. отслюда падеие сокрости)


на датасете с рекордсайз 16К линиейное чтение из файла составила 2МБ\с тоесть еще в 10 раз
мендленее. Или в 52 раза медленнее чем с голого блочного устройства!

и тут пришло озарение -  если хочешь узнать какая будет сокрость линейного чтения
файла на зфс который лежит на датасете с рекордсайз=N то для этого нужно измерить
скорость рандомных чтений на блочном устройстве с размером запроса на чтение равным N.
тоесть хотим узнаь скоррость лин чтения файла на зфс на датасете с рекордсайз 128к.
для этого нужно помераить на голом блочном устройстве RANDOM READ при bs=128k
получить иопсы и умножить их на  128К. получим срупут. этот срупут и есть та скорость
чтения которая нас интересует!!! тоесть меряем рандом рид иопсы умноаем на размер блока 
запроса и получае  срупут личейнного чтения файла на зфс !!!! это полный прикол.

солгласно вот этой нашей талице

RANDON READ 
	1 iodepth
		4k :  61 IOPS
		16k:  60  IOPS
		32k:  60  IOPS
		128k: 55 IOPS
		1M:   35 IOPS


скорсть лин чтеия файла на датасетах с разным рекордсайзом будет

		4k :  61 IOPS * 4к = 240КБ\с
		16k:  60  IOPS *16к = 960 КБ\с
		32k:  60  IOPS *32 = 1920 КБ\с
		128k: 55 IOPS = 8 МБ\с
		1M:   35 IOPS = 35 МБ \с 

офигеть!

если файл меньше фрагменторован то скорость будет выше. это мы посчитали самый
нижний порог.


получсется если я хочу получить скажем на датасет с 128К рекодрдсайзом
который лежит на масиве шпинделей. где в среднем шпиндель дает 150 ипосов на bs=128k
линейную скороть чтения из файла на зфс скорости 300 МБ\с. то это мне надо собрать
в зеркала и страйпы такиш шпинделей 

  150 * 128 =  18,75 МБ\с дает оди шпиндель.
  300 / 19 = 16 дисков. 

это будет 8 страйпов в каждом из которых сидит миррор


еще полезный момент. вот у меня есть файл на датасете на зфс я его начинаю
линейно читать ( линейно относиельно оффсетов файла) и смотрю в гстат сколко ипсов
это занимает.  отношение числа ипсов что я вижу к числу ипсов которое выжал голый диск
на тесте рандом рид 1 поток (условно говоря для шпинделей обычно это одно и тоже цисло 130-150 
иопсов) покажет степень фргментированности данного файла. чем ближе иопсы к 150 тем ближе
файл к своему абсолютной фрагментации и дну по скорости чтения. чем больше ипосов тем файл
еще нетак фрагметирован. так что можно оценить стеепень его фрагментрованности.

получается скажем беру я пул из 8 дисков. 4 страйпа из мироров. 
и рекордсет 128к.

я могу оценить нижний порог лин чтения из файла лежащего на этом зфс.

8 дисков * 150 иопс * 128К = 150 МБ\с.

или выше если файл меньше фрагментирован.






ИТАК. важная инфо. вот я создаю zpool на основе шпиндельного дисков или других 
дисков. для простоты путь пул состоит всего из одного диска. я могу сразу оценить какая
будет на этом пуле на базе этого диска  линейная скоросьть чтения. для этого надо
взять этот диск и померить на нем скорость используя фио как на голом блочном устройстве
на тесте RANDOM READ. iodepth=1 ,  bs (размер блока запроса) взять равным рекордсету
датасета на котором будет лежать файл. нужно померить иопсы. и умножить на размер bs.
или сразу в срупут посмотреть. 
например я буду хранить файл на зпуле на даатсете с рекордсайзом 128К.
я беру диск. котрый будет входит в состав этого пула. 
я на нем запускаю fio и запускаю тест RAND READ c bs=128K iodepth=1
и получаю 200 iops.
тогда 200 iops * 128 K = 25 MB\s будет скорость чтения файла с этого диска на этом
зпуле с этого датасета с его рекордсайзом. 25 МБ\с это самая нижний порог при условии что
файл будет 100% фрагментирован и рабросан 100% хаотично по диску. а так скорость будет 
больше если он будет меньше фрагментрован.
тоест как только мы взяи диск в руки и померяли тест RAND READ его как голое блочное
устрофство мы мгнвоенно знаем как будет вести себя на нем ЗФС.!
если мы возьмем таких дисков 10 щтук. скажем это будет 5 миророров обиедиенных в страйп.
на оперции чтения все миррорры работатют тоже как страйп. так что считай у нас 10 дисков
в страйпе. если один диск выдает 25МБ\с значит 10 выдадут 250МБ\с. все мы мгнвоенно
знаем как будет линейно читаться файл на этом массиве дисков на этом пуле на этом 
датасете с этим рекордсайзом ( потому что все это влияет на реузулттат.)


как правило если это шпиндель то он на однопотоке покывает одинаковые ипосы для разных
блоков запроса. тоесть что для 4К рандом рид что для 1М рандом рид будет один и тот же
ипос = 150 условно. поэтому мы сразу знаем какая будет лин скорость чтения для разных
датасетов с разным рекордсетом. тоесть

  150 * 128 к = 18,75 МБ\с

на этом же диске для рекордсета 1М 

  150 * 1 = 150 МБ\с



ДАлее. еще раз. если я хочу оценить степень фрагментрованности файла то это 
вот так можно сделать.
я беру пул. вижу какой в нем диск. я узнаю сколько иопсов он выжимает на RANDOM READ
на одном потоке (размер bs особо неважен так как шпиндель для всех почти bs покажет одинаковый ипос). скажем полчаем 60 iops. вот как пример  с нашим диском


RANDON READ 
	1 iodepth
		4k :  61 IOPS
		16k:  60  IOPS
		32k:  60  IOPS
		128k: 55 IOPS
		1M:   35 IOPS


тоесть видно что 60 иопс его скорость если не брать особо болтшие BS.

теерь я иду на датасет и смотрю его рекорд сайз

# zfs get recordsize POOL-01/ds2-128K
NAME              PROPERTY    VALUE    SOURCE
POOL-01/ds2-128K  recordsize  128K     local


теперь мне надо начать через fio линейно читать этот файл с размером блока запроса
равным record size тоесть в нашем случае 128К и в gstat смотеть сколько иопсов там
рисуется. чем меньше число иопсов и чем оно ближе к 60 значит тем больше фрагментирован 
файл. а чем больше число иопс тем файл меньше фрагментрован. 
например 

вот я заряжаю тест. в нем указываю :
   linear read
   путь к файлу
   bs=128k (размер рекордсайза)



# export BLOCK_SIZE=128; fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-"$BLOCK_SIZE"K/fio.dat  --bs=$(($BLOCK_SIZE *1024)) --iodepth=1  --size=10G  --readwrite=read --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
bs: 1 (f=1): [R(1)][1.5%][r=14.3MiB/s][r=114 IOPS][eta 12m:11s]


в прицнипе иопсы можно смотреть и здесь. ибо они должны содпать с gstat

dT: 1.002s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    0      0      0      0    0.0      0      0    0.0    0.0| ada0
    1    114    114  14560    8.4      0      0    0.0   95.9| ada1
    1    114    114  14560    8.4      0      0    0.0   95.9| ada1p1
    1    114    114  14560    8.4      0      0    0.0   95.9| gptid/c37863f1-d2dc-11f0-974a-f0def19938c3

    0      0      0      0    0.0      0      0    0.0    0.0| ada0p1
    0      0      0      0    0.0      0      0    0.0    0.0| ada0p2
    0      0      0      0    0.0      0      0    0.0    0.0| gptid/dd1693ec-d2b2-11f0-947e-f0def19938c3



видим что ИОПСЫ равны 114 иопс. 
а у нас  60 на голом диске. значит файл условно фрагмнтрован пока на 50%.





следущий интересный момент.
положим у нас рекордсайз 128К.
это значит что зфс всегда будет чиать с диска блокиФС по 128К в независимости от bs нащего
запроса. и положим мы читаем линейно с файла.

и тогда если наш bs меньше можно получит вот такой прикольный феномен.

значит мой фио читает офсеты лиенейно по 4К. при запросе первого 4К зфс считывает с диска
128К. и при этом надиске в его буфере эти 128К автоматом сохраняются. 
далее мой фио просит следущий 4К. зфс обращется к диску. ( в случае если я отключил кэширование
данных в арк) а тот блок 128К он все еще лежит в памяти буфера кэша диска и диск
мгновенно обратно отвечает. ему ничего со шпинделя чиатть ненадо. и будет наблюдться в 
gstat такая неонятная на первй взгдя фигня что якобы у диска круто увеичился срупут. 
тоесть как бутто шпнидели стали релаьно бытсрее работать. но это не так. просто мы запраиваем
у диска данные которые он уже недавно счиал и положил в кеш . и он нам их отдает не из шпинделей
а из кеша.
и в фио все так будет выгляеть что у нас иопсы круто выросли. что якобы диск по иопсам
круто ускорился. но это все только за счет того что мы спрсили 4К. зфс спросил у диска 128К.
а далее наш фио запрашивает все эти оставшиется 4К куски которые находится как раз в том 128К
куске. тоесть у нас в 128К помещается 32 куска по 4К.  и вот мы делаем через фио поледоеталваьно
запрос к этим кускам. диск вфизически считает 128К всего один раз а нам потом будет оддвавтьа 
все остальые 4К куски уже из буфера.

показываю.
датасет иеммеет рекрдсейт 128К.
возьме файл и почитем из него кусками по 128К.


# export BLOCK_SIZE=128; fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-"$BLOCK_SIZE"K/fio.dat  --bs=$(($BLOCK_SIZE *1024)) --iodepth=1  --size=10G  --readwrite=read --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
^Cbs: 1 (f=1): [R(1)][0.7%][r=13.1MiB/s][r=104 IOPS][eta 12m:38s]
fio: terminating on signal 2



dT: 1.009s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    0      0      0      0    0.0      0      0    0.0    0.0| ada0
    0    115    115  14711    8.5      0      0    0.0   97.3| ada1
    0    115    115  14711    8.5      0      0    0.0   97.4| ada1p1
    0    115    115  14711    8.5      0      0    0.0   97.4| gptid/c37863f1-d2dc-11f0-974a-f0def19938c3



вотвидно что диска пашет на 97% и максимум что он может из себя выжать 
это 115 иопс и 14711 КБ\с срупута.

также видно что средний размер запрса с диска равен 128К что логично.



мы теперь начнем читать этоже файл кусками по 4К


# export BLOCK_SIZE=128; fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-"$BLOCK_SIZE"K/fio.dat  --bs=$((4 *1024)) --iodepth=1  --size=10G  --readwrite=read --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
^Cbs: 1 (f=1): [R(1)][0.3%][r=4047KiB/s][r=1011 IOPS][eta 44m:21s]


dT: 1.002s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    0      0      0      0    0.0      0      0    0.0    0.0| ada0
    0    972    972 124393    0.7      0      0    0.0   68.2| ada1
    0    972    972 124393    0.7      0      0    0.0   68.4| ada1p1
    0    972    972 124393    0.7      0      0    0.0   68.5| gptid/c37863f1-d2dc-11f0-974a-f0def19938c3


и вот видно что мы читаем тот же файл.
диск отдает нам куски по 128К потому что зфс у него запрашиывает по 128К
так как рекродсайз 128К и зфс плевать что мы спросили толко 4К.
тоесть тот же файл читется темиже кусками по 128К но при этом суперкруто вырос срупут
с 14711 КБ до 124393КБ. как так? неужели пластины стали рабоать быстрее. да нет.
просто зфс запрашивает у диска один и тот же блокФС много много раз. а он уже хранится
в буфере диска. вот диск его поэтому легко и отдает поэтому такой огромный срупут.
а все потому что мы запршвиает линейно куски по 4К. 


итак получается с тем как быстро прикинуть какая будет лин скорсть чтения файла
на зфс на пуле котоырый будет лежать на таком то конкретном диске при таком то 
рекрордсайзе - научился. тоест оотвает на вопос. если я возьму вот этот диск  
то какая будет скороть лин чтения файлов если на нем поставит зфс  - это стало понятно.
с оценкой линейной скорости чтения ФАЙЛА разобрались. тоесть она упирается в рандомную скорость
чтения с диска. тоест зфс даелеает удивительную. он делате так что лин скорость чтения из файла
и рандомная скорость чтения из файла становятся одинаковыми !! ))
безусловно если какято часть файла лежит в кеше то это круто ускорит процесс. 
вопрос только в том какая часть файла бует в АРК или в л2АРК и будет ли она там вобще.

рандомная скорость чения файла. она опять же упирается в рандомуню скорст чтения с диска
как блочого устройства. если унас диск выжиаем 60 иопс. значит она и будет 60 иопс.

ну путем экпримента я столкунлся с тем что почемуто скоорост рандом ридов все таки в
иопсах повыще чем скорость линер рид для файла. как это может быть непонятно.

пример 
рекордсайз 128К. диск на 128К голый  выдавал 55 иопсов
согласно тестам выше. 
	1 iodepth
		4k :  61 IOPS
		16k:  60  IOPS
		32k:  60  IOPS
		128k: 55 IOPS
		1M:   35 IOPS


ну будем считать 60.


читаю файл кусками по 128К линейно


# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-128K/fio.dat  --bs=$(( 128 *1024)) --iodepth=1  --size=10G  --readwrite=read --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
^Cbs: 1 (f=1): [R(1)][0.8%][r=11.3MiB/s][r=90 IOPS][eta 17m:31s]

показания и и иопссов и срупутов [r=11.3MiB/s][r=90 IOPS] совпадают с етм что говорит gstat
(ну как и должно быть )

тоесть 90 иопосов.

теперь читаю тот же файл тем же куском 128К но рандомно

# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-128K/fio.dat  --bs=$(( 128 *1024)) --iodepth=1  --size=10G  --readwrite=randread --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
^Cbs: 1 (f=1): [r(1)][1.2%][r=14.6MiB/s][r=116 IOPS][eta 11m:12s]

опят же показания ипосов и срупута [r=14.6MiB/s][r=116 IOPS] совпдают с гстат.
так вот мы получили 116 иопсов.  непонятно как так может быть что рандомное чтение офсетов
файла в итоге приводит что на диске его блоки лежат более "линейно" чем в случае лиейного 
чтения файла. это какойто парадокс.

ну окей. значит можно в прикидке для расчета рандомных ридов добавит +30%  к рандомной скорости
голого диска. 
тоесть елси у нас скажем есть 8 дисков по 150 иопс. то 8*150 = 1200 иопс. плюс еще 30%
это будет 1560 иопс.  вот такая прикидка на счет скорости рандомной чтения с такого то 
пула на таких то дисках!!!


итак с прикидкам по линейному чтениб файла и рандомному чтениб файла мы разобрлаисть.




теперь преходим к прикдкам для записи.

случай когда нет слога и когда он есть.

когда slog нет.

беру прям два датасета. один на 4К рекрордсайз второй на 1М рекордсайз.
и посмтрим какими запросами зфс будет писать данные на диск. и какой будет срупут и какая
будет очередь в гстат (iodepth)


случай когда мы пишем в файл тем же куском котоырм он харнистя на ФС

# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-1024K/fio.dat  --bs=$(( 1024 *1024)) --iodepth=1  --size=10G  --readwrite=write --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
^Cbs: 1 (f=1): [W(1)][64.8%][w=78.9MiB/s][w=78 IOPS][eta 00m:51s]


78 иопсов.
а гстат покывзает выше

dT: 1.003s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    0      0      0      0    0.0      0      0    0.0    0.0| ada0
    1    136      0      0    0.0    134  73005    6.1   97.2| ada1
    1    136      0      0    0.0    134  73005    6.1   97.2| ada1p1
    1    136      0      0    0.0    134  73005    6.1   97.3| gptid/c37863f1-d2dc-11f0-974a-f0def19938c3


136 иопсов. и средний размер записи 500КБ. это значит то что помимо данных моих
зфс еще делает доп запросы иопсы причем мелкие по размеру в которых он чтото уже пищет
от себя тодесть он пшиет метаданные. 
тоесть в целом запись идет кусками по 1МБ. и еще ряд микрозаписей виидим по 4К метаданных.


у меня на голом диске лин скорсть записи на 1М куске была

LINEAR WRITE
	1 iodepth
		1M:   107 MB\s


мы щас видим  73МБ\с. это за счет дописи метаданных.
поэтому тут я делаю вывод что если скорость лин записи голго диска 100% 
то лин запись в файл будет равна 70% от этого(на шпинделях. тут вообще много про шпиндели
ибо ссд по другому себя будут вести)

это я писал в файл который лежит на датссете с рекодсайз 1М и  я  в него писал
куска по 1МБ.

теперь будут пиатть в него кусками по 4К
тоест случай когда мы пишем куском меньше чем кусок ФС


# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-1024K/fio.dat  --bs=$(( 4 *1024)) --iodepth=1  --runtime=20  --readwrite=write --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
Jobs: 1 (f=1): [W(1)][6.0%][w=45.7MiB/s][w=11.7k IOPS][eta 05m:28s] 
test: (groupid=0, jobs=1): err= 0: pid=8473: Sat Dec  6 15:54:35 2025
  write: IOPS=7534, BW=29.4MiB/s (30.9MB/s)(615MiB/20905msec); 0 zone resets



и тут мы ловим такой момент как READ-modify-WRITE
тоесть чтобы перезаписать 4к в блоокеФС 1МБ зфсу нужно считать 1МБ. перезаписать в памяти
и оббратно записат на диск 1МБ. запись идет кусками 1МБ. 
из фио мы видим что пофакту было преезаписано полезного контента 29.4MiB
я здесь расуждаю так. чтобы зменить 4к в блокеФС 1МБ  вначале зфс должен считаь
из памяти 1МБ страницу. диск должен ее рандомно считать.  я посмтеть диск на рандомных 
чтениях 1МБ куска тратить 12.5мс
втоже время диск на линейную запись куска 1МБ размером тратит 5.5мс.
значит у нас как бы за счет чтений с диска увеличислось время записи.
если 77МБ\с это 5.5мс 
то добваление 12.5мс ожидания перед записью нам увелитвает время в 3 раза
что в 3 раза уменьшает в итоге скорость. 
логично. если за 5.5мс мы успеваем передть ползого контента 77МБ.
то если мы передаем за (5.5+12.5)= 18мс теже самые 77МБ то 5.5 отноистя к 18 как 3
значит в итоге наша еолзная скорсть записи падет в 3 раза теость 25.6МБ\с
что мы и видим BW=29.4MiB/s !!!


причем это независит от того каким блоком мы перзеиывем наш ФС блок. толи это 4К
толи 8К толи 500К толи 800К потому что все они потребует считтывания блока 1МБ с диска
а это и ест самое длинная часть операции так как зфс одинаков быстрое изменить что 4к 
что 900К. это быстро.

поэтому я вот что постулирую - если мы линейно пишем вфайл кусками которые меньше чем
рекордсайз то чттбы узнать как у нас при этом упадет скороост записи 
нужно взять скорость с которой на предыдущем шаге тоесть скортст линейной записи в файл
но кускаии равными рекордасайзу 77МБ\с у нас было.  потом надо посмтреть какое время занимает такая запись.
и потом надо посмтреть время поиска блока на диске. потом надо поделить одно время на другое
получим коэфициент 3 к примеру. и нужно поделить 77 на 3 и получим прогноз как сильно
у нас будет падать запись.



значит я взял друго датсет с еркордсйазом 128к.
и получил то что если в него писат куском 128к то по факту зфс пишет новые блоки
на диск теми же макрооращенияеми по 1МБ. теоесть косрост таже самая 77-80 МБ\с
и если я на этот рекордсет пишу мелкими кусками то потеря такая же самая 29МБ\с
(это все верно только для шпинделей)


таким образом лнейная скрорость записив  файл не зависит от рекордсета. 
она одна и таже. и равна 70% от лин скорости записи на голый блочный диск при велчиине
запроса болка на запись 1МБ. 
если мы пишем куском меньше чем рекордсайз. то нужно найти соотношение скрости рандомной поиска
блока на диске. к скорости записи линейной на диск. скорей всего оно всегда равно 1:3
и значит нужно подедить 70% на 3 тоесть это будет 23%



рандомая запись в файл.
если мы пишем в файл кусками раынвми рекордсету то скорость будет такая же как при лин 
записи в файл . тоесть 70%.


если мы пишем в рандномный оффсет файла и куском меньше чем рекордсет то 
перфоманс падает драматически. вот пример.
потому что когда мы переписывали 1МБ блокФС кусками по 4к. то по сути 
мы какоето время с диска зарпшивали один и тот же блокФС который там лежал в кеше.
а вот кода мы каждый раз хотим переписать новый блок ФС то диск вынужен потоянно нам
искать новый блокФС. 
и вот итог

# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-1024K/fio.dat  --bs=$(( 4 *1024)) --iodepth=1  --runtime=20  --readwrite=randwrite --numjobs=1  --group_reporting --loop=1
test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1
fio-3.28
Starting 1 process
Jobs: 1 (f=1): [w(1)][0.0%][w=139KiB/s][w=34 IOPS][eta 01d:07h:39m:13s]
test: (groupid=0, jobs=1): err= 0: pid=8682: Sat Dec  6 16:12:59 2025
  write: IOPS=24, BW=96.6KiB/s (98.9kB/s)(2040KiB/21124msec); 0 zone resets



тоесть BW=96.6KiB/s
на самом деле это занчит что мы за это время запросили с диска 96\4=24 блокаФС 
и диск их искал. 
ну в целом похоже. 24 блока ФС за 1с это  42мс на обработку одной такой транзакции.
ну тут уже трудно чтото прикиыдват. потому что если 12.5мс на поиск. 
5.5мс на запись . хотя я посмтреьл бывает и 15мс на запст. в общем 
тут непонятно из каких цифр нужно наскребвть 42мс.

я бы в целом сказал что если у нас кусок записи не сооветсвтует рекордсайзу то 
у нас наступает драматический пенальти на скороость записи. который хрен 
посчитаешь.

получается зфс охуенно медленно позволяет лиинейно читать файл.
и она охуеть как медленно линейно пишет в файл если толко мы неделаем это 
точно таким ккуском который равен рекордсету  что редкость. 
скажем у нас рекордсайз 16К а мы пишем в файл куском 24КБ.
у нас один блок обновится лехко. а второй потребует чтения с диска блокаФС. 

и мне пришла мысль что кода мы рандомно пишем в файл то у нас неподразумется
выскойи срупут! нас интереуют иопсы! мы получили IOPS=24 
а что это за иопсы. это в 3 раза меньше чем рандомно просто писать кусками по 1МБ
на диск. 
тоесьть я вот что думаю. елси мы рандомно пишем на диск кускам меньше чем
размер рекордсета то сокроть вот как можно пресказать.
она упадет по сраеннению с рандмноыми иопасми записи на диск во столкьо раз 
во сколько сумма (среднее время поиска на чтение + срденее время записи ) поделить
на (срднее время записи)

тоесть у меня время чтение рандомного это 12.5
а время записи это 5.5.
знчит 12.5+5.5 = 18 и 18 \ 5.5 = 3 
значит 60 иопсов которые дает рандом врайт на голый диск поделить на 3 = 20 иопсов
ну вот так типа и есть.


а если добавить SLOG 
как тогда?

если операции записи асинхронные то он типа мало поможет . ну там раз в 5 секунд 
когда срабывает журнал на зфс он будет ускорять. а так он ничего недаст.
другое дело если мы унас все записи синхронные как при NFS тогда у нас и скорости
записи без слог будут чудовищно низкие. не те которые мы рассматривали выше.


тоестя вот я включаю

 zfs set sync=always  POOL-01/ds2-1024K


и смотрим

# fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-01/ds2-1024K/fio.dat  --bs=$(( 1024 *1024)) --iodepth=1  --runtime=20  --readwrite=write --numjobs=1  --group_reporting --loop=1
Jobs: 1 (f=1): [W(1)][3.7%][w=11.7MiB/s][w=11 IOPS][eta 09m:55s]  
test: (groupid=0, jobs=1): err= 0: pid=8940: Sat Dec  6 16:39:48 2025
  write: IOPS=17, BW=17.2MiB/s (18.0MB/s)(382MiB/22272msec); 0 zone resets


там где раньше было 77МБ\с теперь 18МБ\с потмоу что раньше зфс кинул диску в буфер 
и окей. а теперь он труебеует чтоыбы диск из буфера записал на плитку а мы подождем.



про SLOG
я созда датасет с 1МБ рекордсайз. и сделал датсасет sync=always
и я вижу что в SLOG при этом в q=1 идет запис блоками размером 114К

сделаал датает с 128К. при этом в слог идет запись блоками 64-66К

просто как факт

это к тому как узнать под какой режим записи нужно подбирвать SLOG и на каком
режиме его тестировать


то что рандом врайты будут тормзоить за счет четния хрен с ним.

то что линейная запись будет тормозит из за чтения это уже грустно 

и тут даже не спасет slog. потому что риды с диска никто не отменял.


далее. то что я увидел из ппактикаики
1) полная хуня что якобы qemu обраается к стораджу кусками по 64к.
хуня полнейшая. он обращется теми кусками что ему говрит вм

2) посмотрел на slog. он пишет на свой сторадж данны разными кусками.
есть некоторая связь с тем каким размером делаетс запрос фио. не необязательно.

а вот прикол

dT: 1.002s  w: 1.000s
 L(q)  ops/s    r/s   kBps   ms/r    w/s   kBps   ms/w   %busy Name
    2   2099      0      0    0.0   1399  92320    0.2   25.4| ada0
    5    255      0      0    0.0    253  81102   15.5   99.3| ada1
    0      0      0      0    0.0      0      0    0.0    0.0| ada1p1



по идее ops\s = r\s + w\s 
тут r\s=0 при этом ops\s НЕРАЕВЕН  w\s !!!!


а вот еще прикол


root@truenas[~]# bash
[root@truenas ~]# iostat 1
       tty            ada0             ada1            pass0             cpu
 tin  tout KB/t  tps  MB/s  KB/t  tps  MB/s  KB/t  tps  MB/s  us ni sy in id
   0   749 44.0 2543 109.3   907  104  92.2   0.0    0   0.0   1  0 31  0 68
   0   908 44.0 2406 103.4  1024   98  97.8   0.0    0   0.0   0  0 20  1 78
   0   801 44.0 2412 103.6  1005  101  99.1   0.0    0   0.0   2  0 20  1 77
   0   688 44.0 2050  88.0   353  239  82.4   0.0    0   0.0   1  0 30  2 67


tps тут это ops\s в гстат. 
поэтому KB\t это ops\s  \   t 
поэтому если у нас r\s=0 то  эта цифра не покажет w\s \ t  она покажет именно ops\s  \ t
что милислидинг


главная мысль -  слог запись идет как кусками которыми мы пишем из фио. так и другим
кусками. или скорей всего --- запись идет и кусками котоыми мы пищем из фио. и есть 
другие транзакции дургого размера пэтому среднйи разеер тарнзакциии отличается от того что
мы едаем из фио.
особенно это вдно если транзакция прилетает не с хоста а чеерз NFS

напрмиер я с хоста шлю запрс на запись 8К в датасет где рекодрдсаайз равен 128К.
среднй разермер записи в слог это 13-14К
бывает слог пишет точно как мы шлем записиь на диск. бывает вокруг. бывает почти.
это к вопосу какая связь между размером записи у фио и размером записи на слог


далее. путем стрейс я устанвоил что 

   cat
   cp
   mc

они все пишут линейно кусками по 256КБ

значит при нфс  у нас на нфс срвер летят пакеты MTU прописанным на сет  картах
тоесть нарпиер с 1500 байт.


эти пакеты накаплиываюься на сервере и на уровне прилоежнения накапливаются до NFS 
трназакции. и уже этра тразакцния летит далее в ядро. 

я обанунаружил что при вот таком смотрованрномр NFS папке

[lenovo deb12-clon1]# mount -t nfs4 -o rw,vers=4.2,rsize=131072,wsize=131072,sync,noac,nolock,actimeo=0,lookupcache=none,hard 10.254.254.2:/ds2-64K /mnt/NFS
[lenovo deb12-clon1]# 
[lenovo deb12-clon1]# 
[lenovo deb12-clon1]# mount | grep nfs
10.254.254.2:/ds2-64K on /mnt/NFS type nfs4 (rw,relatime,sync,vers=4.2,rsize=131072,wsize=131072,namlen=255,acregmin=0,acregmax=0,acdirmin=0,acdirmax=0,hard,noac,fatal_neterrors=none,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.254.254.1,lookupcache=none,local_lock=none,addr=10.254.254.2)
[lenovo deb12-clon1]# 


при условии что на серверере этот датасет имеет рекордсайз 64К

так вот когда мой клиент пишет в эту папку то  SLOG пишет на свой диск 
как раз таки кусками по 64К


# iostat -t da -d   1

            ada0             ada1 
 KB/t  tps  MB/s  KB/t  tps  MB/s 
 65.0  156   9.9  1000   40  39.0 
 64.6  158  10.0   187   63  11.4 
 65.0  164  10.4   0.0    0   0.0 
 65.0  160  10.2   0.0    0   0.0 
 65.0  160  10.2   0.0    0   0.0 
 65.0  160  10.2   999   39  38.1 
 65.0  160  10.1   192   65  12.2 
 64.6  161  10.2   0.0    0   0.0 
 65.0  160  10.1   0.0    0   0.0 
 65.0  160  10.2   0.0    0   0.0 
 65.0  156   9.9   999   38  37.2 




тоесть диск ada0 это SLOG
и колонка слева KB/t = 64

но это при условии что нфс папка сомнрованан на клиенте в режиме sync
а если эттго нет если нфс клиент ядра на компе клиента  кэширует запросы пержде чем
послать на сервер то тогда размер этого блока будет прыгать. 

это все к вопросу каким укскусками slog пишет на свой диск


соовтаственнно на уже бекенд диск шпиндельный зфс пишет кусками по 1МБ

 999

так как для шпинделя это уже несинхроннная операция для зфс

итак если нфс лежит на датасете с рекордсе 64К и клиент смонтровал папку в режиме sync
и коирутет файл через mc  а унего кстати размер при копировании write(2, ... 256KB) что 
конечно влияет в том плане что  он кратен 64К поэому на зфс датасете не буедет паразитыных
чтений. так вот в слог будет писать кусками 64К а уже не бкеенд шпинелль диски будет 
писать зфс кусками по 1МБ)


исходя из того что cp,mc, cat они пишут кусками по 256КБ это значит что 
с точки зрения LINEAR WRITE можно брать ЛЮБОЙ размер рекордсайз!  потмоу что 
все 4К 8К 16К 32К 128К  они все кратны 256КБ !!! поэтому при записи в любой датасет
у нас не будет наблатьься паразитных чтений!!! тоесть прблема линейной записи решена!!!

в плане записи только остается проблема рандом врайтов, потому что если они ведутся
кусками не кратными рекордсайз то будут паразитные чтения.


далее такой резулбтат. 
нфс папака лежит на датасете с рекрдрдайз 64К
я с клиента пуляю зпрос на рандом врайт кусками по 8К
по факту у меня в слог идет запись кусками 12К
потом пуляю с киента рандом врайт кусками по32К при этмо у меня в слог идет запись кусками по 36К
тоесть ссвязь с размером реквеста на запись наклиенте и размером куска на запист на слог диск
она есть. но она не всегда 1:1 совпадает

===












---


