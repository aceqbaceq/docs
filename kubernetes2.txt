| kubernetes

можно установить через kubespray(см. kubespray.txt)


---
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
возобновил изучение 20 июня 2023

выше  я рассматривал к8 снизу. теперь возвращаюсь к изучению к8
сверху - поды , реплики итд.

тема: понять что такое replica set 
                    vs deployment 
                    vs replication controller 
                    vs statefull set 

pod

начнем с подов

создадим его в yaml

$ cat pod3.yml 
apiVersion: v1

kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: webserver1
    image: nginx:1.14.2


metadata - это информация для к8. так сказать служебная информация
о поде 
для к8. его обвязка. это как упаковка с надписями на коробке. 
как конверт.
на конверте не скзаано о чем содержимое письма. там указано откуда
 и куда оно летит.
это как железный контейнер.


spec - это информация уже о том что внутри коробки хранится. 
о его начинке.

метадата - это информация для почтальона ( к8) 
а spec это информация уже для конечного потребителя.

куб когда оперирует подами он в spec не смотрит. 
он смотрит в метадату.


поды конечно же внутри состоят из контейнеров. в данном случае у 
нас один контенер в поде.

$ kubectl apply -f po3.yml

$ kubectl get pods  pod-1
NAME    READY   STATUS    RESTARTS   AGE
pod-1   1/1     Running   0          63s


посмтреть инфо о поде можно двумя споосбами
$ kubectl describe pods  pod-1

и более подробный
$ kubectl get pods  pod-1 -o json

...
   "metadata": {
        "annotations": {...},
        "creationTimestamp": "2023-06-17T16:54:33Z",
        "name": "pod-1",
        "namespace": "default",
        "resourceVersion": "91087",
        "uid": "e18de211-4fb3-4c59-a1ee-629218d512d4"
    },
...


теперт чуть добавим в метадату хреней. а именно добавим label


$ cat pod4.yml 
apiVersion: v1

kind: Pod
metadata:
  name: mywebapp4
  labels:
    label1: web4
spec:
  containers:
  - name: webserver1
    image: nginx:1.14.2


замечу что label это dictionary а не list (доббоебы)
смотрим как изменилась метадата

$ kubectl get pods mywebapp4 -o json
    "metadata": {
        ...
        "labels": {
            "label1": "web4"
        },
        ...


 лейбл добавился в метадату.
 зачем нужен label. дело в том что поды в к8 они не болтаются 
 обычно сами по себе. за ними обычно ктото 
 присматривает. например replica set. так вот мы в реплике сет 
 прорисываем какие ей лейблы искать
 и тогда реплика сет знает за какими подами ей надо следить.  
 итак лейблы нужно подами для того чтобы
 вышестоящие струкруты могли найти нужные им поды и потом за ними 
 следить. 

на самом деле эту тему очень подробно я рассмотрел выше. так что
смотри выше. 



перейдем к вышестоящей хрени = repliace set
пронее подробно я уже рассказал выше ищи. поэтому
коротенко пройдусь.
replica set = ее задача это найти стенд элон поды которые к ней 
относятся, если подов не хватает то досзодать. все поды 
абсолютно одинаковые внутри кроме IP адреса. далее следить
за этими подами  тоесть если какойто под сдох то поднять 
его заново. если появился какотой то хер знает откуда стенд 
элон под с таким же лейблами то грохнуть его. если человек поменял
манифест в котором изменил число реплик то поменять число реплик.
иинтересный вопрос - что если в свойтсвах пода в манифесте реплики
прописать чтобы под при падении не перезакался. что тогда будет 
делать реплика сет? ведь с одной стороны скажем ей надо следить
чтобы число работающих подов было 10 а с другой стороны в манифесте
указано что если под упал то его перезапускать ненужно. 
щас проверю что будет
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-1
  labels:
    label1: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      label1: nginx
  template:
    metadata:
      labels:
        label1: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.14.0
      restartPolicy: Never


при попытке накатить такой rs нас к8 щлет нахуй
		The ReplicaSet "rs-1" is invalid: 
		spec.template.spec.restartPolicy: Unsupported value: "Never": 
		supported values: "Always"

ага теперь все понятно. rs просто не позволит запустить поды
с рестартполиси never. все понятно.








тут возникают важные вопросы:
- в чем отличие реплики сет (rs) vs replication controller (rc)
я думаю даже отвечать не буду. так как replication controller
это какая то старая хрень. а репликейшн сет ей пришла на смену.
разбираться тоогда нет времени

- поды крутятся в виде кучки контейнеров на хостах. 
и их можно найти на этих хостах. 
а на каком хосте и в каком виде живет и существует реплика сет? 
на каком хосте она сидит? 
в какой форме она заключена. как ее найти. 
ответ - реплика сет сидит исключительно  в таком компоненте как
k8-controller-manager. физически это такой под который крутится 
на том хосте где установлен к8 контрол плейн. вот и все. это кусок
кода контрол плейна.


- когда мы публикуем реплику сет то она сама создает поды 
или они уже должны сущестовать? ответ - если в к8 уже есть 
работающие стэнд алон поды у которых лейблы ровно такие же
как у реплики сет то она их "захыватывает". в зависимости от 
того сколько таких подов оказалось то реплик сет создает
новые поды , ровно столько сколько не хватает до того числа 
что указан в ее манифесте. например у нее в манифесте записано 10
подов. она поднялась. обнаружиа что в к8 уже есть 5 подов которые
стенд элон. и у них такие же лейблы как и в манифесте реплики сет.
тогда она их захыватывает. и еще поднимает 5 подов с нуля. 
итого 10. замечу важное - реплика сет при захывате подов смотрит
только на два момента - под должен быть стэенд элон тоесть он 
недолжен быть под упоавлеием других реплик сет. и у него 
должен быть полный набор лейблов таких же как манифесте реплики сет.
а вот что там внутри пода нашей репике сет вобще похуй. поэтому 
она может захватить поды в которых внурив вращается мускул 
хотя в ее манифесте записан жинкс. во как.


- как она понимает какие поды ееные (за которыми ей надо следить) 
а какие к ней неотносятся
ответ - она понимает это так: если под ееный то в его свойствах
есть строка 
	Controlled by: имя_реплики_сет
значит под ееный. если под неееный то там будет такая же строка
но будет указана другое имя реплики сет. если же такой строки вобще
нет то это стэнд элон под. тогда она смотрит если у него набор лейблсов
такой же как у реплики сет то она "захватвыет " этот под. и он 
становится ееным. при этом обычно она ее сразу грохает. потому что 
число подов превышает указанный в манифесте. значит один надо 
грохнуть. она грохает именно последний по времени. то есть
захываченный.


- что если мы опбуликовали реплику сет а поды которые к ней 
относятся уже есть на хосте?
будет то что поднятая реплика сет прежде всего ищет стэнд элон 
поды которые подоходят под ее лейблсы. и если находит то захыва
тывает их. а уже потом если подов не хватвает то досоздает новые.

- что если реплик сет создала поды а мы еще ей насунули руками подов
ответ - она их захватит и грохнет




далее мы имеем еще более поразительну вещь. вот наш под попал
 под власть реплики. мы можем имзенить конфиурацию
этого пода и реплика на это никак не прореагирует.
берем ямль пода и меняем в нем версию жинкса

$ cat pod-rp3.yml  | grep image
    image: nginx:1.15.2

публикуем под
$ 
kubectl apply -f pod-rp3.yml 
pod/pod-rp3 configured


смотрим свойства пода
$ kubectl get pods pod-rp3 -o json
    "metadata": {

        "labels": {
            "app": "nginx2",
            "custombuild": "1"
        },

        "name": "pod-rp3",

        "ownerReferences": [
            {
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "ReplicaSet",
                "name": "rp3",
            }

    "spec": {
        "containers": [
            {
                "image": "nginx:1.15.2",


тоесть под по прежнему находится по управлением реплики
 и версия жинкса у него
та что указана в магифесте пода а не в магифесте реплики



что поразиельно то что в свойтвах реплики соверещнно нет инфо 
о подах за котоырми она следит - об этом я и ранее писал



тоесть указано что поды есть. но что это за поды в ней не указано. 
разве это не дебилизм.
если я имею рплику как мне иузнать за каким подами она следит.
мне что парсить все поды блядь. а по факту нужно их искать
на основе лейблсов.




из того что я описал выше вытекает важный вывод - те параметры пода 
котоыре указаны в манифесте реплики
если их поменять то это совершенно не повлечет того чтобы 
реплика начала уничтожать поды и заменять их на 
тот что укзаан в ее ямле. нихуя такого нет. параметры пода 
указанные в реплике касаются только тех подов которые
реплика будет создавать сама. а создавать она их будет только
 если один из подов за которым она следит сдох.
если все поды живы а мы обновили ямль реплики то непроизодет НИХУЯ. 
чтобы у нас обновились поды надо 
либо убить их руками, либо уменьшить в ямле реплики число подов
 а потом его увеличтить. вот такое 
важный вывод. эти суки которые в к8. они об этом ничего непишут.
 уебки.
а ведь интутиивно ожидаешь того что если обновил манифест реплики 
то все поды должны быть обновлены. но этого небудет.
об этом  я уже писал и выше.


еще один момент. вот у нас запущена релика сет. у нее есть все 
нужные ей поды. и тут мы создаем руками 
еще один стендэлон под который конечно же неимеет 
"ownerReferences и имеет лейблы такие как в селекторе у реплики сет.
она получается его тоже захватит но так как у нее уже есть все нужные 
ей поды то она удалит именно этот под. я об этом тоже уже писал


что интеересно походу это то что имя подов котоыре создаст реплика 
никак нельзя
прописать в ямле рпелики. только имя контейнеров можно прописать в ямле. но это нето.

что интересно вот эти две команды выдают разное. 
тоесть у них не одни и теже поля

#  kubectl get  rs rp3  -o json
#  kubectl describe rs rp3




- пооходу в реплике  кода ее создали можно менять толко спек пода
а вот спек реплики пошел нхуй менять неьльзя. только уалять ее 
и пересоздавать. это я тоже уже подмечал.



- selctor и labels должен сопваватьвать 
если у нас есть манифест реплики то у нас в ней прописывается
 свойства рпелики и а именно какой у нее селектор
по которому она ищет поды и там же прописывется шаблон пода 
котоырй будет создан если готовых подов не окажется. в шаблоне 
пода просывается лейбл по которому реплика сет будет его искать. 
так вот набор лейблов и набор сеелекторов должны собвпадать.
пример


apiVersion: apps/v1
kind: ReplicaSet

metadata:
  name: rp3

spec:
  selector:
    matchLabels:
      name: vasya
      label: "1"

  replicas: 1

  template:
    metadata:
      labels:
        name: vasya

    spec:
      containers:
      - name: vasya
        image: nginx:1.13.2


вот селектор реплики
  selector:
    matchLabels:
      name: vasya
      label: "1"


вот лейбл щаблона пода
    metadata:
      labels:
        name: vasya

здесь видно что селектор имеет один набор лейблов 
а набор лейблов пода имеют другой набор поэтому
при попытке создать такой реплик сет к8 пошлет нахер

$ kubectl apply -f rp3.yml 
The ReplicaSet "rp3" is invalid: 
spec.template.metadata.labels: Invalid value: 
map[string]string{"name":"vasya"}: 
`selector` does not match template `labels`



- ReplicaSet is the next-generation ReplicationController 
that supports the new set-based label selector. 
It's mainly used by Deployment as a mechanism 
to orchestrate pod creation, deletion and updates. 
Note that we recommend using Deployments instead of 
directly using Replica Sets, unless you require custom update
 orchestration or don't require updates at all.

ага это выдержка из доков к8. полезно для понимания.


а вот обьяснение что такое set-based lable selector 
Set-based label requirements allow filtering keys according 
to a set of values. Three kinds of operators are supported: 
in,notin and exists (only the key identifier). 
тоесть мы можем требовать чтобы селектор не просто был равен чемуто 
а  входил в какоето множество.
пример set-based label селекторов

environment in (production, qa)
эта штука требует чтобы environment был равен любому 
из production или qa


tier notin (frontend, backend)
это селектор требует чтобы tier небыл равен frontend или backend

partition
этому селектору пофиг чему равен partition главное чтобы в 
селекторе присуствовал partition.
говоря другими словами selector = partition.* (мы помним что лейбейлы 
в  к8 это не списки  а dictionary у которого есть
ключ и значение. ) так вот в данном случае мы говорим что 
ключ должен быть равен partition а какое у него значение пофиг


!partition
в этом примере селектор труебует чтобы лейбл имел ключ нерваный партишн.  а валью вобще пофин какое



- repl contoler понимает команду rolling-update а реплика сет нет
когда то можно было заюзать команду kubectl rolling-update для  
автоматчиеского обновления подов за которыми наблюдает 
rc (replication controller) но на данный момент эту функцию выпилили. 
поэтому на данный момент команды
kubectl roling-update нет вообще.




под и реплика сет самостоятельные хрени но реплика сет содержит 
всебе описание пода на случай если его 
надо создать,
реплика добавялет в пода строку "Controlled by ..."
а деплоймент добавляет хэш в имя реплики и еще добавляет 
лейбл  pod-hash... которого нет в манифесте деплоя,
как я писал если стэнд элон под можно заранее создать руками для 
реплики сет , то создать руками стэнд элон реплику сет 
для депломента уже нельзя. точнее можно , но! он ее захватит
и сразу переведет в "старую " реплику. и начнет в ней число подов
уменьшать до нуля.


шас будет повтороение того что выше.
если под ломается rs создает новый. в чем хитриый приздец 
реплики сета это то что если 
мы обновим манифест rs то ей это вобщем то похуй в том плане что за 
этим не последует обновление 
подов за которыми она присматрвиет. она не тронет поды. 
чтобы обновить поды 
их надо убить. либо надо в манифесте rs уменьшить число реплик до 
нуля и применить этот манифест.
только  новые поды будут созданы согласно новому манифесту. 
еще тонкий вопрос - как реплика сет отличает поды за которыми она 
следит от всех остальных.  я это тоже уже описал выше.



далее еще более сложный момент - что если у нас реплика создана
 не руками а деплойментом.
тогда вот что. делпоймент доаблавяем в своства реплики не толкьо тот 
лейбл который мы прописали в манифесте
дплоймента а еще доп лейбл 
    "pod-template-hash": "5547ff7cd6"
подчеркну этот лейбел отсутсвует в манифесте деплоймента. мы его
туда не помещали. это сам деплоймент
добавит допонилельный лейбел к свойствам rs, а она в свою очередь
добавит к поду который создаст. 
прада непонятно нахуй нужен этот доп лейбл.
у нас как происходит обноавлние подов за которым набллюдает
деплйомент: деплоймент создает +1 rs имя которой
строится из названия деплоймента. если у нас есть два деплоймента то у
нас они имеют разные имена. значит и rs которые 
создают эти деплойменты(сокарщенно deploy) будут гарантрованно разные. 
имена подов генеирируется на основе имен rs 
раз имена rs разыне то и иимена подов будут разные. в каждой rs
которая создана пропиывается какой deploy ею руководит 
поэтому опят же никкой путаницы. те поды которые будутсозданы этими 
rs в них каждая rs пропыивает в Controlled by
что она руководит этим подом поэтмоу путаницы о том что под был
 создан одной rs скажем от первого deploy 
а в итоге под попал под захват другой rs от другого deploy на мой 
взгляди исчлючен. поэтому нахуй нужен 
это доп лейбл "pod-template-hash" не понимаю. 
как бутто это перестраховка.

если  у нас в свойтвах пода при его создании в "Controlled by"
 прописано какая релика является его владельцем
а в свойствах реплики в свою очередь пропиано какой deploy 
является ее владельцем то непонятно нахуй было еще 
доп лейблы городить.




вот пример

$ kubectl describe  deploy  rss-site
...
NewReplicaSet:   rss-site-7cdf5b859 (2/2 replicas created)


$ kubectl describe rs rss-site-7cdf5b859
Labels:         app=web
                pod-template-hash=7cdf5b859
Selector:       app=web,pod-template-hash=7cdf5b859
Controlled By:  Deployment/rss-site
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed

здесь в описании реплики видно какой deploy ей заведует.
а также виден доп лейбл 
	pod-template-hash=7cdf5b859
который деплой прилепил к реплике сет.  а также он его
прилепил и секции селектор.

ха! но ведь эта секция Labels в реплике сет она не относится к 
поиску подов. за это ответчает секкция Seelector.
сеция Label отвечает за то что она метит текущий 
обьект (rs в данном сулчае) чтобы ее (rs) мог найти некоторый 
вышестоящий api обьект среди других !! 
тоесть pod-template-hash=7cdf5b859 эта 
лейбл по которому deploy который рулит этой rs 
ищет ее среди других rs. все что в лейбел не имеет отношения 
к поиску подов. Но с другой стороны нахер деплою искать свои rs
через секцию лейблов когда в rs есть строка "Controlled by"
в которой указано имя деплоя который ею владеет.


единсвтенное что  я могу предположить на счет нахера деплою лепить
на rs доп метку 
		label:pod-template-hash=7cdf5b859
состоит в том что деплой он в процессе обновления подов
 через rs он получается постоянно создает 
новые rs и удаляет прежние rs и возможно он запиывает себе 
эти метки куда то чтобы знать какая rs была создана 
щас а какая в более старые времена 
(например для процесса отката обновлени ??)
но опять же нахер ему метка для этого - он же может для этого 
запоминать у себя названия rs они ведь 
уникальные. историю rs можно вести по имнам этих rs без
 импользвования доп меток... не понимаю..

ксати! я же помню из практики что в манифесте rs должны 
совпдаать поля в селекторе и в спеке пода.
поэтому раз в свойствах rs указано в селекторе 
	Selector:       app=web,pod-template-hash=7cdf5b859
значит следует конено же ожидаь что в поде будут точно такие же лейбелы. и точно1

$ kubectl describe pods rss-site-7cdf5b859-7mg9r
Labels:           app=web
                  pod-template-hash=7cdf5b859
Controlled By:  ReplicaSet/rss-site-7cdf5b859

вот видно что этот под контроливется нашей rs. и то что у него в 
лейблах пристувует 
	pod-template-hash=7cdf5b859
так что этот лейбл долаяяется нетолько деплоем в rs но и rs 
 в конечном итоге егодобалвяет в под! об этом я тоже упоминал 
 уже ранее выше. это все повтор.


покаызаваю манифест деплоя

$ cat d4.yaml 
kind: Deployment
apiVersion: apps/v1

metadata:
  name: rss-site
  labels:
    app: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web



  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: front-end
          image: nginx


как видим в маниесте делоя мы не прописываем лейбл
	pod-template-hash=7cdf5b859
это все отсебятина деплоя



 с разбором деплоя я конечно малек забежал  вперед ведь щас мы 
 говориим об rs


 - дебилый выввод статуса rs

 $ kubectl get rs
NAME                  DESIRED   CURRENT   READY   AGE
rp3                   10        10        10      25h


вот эти цифры они показывают несколько rs в штуках развернуто. нет. 
они покзывают сколько подов находится под управлением\присмотром 
данной ОДНОЙ rs.
тоесть rs одна а подов под присмотром 10.
это по мне очень дебильная система вывода инфо. 
интуитивно кажется что развенут 10 штук rs.

а вот еще дебилизм вывода  с другой стророны


$ kubectl describe rs rp3
Name:         rp3
Replicas:     10 current / 10 desired
Pods Status:  10 Running / 0 Waiting / 0 Succeeded / 0 Failed



Replicas:     10  = это тоже не про то что у нас 10 штук rs разврнуто.
 это прото что у нас 10 подов находится
под присмотром. причем развернуть их могли и до создания rs. 
тоесть нефакт что их создала наща rs.
так вот если replicas уже отображает сколько подов находится под
 присмотром нахуй еще раз эту инфо дублировать 
в следудщей строке. Pods Status:  10 Running = говорит ровно о том же
 самом. 
единсвтенное что я могу педположить что Replicas: 10   возможно значит
 сколько подов для наблюдения указано
в манифесте rs, а  Pods Status:  10 показывает сколько по факту
 подов находится шас под наблюдением. но опятьже 
по моему все это уже отображено в строке 
"Replicas: 10 current / 10 desired" тоесть у нас указано что
в манифесте указано 10 подов и 10 подов у нас и наблюдается. 
нахуй тогда дублировать эту инфо в строке  Pods Status:  10 Running
непонятно




| replication controller
в целом это старая хрень ее вообще можно не изучать.
расссмотрим его.

как я писал выше его юзать уже ненадо.
это устаревшая версия rs

эта секцтя про 
replication controller



apiVersion: v1

kind: ReplicationController
metadata:
  name: repl-contr-vasya
spec:
  selector:
    property: nginx-vasya

  replicas: 2


  template:
    metadata:
      name:  pod-vasya
      labels:
        property: nginx-vasya
    spec:
      containers:
        - name: cont-vasya
          image: nginx:1.15.2
          ports:
            - containerPort: 80



$ kubectl apply -f repl-contr-vasya.yml 
replicationcontroller/repl-contr-vasya created

$ kubectl get rc
NAME               DESIRED   CURRENT   READY   AGE
repl-contr-vasya   2         2         2       7m17s

rc это репликейшн контроллер
rs это реплика сет 
смотреть спмсок хреновин куба можно через 
$ kubectl api-resources
там можно подсмотреть не только список всех хреновик которые может созавать куб
но и их укороченные названия


при этом получим поды с именами
repl-contr-vasya-8wvhd 
repl-contr-vasya-ddxxx 

приклоно что строка   name:  pod-vasya неимеет никаткго занчения
иимена подов строятся на основе имени репл контроллера + хэш


вот что покзывает оисание репл контроллера


$ kubectl describe rc repl-contr-vasya
Name:         repl-contr-vasya
Namespace:    default
Selector:     property=nginx-vasya
Labels:       property=nginx-vasya
Annotations:  <none>
Replicas:     2 current / 2 desired
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  property=nginx-vasya
  Containers:
   cont-vasya:
    Image:        nginx:1.15.2
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  14m   replication-controller  Created pod: repl-contr-vasya-ddxxx
  Normal  SuccessfulCreate  14m   replication-controller  Created pod: repl-contr-vasya-8wvhd


в целом все похоже как и у репл сет.


теперь я хочу сравнить ямль для реплики сета и для репл контроллера.
в целом разница только в том как опиываетс селектор
у реплики сет вот так

spec:
  selector:
    matchLabels:
      name: vasya


у репл контроллера вот так

spec:
  selector:
    property: nginx-vasya



но далее чувак пишет что у рпелики сет более мощный аппарат по распиыванию селектора.
тоесть реплик сет например может иметь вот такую дуру в селекторе


spec:
   selector:
     matchExpressions:
      - {key: app, operator: In, values: [soaktestrs, soaktestrs, soaktest]}
      - {key: teir, operator: NotIn, values: [production]}

а видимо реклика контроллер нет

расшифровка этой дуры такая что  app может быть либо soaktestrs, либо soaktestrs, либо soaktest
а teir не может быть production


еще раз упомяню что если мы создали ручной rs или rc то обновлять поды за которыми они следят 
надо по схеме :
 As explained in #1353, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.

а вот фраза от том что ненадо публиковат просто standalone поды. ибо если с ним чтот случится то никто под перезапусткать
небудет.ужно запускать под только под присмотром rs или rc. он за ним будет следить и перезапустит его в случае
если он terminated или deleted : 
Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.


далее полезнякшки о том когда ненадо юзеать rc\rs а юзать другой тип контроллера:
Use a Job instead of a ReplicationController for pods that are expected to terminate on their own (that is, batch jobs).
тоесть если мы ожидаем что под будет рабтать невечно тоесть типа как cron то надо юзать контроллер Job.
видимо дальше с ним  я будут знакомиться

далее. ои пишут что если работа пода связана с конкретным хостом на котором он крутится то надо юзать DaemonSet:
Use a DaemonSet instead of a ReplicationController for pods that provide a machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied to a machine lifetime: the pod needs to be running on the machine before other pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.


еще раз про то какова история перехода от rc к rs  и как обновляли поды котоыре под их присмотром
раньше:
вначале был replication controller и ручной процесс обноавления подов 
через : the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.


потом они чуть облегчили ручной труд и добавили

$ kubect rolling-update названия_rc -f имя_нового_rc


потом они придумали replicaset + deployment , выпилили kubectl rolling-update
и сказали забыть про rc . так что его в целом можно неизучать!


получается раньше репликейшн контроллер как плюс имел то что можно было его поды обновить 
легко через rollinh-update. щас это выпилили. также рпликшеншн контролер неимеет подержки крутых селектов
как это есть у реплики сета. так что наданный момент репликешйн контролллр неимеет нкиаких плюсов.


я думаю  что rc также как rs необновляет поды которые уже крутятся если в ямле rc заменить спеку у 
подов.



полчаетс что rc это старье и атавизм о котором можно забыть.
подами управялет реплика сет . реплкой упралявет деплоймент. 




схема обновления подов через rc\rs руками - создаем +1  rc\rs. у него 
обычно все тоже самое кроме скажем верссии докер имаджа. началное чиcло реплик = 1.
далее на старом контроллере уменьшаем число реплик на -1 а на новом увеличиваем на +1.
возникает вопрос почему новый под созданный на новом контроллере не подпадает под юрисдикцию 
старого контроллера . ответ потому что когда контроллер создает новый под он сразу же в нем прописывает
поле "ownerReferences" поэтому когда под создан старый контроллер это видит и мгновенно отваливает от этого пода

возикет идея.  а почему бы нам не создавать новый контроллер а обновить магифест старого и потом делать так - уменьшаем 
число подов на 1 потом обратно прибавляем. потом опять уменьшаем число подов на 1 потом прибваляем. 
может быть тогда он так удалит все старые поды и оставит толко новые? проверим

имеем 

s$ cat repl-contr-vasya.yml 
apiVersion: v1

kind: ReplicationController
metadata:
  name: repl-contr-vasya
spec:
  replicas: 2
  selector:
    property: nginx-vasya


  template:
    metadata:
      name:  pod-vasya
      labels:
        property: nginx-vasya
    spec:
      containers:
        - name: cont-vasya
          image: nginx:1.15.2
          ports:
            - containerPort: 80



его поды

$ kubectl describe pods repl-contr-vasya-8wvhd   | grep -i image:
    Image:          nginx:1.15.2

$ kubectl describe pods repl-contr-vasya-ddxxx   | grep -i image:
    Image:          nginx:1.15.2


обнволяем манифест контроллера

$ cat repl-contr-vasya.yml | grep -i image
          image: nginx:1.13.2

применям обновлванный манифест контроллера

$ kubectl apply -f repl-contr-vasya.yml 
replicationcontroller/repl-contr-vasya configured

проверяем что сейчас контроллер содержит новую версию имаджа


$ kubectl  describe rc repl-contr-vasya | grep -i image
    Image:        nginx:1.13.2


теперь меняем число реплик на -1 и +1

$ kubectl scale rc   repl-contr-vasya    --replicas=1
replicationcontroller/repl-contr-vasya scaled

$ kubectl scale rc   repl-contr-vasya    --replicas=2
replicationcontroller/repl-contr-vasya scaled

проверяем поды
$ kubectl describe pods  repl-contr-vasya-8wvhd   | grep -i image:
    Image:          nginx:1.15.2
 @test:~/test/repl-controllers$ kubectl describe pods  repl-contr-vasya-cchkj  | grep -i image:
    Image:          nginx:1.13.2

один под обновился. 

теперь еще раз -1 и +1


$ kubectl scale rc   repl-contr-vasya    --replicas=1
replicationcontroller/repl-contr-vasya scaled

$ kubectl scale rc   repl-contr-vasya    --replicas=2
replicationcontroller/repl-contr-vasya scaled


$ kubectl describe pods  repl-contr-vasya-8nrgj  | grep -i image:
    Image:          nginx:1.13.2
 @test:~/test/repl-controllers$ kubectl describe pods  repl-contr-vasya-8wvhd  | grep -i image:
    Image:          nginx:1.15.2

ага. видно вот что. походу с точки зрения rc его поды имеют некий id. 
и если мы уменьшаем scale то он удаляет не тот под который самый стартый а тот у которого id самый меньший. и это 
всегда будет последний запущенный под. поэтому  условно говоря дераргая scale туда сюда на единицу мы будем толкьо
пересоздавать последний под. а первые будут отсавться безизменныйми.
таким макаром дергая скейл мы поды не обновим. также полчается чтоу нас скейл фактор при этом уменшается на один а если у нас
изчанально один под то вобще неможем такой тактиктой полтзватся.
и еще вот мы обновили ямл rc в нем заявлены уже новые версии подов. а по факту на проде у нас будет сборная солянка
из старых и новх версий . полуается состояние на проде отлвитчает от того что написано  в ямле rc. вобщем у
ткого подхода обровления подов одни минусы. поэтому понятнно что они говорят что надо обновлятьь поды под дургоуму.
создем +1 новый rc и потом в старом удаляем -1 scale а в новом прибавляем +1 scale тогда у нас все хоороо на кажом этапе.
пока на новом rc непоявится +1 здоровый под мы на старом ничего не удаляем. все надежно. и хорошо.


а вот такой пост про сравнение rc vs rs vs deployment 
написал на стековерфлоу

Replication Controller VS Deployment in Kubernetes
replication controller is the old version of replica set.
so replication controller(rc) vs replica set(rs) vs deployment(deploy).
rs can work with set-based selectors. rc can not.
some time before we could automatically update pods of rc via
kubectl rolling-update old_rc_manifest -f new_rc_manifest
now this feature is removed from kubectl.
if we create rs manually we can not automatically update its pods 
- that is there is no any command that delete old pods one by one 
and create new pods one by one. we should do it manually. (я даже
добавлю что сделать это можно только уменьшив число реплик 
до нуля убив текущие поды и потом прибавив число реплик до нуж
ного числа чтобы rs создала новые поды на основе нового манифеста)
However if we create rs via deployment then we can update
 pods automatically using deploy features.
so rc is the old stuff with no pros. instead use rs. 
if we want to update pods of rs in a usefull manner
 do not create rs manually , instead use deploy. 
 deploy automatically create rs, rs create pods.
в общем ничего нового. у них все описано верно но по верхам, я же
расписал все глубоко и из практики.




| minikube
чтобы не ставить к8 руками то побыстрому можно поставить minikube

стартует \ стопим его череез

$ minukube start
$ minikube stop

при этом kubectl надо ставить отдльно руками
потому что встроенный  minikube kubectl  это гавно



| как узнать версию k8

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}

Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.3", GitCommit:"9e644106593f3f4aa98f8a84b23db5fa378900bd", 
GitTreeState:"clean", BuildDate:"2023-03-15T13:33:12Z", GoVersion:"go1.19.7", Compiler:"gc", Platform:"linux/amd64"}

версия куба это "server version". в даннном случае это "v1.26.3"




| api

запускаем мы разворот манифеста а нам на экране ошибка

no matches for kind "Deployment" in version "extensions/v1beta1"

прикол в том что для разворота разных контроллеров нужны 
разные версии api.
скажем мы хотим деплоймент развернуть тогда нужно
узнать какую версию апи мы хотим заюзать (по ходу у дебилов
из к8 присуттувует сразу несолкьо версий апи)

так вот

$ kubectl api-resources | grep -E "APIVERSION|Deployment"
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
deployments                       deploy       apps/v1                                true         Deployment


получается для разворота манифеста деплоймента надо юзать apps/v1
тоесть

# cat d4.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: rss-site
...

охренеть....





| labels
| selector

я уже выше их обсуждал.
щас хочу сказать то что если у обьекта есть селектор это значит 
что на основе 
него он ищет некие дочерние обьекты. например на основе 
селектора rs ищет свои поды

а если у обьекта есть labels это значит что на основе этих labels
 уже этот обьект будет 
искать какйто более выскопоставленыый родителсткий обьект. 
например лейблы в rs говорят о том 
что на основе этих лейблов этот rs может быть обьектом поиска 
для deployment


итак label характирирузует сам обьект. а selector характириузует какойто другой обьект который этот обьект
будет искать.

охренеть.....

пример rs


$ kubectl describe rs rss-site-7cdf5b859
Name:           rss-site-7cdf5b859
Namespace:      default
Selector:       app=web,pod-template-hash=7cdf5b859
Labels:         app=web
                pod-template-hash=7cdf5b859
Controlled By:  Deployment/rss-site
...
...




его поле Selector:       app=web,pod-template-hash=7cdf5b859
показывает те лейблы на основе которых он ищет свои поды


а  
Labels:         app=web
                pod-template-hash=7cdf5b859

показыает что на основе этого лейбла его ищет родитеслкий 
контроллер  Deployment/rss-site







| полезняшка

отличный способ узнать что с контрол панелью куба все окей
надо всего навсего посмотреть успешно ли стартанули все его 

системные поды

# kubectl get pods  -n kube-system


# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS       AGE
coredns-5dd5756b68-ck9fd           1/1     Running   6 (26h ago)    7d5h
etcd-minikube                      1/1     Running   6 (26h ago)    7d5h
kube-apiserver-minikube            1/1     Running   6 (26h ago)    7d5h
kube-controller-manager-minikube   1/1     Running   6 (26h ago)    7d5h
kube-proxy-7rxr9                   1/1     Running   5 (26h ago)    7d5h
kube-scheduler-minikube            1/1     Running   6 (26h ago)    7d5h
storage-provisioner                1/1     Running   19 (20h ago)   7d5h




ну а второй шаг это посмотреть в 

# kubectl describe имя_ноды 

как там с pressure. хватает ли памяти, дисков. 
и все ли окей с оверлейной сетью.
причем прикол. в kubectl describe имя_ноды может писать 
что с овеолейной сетю все окей
а состояние системного пода который отвечает за  оверлейную
 сеть будет failure так что тут надо с двух 
сторон смотреть
пиздец....





| stuck
| terminate

если под висит бесконечно в статусе "terminate"
то его надо убить рукам 

# kubectl delete pod <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>
или 
# kubectl delete pod POD_NAME --grace-period=0 --force -n NAMESPACE_NAME




| runtime

как узнать какой тип конейнеризации используется на к8. 
говоря по заумному
какой рантайм.

ответ надо посмореть в настройках кубелета

/usr/local/bin/kubelet
--v=2
--node-ip=10.42.147.39
--hostname-override=nl-k8-01
--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--config=/etc/kubernetes/kubelet-config.yaml
--kubeconfig=/etc/kubernetes/kubelet.conf
--container-runtime=remote
--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
--runtime-cgroups=/system.slice/containerd.service


здесь райнтайм это containerd
почему смотреть именно в насройках кубелета? потому что походу 
на хосте кубелет это агент к8. это его удаленные руки. и контейнеры
поднимаются на хосте через кубелет. он их поднимает.








| deployment

как говорится только что был взгляд на к8 снизу.
возврашаемся на изучение к8 сверху

одна из фишек deploy это 

* Managing the state of an application: Deployments 
can be paused, edited, and rolled back, so you can make changes 
with a minimum of fuss.

paused и rollback это прикольно

вторая фишка
* Easily exposing a workload outside the cluster: It might 
not sound like much, but being able to create a service that
 connects your application with the outside world with 
 a single command is more than a little convenient.
правда я не уверен что эти слова отностя к деплою. по мне 
это вобще то должно относится к Service. страннно....


буду ниже смотреть как это на практике

забегая вперед. под - эта ровно та хрень где физически
сидит наше приложение. наша юзерская программа. ее код. ее процесс.
. за ней следит rs. а за rs следит деплой. 
по дефолту поды досутпны только по сети только внутри куб кластера.
плюс если под убили а другой создали то поменлся  IP 
у пода . поэтому service(svc)
дает единый IP кооторый неменяется. 
плюс он пробрасывает порт наружу хоста.
честно говоря я не понял зачем я в кусок посвященный деплою 
я сюда засунул описание Service.


про деплой. деплой в отличие от rs  при обновлении
 шаблона пода  в его манифесте
он сразу начинает обновлять свои поды. 
и у него есть 
две стратегии обновления подов.
одна стратегия это когда он полностью удаляет все старые поды
и только после этого начинает создавать новые. эта стратегия в 
манифесте
прописывается вот так

  replicas: 1  
  strategy:
    type: Recreate
 

вторая стратегия это когда он понемногу создает новые поды и
 понемног удаляет старые
подые. эта стратегия в конфиге прописывется вот так

replicas: 3  
strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0


пр maxsurge и  maxUnavailable 
если кратко для чего они

maxSurge: 2        # how many pods we can add at a time
maxUnavailable: 0  # how many pods can be unavailable
                   # during the rolling update

но это если кратко и малопонятно   а так надо говорить отдельно 
для этого смотри "kubernetes PV.txt" файл, там я это расписал.



по поводу того как откатывать поды в деплое к предыдущему релизу.
с одной стороны можно зайти в манифест деплоя и руками скажем 
поправить обратно 
версии имаджей на предыдующую версию и накатить манифест.
 по мне метод норм. а можно 
сделать по другому.
дело в том что деплой  у нас обноавляется как? он создает
 новый rs в нем приавбяеляет
число реплик в старой rs убавляет до нуля. так вот по окончании
 процесса обновления
он старую реплику неудаляет. она так и болтается в к8. просто у нее
число подов указано ноль.
 по дефолту таких реплик деплой
хранит толи 3 толи 10 штук. можно задать чтобы он это хранил и больше.
вот
хоршая статья про это (https://learnk8s.io/kubernetes-rollbacks)
так вот можно увелчить это число, для этого в манифесте деплоя 
надо вписать

 replicas: 3
  revisionHistoryLimit: 100


а вот доказетельство

# kubectl get rs
NAME                                DESIRED   CURRENT   READY   AGE
depl3-599bb89957                    2         2         2       14s
depl3-9759cc7f6                     0         0         0       40h
depl3-fc57c9788                     0         0         0       40h

это rs принадлежщие деплою.видно что висит один рабочий rs с текущими
подами и два старых rs без подов.

по поводу DESIRED, CURRENT, READY.
DESIRED = это число реплик(подов) прописанное в манифесте rs
CURRENT = это число подов которое создано  в системе номинально
( на уровне созданных контейнеров) но совершенно не факт что 
эти контейнеры полностью раскрутились и ready проверки прошли 
успешную проверку
READY = это число подов которые стартанули, полностью раскрутились
и прошли все ready проверки

в чем разница между READY и CURRENT. CURRENT это под который форма
льно был создан но процесс который там внутри он еще не сделал все
хреновины (нераскрутился) и поэтому проверки которые есть внутри
пода он еще не прошел. пока я мало понимаю на чем основаны
эти проверки. но по моему их можно и самому вписывать в под.
хорший привер скажем под с эластиком. должно пройти много времени
с момента старта контейнера с эластиком до момента коогда он 
там реально раскрутится. READY это уже под в котором программа 
которая внутри работает уже реально готова к приему запросов снаружи
тоесть она уже реально готова к рабочей нагрузке к труду и обороне. 

кстати практический вопрос. вот мы опубликовали деплой.
или мы модифицировали его манифест и переопубликовали.
вопрос - как узнать что все поды обновлены? вот так

# kubectl  describe  deploy depl1 | grep Replicas:
Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable

тоест нам ненужно лазить и искать какую реплику сет породил 
деплой и смотреть ее статистику. мы все сразу можем найти 
в describe самого манифеста. 

а вот как нам из describe деплоя узнать какая rs сейчас активный

# kubectl  describe  deploy depl1 | grep NewReplicaSet:
NewReplicaSet:   depl1-586d9d8464 (2/2 replicas created)

если эти две команды собрать в одну. тоесть мы хотим знать
сколько подов новых уже развернуть и какая rs явялется активной

# kubectl  describe  deploy depl1 | grep -E "Replicas:|NewReplicaSet:"
Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable
NewReplicaSet:   depl1-586d9d8464 (2/2 replicas created)

при желании зная имя активной rs можно посмотреть ее статистику
через команду

# kubectl get rs | grep -E "READY|depl1-586d9d8464"
NAME               DESIRED   CURRENT   READY   AGE
depl1-586d9d8464   2         2         2       16m

скорее всего "2 available" в статистике деплоя 
это тоже самое что "2 READY" в статистике rs





интересный вопрос хранит ли в describe  деплой 
список старых rs, ответ нет. 
как он их находит - находит он их просто - во первых 
он их владелец, 
во вторых они rs имеют теже лейблы которые прописаны в селекторе деплоя.
окей - деплой знает про свою кучу rs. как он определяет 
в какой последовательности
эти rs он создавал. ответ такой что эта инфа содержится в аннотации rs
в аннотации есть поле "revision"
по нему деплой понимает какой rs был опубликован за каким
другой вопрос как нам узнать список "старых" rs для деплоя? 
ответ - незнаю,

это текущий последний(активный) rs
# kubectl describe rs  depl3-599bb89957  | head
Name:           depl3-599bb89957
Annotations:    
                deployment.kubernetes.io/revision: 3

кстати говоря та же самая инфо есть и в аннотации в деплое!


это прердыдущий rs
# kubectl describe rs  depl3-9759cc7f6  | head | grep revision
                deployment.kubernetes.io/revision: 2

это самый старый rs
# kubectl describe rs  depl3-fc57c9788  | head | grep revision
                deployment.kubernetes.io/revision: 1

поэтому если деплой хочет откатить поды к предыдущей rs он
ищет в "своих старых " rs ту rs у которой в ревизии находится
ревизия на 1 меньше. или ближайшая предыдушая по цифре rs,

с этим разобрались. теперь вопрос как нам откатсят на преддущую 
версию подов
используя это ?еще раз покажу какой rs щас последний
 и какая версия имаджа там стоит

# kubectl describe rs  depl3-599bb89957  | grep -E "revision|Image"
                deployment.kubernetes.io/revision: 3
    Image:        nginx:1.7.6


а вотпредыдущая версия rs

# kubectl describe rs  depl3-9759cc7f6  |  grep -E "revision|Image"
                deployment.kubernetes.io/revision: 2
    Image:        nginx:1.7.9


откатвытеися на предудущю весрию подов
# kubectl rollout undo deployment/depl3  --to-revision=2
deployment.apps/depl3 rolled back


вот такая у нас была ситуация

# kubectl get rs
NAME                                DESIRED   CURRENT   READY   AGE
depl3-599bb89957                    2         2         2       14s
depl3-9759cc7f6                     0         0         0       40h
depl3-fc57c9788                     0         0         0       40h


вот такая у нас стала ситуация

# kubectl get rs | grep -E "NAME|depl3"
NAME                                DESIRED   CURRENT   READY   AGE
depl3-599bb89957                    0         0         0       10m
depl3-9759cc7f6                     2         2         2       40h
depl3-fc57c9788                     0         0         0       40h



вот такие номера ревизий были

# kubectl describe rs  depl3-599bb89957  | head
Name:           depl3-599bb89957
Annotations:    
                deployment.kubernetes.io/revision: 3



# kubectl describe rs  depl3-9759cc7f6  | head | grep revision
                deployment.kubernetes.io/revision: 2

# kubectl describe rs  depl3-fc57c9788  | head | grep revision
                deployment.kubernetes.io/revision: 1



а вот такие стали
# kubectl describe rs  depl3-599bb89957  | grep -E "revision|Image"
                deployment.kubernetes.io/revision: 3
    Image:        nginx:1.7.6

# kubectl describe rs  depl3-9759cc7f6  |  grep -E "revision|Image"
                deployment.kubernetes.io/revision: 4
    Image:        nginx:1.7.9



видно что тот rs куда деплой перешел в результате роллоута 
у него прибавилась 
версия ревизии. была 2 а стала 4. а почему она стала 4?
потому что у нас номер ревизии нашей прошлой активной rs был 3
значит новый rs будет иметь ревизию 3+1=4, и при этом неважно
наш активный rs это аболютно новый rs которого до этого не было в 
природе или мы вернулись к одному из rs который уже до этого сущест
вовал в системе просто мы к нему вернулись,

вот такая схемотехника. поэтому по номеру ревизии всегда можно 
узнать какая реплика сет шла за какой в деплое,
внезависимости скачем ли мы по репликам через роллоут(роллоут
это когда мы прыгаем на одну из старых реплик) или мы модифицировали
манифест самого деплоя накатили его и деплой создал абсолютно
новый rs .

как происходит rollout - деплой  берет указанную старую rs 
и делает ее активной а ту rs которая была активной делает 
ее старой, и далее он в старой rs уменьшает число подов 
до нуля а в активной rs прибавляет поды до указанного числа.
обычно он делает это так, он на -1 уменьшает число подов в старой
rs и прибавляет на +1 число подов в новой.
также деплой когда он берет активную rs то у нее изменяет 
номер ревизии. он выставляет в текущей активной rs номер 
ревизии на +1 больше от того номера ревизии который был у прошлой
активной rs. например. номер ревизии в текущей активной rs равно 13,
значит у следущей активной rs будет номер ревизии 13+1=14, подчеркну
что неважно текущая активная rs она взята из каких то "старых"
либо она создана с нуля.


едиснвтенное что надо помнить что если после такого роллоута  нам надо 
в нашем текстовом файле манифеста деплоя поменять номера имаджей 
на текущий.
а то будет несоовтасвие манифетса на к8 и  у нас на компе с которого
 мы разворачивали
этот деплой.

процесс  рооллоута в динамике (тоесть сколько подов 
уже обновлено)
можно наблдать  перидически вызывая 

# kubectl get rs

а можно через комкнду
# kubectl rollout status deployment/depl

елинстевеное что эта команда ебнутая. показыает некоеректные резултатты. 
об этом подробно в "kubernetes PV.txt" я расписал.

поэтому да - вопрс как наблюдать динамику сколько подов обновлено 
при деплое 
или роллоуте - я рекомендую неюзать 
команду  kubectl rollout status deployment/depl 
а юзать банально   kubectl get rs
а еще лучше команду

# kubectl  describe  deploy depl1 | grep -E "Replicas:|NewReplicaSet:"
Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable
NewReplicaSet:   depl1-586d9d8464 (2/2 replicas created)


еще вот такая команда есть, это еще одна ебанутая команда
# kubectl rollout history deployment/depl3
deployment.apps/depl3 
REVISION  CHANGE-CAUSE
1         <none>
3         <none>
4         <none>



она по идее должна показывать чтото типа такого
kubectl rollout history deployment/app
REVISION  CHANGE-CAUSE
1         kubectl create --filename=deployment.yaml --record=true
2         kubectl apply --filename=deployment.yaml --record=true


но она такого никогда непокаже потому что для этого надо при накатке 
деплоя в командной строке указать доп флаг --record

# kubectl apply -f deployment.yaml --record
или вот так
# kubectl apply -f deployment.yaml --record=true

так вот проблема в том что если мы хотим задать этот флаг в 
манифесте деплоя
то в нем такой опции нет. так что пошел нахуй называется.
еще я не знаю нужно ли при каждлом вызове деплоя вставлять этот флаг 
или достатно только один раз.



продолжаю про деплоймент
важная вещь - положим новая версия деплоя неправильная неудачная.
как ее откатить?

показываю

вот текущая версия имаджа у подов
# kubectl describe rs depl3-fc57c9788 | grep -i image
 Image:          nginx:1.13.10

хотя я использовал неочень верную команду для этого, я искал
в rs а лучше искать сразу в deploy, то есть

# kubectl describe  deploy  depl1 | grep Image:
    Image:        nginx:1.13.10



изменили деплой. накатываем несуществующую версию жинкс


# kubectl get pod | grep -E "NAME|depl3"
NAME                                      READY   STATUS         RESTARTS        AGE
depl3-554ddc4978-4vf4t                    0/1     ErrImagePull   0               5s
depl3-7c7f7bb655-h8465                    1/1     Running        0               2m36s
depl3-7c7f7bb655-jl8xw                    1/1     Running        0               2m24s


смотрим в дескрайб пода
Error: ImagePullBackOff



# kubectl get rs | grep depl3
depl3-554ddc4978                    1         1         0       3m29s
depl3-7c7f7bb655                    2         2         2       6m1s


понятно что где "2  2   2" это старая rs а где  "1  1  0"
это новая активная rs, 

а вот что к слову показывает status деплоя
# kubectl rollout status deploy depl3
Waiting for deployment "depl3" rollout 
to finish: 1 out of 2 new replicas have been updated...


откатываем деплой обратно (та самая команда)
# kubectl rollout   undo  deploy depl3
deployment.apps/depl3 rolled back

смотрим что стало с rs
вот так было на момент херового деплоя
depl3-554ddc4978                    1         1         0       3m29s
depl3-7c7f7bb655                    2         2         2       6m1s


вот так стало после rollout
# kubectl get rs | grep depl3
depl3-554ddc4978                    0         0         0       7m42s
depl3-7c7f7bb655                    2         2         2       10m




# kubectl describe deploy depl3 | grep NewReplicaSet:
NewReplicaSet:   depl3-7c7f7bb655 (2/2 replicas created)

таким образом rollout undo меняет обратно  активную реплику сет 
на предыдущую.
на херовой rs число подов сдувается до нуля. а на предыдущей
rs надувается до максимума.

кстати важно что  при этом   стремная реплика неудаляется.
и это очень плохо. потому что потом мы забудем что эта rs 
непросто старая она " с ошибкой" и мы можем на нее откатиться.


щая я проверю а можно ли таким же rollout октатить успешный деплой?
краткий ответ - да. rollout undo пофигу почему мы откатываемся
назад. он просто откатвыается назад на -1 rs. а почему
мы это делаем rollout undo неважно,

получается это просто укороченная версия команды
# kubectl rollout undo deployment/depl3  --to-revision=2

понятно.



продолжаю про деплоймент,
я уже скзаал что деплой обновляет поды по двум сценариям.
один сценарий это recreate
A deployment defined with a strategy of type Recreate will terminate 
all the running instances then recreate them with the newer version.
тоесть вначале удаляются полнстью все старые поды и  только после
 этого создаются 
новые поды.
The recreate strategy is a dummy deployment which consists of
 shutting down version A then deploying version B after version A
  is turned off. This technique implies downtime of the service
   that depends on both shutdown and boot duration of the application.

еще про rollout там есть rollout pause

# kubectl rollout pause deploy depl3
deployment.apps/depl3 paused


что это дает. работающие поды будут продолжать работать 
как я понял попытки обновления подов через модификацию
манифеста деплоя перестанут работать.
также ( я непроверял) если процесс деплоя идет и в это время 
ставим пауузу
то как я понимаю то накатка оставшихся подов прекращается.
тоесть rollout pause она запрещает обновлять деплой?


следущий момент соупуствующий это port-forward
эта штука которая позволяет настроить порт форвардинг проксирования 
 с локального
локалхоста до пода

вот у меня есть под  whoami

# curl 10.233.80.216
Hostname: whoami-deploy-1-75d55b64f6-gqldp
IP: 10.233.80.216   (<=== ip пода)
RemoteAddr: 10.233.66.128:58432   (<==== src ip нашего пакета)


# kubectl port-forward   whoami-deploy-1-75d55b64f6-gqldp  4545:80
Forwarding from 127.0.0.1:4545 -> 80
Forwarding from [::1]:4545 -> 80
Handling connection for 4545

этой командой мы пробрасываем порт 4545 с нашего локалхоста 
в порт 80 на удаленном поде.

# netstat -tnlp | grep 4545
tcp        0      0 127.0.0.1:4545          0.0.0.0:*               LISTEN      2395642/kubectl     

#  curl localhost:4545
Hostname: whoami-deploy-1-75d55b64f6-gqldp
IP: 10.233.80.216
RemoteAddr: 127.0.0.1:47906

вопрос - а зачем нам это надо, какой толк? хороший пример когда
к8 у нас поднят на основе миникуба. там схема такая  - у нас есть 
комп(хост). миникуб создает на хосте докер контейнер. кстати 
также миникуб создает отдельную bridge сеть отличную от дефолтовой
докер сети и он прикрепляет контейнер к той бридж сети. это дает 
то что если мы будем на компе еще создавать какието докер контейн
еры то они по дефолту не будут прикрепляться к этой новой бридж
сети. и это тоже умнО. внутри этого 
контейнера миникуб крутит мутит и поднимает  к8. 
далее получается то что сет карта которая видна изнутри контейнера
и ее IP (скажем 192.168.49.2) она видна как изнутри контейнера
так и снаружи из хоста. это значит что если изнутри контейнера служба
забиндиится на эту карту сетевую по какому то порту то и снаружи
из хоста можно будет обратиться по 192.168.49.2 по порту и достучаться
до сервиса внутри контейнера. ровно поэтому можно через kubectl кото
рый установлен на хосте достучаться до контрол панели к8  внутри
контейнера. через https://192.168.49.2:8443,  но ! внутри контейнера
с к8 также поднята внутренняя "оверлейная" сеть куба которая исполь
зуется внутри контейнера чтобы выдавать из этой сети IP адреса
подам которые поднимаются внутри контейнера. скажем изнутри 
контейнера эта сеть выглядит как IP=10.244.0.1/16
это бридж сеть. она видна только изнутри контейнера. а снаружи
контенера конечно мы ее не видим на нашем хосте. также я подчеркну
что эта бридж сеть она поднимается внутри контейнера не за счет
docker network внутри контейнера а за счет cni плагина. что такое
cni плагин и все такое это я будут завтра отдельно рассматривать.
а пока важно то что внутри докер контейнера там где развернут куб
там есть вирт сеть L3 с набором IP. и поды внутри этого контейнера
получают IP которые видны только внутри контейнера. тоесть
системные поды к8 на которых поднят контрол плейн к8 они забиндены
на такую сет карту которая видна и изнутри докер контейнера 
и снаружи - на хосте. а поды видят сет карту которая доступна 
только внутри докер контейнера но снаружи докер контейнера - на хосте
эти IP недоступны. тоест поясню на примере. внутри докер контейнера
есть сет карта IP=192.168.49.2 которая видна как внутри докер
контейнера так и снаружи на хосте. и на эту карту забиндены поды
на которых крутится контрол панель к8. поэтому сидя на хосте и 
запуская kubectl мы без проблем можем достучаться до контрол панели.
а еще внутри докер контейера есть сеть IP=10.242.0.0/16
и из этой сети назначаются IP подам. но эти IP недоступны с хоста.
получается с хоста мы к подам достучаться подключиться не можем.
нужно заходить внутрь докер контейнера и уже оттуда запускать curl,
это неудобно. мы хотим как то с хоста достукиваться до подов.
И ВОТ ТУТ ТО нам и помогает kubectl port-forward. он позволяет
поднять прокси службу. обращаясь по сокету на хосте мы будем 
достукиваться до подов внутри докер контейнера. вот когда этот
port-forward пригождается !
для этого нам нужно имя пода. выясняем его так или иначе
например через kubectl get pods, пусть имя пода 
whoami-deploy-1-75d55b64f6-gqldp
тогда чтобы до него достучатся запускаем прокси

# kubectl port-forward   whoami-deploy-1-75d55b64f6-gqldp  4545:80

теперь если мы будем стучать на 127.0.0.1:4545 
а в итоге будем попадать на под whoami-deploy-1-75d55b64f6-gqldp
на порт 80
вот!





еще одна полезняшка. как в сервисе изменить селектор под которым
он работает и ищет поды
я не очень понял почему я это тут написал. ведь я рассматриваю
деплой. а не сервис

# kubectl patch service my-app -p '{"spec":{"selector":{"version":"v1.0.0"}}}'



еще полезняшка как зайти в под
# kubectl exec --stdin --tty  название_пода -- /bin/bash
о смысле -- я подробнейше рассмотрел гораздо выше



двигаю  дальше.
если у нас есть деплоймент. и в нем мы пропишем подключение PV
через PVC а этот PV имеет режим ReadWriteOnce который означает то что
этот PV можно монтировать только на одной ноде и  поды которые крутятся
на этой ноде могут ее себе монтировать. но поды на другой ноде
уже не могут ее монтировать. так вот если запустить деплоймент
и число реплик равно 2 и больше то деплоймент будет стараться 
раскидывать 
поды по разным нодам и деплой при этом плевал что у нас получится 
ситуация
что на одной ноде под смонтирует себе PV а на другой ноде под уже
 получит отлуп
и в логах пода будет запись

Multi-Attach error for volume Volume is already used by pod(s)

так вот эта запись она пудрящая мозги. дело не в том что 
PV уже примонтирован к поду. это хуйня. проблема совсем в другом,
в том что деплой плевал на требования PV 
что все поды 
дожны быть на одной ноде. ошибка об этом. 
если мы заставим деплой поднимать поды только на одном хосте,
то мы без проблем этот PV присобачим к миллиону подов,
реешение такое - надо 
заставить к8 создавать поды на одной ноде. тогда никакой проблемы 
примонтиовать 
этот PV хоть на 100 000 подов нет. для того чтобы заставить к8 создавать поды
на одной ноде надо в манифест деплоя добавить NodeSelector

пример деплой с NodeSelector

apiVersion: apps/v1
kind: Deployment


metadata:
  name: depl3


spec:
  replicas: 10
  revisionHistoryLimit: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 5
      maxUnavailable: 0
  selector:
    matchLabels:
      app: nginx3


  template:
    metadata:
      labels:
        app: nginx3
    spec:
      nodeSelector:
        kubernetes.io/hostname: nl-test-02
      containers:
      - name: nginx
        image: nginx:1.7.6
        ports:
        - containerPort: 80
        volumeMounts:
        - name: data
          mountPath: "/usr/share/nginx/html"
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: pvc-02



| CNM DOCKER
| CNI K8

далее в целом я планирую учить к8 сверху - тоесть такие штуки
как Service, StateFullSet итд.
но поскольку всплыло я планирую все таки изучить некоторый вопрос
про к8 снизу а именно CNI и его плагин bridge. и как увязать CNI
плагин с докером. потому что миникуб поднимает к8 внутри докер
контейнера. и он контейнеры создает через докер. но сеть к 
контейнерам он прилевляет не от докер сети а от CNI. хотя
тут надо уточнить. вроде как системыне поды подключены к host 
докер сети. а поды уже к bridge CNI сети.

вобщем есть очень большие темы. а именно про то что есть container
runtime эта типа хрень которая создает контейнер. тоесть создает
неймспейсы и процесс внутри. теперь к контейнеру нужно присобачить
сеть. что значит присобачить сеть. это значит что внутри сетевого
неймспейса контейнера нужно ему назначить IP. и нужно как то соед
инить сетеовой неймспейс контейнера с сетевым неймспейсом хоста - 
я так это понимаю. например как это обычно делает докер - он
в сет неймспейсе контейнера создает veth L3 интерфейс. это и 
ест сет карта контейнера. второй конец veth интерфейс создается
в неймспейсе хоста и втыкается в вирт свич (он у линкса называ
ется вирт бридж. ебанаты). этот veth интерфейс не имеет IP 
он L2. ровно по аналогии с компами в офисе. есть комп у него сет
карточка L3 имеет IP. от нее провод идет в свич. в свиче порт
не имеет IP он L2. хотя если это роутер то он имеет внутри себя 
вирт порт с L3. тоесть с IP. вот так и на линуксе. этот вирт 
свич имеет вирт порт с L3. он имеет имя docker0. вирт свич создает
докер. это его рук дело. вот таккую имее картуинку

 ns контейнера        |              ns хоста                       |
    eth0(veth, L3)---------[veth1 (L2) --- docker0 (L3)] --- ethX(L3)
 
eth0=172.17.0.2
docker0=172.17.0.1
ethX=192.168.0.10

итак у нас на компе ip=172.17.0.2 сидит он на veth L3 порту
в роутах контейнера указано что дефолт гейтвей это 172.17.0.1
это порт docker0 уже в неймспейсе компа. 
eth0, veth1, docker0 это порты которые сидят в одном L2 домене.
поэтому там работает arp там работает броадкаст MAC хрень.
поэтому пакет с контейнера без проблем долетает до docker0.
таким образом у нас пакет с одного сет неймспейса проникает
в другой неймспейс. по правде говоря портам насрать в каких 
неймспейсах они сидят. они их не чуствуют не видят. неймспейс
только определяет какие сет порты "видит" процесс, те порты
которы процесс может выбрать для начальной отправки пакета.
порты в которые процесс может пихнуть пакет как начальную точку
отправки его. далее неймспейсы идут нахуй. дальше у нас идет
классическая сетевая хрень. есть порты. есть броадкаст домен.
есть IP плоская сеть. наша задача чтобы у нас порт который гейтвей
был доступен нам в рамках нашего броадкаст домена. и все. 
тоесть с eth0 вылетает arp запрос мол какой MAC у порта с 
IP=172.17.0.1 этот запрос с eth0 летит до veth1 а из него до docker0
тот обратно отвечает. наш eth0 узнает MAC порта с docker0 и тогда
мы шлем dst_MAC=MAC_docker0 dst_IP=172.17.0.1 
все мы допинговались до порта docker0 с нашего контейнера.
если мы хотим послать пакет куда то  в интернет то мы шлем
dst_MAC=MAC_docker0 dst_IP=8.8.8.8
и далее если iptables правила разрешают маршрутизацию между 
портами docker0 и ethX портами то пакет полетит дальше.
очень кратко и хорошо я это описал в файле "iptables.txt" смотри
там.

так вот я теперь возвращаюсь к теме нашей - как я понял задача
создания вирт свича, создания veth интерфейсов, назначение IP 
адресов внутри контейнера и docker0 L3 порту это все называется
организация сети для контейнеров - это все выделили в отдельную
задачу. и пошли двумя путями. один путь реализован в докере
и называется CNM (Container Network Model), второй путь выбран вы
к8 и называется CNI (Container Network Interface).
CNM и CNI это типа спецификации. а у этих спецификаций есть 
реализации. пздц. CNM придумал докер. CNI придумал вроде как coreos
а к8 его взял себе на вооружение. почему к8 не взял себе CNM?
как  я понял читая горы шлака что к8 програмисты посчитали что 
CNM имеет в своем стандарте много привязок именно к докеру (к
продукту докер) а так как в качестве контейнеризатора (хуй знает 
что это конкретно значит) в к8 используется не только докер то 
им нужна была хрень более общая не привязанная конкретно к докеру
поэтому они послали CNM и выбрали CNI.

как я понял libnetwork программа является реализацией стандарта CNM
( https://github.com/moby/libnetwork/blob/master/docs/design.md )

значит CNM говорит о том что контейнерная сеть состоит из таких 
частей как :
		sandbox
		endpoints
		networks
		driver

sandbox это такая хрень которая обеспечивает внутри контейнера 
такие штуки как route table, dns, endpoints. в общем с одной стороны
понятно с другой нет. в линуксе сендбокс обеспечивается такой 
хренью как network namespace. тоесть если мы создали нетворк нейм
спейс то мы создали и sandbox. 

endpont это сет карточка внутри этого сэндбокса. в линуксе это 
обычно veth интерфейс

набор ендпоинтов образует network. требование к сети следующее - 
все ендпоинты входящие в сеть должны директли иметь связь друг 
с другом. в линуксе эндпоинты это эзернет интерфейсы и они имеют
прямую связь друг с другом через MAC адресацию. прямая связь озна
чаает насколько я понимаю это такая связь когда  ненужны никакие 
посредники имееется ввиду роутеры между двумя эндпоинтами

эндпоинты принадлежащие network обслуживаются некоторым driver.
таковыми могут быть : none, bridge, overlay.
none = это какая то хрень неинтересна на практкие. 
bridge = это самая частая вещь. это означае что в линуксе будет
создан вирт свич (вирт бридж как они его обзывают).  такая сеть
позволяет общаться контейнерам друг с другом внутри одного хоста.
overlay = такой драйвер  и такая сеть позволяет "прозрачно" общаться
контейнерам сидящими на разных хостах. связь между хостами обычно
обеспечивается через технологию VXLAN.
host = этот драйвер элементарный. это всего навсего хостовый
сетевой неймспейс присоединяется к контейнеру. таким образом
процесс в контейнере видит все сет карты которые есть на хосте.

в докере можно посмотреть какие сети на хосте созданы и какой у
них драйвер

# docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
de425cb5bdc8   bridge    bridge    local
1b44888526e2   host      host      local
b1eef3873be8   none      null      local

соответсвенно кода создаем контейнер то можно указать к какой сети
мы хотими подключиться.

# docker run --name "bb2" --network de425cb5bdc8  -it  busybox sh
(кстати мега полезная оация --name чтобы при запуске сразу
назвать контейнер удобным имеенм чтобы потом было удобно к нему
обращаться. )

кстати по дефолту ( и помеому это неизменить) если у нас один
контенер сидит в одной сети а второй контейнер во второй сети
(докер сети) то связь между ними не работает.

создаю еще одну bridge сеть
$ docker network create --attachable --driver bridge   net2

пихаю в нее контейнер
$ docker run --name "bb4" --network  81e0d9ff7558  -it  busybox sh

далее пробую пинговать контейнер в дргуой сети и пошел нахер.

что присходит когда создается новая bridge сеть в докере? он создает
еще один вирт свич. 
если поставить на комп пакет 

# apt-get install bridge-utils

то можно увидеть такую картину

# brctl show
bridge name	       bridge id		     STP enabled	  interfaces
br-81e0d9ff7558		 8000.0242782739f1	no		        vethedd4829
docker0		         8000.0242ddd01a24	no		        veth91534ca

в правой колонке указаны veth интерфейсы которые воткнуты в 
свич. соовтствено второй veth интерфейс уже лежит на стороне конт
ейнера в другом неймспейсе,
в левой колонке указан L3 интерфейс который принадлежит
вирт свичу. этот L3 IP указывается в контейнерах как дефолт гейтвей

# ip -c a | grep -E "docker|br-"
3: docker0:  
    inet 172.17.0.1/16 
22: br-81e0d9ff7558:  
    inet 172.18.0.1/16 


в свойствах контейнера можно увидеть термины такие как sandbox,
endpoint. щас покажу

$ docker inspect bb1
        "NetworkSettings": {
            "SandboxID": "92260cbb58bbe4825cdd7d164f13664df47f9fe38f90e9fbd613409cead63e00",
            "SandboxKey": "/var/run/docker/netns/92260cbb58bb",
            "EndpointID": "7422d092eff9297fb5812a2e85eda9cf27c02b52d3a7d686e6fd63cb828a7a38",
            "Networks": {
                "bridge": {
                    "IPAMConfig": null,
                    "NetworkID": "de425cb5bdc8d6a78e9a057255b81b1cca3894eb49107a8e1f4702323ec85f63",
                    "EndpointID": "7422d092eff9297fb5812a2e85eda9cf27c02b52d3a7d686e6fd63cb828a7a38",
                    "IPAddress": "172.17.0.2"
                }
            }


IPAM это тоже модуль. который отвечает за то какой IP адрес 
назначить сет карту внутри контейнера. 

кстати виден ключ SandboxKey и в нем указано как найти сетевой
неймспейс этого контейнера. тоесть опять же мы видим потдверждение
того что sandbox организован через сетевой неймспейс.
кстати зная сет неймспейс мы можем создать новый процесс,
подключить его к этому сет неймспейсе и например
посмотреть какие сет интерфейсы сидят в этом неймспейсе

# nsenter --net=/var/run/docker/netns/92260cbb58bb ip -c -f  inet address show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
14: eth0@if15: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever


видим 172.17.0.2, ровно тоже что в свойствах контейнера.

далее про docker network ls, 

# docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
de425cb5bdc8   bridge    bridge    local
1b44888526e2   host      host      local
81e0d9ff7558   net2      bridge    local
b1eef3873be8   none      null      local

если мы запустим docker inspect имя_сети
то в свойствах сети мы увидим все контейнеры которые сидят в этой
сети

  "Containers": {
            "01b4f51b20fa8f59773099ca34be843892da1410d2f962111a4044b3bf37d6e5": {
                "Name": "bb1",
                "EndpointID": "7422d092eff9297fb5812a2e85eda9cf27c02b52d3a7d686e6fd63cb828a7a38",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
            }

что еще хорошо это в свойствах сети указано как называется
сетевая карточка в неймспейсе хоста которая входит в эту сеть.
(тот самый бридж сетевой интерфейс)

например если мы смотрим в свойства сети типа бридж мы увидим

# docker network inspect 34324fwdf
...
...
"com.docker.network.bridge.name": "docker0",

в свойствах сети типа host уже что логично никакого имени
сетвого интерфеса нет.

кстати в свойствах любой сети есть инфо и об IPAM(модуле который
занимается назначением IP адреса внутри контейнера)

 "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": null
        },


в сети с драйвером NONE в свойствах тоже не бует инфо ни о 
какой сетевой карточке что логично.

также замечу что когда смотрим свойтва сети то можно это делать
двумя путями, точнее тремя

$ docker network inspect имя_сети
$ docker inspect имя_сети
$ docker inspect id_сети

тоесть во первых можно указывать как id сети так и имя сети.
и во вторых можно указать network inspect обьясняя что именно
мы хотим инспектировать. а можно просто указать inspect

при попытке удалить сеть если в ней есть хотя бы один контенер
то докер недаст удалить эту сеть

забавно то что в описании докера указано что есть драйвер
NONE а в docker network ls в графе драйер он сука пишет вместо
этого NULL. дебилы

что еще интересно. мы обычно пишем docker run или docker exec
если хотим чтото сделаем с контейнерами, но это окзывается сокра
щение от полного формата 
	$ docker conatiner run ...
	$ docker container exec ...

это стало понятно когда юзаешь 
	$ docker network ...

кстати контейнер можно запаузить средствами самого докера
	$ docker container pause   имя_конейнера

что еще странно. если у нас контейенер подклюючен к сети типа NULL/NONE
это значит что в сет неймспейсе контейнера есть только lo интерфейс.
так вот в свойствах этого контейнера будет указано что у него
есть Endpoint , возникает вопрос за какую сет карточку отвечает
этот Endpoint? за lo интерфейс? но тогда разбивается другой пример
если я подключают контейнер к бридж сети то понятно что конйенер
будет иметь еще одну нормальную сетевую карту помимо lo. 
так вот в свойствах этого контейнера ВСЕ РАВНО УКАЗАН ВСЕГО ОДИН 
ENDPOINT . тоест еще раз. у нас есть контейнер он подключен к сети
типа NONE\NULL значит внутри контенера есть только lo сетевая карта,
в свойствах этого контейнера будет указано что внутри контйенера
есть один Endpoint. возникает предподожение что он отвечает за 
lo сет карту. но тогда это проииворчечит другому примеру когда сет
карта подключена к сети бридж. эта карта будет иметь две сет карты
внутри себя - lo и еще карта с IP адресом. тогда по идее в свойствах
карты должно быть ДВА Endpoint. но нет. в свойстваэ будет всего
один ENdpoint. что за хрень. и  отвечает ли Endpoint за lo интерфейс
или нет. непонятно.



итак с докером. и то как он организует сеть для контейнеров 
познакомились. мы смогли пощупать CNM путем того что посмотрели
список networks, посоздавали новые networks. подключили к ним
новые контейнеры. увидели что bridge network это по сути
линукс бридж свойства которого можно посмотреть через brctl 
утилиту. тоесть прям вот глубоко как устроен и как работает CNM
(на примере его реализации libnetwork) непонятно. но чтото 
по верхам удалось пощупать.



теперь перехожу к к8. как он организует сеть 
для своих подов\контейнеров. он использует cni. cni это такой 
стандарт. в реализации это кучка бинарников. мы запускаем
бинарник, на STDIN ему кидаем конфиг. Также переменные окружения
должны содержать тоже часть инфо. и тогда этот бинарник делает
некие телодвижения на хосте он создает внутри контейнера veth
интерфейс. создает на хосте veth интерфейс. также он юзает другой
бинарник который назначает этоим интерфейсам IP адреса. и таким
макаром наш контейнер получает сетевые настройки и доступ в сеть.
насколько cni отличается от cnm да хуй знает. щас просто посмотрим
как этим cni можно руками попользоваться. 

значит CNI на практике идет как кучка бинарников. они называются
плагинами.  нам нужно написать конфиг в json формате и скормить
его на stdin одного из бинарников. также нам нужно в переменных
окружения задать ряд переменных. тогда бинарник используя эти
переменные окружения и конфиг подключить наш контейнер (его сетевой
неймспейс) к сети. а именно создаст сет карты на хосте и внутри
сет неймспейса контйенера, присвоит IP адреса, задаст роуты.
проект CNI лежит тут 
	 	https://github.com/containernetworking/cni
я еще к нему вернусь. а пока начнем с одной супер статьи посвя
щенной cni
		https://www.dasblinkenlichten.com/understanding-cni-container-networking-interface/

качаем бинарники cni
# wget https://github.com/containernetworking/cni/releases/download/v0.4.0/cni-amd64-v0.4.0.tgz
# tar -xvzf ./cni-amd64-v0.4.0.tgz


теперь создаем конфиг новой бридж сети 

# cat > mybridge.conf <<"EOF"
{
    "cniVersion": "0.2.0",
    "name": "mybridge",
    "type": "bridge",
    "bridge": "cni_bridge0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "subnet": "10.15.20.0/24",
        "routes": [
            { "dst": "0.0.0.0/0" },
            { "dst": "1.1.1.1/32", "gw":"10.15.20.1"}
        ]
    }
}
EOF

как видно создается сеть типа бридж. значит на компе будет создан
бридж. тобишь свич сука. 

cni_bridge0 = это имя сет карточки которая будет в рамках свича 
сидеть в сет неймспейсе хоста
isGateway = это вроде как то что cni_bridge0 будет присвоен IP
и он будет назначаен дефолт гейтвееем внутри контейнера в таблице
маршрутизации
ipMasq = это вроде о том что пакеты вылетающие во вне из
cni_bridge0 у них будет заменяться src_IP на IP cni_bridge0
ipam= эта хрень о том что нужно назначить IP сет карте внутри
контейнера. далее идет расшифровка а как это делать
host-local = это имя бинарника который будет конкретно 
заниматься назначением IP внури контейнера. вместо этого 
бинарника можно заюзать бинарник dhcp но тогда его dhcp бинарник
нужно запускать в отдельном окне. тоесть host-local он назначает
адрес IP внутри контенера и делает об этом записи на хосте в 
папке которую я дальше укажу. при этом host-local ненужно запускать
на компе как демон. а вот если указать dhcp то он записей вроде
как нигде на фс неделает да еще его нужно запускать как демона,
да еще судя по статье он нихуя как надо не работает.
cniVersion": "0.2.0" = эта херня означает цитирую The version of the CNI spec in which the definition works with. что это значит
по факту хуй знает

далее вот что. мы создаем на хосте новый сетевой неймспейс.

# ip netns add 1234567890
# ip netns list

теперь запускаем бинарник. ему мы подсовываем наш конфиг.
и также ему мы подсоваываем переменные окружения.
они вот что значат
CNI_COMMAND=ADD - подключить сеть к контейнеру. (под контейнером
понимается сетевой неймспейс)
CNI_CONTAINERID=1234567890 - имя сет неймспейса
CNI_NETNS=/var/run/netns/1234567890 - в этой папке создается
такой файл после того как мы создали сетевой ns. так делает
ядро автоматом
CNI_IFNAME=eth12 - какое мы хотим видеть имя сетевой карты которую
создаст бинарник внутри контйнера
CNI_PATH=`pwd` - нужно указать полный путь к бинарникам CNI


# sudo CNI_COMMAND=ADD \
       CNI_CONTAINERID=1234567890 \
       CNI_NETNS=/var/run/netns/1234567890 \
       CNI_IFNAME=eth12 \
       CNI_PATH=`pwd` \
       ./bridge <mybridge.conf

далее вот что. это были примеры из статьи.  я их уже ранеее
проделал и поэтому я щас малек их изменю:


# cat mybridge2.conf 
{
    "cniVersion": "0.2.0",
    "name": "vasya-bridge",
    "type": "bridge",
    "bridge": "cni_bridge2",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "subnet": "172.30.30.0/24",
        "routes": [
            { "dst": "0.0.0.0/0" },
            { "dst": "8.8.8.8/32", "gw":"172.30.30.1"}
        ]
    }
}



# ip netns add 6778


# sudo CNI_COMMAND=ADD \
       CNI_CONTAINERID=6778 \
       CNI_NETNS=/var/run/netns/6778 \
       CNI_IFNAME=ef1 \
       CNI_PATH=`pwd` \
       ./bridge <mybridge2.conf
при запуске этой команды на экране выскочит

2024/04/18 06:01:40 Error retriving last reserved ip: Failed to retrieve last reserved ip: open /var/lib/cni/networks/vasya-bridge/last_reserved_ip: no such file or directory
{
    "ip4": {
        "ip": "172.30.30.2/24",
        "gateway": "172.30.30.1",
        "routes": [
            {
                "dst": "0.0.0.0/0"
            },
            {
                "dst": "8.8.8.8/32",
                "gw": "172.30.30.1"
            }
        ]
    },
    "dns": {}
}


верхушка сообщения : 2024/04/18 06:01:40 Error retriving last reserved ip: Failed to retrieve last reserved ip: open /var/lib/cni/networks/vasya-bridge/last_reserved_ip: no such file or directory

говорит о том что бинарник ./bridge полез в /var/lib/cni/networks/vasya-bridge/last_reserved_ip и там нихрена не нашел а точнее 
на самом деле как я понимаю это сделал бинарник ./host-local потому
что именно он занимается записью в  /var/lib/cni/networks/vasya-bridge/last_reserved_ip, ненашел там ничего и выругался. то что он там
ничего ненашел это понятно. я щас обьясню чуть ниже. в целом
он просто мы увидели WARNING на экране. никакой ошибки нет.
команда отработала успешно.
оставшаяся часть сообщения
{
    "ip4": {
        "ip": "172.30.30.2/24",
        "gateway": "172.30.30.1",
        "routes": [
            {
                "dst": "0.0.0.0/0"
            },
            {
                "dst": "8.8.8.8/32",
                "gw": "172.30.30.1"
            }
        ]
    },
    "dns": {}
}

показывает статус того что было сделано - сеть была воткнута в 
неймсмпейс. и показаны ее параметры. тоесть итого команда отработала
успешно. что же было сделано?
во первых был создана сет картчока на хосте

# ip -c a  s dev cni_bridge2
cni_bridge2: 
    inet 172.30.30.1/24 

во вторых была создана не просто карточка cni_bridge2,
был создан целый свич. и эта карточка cni_bridge2 является
ее портом, этот порт сидит в сет неймспейсе хоста,

# brctl show cni_bridge2
bridge name	       bridge id		   STP enabled	     interfaces
cni_bridge2		8000.0a58ac1e1e01	        no		       veth2937c950

также как видно это не единственный порт который был создан и 
который тоже входит в сет неймспейс хоста и который входит 
в состав свича. еще есть порт veth2937c950
и действительно

# ip -c a  s dev veth2937c950
33: veth2937c950@if3:

итак в сет неймспейсе хоста было создано две сет карточки. и был
создан сет свич. и эти два порта входят в этот свич. что значит
что эти два порта входят в свич. это значит что эти два порта 
входят в один броадкаст домен L2. если мы пошлем в один из портов
L2 фрейм с широковещательным MAC адресом то этот фрейм будет 
послан в оба эти порта. возникает вопрос а в чем разница между
портами cni_bridge2		и  veth2937c950. я не знаю. возможно 
тем что IP можно назначить только порту cni_bridge2, а может
тем что порт cni_bridge2 грубо говоря смотрит в сторону всех 
остальных физ портов компа которые сидят в этом же неймспейсе 
компа а порт veth2937c950 он ведет в другие сетевые неймспейсы.
тоесть если на компе есть физ карточка  eth0 то пакет прилетевший
в нее может попасть внутрь свича только через порт cni_bridge2,
этот порт является входными воротами чтобы из вне из внешнего мира
попасть внутри этого вирт свича. а порт veth2937c950 он является
воротами для других сет неймспейсов чтобы попасть внутрь свича.

теперт заглянем в наш тот другой сетевой неймспейс. что там
творится

# ip netns exec 6778 ip -c a
1: lo: <LOOPBACK> 

3: ef1@if33: 
    inet 172.30.30.2/24 scope global ef1
       

мы видим что там была создана сет карточка ef1 как мы заказывали.
и ей присвоен IP адрес из сети которую мы заказывали

а что там с таблицей роутинга?
# ip netns exec 6778 ip -c r s
default via 172.30.30.1 dev ef1 
8.8.8.8 via 172.30.30.1 dev ef1 
172.30.30.0/24 dev ef1 proto kernel scope link src 172.30.30.2 

как видим были прописаны маршруты

ну ка пинганем изнутри "контейнера" наш cni_bridge2
# ip netns exec 6778 ping -c 3 172.30.30.1
PING 172.30.30.1 (172.30.30.1) 56(84) bytes of data.
64 bytes from 172.30.30.1: icmp_seq=1 ttl=64 time=0.237 ms
64 bytes from 172.30.30.1: icmp_seq=2 ttl=64 time=0.107 ms

успех

и еще вот что, бинарник host-local он сделал запись на хосте
о том какой IP он назначил контейнеру. 
он создал вот такую папку. и в ней два файла

# ls -1 /var/lib/cni/networks/vasya-bridge
172.30.30.2
last_reserved_ip

# cat 172.30.30.2 
6778

тоесть записано что сет неймсейсу 6778 присвоен IP 172.30.30.2

и 

# cat last_reserved_ip 
172.30.30.2

записано какой последний IP из пула был уже использован

вот как отработал cni бинарник. что он сделал. 
как он это сделал. куда он чо записал. в итоге нащ "контейнер" его
сет стек. имеет карточку, IP, таблицу маощрутизации. и может пинго
вать по крайней мере свой гейтвей.

создам еще один сет неймспейс. и введу его в туже сеть.

# ip netns add 3450

кстати после этой команды в нащем новом сетевом неймспейсе
автоматом уже есть lo сет карточка. ее создает ядро

# ip netns exec 3450 ip -c a
1: lo: <LOOPBACK> 


#      CNI_COMMAND=ADD \
       CNI_CONTAINERID=3450 \
       CNI_NETNS=/var/run/netns/3450 \
       CNI_IFNAME=ef1 \
       CNI_PATH=`pwd` \
       ./bridge <mybridge2.conf
{
    "ip4": {
        "ip": "172.30.30.3/24",
        "gateway": "172.30.30.1",
        "routes": [
            {
                "dst": "0.0.0.0/0"
            },
            {
                "dst": "8.8.8.8/32",
                "gw": "172.30.30.1"
            }
        ]
    },
    "dns": {}
}

команда отрабоала успешно. нащ сет неймспейс 3450 теперь тоже
подключен к сети 172.30.30.0\24 через этот же свич.
появились соответсвтующие записи

# cat 172.30.30.3
3450
# cat  last_reserved_ip 
172.30.30.3

смотрим в контейнер

# ip netns exec 3450 ip -c a
1: lo:  
3: ef1@if34: 
    inet 172.30.30.3/24 

пинганем из этого контенера нащ прошлый контенер
и хуякс... пинг между контенерами не работает. тоесть с 172.30.30.2
не пингуется 172.30.30.3
а не работает потому что написано в файле "bridge.txt" -
потому что у нас схема потока вот такая

                                                     | свич
                   |     <cni_bridge2>---------<порт>|(    )
                   |                                 |(    ) 
 контейнер1 <eth1>-|--------------------------<veth1>|(    )
 контейнер2 <eth2>-|--------------------------<veth2>|(    )
                   |                                 |
                   |                                 |
 сетевые неймспесы |   сетевой неймспейс хоста       |  ядро
 контейнеров       |                                 |
                   |     правила iptables            |



так вот дело вот в чем. когда у нас поток течет между контейором1
и контейром2 то поток течет через порты veth1 и veth2 которые 
сидят в сет неймсейске хоста в котром разадны правила iptables
поэтому поэтому так как поток ядром перабрасывается между интфрейсами
veth1 и veth2 то этот поток отноится к цепочке FORWARD. единстенное
что надо замтетиь что обычно в цепочке foreard идут пакеты которые 
перебрасываются между интфрейсами на основе таблицы маршрутизации
роутинга (тоесть исходная сеть пакета  и конечная сеть пакета 
отлиачаются поэтому выходной порт обпредеяляется на оснвое талицы
руотуинга ) а в нашем случае у нас исходная ip сеть и конечная
ip сеть они одинаковые поэтому переброс с порта на порт идет на 
основе таблицы свичинга. происходит не роутинг а свичинг. тем
не менее раз фрйем переабрасывается с порта на порт то они идут
через цеопчку FORWARD (опять же потому что фрейм не предназначен для
сет карты самого хоста а значит это не LOCAL это FORWARD)
так вот по дефолту на хосте fORWARD дропает все пакеты кроме тех
которые разрешены в явном виде! поэтому наща связт и не раобтает!
первый выход - отклчить обработку пакетов которые свичатся (которые
текут искбчиетльно между бриджевыми портами) на цепочках iptables.
об этом я пишу в bridge.txt для этого нужно в эти файлы

# ls -1 /proc/sys/net/bridge/
bridge-nf-call-arptables
bridge-nf-call-ip6tables
bridge-nf-call-iptables
bridge-nf-filter-pppoe-tagged
bridge-nf-filter-vlan-tagged
bridge-nf-pass-vlan-input-dev

везде записать 0.

другое решение.
это добаить вот это правило

-A FORWARD -m physdev --physdev-is-bridged -j ACCEPT

по сути оно разрешает по цепочке FORDWARD все пакеты которые
ходят между свичевыми портами!

вот почему связь не работала!

<===== закончил здесь
 





для начала еще раз возвращаю внимание к файлу "kubernetes-flannel.txt"
надо его с самого верах обязательно прочиать чтобы отлично понимать
как устроена сеть докера, как устроен вирт свич. как работает связь
меджу контейром который сидит за свичем и сет стеком хоста, или
между двумя контйенерами каждй из котоых сидит за своим свичем 
и между ними лежит сет стек хоста. это все надо хороеноко понимать
прежде чем двиатьс дальше!

собираем тестовую площадку тогда используя неймпейсы, veth, бридж.
( кстати работа с veth отлично описана в этом же файле kubernetes.txt
только выше ищи по строке ip link add , ip link set)
(ксати там же описано то что если я зайду в неймспейс  котором есть
несколько сет карточек и попробую попинговать их же , самих себя 
то по дефолту нихуя не сработает. казалось бы что за нахуй, оказывается
все дело в lo интерфейсе. надо ему назначить ip и поднять его. 
как только это будет сделано то пинги самих себя карточек заработтает,
но это к нашей текущей прблеме не имеет отношения). дальше я хочу
сказать что тема линукс свича\бриджа суперподробно изложена в 
файле kubernetes-flannel.txt прям с самого начала докумнета. поэтмоу
прям сейчас отправляемся туда и читаем. итак я щас собиарюсь понять
почему между контйенерами нет пинга. для этго я хочу собрат тестовую 
систему.
КСТАТИ про CNI я еще хотел добавит что бинарник cni когда
мы его юзаем он еще добавляет правила в iptables и он 
еще и комментарии всятавляет например

-A CNI-26633426ea992aa1f0477097 -d 10.15.20.0/24 -m comment --comment "name: \"mybridge\" id: \"1234567890\"" -j ACCEPT









ВОПРОС: если мы создали докер контейнер. поключили его к NONE\NULL
докер сети.  а потом внутрь этого коненера подключили сет карту
на основе CNI. возникает вопрос появиттся ли какая нибудь новая 
сеть в выводе 
	$ docker network ls
ответ - НЕТ
это очень важный вывод. очень полезный. тоесть docker network ls
тоеть docker никоим образом не видит не чуствует сети созданные
с помощью CNI если они даже подключены внутрь докер контейнеров.
это очень важный вывод.










user@ubuntu-1:~/cni$ sudo ip netns exec 1234567890 ifconfig
eth12     Link encap:Ethernet  HWaddr 0a:58:0a:0f:14:02
          inet addr:10.15.20.2  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::d861:8ff:fe46:33ac/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:16 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:1296 (1.2 KB)  TX bytes:648 (648.0 B)

user@ubuntu-1:~/cni$ sudo ip netns exec 1234567890 ip route
default via 10.15.20.1 dev eth12
1.1.1.1 via 10.15.20.1 dev eth12
10.15.20.0/24 dev eth12  proto kernel  scope link  src 10.15.20.2
user@ubuntu-1:~/cni$


docker network ls покажет что есть сеть и у нее драйвер null


cat > mybridge2.conf <<"EOF"
{
    "cniVersion": "0.2.0",
    "name": "mybridge",
    "type": "bridge",
    "bridge": "cni_bridge1",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "subnet": "10.15.30.0/24",
        "routes": [
            { "dst": "0.0.0.0/0" },
            { "dst": "1.1.1.1/32", "gw":"10.15.30.1"}
        ],
        "rangeStart": "10.15.30.100",
        "rangeEnd": "10.15.30.200",
        "gateway": "10.15.30.99"
    }
}
EOF







user@ubuntu-1:~$ sudo rkt run --interactive --net=customrktbridge quay.io/coreos/alpine-sh
pubkey: prefix: "quay.io/coreos/alpine-sh"
key: "https://quay.io/aci-signing-key"
gpg key fingerprint is: BFF3 13CD AA56 0B16 A898 7B8F 72AB F5F6 799D 33BC
 Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
Are you sure you want to trust this key (yes/no)?
yes
Trusting "https://quay.io/aci-signing-key" for prefix "quay.io/coreos/alpine-sh" after fingerprint review.
Added key for prefix "quay.io/coreos/alpine-sh" at "/etc/rkt/trustedkeys/prefix.d/quay.io/coreos/alpine-sh/bff313cdaa560b16a8987b8f72abf5f6799d33bc"
Downloading signature: [=======================================] 473 B/473 B
Downloading ACI: [=============================================] 2.65 MB/2.65 MB
image: signature verified:
 Quay.io ACI Converter (ACI conversion signing key) <support@quay.io>
/ #
/ # ifconfig
eth0 Link encap:Ethernet HWaddr 62:5C:46:9F:57:3A
 inet addr:10.11.0.2 Bcast:0.0.0.0 Mask:255.255.0.0
 inet6 addr: fe80::605c:46ff:fe9f:573a/64 Scope:Link
 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1
 RX packets:6 errors:0 dropped:0 overruns:0 frame:0
 TX packets:7 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0
 RX bytes:508 (508.0 B) TX bytes:578 (578.0 B)

eth1 Link encap:Ethernet HWaddr A2:EE:49:17:03:EA
 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0
 inet6 addr: fe80::a0ee:49ff:fe17:3ea/64 Scope:Link
 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1
 RX packets:7 errors:0 dropped:0 overruns:0 frame:0
 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0
 RX bytes:578 (578.0 B) TX bytes:508 (508.0 B)

lo Link encap:Local Loopback
 inet addr:127.0.0.1 Mask:255.0.0.0
 inet6 addr: ::1/128 Scope:Host
 UP LOOPBACK RUNNING MTU:65536 Metric:1
 RX packets:0 errors:0 dropped:0 overruns:0 frame:0
 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:1
 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)

/ #
/ # Container rkt-596c724a-f3de-4892-aebf-83529d0f386f terminated by signal KILL.
user@ubuntu-1:~$

