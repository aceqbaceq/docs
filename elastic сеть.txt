когда нода стартанула
то эластик коннектит друг с другом 



нода ищет соседей используя ip сокеты указанные в
	discovery.seed_hosts: [ip:port , dns_name:port  ]
	
ВАЖНО при этом заметить что мы можем указать нетолько IP но ПОРТ! 
это дейсивительно важно потому что когда нода стартанула и ищет соседей одного ip ей недостаточно. ей надо знать и порт на который ей стучаться.
обычно мы порт неуказыаем и система тогда считает что он равен деволтовому 9300.
 к чему это приводит. если у соседй эластик принимает запросы на порт 9300
 тогда проблем нет. наш порт 9300 и мы тогда в discovery.seed_hosts можем указать DNS записи вместо Ip адресов. это очень удобно порой. потому что если чтото поменялось то мы просто заходим на DNS сервер и меняем там а на конечных нодах менять ничего ненужно. а так получается что надо менять на каждой ноде в кластере и потом еще эту ноду перезапускать что жопа. 
   но если у нас соседи слушают не 9300 а другой порт тогда получается что в discovery.seed_hosts мы неможем указать DNS записи потому что DNS запись типа A она содержит только IP. номер порта туда никак не пропихнешь. и тогда остается отказаться от DNS записей в discovery.seed_hosts и вписывать туда IP с портами. как бы получается захаркдоживать туда это. а это оочень неудобно. потому что если чтото поменяется надо будет менять на всех нодах в конфигах и эти ноды еще и перезагружать а это все жопа.
 по идее  с помощью dns можно было бы прописыват на DNS кастомные порты
через SRV записи. но я поиска вроде как эалстик неподдерживает работу с srv записями для дискаверинга. есть какието плагины внешние но для суперстарых эластиков.
вообще процесс поиска соседей описан вот здесь 
 https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-hosts-providers.html#file-based-hosts-provider  

там кстати сказано что если в discovery.seed_hosts неуказаны порты 
то наша стартующая нода использует порт прописанный в
	transport.profiles.default.port
а если этой настройки нет то использует тогда
	transport.port
 
 но трогать эти настройки для обнаружения соседей нам чревато потому что
 изначально они предназначены для того чтобы на нашей ноде прописать какой порты она будет слущать и какой порт она сообщить соседям. то есть этих настроек изначально совсем другое предназначение не про соседские настройки они а про настройки нашей ноды. так что я считаю что их лучше нетрогать для целей "как найти порт моего соседа".
 
 но! в той же ссылке написано что эластик может использовать вместо настрройки discovery.seed_hosts внешний файл. причем более того невместо а в дополнение. мы в конфиге эластика прописываем строчку
	discovery.seed_providers: имя_файла
кладем текстовый файл туда же где лежт конфиг эластика. вэтом файле
мы прописываем

	10.10.10.5
	10.10.10.6:9305
	10.10.10.5:10005
	
и что еще класно что эластик он автоматом перечитыает этот файл если мы его изменим и при этом эластик ненужно перезгружать.

а что файл это выход он дает интересные возмжонсти!
 
далее.
	
 discovery.seed_hosts: ["192.168.7.234:9300", "192.168.7.224:10000", "192.168.7.223:30890", "192.168.7.221", "192.168.7.220" ] - это список в целом
 где искать мастеров. из него непонятно какой мастер кроется за каким IP
 
 когда нода-А тыкается вслепую в ip сокеты из списка discovery.seed_hosts 
 и попадает на некоторую ноду-Б то эта нода-Б ей персонально представляется и сообщает через какой ip сокет с ней связываться в дальнейшем.
 на ноде-Б как с ней связываться в дальнейшем указывается через 
 transport.publish_host: [ "192.168.8.500" ] 
 поэтому список  discovery.seed_hosts испольщуется только один раз 
 когда нода-А стартует и ищет соседей. потом она вобще им непользутся.
 она пользуется сокетами которые ей сообщили сами соседи.

полуается что изначально нода-А связывается с нодой-Б путем перебора
через "192.168.7.224" а потом от самой ноды-Б получает указ связываться с ней через "192.168.8.500". и реаьно будет общаться с ней на постоянной основе через "192.168.8.500" а "192.168.7.224" уже никак небудет использщоваться. 
вот такой странный механизм механизм.
еще раз в этом списке
	discovery.seed_hosts: ["192.168.7.234", "192.168.7.224", "192.168.7.223", "192.168.7.221", "192.168.7.220" ]
указано как связыавться с другими нодами при старте первый раз.
важно что только один раз. первый раз. и важно то что из этого списка
непонятно какой ip за какого соседа отвечает.
а эта опция
	transport.publish_host: [ "192.168.8.500" ]
указвыает как с нодой общаться на постоянной основе 
конечно в discovery.seed_hosts можно составить из  ip transport.publish_host так обычно и делают. но в целом они необязаны совпадать. условно говоря discovery.seed_hosts вот так обычно составляют

	discovery.seed_hosts: ["transport.publish_host ноды-А", "transport.publish_host ноды-Б", "transport.publish_host ноды-С" итд]
но еще раз говорю это необязательно а опционально.
в целом очень странная  архитектура.

 
далее идет длинное обьяснение про тоже самое но оно более старое и неправильное. но возможно там есть полезные детали:

		сокеты которые указаны в этой опции они отвечают за нахождение 
		нод между друг дружкой на стадии фаза-1. 
		причем невсех нод а масетров только 
		фаза-1(первый контакт) это фаза когда нода хочет связаться с соседом один мастер хочет
		получить ответ от другого мастера. фишка втом что указанные сокеты они небудут использоваться потом для постоянной связи масетров друг с другом.
		мастер свзяыается с друним мастеров по указаннму сокету. далее тот мастер указывает сам по какому сокету сним общатся на постоянной основе.
		поэтому на фазе-2 (постояння связь) мастер с мастером неиспользует сокет
		указанный в discovery.seed_hosts: он исольузует тот сокет который ему сообдлил его сосед мастер сам. зачем так дебильно сделано непонятно.
		потому что по факту витоге все равно настраиваешь мастеров чтобы они собщали для фазы-2 ровно теже сокеты что и указаны в discovery.seed_hosts: на фазе1.

		пример 

		фаза-1. нода-1 смотрит в discovery.seed_hosts видит там ip-1:9300
		и по нему ищет соседа.
		сосед нода-2 сообщает что с ним надо связываться для фазы-2 через ip-1:9300
		в итоге нода-1 общается с нодой-2 через ip-1:9300

		на ноде сокет для фазы-2 указываетс янатсройкой

		transport.publish_host
		transport.publish_port
		либо
		transport.profiles.default.publish_host
		transport.publish_port

		небольшой замечение всторону от темызамечу что такой настройки как 
		transport.profiles.default.publish_port
		нет


		кстатив этой строке указываем только мастеров.

		пример

			discovery.seed_hosts: [ "svc-master-01:9300, svc-master-02:9300, test-kub-02.mk.local:30900, test-kub-02.mk.local:30901, test-kub-14.mk.local:30902" ]
			
		он при своем старте вначале резолвит эти DNS имена в IP (делает 
		он это всего один раз только при старте. и потом если ip поменялись
		у данных dns имен то нода об этом понятия неимеет.)

		вот зарезволвила нода эти dns имена и имеет список IP:порт, ip:порт
		и туда нода начинает долбиться. если порт неуказан то исопльзуется дефолтовый 9300

		таким макаром нода теперь знает где ей искать мастеров. но ! привол в том
		что дальше все происходит ОСОБЕННО. ака дебильно.
		нода делает первое обращение по сокету из списка и получает в ответ 
		от той ноды (внимание!) сокет по которому этой ноде нужно вдальнейшем
		к той обрашаться.дада! обана! таким образом та нода может сообщить совершенно другой сокет и теперь наша нода будет обрашаться к той ноде 
		по новому сокету.
		таким образом сокеты в discovery.seed_hosts это всего навсего фаза1, начальные сокеты по которым нода ищет соседей-мастеров. и если они откликаются то они ей (непонятно зачем) сообщают новые сокеты по которым наша ноды к тем нодам должна обращаться.
		ваУ! эйнштейн отдыхает.

		нарисую схему как надо настрить нашу и ту ноду если их разделяет 
		проброс портов.

		пусть у нас 
			нода-1 вращается в поде на хосте-А
			нода-2 вращается в поде на хосте-Б

		хост-А имеет ip-A
		хост-Б имеет ip-B

		нода-1 пусть имеет транспортный порт 9300,
		с помощью сервиса мы пробрасыаем 9300 из пода наружу ноды на порт 39000

		таким образом если мы обратимся на ip-A:39000 то мы попадем внутрь пода на порт 9300

		ip-A:39000 ---> под (нода-1 порт 9300)

		таким образом мы сидя в сети можем обраться на ip-A хоста и в итоге
		попасть на под.

		тоже самое для ноды-2 на хосте-Б

		ip-Б:39002 ---> под (нода-2 порт 9300)


		тогда общая схема как под с подом может связаться друг с другом.
		фишка в том что хост-А и хост-Б они сидят в разных кубернетесах
		и между ними нет прозрачной оверлейной связи


		под(нода-1 порт 9300)  ---> хост(ip-A:39000)  --> сеть <-- хост(ip-Б:39002) <--- (нода-2 порт 9300)


		конфиг (нода-1)
			 - name: "discovery.seed_hosts"
			   value: "ip-Б:30902"  (внешний ip того хоста)

			 - name: "transport.profiles.default.publish_host"
			   value: "ip-А"  (внешний ip этого хоста)
			 - name: "transport.publish_port"
			   value: "30900"  (внешний порт этого хоста)



			
		конфиг (нода-2)
			- name: "discovery.seed_hosts"
			  value: "ip-A:30900"  (внешний ip того хоста)

			 - name: "transport.profiles.default.publish_host"
			   value: "ip-Б"  (внешний ip этого хоста)
			 - name: "transport.publish_port"
			   value: "30902"  (внешний порт этого хоста)


		еще раз как это будет работать
		нода-1  на стадии "фаза-1" смотрит в конфиг в discovery.seed_hosts
		и стучиться на ip-Б:30902 получает ответ от той ноды-2 которая ей еще раз 
		для "фазы-2" говорит чтобы она к ней обращалась через ip-Б:30902

		вот такой дебилизм.

		ЧТО ЕЩЕ СУПЕР ВАЖНО ОТМЕТИТЬ:
			как видно в конфиге я использовал такую штуку как профили.
			так вот ОЧЕНЬ ВАЖНО ОТМЕТИТЬ  что параметр 
			
			что это за хрень профили. ооооооо... это отдельная песня.
			
			
			port: The port to which to bind.
			bind_host: The host to which to bind.
			publish_host:

			
		!! transport publish_port в рамках профилей неработает хотя они завялвляют
		что работает.

		!! из куба из пода обраиться на внешний порт хоста своего = работатет . 

		это неработает
		   - name: "transport.profiles.default.publish_port"
					value: "9400"

		в профилях опция publish_port НЕРАБОТАЕТ

		это работает
		   - name: "transport.publish_port"
					value: "9400"



		 1)написать про фазы
		 2)про сеть профили
		 3)про логи что в логах указн компонент который в лог пишет
		 4)
		 
		 {"type": "server", "timestamp": "2020-11-25T23:26:20,886Z", "level": "WARN", "component": "o.e.d.HandshakingTransportAddressConnector", "cluster.name": "es-cl-03", "node.name": "master-03-0", "message": "[connectToRemoteMasterNode[172.16.102.31:30900]] completed handshake with [{master-01-0}{1c7F8xZETEyTDgFSE51OKw}{hKHiG0-zQ02ozD8dcegmPg}{svc-master-01}{10.101.253.96:9300}{mr}{xpack.installed=true, transform.node=false}] but followup connection failed",
		"stacktrace": ["org.elasticsearch.transport.ConnectTransportException: [master-01-0][10.101.253.96:9300] connect_timeout[30s]",
		"at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.onTimeout(TcpTransport.java:1004) ~[elasticsearch-7.7.1.jar:7.7.1]",
		"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) ~[elasticsearch-7.7.1.jar:7.7.1]",
		"at java.util.concurrent.ThreadPoo

		из чего я делаю вывод что если на ноде несколько профилей
		когда к ней орашается сосед то она для связи отдает именно дефлотовый профиль.

		получатес янгесколько профилей заводить можно но толкеу от них ноль.
		они работают только на фазе1. 
		на фазе 2 все равно соседполучит инфо чтобы к соседу он обращался по сокету от дефолтового профиля

		непонятно какую роль играет cluster iunital nodes.
		скажем там 7 мастеров. указано.
		потом мы динамически расштряем до 10 масетров. вопрос
		сколько мастеров нужно клкстеру для кворума? 4 или 6.

		удадение pvc из под sts и перепулбикация пода ничего недадут.
		Pvc невосстаналивется. поможет только вот что. уменщшить число реплик
		в sts а потом обратно увелчичтть. тогда sts пересоздать pvc.
		но опять же первой удаляетс прослоедняя реплика. поэтому если мы уадили
		pvc первого пода то нам придется уменьшать число рпелик аж до нуля.

		!!! при переносе пода с куба на другой куб надо поменять сторадж
		класс на соответующий
