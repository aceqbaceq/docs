kubernetes + elastic
  
  значит есть изначальная проблема:
  есть кластер из нескольких физ серверов. 
  на них крутится эластик.
  
  если один физ сервер вырубить (физически , считай что как типа авария)
  то эластик  начинает тормозить.
  
  экспериментальным путем выяснено что дело совершенно не вэластике
  а в самом кубе.
  
  выглядит это так :
  скажем веб приложение которое зависит от эластика когда все работает
  отвечает на 200мс.
  если с одного физ сервера убрать все поды эластика через  drain cordon
  
#  kubectl cordon mk-kub2-07
#  kubectl drain mk-kub2-07

  то есть поды мы убрали но связь с сервером остается по прежнему. по прежнему сам сервер виден кубу и он в строю 
  
  причем 
  эти поды они на других серверах незапускаются потому что дисковый сторадж локальный. то есть с точки зрения эластика часть нод
  потеряна. но главное что при этом на физ серверер никаких подов
  эластика нет и связь с ним по прежнему есть. так вот при этом веб прилоржения начинает отвечать 
  300мс пока эластик перкидывает шарды на оставшиеся ноды а когда эластик закончит этот процесс то веб приложение отвечает за прежние 200мс. итак эластик вобще ни при чем. так вот если после этого выключить сервер или отключить на нем сетевые карточки то вэб приложение начинает отвечать уже на скорости 700-800мс.
  это показывает что проблема лежит в самой архитуектуре эластика.
  в его сетевой части.
  
  вот список подов который остается крутится после того как поды
  эластика убраны
  
# kubectl get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name --all-namespaces -n elastic-prod | grep kub2-07

mk-kub2-07   local-provisioner-tp9cd
mk-kub2-07   kube-flannel-ds-dqj8z
mk-kub2-07   kube-proxy-rdfcw

получается к какомуто из эьих компнентнов идет обращение. он недоступен
из за этого все остальное тормозит.

получается проблемк если мы хотим обслужить ноду или нода выпла из кластера из за этого все оставщшееся будем тормозить.

возникает вопрос как легко удалить и потом обратно восстановить 
вот эти поды на этом физ сервере чтоы решить эту детскую проблему.
куб как платформа неочень получается по сравеннению с классическимим VM.

как еще говнище мы имеем связанное с этой темой.
вот у нас есть три пода входящие в состав реплики сет.

elastic-prod-interface-0   1/1     Running    1          17d
elastic-prod-interface-1   1/1     Running    0          61m
elastic-prod-interface-2   0/1     Init:1/2   0          2s

предположим физ сервер упал один из и interface-1 теперь недоступен.
казалось бы щас дадим команду на уменьшение числа реплик 

# kubectl scale  statefulsets elastic-prod-interface  --replicas=3 -n elastic-prod
  
 и умный куб удалить из реплики именно недоступный под. а нихуя.
он напишет что все окей после запуска команды но по факту он ничего 
небудет делать он будет ждать пока физ сервер вернтся в строй  
потом куб запустит заново interface-1 и удалит interface-2

тоесть куб тупой он всегда удаляет в реплике сете самый последний под
и никак иначе. 

получсется если мы хотим удалить из реплики сета определенный под это проблема. единственно возможный вариант это удалить под руками и тут же уменьшить число реплик. может быть поможет

=====

 