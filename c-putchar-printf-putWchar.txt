| c 
| putchar
| putwchar
| printf

я хотел в этом куске поговррить про то как
работает печать символов у программ на си на 
стдаут. если мы хотим напечатать текст то 
как нам это делать какие подьебки тут есть. 
как это устроено.

что такое печать с точки зрения программы\процесса.
это то что мы просто пихаем байт куда то.
в си есть два пути куда можно пихнуть байт.


первый путь это мы пихаем байт в дескриптор fd/1
второй путь это мы пихаем байт в "стрим" stdout

расказываю. наша прога работаем вунтри контейнера
под названием процесс. этот процесс имеет со стороны
ос три файла открытых. связт процесса и этих 
файлов идет через дескрипторы. дескриптор 1 связан
с файлом и этот дескриптор открыт с правом на запись.
обычно этот дескриптор ведет на файл который ведет
на терминал. поэтому если мы сунем байт в fd/1
то он попадет на термиал и на его дисплее обычно
чтото будет нарисовано. в си пихнуть байт в fd/1
можно через глбиц фунцию write()

    char cc = 0x71;
    write(1, &cc, sizeof(cc));
    cc = 0x0A;
    write(1, &cc, sizeof(cc));


во врайт мы указываем декриптор файла куда 
мы хотим пихнуть байт. указываем поинтер на кусок
памяти где лежат байты которы мы хотим пихнуть
и указываем сколько байт мы хотим из поинтера
пихунть
итак мы можем пихать байты непосредствтенно в 
файл чеерез дескриптор. тоесть сисколл берет байты
из памяти процесса и пихает их файл. ну а так как
файл ведет на терминал то в итоге мы пихнули их
на терминал. 


второй путь который я уже указал это
 мы пихаем байт в "стрим" stdout

что такое стрим. когда мы компиируем нашу прогу
то помимо нашего кода  в програму встраивается код
глибц. и он тоже выполняется когда наша прога
запускается. этот код он создает в памяти процесса
буфер. и создает поинтер на этот буфер. и еще есть
глибц функции которые умеют работать с этим буфером
через этот поинтер. поинтер имеет имя stdout
и схема выглядит так. мы берем глибц фуцнкцию и 
говорим ей - засунть вот такой то байт в буфер
внутри памяти процесса используя поинтер stdout

    putc(0x71, stdout);
    putc(0x0A, stdout);

и тода прроисходит вот что. putc он пихает 0x71 
и 0x0A байты в буфер в памяти самого процесса 
по адресу который записан в поинтере stdout.
если этот буфер переполняется то putc() вызывает write() который берет байты из буфера внутри памяти
процесса и пихает их на fd/1 
либо если буфер еще не переполнился но программа
уже хочет выйти то кусок кода глибц также взывает
write() чтобы он скопироал байты из буфера внутри
памяти процесса на fd/1

значит в чем разница между использованием write()
и putc()
она очень большая. когда мы вызываем write() то 
он сразу выывзет СИСКОЛЛ write() который копирует
байты из памяти процесса в файл.
а когда мы вызываем putc() обычно сисколл write()
невзывается а просто силами юзер кода байты копиру
ются из одной области памяти процесса в другую
область памяти процесса. и если только буфер
который сидит за *stdout пеерполнился только тогда
взывается сисколл write() это дает то что с одной
стороны вывод в файл прооисходит на сразу он 
задерживается (как говорят буферизуется) но! - при 
этом мы круто эконоимим на запусках сисколла write()
а это дает то что мы круто разгружаем цпу на компе
дело в том что вызов сисколла это дорогая операция
с точки зрения цпу. нужно сделать кучу хреней чтобы
ссиколл выполнился.поэтом желательно сискооллы по 
пустками не вызывать. поэтому более разумно с точки
зрения разгрузки цпу собрать данные которые мы хотим
тиснуть в файл в кучку. а потом вызывать один
раз сисколл который их всей пачкой и скопирует из
памяти процесса в файл. да с точки зрения юзер
эксприенса и приложения будет задержка. но с точки
зрения разгрузки цпу на хосте будет очень круто.
так вот вызоыв глибц write() сразу запускает
сисколл write() а вызов глибц putc() врят ли к этому
приведет. 


итак разницу между write() и   putc() я обьяснил.
под write()  яи мю ввиду глибц функцию. далее если
особо неоговоерно я говорю только про фунции. 
ссиколл я будут особо выделять.

итак грубо говоря можно сразу "вызывать" сисколл
и писать в файл. а можно копировать во внутренний
буфер. из которого уже потом асинхронно скопом 
куча байтов будет когда то там через сисколл
записана  в файл. 

еще раз посмотрим на первый способ

    char cc = 0x71;
    write(1, &cc, sizeof(cc));

вопрос что если я хочу записать кучу байтов как
это организовать. нам нужен способ насоават кучу
байтов в память процесса. 
это можно сделать нарпимер вот
так 

    long int i1 = 0x0A77767574737271;

я взял тип данных макс большой из простых тоесть
это данные длиной 8 байт. тоесть я организовал 
кусок в памяти длиной 8 байт . и потом попросил
ОС взять эти байты и засунуть их в файл. 


    write(1, &i1, sizeof(i1));


тут важный моментик. в файл данный кусок байтов
будет пихаться байтами в обратном порядке
тоесть вот так

  71 72 73 74 75 76 77 0А

потому что на бумаге байты в хекс мы запиываем
в биг ендиан порядке а цпу работает с памятью и 
файлами в лоу ендиан порядке
но это так заметка.

главное то что данный метод позоволяет нам софрмиро
вать в памяти кусок размером 8 байтов это максимум
. аесли мне нужен кусок размером больше то 
нужно использовать МАССИВ.

что такое массив в си. это набор однородных элементов.
можно сделать массив где каждый элмент 1 байт
по размеру

   char c1[] = { 0xd0,  0xae,  0x0A };

ксати элемента массива уже сразу суются в лоу ендиан
прядке. то есть как видим так и он их засунет.

можно сделать массив из элентом где каджый по 
два байта по размеру


    short int i2[] = { 0xAED0,  0x7271,  0x000A };

сами елменты указаны в том пордке как они буду
засунуты в память.  тоесть первым будет засунут 0xAED0
хитрость в том как его байты будут в памяти сидеть.
так как хекс мы запывем в биг ендиан на бумаге
а цпу пишет в память в порядке лоу ендиан то
по факту байты пкрвого элементв  памятьи будут 
сидеть в обратно поряке

    D0  AE

а в целом массив в памяти будет иметь вид

   D0  AE   71  72   0A  00

сосвбтвенно далее драйвер терминала будет
брать по одному байту и пихать на терминал
начиная с байта котоырй сидит на минимальном 
офсете. тоест с D0.

ос когда вощьмет этот кусок памяти то оно 
засунет эти байты в файл по одной штуке начиная
слева тоесть прмы будет запиан байт D0 а посегим
будет записан файл 00
поэтмоу на кране мы увидим байты в точно таком
порядке

   D0  AE   71  72   0A  00

по факту мы увидим на терминале

    Юqr


вот мы втдим "Ю" потому что терминал понимает 
ютф-8 кодировку. он получает у нас D0 а потом AE
тоесть он получает массив из двух байтов. 
первый байт это D0 а второйбайт из масива это AE
и он понимает что это 0xD0 0xAE что это "Ю"
вточнсоти как узано в ютф-8

и наэтом месте прочитай читай "utf-8-16-32.txt"

таким образом я в тексте программы заиписываю
массив из однобатывых либо многтбатовых
хреней. компилято их сует в память. далее
я через write() прошу ядро записать этот кусок
памяти в буфе драйвера ттай. а он берет оттуда 
по одному байту и сует на терминал. а тот 
декодирует и рисует глифы. 
вот еще раз пример

    short int i2[] = { 0xAED0,  0x7271,  0x000A };
    short int  *buf = &i2[0];
    write(1, buf, sizeof(i2));

в  целом для write() вобще все равно какая
структура данных внутри области памяти которую 
он копирует. тоесть для него это просто набор 
отдьеных байтов. и драйвер ттай копирует в терминал
все по одному байту из своего буфера.
так что в плане копирования write() и влпане 
кпроания из драйвера ттай в термиал  - енинесс
не играет ниакой роли. невреен на счет если 
термианао рабоиает в реиме ютф-16 но точно
не играет роли если он рабоает в режиме ютф-8

я показал выше как можно создать массив байтов.
вот так

    short int i2[] = { 0xAED0,  0x7271,  0x000A };
или вот так
   char c1[] = { 0xd0,  0xae,  0x0A };

а можно еще вот так

   char c1[]= "asdasdasdas";

хрень в виде скобочек это просто такая заувилованная
форма массива. тоесть 

"asdasdasdas" = { 'a', 's', 'd',...'s', 0x00 };

толко еще на конец ноль байт. 
кмпоилтятор так и пишет что "......" эта хрень
имеет тип данных char*[]
но тут важно понять что "........" это всего навсего
алиас для формы обозначени массива { , , , , ,,, , }
причем именно для массива где элементами явлаяются
однобайтвые элементы. тоесть массив двухбайтовых
элементов уже такой формы неимеет.
тоесть

"........" = { байт1, байт2, байт3, ... байтН}

тоесть если я хочу заать масив из однобтыйх эемегтов.
то я могу поступить двумя пуяти могу вот так
 
char p[]={ байт1, байт2, байт3, ... байтН}
а могу вот так

char p[]="байт1байт2байт3байтН";

едисвнетнное что если я использую фрму в виде кавычек
то байты можно укзаать вот таким споосбом

char p[]="\x01\xA0\xNN...\xMM";

либо вот таким

char p[]="saasdasadadsasd";

причем в плане символов интересно. по сути кажый
символ это набор байтов потом в файле. эти байты
читает мой терминал и на оснвое кодировки терминага
рисует символ на экране в редакторе текста програмы
на си. тоесть символы необязаны быть асккии. 
воббще поххер. любые байты могут стоять между квычкими
. симвыолы это просто как мой терминал иенптерутреи
эти байты в симолы на освное той кодировки в котрой
этот термиал рабоает. в даннмом сулчае щас терминалы
работаютв ютф-8 кодроуке. компилиятор коода я его
натрваливаю на этот текст проагмы на си. то он 
по дефолту считает что байты которые стоят между
кавыяками это ютф-8 кодировка. а вобще это 
задается в флагах комплиятора 

 -finput-charset=charset
Set the input character set, used for translation from the character set of the input file to
the source character set used by GCC.  If the locale does not specify, or GCC cannot get this
information  from  the  locale,  the  default is UTF-8.  This can be overridden by either the
locale or this command-line option.  Currently the command-line option  takes  precedence  if
there's  a  conflict.   charset can be any encoding supported by the system's "iconv" library
routine.

в данном случае в целом это неимеет значения 
потому что мы через write() прото туппо берем
байты из текстковго файла прогарммы и суем 
их в буфер ттай драйвера. но это будет иметь
значние кодга я передй к рассмотрению wchar_t 
и так называемый wide charcters. пока забудем.


вобщем как у нас рабоает петчать через write()
я создаю массив из байтов которые в ютф-8 
либо так 

    short int i2[] = { 0xAED0,  0x7271,  0x000A };
или вот так

   char c1[] = { 0xd0,  0xae,  0x0A };

или вот так

char p[]="\x01\xA0\xNN...\xMM";

тут еще напомю что если квычки то он еще к этому
масиву вхвеост добавляет 0х00 байт.

и потом я просто этот массив через write() кпирую
в буфер ттай дрйвера. а он берет по байтику и 
пихает на терминал. а тотраспознает ютф-8 
и петчает глифы.


при вот этом способе у нас никак не 
участвует локаль. локаль это отдельная тема.
но эта такая хрень. которая имеет рекомендаттный 
характер. и мой программа может ее соблюдать а
может нет. так вот для write() плевать на локаль.

а вот другая фгуция putc() ей на локаль уже не 
плевать. потому что она более софистикетейд.
во перых она копирует данные не в дескиптор
а в буфер внутри памяти процесса (так назымый стрим
хотя название идиотское). и также эта фунция 
она консльтрируется что там за локаль. и например
это может приводит к тому что я указал в аргумете
один байт код а она его на оснвое локали замненит
на другой байт код.  правда putc() моет неоен 
удачная фнуция потому что она вообще то ппреднана
ена для печати всего одного граф символа а не 
кучи. тоест аналоги более знаковые такие как 
pritnf() например который неимеет этих огарниений.


значит putc() он как работает. он был придуман в
те времна когда кодировки были однбтайтовые.
поэтому он на входе брал char 1 байт и на основе
него рисовал один символ , тоесть по факту  он 
совал его без измеений в стрим stdout и он летел
на терминал и там он рисовался. получается была
схема - я пихаю 1 байт в putc() и у нас на терминале
появялется +1 глиф. важно подчеркнуть что putc() он 
никак не меняет этот байт. а сует его дальше как есть.
но щас терминалы рабоаюта на кодировке UTF-8 котррая
один глиф может кодровать как одним байтом так 
и четырьмя байтами. например 0xD0 0xAE это глиф 'Ю'
поэтому если мы тпперь суем байт в putc() то у нас на
терминале может нихуйя и не нарисоаться. а чтобы
анпрмиер Ю нарсиовать то надо сделать

  putc(0xD0, stdout);
  putc(0xAE, stdout);

полчется можно в целом без прлем байты любой кодировки
пихать в путс. потому что он их в итге просто сунет
дальше. но если мы себе ставим заадчу что мы пихаем
байт в путс и на экране ристся +1глиф то он нам 
в этом не поможет. если мы в него будем совать
байты из аскии кодировки то тогда окей. 
но в целом посвторюсь что путс с другой стоороны под
ходит для пеати текста в любой кодироваке. просто
надо потинмать что елимы мы в него байт сунули
то это еще не факт что на эранке вылезет глиф.

но опять же почдеркну что путс глубоко похер на 
кодировки. его дело малое- он на входе ждет 
тип аргумента char и он его просто сует в стрим
stdout вот и все.  адльше это уже проблем терминала
как он транслирует этот байт. 


0х0А ---> putc (0x0A) ---> стрим *stdout --> /fd/1 ---> ттай драйвер --->  терминал


дале в си есть два понятия хуево обьсяненых.
одно это multibyte character 
второе это wide character

мультибайт символ - это такой символ который кодируется через такую кодировку которая 
исползует один или несколько байтов для кодрования
символа. байт может быть один их может быть несоклько
тоесть это один байт или массив байтов. но это
не многобайтовое число. это именно массив из едичных
байтиков. напрмиер такая кодирка это UTF-8

  символ Ю это 0x0D 0xAE

подчернкуть что это не 0x0DAE нет!
это именно массив из одиночных байтов! массив
может быть в ютф-8 состоять как из одного байтта
так и до четырех байтов. такая кодировка называется
мультивабайтовая. а смивол закоиорвыанный ей
называется мультибайтовым.

wide character (тема вобще ужасно изложена в 
инете). это такой символ который закодирован такой
кодировкой которая в себя включает один или несклоко
хреней каждая из которых БОЛЛЬШЕ чем 8бит тоесть 
она болше чем  1 байт. напримет такая кодиорвка это 
UTF-16  
эта кодиовака она кодирвет символ либо через 
одно двухбайтовое число например 
   
   0х0443

либо через массив из двух двухбайтовых чисел

  0xD800 0xDE9C

тоесть суть в том что данная кодровка у нее элемент
в массиве имеет длину больше чем 1 байт.в  данном
случае ютф-16 использует двухбайтовые хрени.
насколко я понял - именно это и явлется признаком
wide character кодировки. 

есть еще wide cahrecater кодровка. это utf-32
у нее всегда один член в массиве. но он тоже 
ддиной больше чем 1 байт. а точнее он в длину 
4 байта. это не массив из четрыех однобатоыйых 
хреней а это один элемент длиной 4 байта.
например 

   0x0001029C

итак если где тоговоярт про мулбтибайт керектер
это значит что его кдирует такая кодорвка у которой
массив состоит из элементов каждый из которых
по длине 1 байт. напрмиер ascii или utf-8

а если говорят про wide керектер то это значит что
он кодиурется такой кодиорвакой которая кодрует
символы через массив элеметов каждый из которых
по длине точно длинее чем 1 байт.  нарпмиер utf-16
или utf-32

в виновсе для вайл керекетер используется 
utf-16
а в линуксе исполузется utf-32

utf-16 ксати хитрая кодировка. об читай в
	https://en.wikipedia.org/wiki/UTF-16

суть в том что для символоов у которых юникод потнинт
<= U-FFFF
в ютф-16 испольузуется одно двухбайтовая хрень 0xABCD

а для юникод поинтов которые больше уже исползуется
две двухбаоывые хрени  и они очень хитро вычисляются.
для этго из диапазона 0x0000 - 0xFFFF вырезается
спец диапазоны 0xD800–0xDFFF и 0x000–0x3FF
и эти байт коды уже немогу исопльзоваться для 
сами по себе для кодирования символа. если у нас
хрень лежит в диапазоне 0xD800–0xDFFF то это 
обязывает добавить второй элемент к массиву которй
лежит в диапазоне 0x000–0x3FF 
таким макаромы частично жертуем диапазоном симовлов
ктторый мы можем выразить через один эемент 
в массиве но зато у нас не будет путаницы в деко
дровании. чтобы имея рряд двухбайтовых херней мы 
четко знали в каком случае одна хрень кодирует
символ а в каком две. подробнее читай в в вики.
в любому сулчае ютф-16 исполузется в виндовс в
функкциях котоыре рабоютают  с wide charcater.


возвращаемся в линукс. в ливнскксе когда работают
с wide charcter то испольщуют utf-32. с ним проще.
нет никакой мудоты. ютф-32 это всегда массив
состоящий из одного элемента длиной 4 байта

     0x11223344

и работет это так. у нас в си егсть глибц фнуции
которые на входе ждут аргумент который имеет
тип wchar_t по факту это просто хрень длиной
4 байта. и если мы его подтвляе в функцию наример
такую как 

   putwchar()

то она интепретутирует эти 4 байта так что это 
ютф-32 кодировка символа. сам wchar_t никакой
магией не обладает. это просто тип данных кторый
по сути всего навсего задает ДЛИНУ в байтах скоько
этпеременная будет занимать в памяти. 
магия состоит в функциях которые работают с этим
типом данных. так вот putwchar() ожидает что в нее
всунуть ютф-32 кодировку. тоесть это не аскии
это не ютф-8 кодрорвка. а именно ютф-32
например я показываю чему равны кодровки для
символа "у" 

UTF-8 Encoding:	   0xD1 0x83
UTF-16 Encoding:	0x0443
UTF-32 Encoding:	0x00000443


тоесть если мы всунем в wchar_t вот такой контент

  wchar_t w1 = 0x00000443;
  putwchar(w1);

то putwchar он интерпетирует байты которые мы в 
него всунули как ютф-32 закодированный символ. 
далее эта фугция делает очень хитрую вещь - она
считывает локаль для данного процесса. у локали
есть параметр чармап котрый как раз таки задает
таблицу между юникод код поинтами и байтами которые
нужно выдавать на стдоут для этого юникод код 
поинта. ксати говоря ютф-32 кодровка и номер 
юникод код поинта они всегда совпдают. хотя по 
сути они говорят о разных вещах. так вот 
putwchar лезет в локаль. там находит чармап. 
далее эта фунция делает вот что. 
вот у нее есть байты которые сооветсвюте кодировке
ютф-32. она по этим байтам опрделяет код поинт
для символа. для ютф-32 байты в кодровае и номер
код поинта совдпают. так что это очень простой
процесс. далее найдя код поинт она леелзет в 
локаль данного пцросса. находит там чармап.
а чрамап это таблица между юнико код пинтами
и байтами которые  в рамках этого экндоидинга 
ему совтетввуют. и она находит там байты. и уже
их она сует в  стрим stdput! сука!
например мы сдедали

  wchar_t w1 = 0x00000443;
  putwchar(w1);


значит ютф-32  0x00000443 , его код поинт это
0x00000443. далее положим локаль у нас имеет
чармап utf-8 в рамках этого енкодинга
для код поинта  0x00000443 соовотввтуеют байты
кодировки  0xD1 0x83
поэтому данная фуцния сует на стрим stdout 
не хрень 0x00000443 а массив байтов 0xD1 0x83

вот как все хитро работает. 
как putwchar шарится по локали. а вот как 
наш процесс вклчаюе в себя глибц код. а он 
прис тарте процесса октрывает файл 
  /usr/lib/locale/locale-archive 
в котором  содержатся все локали данного компа.
и также в перменых окружения процесса есть 
перменная

LC_CTYPE="en_US.UTF-8"

из нее глибц   putwchar() знает какюу конкретно
локлаьь нужно искать внутри  /usr/lib/locale/locale-archive

вот такой пзидец.
если у нас упроцесса LC_CTYPE="CP1255"
то вот эта хуйяня а входе wchar_t w1 = 0x00000443;
породит на выходе из putwchar() уже 
не байты 0xD1 0x83 а уже чтото другое!

посмтреть какие код поинты кодирую какаие байты
на выходе в разным кодровках(чармапах) монжно
вот тут

  /usr/share/i18n/charmaps

там как раз ест и ютф-8 и cp1255 и iso-8859-8 
итп




ксати  в си если у нас в стрингах можно юзать 
юникод код понты

  "\u1234\u1235"

если у нас номер код поинта четырехзначный то
есть двух байтовый то буква \u а елси чтрыех байтовый
то \U

  "\U11223344\U11223344"



ТАК ВОТ  когда в си начианают пиздеть про wide 
characters и про фунции котоыре работают  с  wide 
characters то надо понимать что это значит то что 
сами символы каким были такими и остались у них 
с "шириной" графической все в порядке она без изменений они не являюст никакими ни широкими ни 
выскомми. это все не про то! просто очеедной
идиотский неходроший термин. это символ широоктий
а это КОДИРОВКА  этого символа такова что она
кодрует символ через ШИРОКИЕ многобайтые хрени!
именно хрени широкие. щирокие они в том смысле что
они шире чем 8бит. теость когда в линуксе говорят
эта фуция печаает широкий символ. то   это всего
навсего значит что у этой функции аругмент это 
кодировка которая при кодировании испозует широкие
битовые хрени. шире чем 8 бит. тоесть речь идет
лишь о том что нкотоаря фунция в си на входе
раотает  с особой кодировкой символов. именно
кодировака широкая а не символ. символ обычный
на экране будет. широкая кодровка. широкая 
она в том плане что массив который она исползует
он имеет элементы шире чем 1 байт. 
в линуксе фнуции котоыре работают с широкой 
кодировкой они все раотают с кодорвакой ютф-32
а ютф-32 для кдовоания символа исоплзует хрень
шириной 4 байта. 
тоесьт мы берем фоунцию   putwchar();
она работает с широикими символаами (говоря по
человекски с широкой кодировкой, говоря еще более
по человекчски с кодровкой ютф-32 . вопрос зачем
так было на пустом месте заебыать мозги людям?)

мы ей скармирываем кодированную хрень юфт-32

  wchar_t w1 = 0x00000443;
  putwchar(w1);

она понимает что 0x00000443 это ютф-32 хрень
а за ней кроется символ. она дяля этой эрени
находит юникод поинт. потом лезет в локаль. 
находит чармап.  в нем находит этот поинт. и для 
него там находит поток байтов. и уже этот поток
байтов она сует в стрим стдоут. а отуда этот поток
байтов уже летит на /fd/1 а оттда на ттай драувер
а отуда на териаал. таким образрм что это дает.
мы можем в прогармме стзозздать массив котоырый
кодирует текст в формате utf-32
а потом его скормиить его в putwchar()
а он его скенрутрует в ту кодировку которая кокретно
есть на этом компе согласно локали. будт то ютф-8
или cp1255 итп. вот в чем прикол этого мудежа 
с широкиими кодровками.

значит сопустующая херня.
в си хрент вида

  "........." имеет тип char *[]

поэтому ее нельзя присвоиь массиву из элементов
wchar_t
но и для него похождая хрент это 

 L"........." вот она имеет тип wchar_t *[]


тоест 

    char    *c1[] = "kwelrkwejlrkjwelk";
    wchar_t *w1[] = "ewlkrjlewrjlkkelj";


компилятор первую строчку вот как обрабтает.
он возьмет кажый символв внутри и конервериует
его в поток utf-8 айтов и ихсунет в память.

а вторая строчка будет по друнрму. он возьмем
каждый символ и конертиурет его в utf-32 четыерех
байтовую хрень. а потом еше используя ендинеесс
эту хрень преобразует в массив однобатовых хреней
и засунет в память. покажу на примере


    char    *c1[] = "qЮ";

q = (utf-8) = 0x71
Ю = (utf-8) = 0x0D 0xAE

в итоге памяти будет вот такой набор байтов


 71 0D AE


теперь 
    wchar_t *w1[] =  = "qЮ";

q = (utf-32) =  0x00000071 
Ю = (utf-32) =  0x0000042E

теперь нам надо прервить это гробы в массив байтов
так как память у нас не умеет хранить четырехбайтовые
хрени. она умеет харнить только однобатыйоые хрени.
значит юзам правило ЛОУ енинесс. тогда в итоге
в памяти будет вот такой набор байтов
 
 71 00 00 00  2E 04 00 00

еще раз помрим на строки в прогармме и что 
будет в итоге в памяти

    char    *c1[] = "qЮ";
    71 0D AE

    wchar_t *w1[] =  = "qЮ";
    71 00 00 00  2E 04 00 00


тоест разница просто охуенная.

ну свотвтетанно примернно ставноится поняным
пчему нелтщя в программе смешивать  функции которые
пишут в стрим "просто байты" как это делает
putc() printf() 
и фнуции котоыре пишут в стрим ютф-32 байты.

как я понимаю дело вот в чем. 
вот я беру

    char    *c1[] = "qЮ";
    printf("%s", c1);

он засунет в стрим вот эти байты 71 0D AE

потом я делаю вот так

    wchar_t *w1[] =  = "qЮ";
    wprintf();
он засунет в стрим байты 71 00 00 00  2E 04 00 00

в итоге у нас в стриме смесь байтов разной 
природы. у нас смесь байтов utf-8 и utf-23
а далее я незнаю точно как оно там работает но
ясно одно - что далее нужно из стрима байты
уже совать в декриптор /fd/1 и тут вопрс что 
делать глибц коду. то ли все байты просто сунуть
как есть то ли все байты в стриме расамаатривать
что они utf-32 и их еще нужно на основе чармапа 
локали преобразовать в текущую кодроровку тоесть 
в utf-8
как глибц в итоге определяет для себя в какой
кодировке сидят байты в его стриме и что с ними
делать но факт есть факт что если в пргрммме
исползвать функции котоыре суют в стрим 
данные в разных кодорвках то уже в /fd/1 у нас 
полетит какойто ебаный мусор. поэтому и говорят
что нельзя смешивать. если бы putwchar() совал 
в стрим уже траснфорваирванный из ютф-32 
в байт поток на основе локали то прбелмы бы не было.
но он этого недалет. этту тарнрасфонрвацию делает
кактто другой глбиц код уже работая с байтами 
запсанными в буфер стрима.

вот такая плюс минус хуйня.
это и то я пишу все макстмаьно коротки и бегом.
ибо нет времени.

так что если фнция рабоает с wide charcater то 
по руски это значит что в она ождидает что ты в нее
вставишь код символа на основе кодорвки utf-32
что у нее аргумент это ютф-32 . вот и все. 
если у обычной печатющей фунции аргументом
обычно ялется массив из отденых байтиков. и ей
вобщем то похуй что там за байткики. она их 
просто копирет из памяти в стрим. ей похуй в какой
они кодровке. для нее это просто массив отделных
байтов. ее задача их просто скопровать из одного
куска памяти в другой. тоесть вот пример

 char *[]a = "3232234eewr";
 printf("%s", a);

это задача компилятора перобразовать 3232234eewr 
в набор батов и сунуть их в память.
а принтф просто беерт поинтер на этот кусок памяти
и копирует байты что там лежат по одному штуке
в поинтер ктторый укзывает на стрим. до тех пор
пока она не встертит 0х00 байт. вот ився ее работа.
хотя она чуть полсложнее рабатет. она формирует
свой массив байтиков "%s" внутрт когторого в 
задынне места чрез ключ %s она коприует массив 
"3232234eewr" а потом уже суммарный массив 
она кпрует в поинтер стрим. 

у компилятора есть ключи которые задают 
кодировки внутри стрингов. 

 -finput-charset=charset
 -fwide-exec-charset=charset

 читаю в man gcc

   -finput-charset=charset
      Set the input character set, used for translation from the character set of the input file to
      the source character set used by GCC.  If the locale does not specify, or GCC cannot get this
      information  from  the  locale,  the  default is UTF-8.  This can be overridden by either the
      locale or this command-line option.  Currently the command-line option  takes  precedence  if
      there's  a  conflict.   charset can be any encoding supported by the system's "iconv" library
      routine.


     -fwide-exec-charset=charset
       Set the wide execution character set, used for wide  string  and  character  constants.   The
       default  is  one  of  UTF-32BE, UTF-32LE, UTF-16BE, or UTF-16LE, whichever corresponds to the
       width of "wchar_t" and the big-endian  or  little-endian  byte  order  being  used  for  code
       generation.   As  with  -fexec-charset, charset can be any encoding supported by the system's
       "iconv" library routine; however, you will have problems  with  encodings  that  do  not  fit
       exactly in "wchar_t".


вобщем очередная тема пиздец.


