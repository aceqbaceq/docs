| ceph


$ sudo ceph osd pool ls
.mgr
ceph_pool






$ sudo ceph osd pool get ceph_pool size
size: 3
это сколкьо копий одного PG






$ sudo ceph -s
  cluster:
    id:     67bf77ff-87ea-483a-8b66-9a45c34f5cc6
    health: HEALTH_OK
 
  services:
    mon: 5 daemons, quorum hst3,hst4,hst6,hst7,hst2 (age 3w)
    mgr: hst7(active, since 3w), standbys: hst5, hst4, hst1, hst6, hst3
    osd: 24 osds: 24 up (since 5d), 23 in (since 12m); 58 remapped pgs
 
  data:
    pools:   2 pools, 513 pgs
    objects: 1.72M objects, 6.0 TiB
    usage:   13 TiB used, 11 TiB / 24 TiB avail
    pgs:     153860/5151111 objects misplaced (2.987%)
             455 active+clean
             58  active+remapped+backfilling
 
  io:
    client:   521 KiB/s rd, 19 MiB/s wr, 33 op/s rd, 1.00k op/s wr
    recovery: 193 MiB/s, 50 objects/s
 



тоесть
             455 active+clean
             58  active+remapped+backfilling


так вот 455+58=513  уникальных PGS. 
сучетом что их три копии то полное число PGS = 513*3 ~ 1500 штук












$ sudo ceph df
--- RAW STORAGE ---
CLASS    SIZE   AVAIL    USED  RAW USED  %RAW USED
hdd    24 TiB  11 TiB  13 TiB    13 TiB      54.05
TOTAL  24 TiB  11 TiB  13 TiB    13 TiB      54.05
 
--- POOLS ---
POOL       ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr        6    1  115 MiB       30  346 MiB      0    2.6 TiB
ceph_pool  10  512  6.2 TiB    1.72M   11 TiB  59.25    2.6 TiB



$ sudo ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP     META     AVAIL    %USE   VAR   PGS  STATUS
 0    hdd  1.20070   1.00000  1.2 TiB  728 GiB  616 GiB   10 MiB  1.1 GiB  501 GiB  59.21  1.10   76      up
 3    hdd  0.90059   1.00000  922 GiB  563 GiB  479 GiB  8.1 MiB  890 MiB  359 GiB  61.04  1.13   59      up
 4    hdd  1.20070   1.00000  1.2 TiB  752 GiB  640 GiB   11 MiB  4.7 GiB  478 GiB  61.13  1.13   85      up
 2    hdd  0.90059   1.00000  922 GiB  573 GiB  489 GiB  8.2 MiB  891 MiB  350 GiB  62.09  1.15   60      up
 5    hdd  1.20070   1.00000  1.2 TiB  745 GiB  633 GiB   10 MiB  1.1 GiB  485 GiB  60.57  1.12   79      up
 6    hdd  0.90059   1.00000  922 GiB  514 GiB  430 GiB  7.2 MiB  815 MiB  408 GiB  55.76  1.03   52      up
 7    hdd  0.90059   1.00000  922 GiB  448 GiB  364 GiB  6.1 MiB  633 MiB  475 GiB  48.54  0.90   44      up
 8    hdd  0.90059         0      0 B      0 B      0 B      0 B      0 B      0 B      0     0   58      up
11    hdd  1.20070   1.00000  1.2 TiB  770 GiB  658 GiB   11 MiB  1.1 GiB  460 GiB  62.59  1.16   79      up
 9    hdd  0.90059   1.00000  922 GiB  512 GiB  429 GiB  7.3 MiB  915 MiB  410 GiB  55.56  1.03   53      up
10    hdd  0.90059   1.00000  922 GiB  577 GiB  493 GiB  8.2 MiB  968 MiB  345 GiB  62.58  1.16   60      up
19    hdd  1.20070   1.00000  1.2 TiB  615 GiB  503 GiB  8.9 MiB  3.8 GiB  614 GiB  50.03  0.93   67      up
 1    hdd  1.20070   1.00000  1.2 TiB  662 GiB  550 GiB  9.2 MiB  1.0 GiB  567 GiB  53.86  1.00   65      up
13    hdd  0.90059   1.00000  922 GiB  447 GiB  363 GiB  6.0 MiB  645 MiB  475 GiB  48.50  0.90   45      up
17    hdd  1.20070   1.00000  1.2 TiB  664 GiB  552 GiB  9.5 MiB  1.2 GiB  566 GiB  53.99  1.00   68      up
12    hdd  0.90059   1.00000  922 GiB  509 GiB  425 GiB  7.1 MiB  880 MiB  413 GiB  55.19  1.02   52      up
14    hdd  1.20070   1.00000  1.2 TiB  664 GiB  552 GiB  9.9 MiB  4.2 GiB  565 GiB  54.01  1.00   74      up
18    hdd  0.90059   1.00000  922 GiB  584 GiB  500 GiB  8.4 MiB  884 MiB  338 GiB  63.31  1.17   61      up
15    hdd  1.20079   1.00000  1.2 TiB  745 GiB  633 GiB   11 MiB  1.1 GiB  485 GiB  60.58  1.12   78      up
16    hdd  1.20079   1.00000  1.2 TiB  767 GiB  655 GiB   11 MiB  1.2 GiB  463 GiB  62.37  1.15   80      up
20    hdd  1.20079   1.00000  1.2 TiB  720 GiB  609 GiB   10 MiB  1.3 GiB  509 GiB  58.60  1.08   75      up
21    hdd  0.87329   1.00000  894 GiB  250 GiB  248 GiB  8.2 MiB  2.2 GiB  644 GiB  28.00  0.52   62      up
22    hdd  0.87329   1.00000  894 GiB  212 GiB  210 GiB  6.8 MiB  1.6 GiB  682 GiB  23.70  0.44   51      up
23    hdd  0.87329   1.00000  894 GiB  231 GiB  229 GiB  7.5 MiB  1.9 GiB  663 GiB  25.84  0.48   56      up
                       TOTAL   24 TiB   13 TiB   11 TiB  202 MiB   35 GiB   11 TiB  54.07                   
MIN/MAX VAR: 0.44/1.17  STDDEV: 11.57







$ sudo ceph-volume lvm list


====== osd.11 ======

  [db]          /dev/ceph-4040b672-b148-474e-a662-229734266b8f/osd-db-da218459-89f3-4088-b8ff-2ca257a99a37

      block device              /dev/ceph-bce978c8-1164-4acd-aaf0-145dc208ca1c/osd-block-ccca1def-e99e-44b9-b2fa-55490c6ed472
      block uuid                MdBdts-W10x-shKO-TANr-PMqL-mztX-pStM3C
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        None
      db device                 /dev/ceph-4040b672-b148-474e-a662-229734266b8f/osd-db-da218459-89f3-4088-b8ff-2ca257a99a37
      db uuid                   XwVLOR-p7ef-UA8I-neZH-8dck-Dtz0-LNCdJi
      encrypted                 0
      osd fsid                  ccca1def-e99e-44b9-b2fa-55490c6ed472
      osd id                    11
      osdspec affinity          
      type                      db
      vdo                       0
      devices                   /dev/sdd

  [block]       /dev/ceph-bce978c8-1164-4acd-aaf0-145dc208ca1c/osd-block-ccca1def-e99e-44b9-b2fa-55490c6ed472

      block device              /dev/ceph-bce978c8-1164-4acd-aaf0-145dc208ca1c/osd-block-ccca1def-e99e-44b9-b2fa-55490c6ed472
      block uuid                MdBdts-W10x-shKO-TANr-PMqL-mztX-pStM3C
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        None
      db device                 /dev/ceph-4040b672-b148-474e-a662-229734266b8f/osd-db-da218459-89f3-4088-b8ff-2ca257a99a37
      db uuid                   XwVLOR-p7ef-UA8I-neZH-8dck-Dtz0-LNCdJi
      encrypted                 0
      osd fsid                  ccca1def-e99e-44b9-b2fa-55490c6ed472
      osd id                    11
      osdspec affinity          
      type                      block
      vdo                       0
      devices                   /dev/sdf

====== osd.7 =======

  [block]       /dev/ceph-644c33c1-2d63-499f-b0df-0fb492731de1/osd-block-4f64af4f-3f70-4b32-8231-361323a6b124

      block device              /dev/ceph-644c33c1-2d63-499f-b0df-0fb492731de1/osd-block-4f64af4f-3f70-4b32-8231-361323a6b124
      block uuid                Gi5vuw-3FLJ-HPUu-aEMB-I9q6-VafB-lfcRAx
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        hdd
      db device                 /dev/ceph-87836a31-9722-4287-8046-bbc22f99c11a/osd-db-666e6a5c-b9c7-49cf-942e-42674493cde1
      db uuid                   ARpxgW-47x2-R7x7-7pxi-K9KA-rBfZ-3a3rmb
      encrypted                 0
      osd fsid                  4f64af4f-3f70-4b32-8231-361323a6b124
      osd id                    7
      osdspec affinity          
      type                      block
      vdo                       0
      devices                   /dev/sdb

  [db]          /dev/ceph-87836a31-9722-4287-8046-bbc22f99c11a/osd-db-666e6a5c-b9c7-49cf-942e-42674493cde1

      block device              /dev/ceph-644c33c1-2d63-499f-b0df-0fb492731de1/osd-block-4f64af4f-3f70-4b32-8231-361323a6b124
      block uuid                Gi5vuw-3FLJ-HPUu-aEMB-I9q6-VafB-lfcRAx
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        hdd
      db device                 /dev/ceph-87836a31-9722-4287-8046-bbc22f99c11a/osd-db-666e6a5c-b9c7-49cf-942e-42674493cde1
      db uuid                   ARpxgW-47x2-R7x7-7pxi-K9KA-rBfZ-3a3rmb
      encrypted                 0
      osd fsid                  4f64af4f-3f70-4b32-8231-361323a6b124
      osd id                    7
      osdspec affinity          
      type                      db
      vdo                       0
      devices                   /dev/sdg

====== osd.8 =======

  [db]          /dev/ceph-1ba4a9b9-795c-4ece-afa1-bf1556be9592/osd-db-46705324-5b49-4be9-ad50-8bcd4f668802

      block device              /dev/ceph-3c422b3c-7244-4b72-90c1-275d4f67d07c/osd-block-d82b58ee-498d-45f8-9787-c94e92ee2fbc
      block uuid                NuIK8H-TjV9-jym0-UNrx-EKVG-gsX4-5kIVik
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        
      db device                 /dev/ceph-1ba4a9b9-795c-4ece-afa1-bf1556be9592/osd-db-46705324-5b49-4be9-ad50-8bcd4f668802
      db uuid                   oPOp4s-izX8-ikJ3-0tpy-ZyZ0-sG75-M3eLDA
      encrypted                 0
      osd fsid                  d82b58ee-498d-45f8-9787-c94e92ee2fbc
      osd id                    8
      osdspec affinity          
      type                      db
      vdo                       0
      devices                   /dev/sde

  [block]       /dev/ceph-3c422b3c-7244-4b72-90c1-275d4f67d07c/osd-block-d82b58ee-498d-45f8-9787-c94e92ee2fbc

      block device              /dev/ceph-3c422b3c-7244-4b72-90c1-275d4f67d07c/osd-block-d82b58ee-498d-45f8-9787-c94e92ee2fbc
      block uuid                NuIK8H-TjV9-jym0-UNrx-EKVG-gsX4-5kIVik
      cephx lockbox secret      
      cluster fsid              67bf77ff-87ea-483a-8b66-9a45c34f5cc6
      cluster name              ceph
      crush device class        
      db device                 /dev/ceph-1ba4a9b9-795c-4ece-afa1-bf1556be9592/osd-db-46705324-5b49-4be9-ad50-8bcd4f668802
      db uuid                   oPOp4s-izX8-ikJ3-0tpy-ZyZ0-sG75-M3eLDA
      encrypted                 0
      osd fsid                  d82b58ee-498d-45f8-9787-c94e92ee2fbc
      osd id                    8
      osdspec affinity          
      type                      block
      vdo                       0
      devices                   /dev/sdc







как псмотреть лейтенси на OSD

$ sudo ceph osd perf
osd  commit_latency(ms)  apply_latency(ms)
 11                  10                 10
  8                   0                  0
  7                   3                  3
 18                   3                  3
 12                   3                  3
 23                   1                  1
 22                   1                  1
 21                   0                  0
 20                   3                  3
 19                   7                  7
 17                   2                  2
  0                   3                  3
 13                   3                  3
  1                   2                  2
 14                  11                 11
  2                   6                  6
 15                   3                  3
  3                   2                  2
 16                   3                  3
  4                   6                  6
  5                   9                  9
  6                   8                  8
  9                   3                  3
 10                   2                  2


 





 ~$ sudo ceph osd tree
ID   CLASS  WEIGHT    TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         22.73244  root default                             
 -9          3.30199      host hst1                            
  0    hdd   1.20070          osd.0       up   1.00000  1.00000
  3    hdd   0.90059          osd.3       up   1.00000  1.00000
  4    hdd   1.20070          osd.4       up   0.84999  1.00000
 -3          3.00188      host hst2                            
  2    hdd   0.90059          osd.2       up   1.00000  1.00000
  5    hdd   1.20070          osd.5       up   1.00000  1.00000
  6    hdd   0.90059          osd.6       up   1.00000  1.00000
 -5          0.90059      host hst3                            
  7    hdd   0.90059          osd.7       up   0.79999  1.00000
 -7          3.00188      host hst4                            
  9    hdd   0.90059          osd.9       up   1.00000  1.00000
 10    hdd   0.90059          osd.10      up   1.00000  1.00000
 19    hdd   1.20070          osd.19      up   1.00000  1.00000
-13          3.30199      host hst5                            
  1    hdd   1.20070          osd.1       up   1.00000  1.00000
 13    hdd   0.90059          osd.13      up   1.00000  1.00000
 17    hdd   1.20070          osd.17      up   1.00000  1.00000
-11          3.00188      host hst6                            
 12    hdd   0.90059          osd.12      up   1.00000  1.00000
 14    hdd   1.20070          osd.14      up   1.00000  1.00000
 18    hdd   0.90059          osd.18      up   1.00000  1.00000
-76          3.60237      host hst7                            
 15    hdd   1.20079          osd.15      up   1.00000  1.00000
 16    hdd   1.20079          osd.16      up   1.00000  1.00000
 20    hdd   1.20079          osd.20      up   1.00000  1.00000
-15          2.61987      host hst8                            
 21    hdd   0.87329          osd.21      up   1.00000  1.00000
 22    hdd   0.87329          osd.22      up   1.00000  1.00000
 23    hdd   0.87329          osd.23      up   1.00000  1.00000
  8                0  osd.8             down         0  1.00000





$ sudo ceph osd reweight osd.4 1.0



число мониторов должно быть нечет. для кворума.
число менеждеров пофиг потому что один актив остальные стендбай.



мура какая то с OSD
есть оперция удления OSD
а есть замена OSD которая мало понятна. 
а еще есть какойто ceph orch  который в цефе прокмокса не устанолвен

==

увидеь список lxc контееноов на проксмокс


$ sudo pct list
[sudo] password for krivosheeva: 
VMID       Status     Lock         Name                
100        running                 dns1-cn03-ct1       
258        running                 lxc-test-01         


войти внутрь контейнера

# pct enter 258

===

проц цеф.

клиент звонит на монитор и оттуда узнает карту цефа. тоеть ип адреса OSD и прочее.

потом клиент определяет на каком OSD лежит нужный ему PG
далее он узнает на какой OSD ему нужно идти. и тут дело в том что на хосте цефа поднимается
служба для каждого OSD. и эта служба выставляет тцп сокеты в  public network туда кудаже
и мониторы глядят и клиент (внимание) стучится напрямую на OSD на его тцп сокеты. и оттуда качает.
тоесть цеф это куча выставленны в сеть островков тцп сокетов (осд) на которых лежит PG.
и клиент сам лезет на эти OSD  и оттуда качает. тоесть у цефа нет  спец точек спец
выделенных через которые бы цеф отдавал бы данные от OSD. цеф дает тебе карту с OSD.
это похоже на супермаркет где нет продавцов которые выдают товар. 
ты сам ходишь по полкам и берешь что надо.


чтоеще.
мы создаем OSD. скажем из двух дисков  - один под дата второй под базу.
так вот цеф при этом что делает с этимм дисками оно создает +1  LVM VG на каждый диск.
и потом на каждом из VG он создает 1 LVM , это все можно увидеть через pvs, vgs, lvs
когда создан lvs то его ядро выставляет для юзер процессов  в виде блочного устройства
вида /dev/mapper/dm-N
так вот osd процесс он открывает этот /dev/mapper/dm-N и работает с ним.

получается схема такая.  осд процесс с одной стороны в публичную сеть выставляет 
несклкьо слушающих тцп сокетов. на них стучится клиент. с дурогой стороны он открывает
/dev/mapper/dm-N устройство. 



клиент --------> tcp socket   <  OSD > ------> /dev/dm-5 ---- жест диск


я уже говорил что OSD можно создать на базе двух дисков. один под данные
второй под базу. тогда OSD процесс открывает оба LVS файла. 


вот как это можно увидеть:


$ systemctl list-units | grep osd | awk '{print $1}'
ceph-osd@0.service
ceph-osd@3.service
ceph-osd@4.service


$ systemctl status ceph-osd@0.service | grep PID
   Main PID: 5611 (ceph-osd)


$ sudo lsof -Pnp 5611 | grep LISTEN
ceph-osd 5611 ceph   18u     IPv4              46473      0t0     TCP 10.10.10.151:6806 (LISTEN)
ceph-osd 5611 ceph   19u     IPv4              46482      0t0     TCP 10.10.10.151:6808 (LISTEN)
ceph-osd 5611 ceph   20u     IPv4          229985294      0t0     TCP 10.0.0.151:6804 (LISTEN)
ceph-osd 5611 ceph   21u     IPv4          229972592      0t0     TCP 10.0.0.151:6806 (LISTEN)
ceph-osd 5611 ceph   22u     IPv4              46507      0t0     TCP 10.10.10.151:6812 (LISTEN)
ceph-osd 5611 ceph   23u     IPv4              46521      0t0     TCP 10.10.10.151:6813 (LISTEN)
ceph-osd 5611 ceph   24u     IPv4              46532      0t0     TCP 10.0.0.151:6810 (LISTEN)
ceph-osd 5611 ceph   25u     IPv4              46544      0t0     TCP 10.0.0.151:6811 (LISTEN)



тоесть видно что слушает тцп сокеты. один ип для паблик сети для запрсов 
от клиентов. тоесть чтобы пинимат ьзапросы от клиентов и отдавать нужный им материал.
второй ип для внутрнней сети тоесть чтобы принимат запросы от самого цефа и тоже
отдаваь и принимать служебный материал


hst1 $ sudo lsof -Pnp 5611 | grep dm-
ceph-osd 5611 ceph   32u      BLK              253,4      0t0     463 /dev/dm-4
...
...
ceph-osd 5611 ceph   44u      BLK              253,8      0t0     475 /dev/dm-8
...
...


тоесть видно что он открыл два болчных устрйоства.


один из них dm-4 это LVS от шпиндельного диска для данных
и втрой dm-8 это LVS от ссд диска под базу

подрорую инфо по блочным усьтройствам можно 
увидеть через 

 $ sudo udevadm info /dev/dm-4
 $ sudo udevadm info /dev/dm-8


забавно то что скажем при созлданпии osd мы указывает два обычных 
диска в виде /dev/sdb  /dev/sdc
потом когда цеф на них навешивает lvm ( vgs, lvs )
то в системе появляются новые болчные устройства вида /dev/dm-N или /dev/mapper/dm-N
и если мы потом в 

   nmon 

смотрим то  у нас целая куча типа дисков показана. но часть из них 
это считай что просто дубликаты. тоесть /dev/sdc имеет прямую связь с /dev/dm-4 
да немножко другое блочное устройство но сидит на одной физике

вот еще можно увидеть


 $ sudo lsblk
NAME                                                                                                  MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sdb                                                                                                     8:16   0 838.3G  0 disk 
└─ceph--4ea7bc65--5f81--4fe8--b764--254e10ed98a9-osd--block--11979a88--4b56--491b--adb2--6c9db70f653f 253:3    0 838.3G  0 lvm  


забавно то что на одно физ устройство udev генерирует целую тьму 
блочных спец устройств. это можно легко увиедт через 
 
 # udevadm info /dev/sdb

так вот nmon покзыает одну часть из них
а lsblk показыывает другую часть

тоесть в lsblk мы увидим устрсйтво

ceph--4ea7bc65--5f81--4fe8--b764--254e10ed98a9-osd--block--11979a88--4b56--491b--adb2--6c9db70f653f 253:3    0 838.3G  0 lvm  

но не увидим уствройство /dev/mapper/dm-4
почему непонятно




если мы укаызваем присоздании OSD только диск под данные
то на него идет и запись базы изапись AWL

если я создаю осд и выделяю диск под данные и под AWL то 
диск под базу идет на диск под данные как я пнмимаю


если я создаю осд и выдяеляю диск под данынеи и под базу то AWL пишется на базу

