| disk
| HGST HUS724020AL A8B0
| ST2000NM0033-9ZM SN06



тест характеристик




HGST HUS724020AL A8B0
SN PN1181P5H6A19W
(скорсть его интфрейса 6Gb/s)

# geom disk list da3
Geom name: da3
Providers:
1. Name: da3
   Mediasize: 2000398934016 (1.8T)
   Sectorsize: 512
   Mode: r0w0e0
   descr: ATA HGST HUS724020AL
   lunid: 5000cca24ed0f76d
   ident: PN1181P5H6A19W
   rotationrate: 7200
   fwsectors: 63
   fwheads: 255


ЛИН ЧТЕНИЕ 

 fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=161MiB/s][r=161 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2588: Sun Jan  4 13:50:23 2026
  read: IOPS=162, BW=163MiB/s (171MB/s)(4926MiB/30234msec)


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=167MiB/s][r=167 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2586: Sun Jan  4 13:49:17 2026
  read: IOPS=162, BW=163MiB/s (171MB/s)(4926MiB/30239msec)


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((128 *1024)) --iodepth=1  --runtime=60  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=161MiB/s][r=1291 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2577: Sun Jan  4 13:48:07 2026
  read: IOPS=1302, BW=163MiB/s (171MB/s)(9776MiB/60051msec)





ЛИН ЗАПИСЬ

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=write --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=160MiB/s][w=160 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2595: Sun Jan  4 13:51:29 2026
  write: IOPS=163, BW=163MiB/s (171MB/s)(4926MiB/30197msec); 0 zone resets



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=write --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=166MiB/s][w=166 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2603: Sun Jan  4 13:52:20 2026
  write: IOPS=163, BW=163MiB/s (171MB/s)(4926MiB/30165msec); 0 zone resets




# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((128 *1024)) --iodepth=1  --runtime=30  --readwrite=write --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=166MiB/s][w=1330 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2611: Sun Jan  4 13:53:07 2026
  write: IOPS=1304, BW=163MiB/s (171MB/s)(4896MiB/30029msec); 0 zone resets


резюме и лин чтение и лин запись по 160 МБ/с




РАНДОМ РИД

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=640KiB/s][r=80 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2616: Sun Jan  4 13:55:08 2026
  read: IOPS=84, BW=680KiB/s (696kB/s)(20.0MiB/30097msec)



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=1520KiB/s][r=190 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2627: Sun Jan  4 13:55:46 2026
  read: IOPS=180, BW=1445KiB/s (1479kB/s)(43.0MiB/30467msec)





РАНДОМ ВРАЙТ

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=1640KiB/s][w=205 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2632: Sun Jan  4 13:57:02 2026
  write: IOPS=197, BW=1577KiB/s (1615kB/s)(46.5MiB/30189msec); 0 zone resets


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [f(1)][100.0%][w=744KiB/s][w=93 IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2640: Sun Jan  4 13:57:42 2026
  write: IOPS=196, BW=1573KiB/s (1610kB/s)(46.5MiB/30267msec); 0 zone resets



резюме рандом рид врайт 80-200 иопс



========



ST2000NM0033-9ZM SN06
SN Z1X415HY
(его макс скорость интерфейса 6Gb/s)

# geom disk list da2
Geom name: da2
Providers:
1. Name: da2
   Mediasize: 2000398934016 (1.8T)
   Sectorsize: 512
   Mode: r0w0e0
   descr: ATA ST2000NM0033-9ZM
   lunid: 5000c5007a6890b2
   ident: Z1X415HY
   rotationrate: 7200
   fwsectors: 63
   fwheads: 255




вначале я вот что проверю.
смотрим фичи

# smartctl -g all /dev/da2

AAM feature is:   Unavailable
APM feature is:   Unavailable
Rd look-ahead is: Enabled
Write cache is:   Enabled
DSN feature is:   Unavailable
ATA Security is:  Disabled, NOT FROZEN [SEC1]



вырубаю лукахед
и смотрю как себя ведет диск

# smartctl  -s lookahead,off /dev/da2

 bs1=1M , iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=73.0MiB/s][r=73 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=6998: Tue Jan  6 12:10:49 2026
  read: IOPS=72, BW=72.8MiB/s (76.3MB/s)(766MiB/10521msec)

 bs1=1M , iodepth=2

 #  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=2  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=2
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=73.1MiB/s][r=73 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7007: Tue Jan  6 12:11:22 2026
  read: IOPS=72, BW=72.6MiB/s (76.1MB/s)(766MiB/10551msec)

удвительно iodepth=2 ничего недае как буттто там ncq неработает

 bs1=1M , iodepth=10

 iostat -x -d da2 1
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
da2           97       0  99662.7      0.0   100     0     0   100   10 100 

видим что ос реально на диск сует по 10 команд за раз

заодно проверчею что ос читает рреально с диска 1МБ кусками

# iostat -d da2 1
             da2 
KB/t   tps  MB/s 
1021     0  0.06 
1024    98 97.94 
1024   110 109.73 
1024   110 109.83 
1024    97 96.64 



#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=10  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=10
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=113MiB/s][r=113 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7009: Tue Jan  6 12:12:02 2026
  read: IOPS=113, BW=113MiB/s (119MB/s)(1150MiB/10149msec)


обалдеьб. NCQ вобще работать нехочет



 bs1=1M , iodepth=32

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=32  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=101MiB/s][r=101 IOPS][eta 00m:00s]


тоесть без лукахед вобше конь непашет


еще рад интереса тест на чтение кусками по 128К ибо это тоже интересно

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=109, BW=13.7MiB/s (14.4MB/s)(144MiB/10455msec)

[root@dell-r720-freebsd ~]#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=10  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=380, BW=47.5MiB/s (49.8MB/s)(480MiB/10097msec)


[root@dell-r720-freebsd ~]#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=32  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=462, BW=57.8MiB/s (60.7MB/s)(592MiB/10230msec)




теперь врубаю лукахед


[root@dell-r720-freebsd ~]# smartctl  -s lookahead,on /dev/da2
Read look-ahead enabled

[root@dell-r720-freebsd ~]# smartctl  -g  all /dev/da2
Rd look-ahead is: Enabled
Write cache is:   Enabled
DSN feature is:   Unavailable
ATA Security is:  Disabled, NOT FROZEN [SEC1]


BS=1M iodepth=1


#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=179MiB/s][r=179 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7026: Tue Jan  6 12:17:54 2026
  read: IOPS=185, BW=186MiB/s (195MB/s)(1918MiB/10316msec)


шарманка сразу заработала

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=32  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=166MiB/s][r=166 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7028: Tue Jan  6 12:18:28 2026
  read: IOPS=185, BW=185MiB/s (194MB/s)(1918MiB/10342msec)



и для bs=128K iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=1489, BW=186MiB/s (195MB/s)(1864MiB/10009msec)



тоесть даже на 128К idepth=1 он дожлен на лин нагрузке читать на полной скорости



ТЕПЕРЬ я плодключаюсь к этому диску когда он воткнут в Fc полку acer altos 200F


# smartctl -g all /dev/sdaa
smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.8.12-9-pve] (local build)
Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org

Read Cache is:        Enabled
Writeback Cache is:   Enabled


# smartctl -s  rcache,off /dev/sdaa
Read cache disable failed: RCD bit not changeable


# sdparm --get=RCD /dev/sdaa
    /dev/sdaa: ST2000NM  0033-9ZM175       SN06
RCD           0  [cha: n, def:  0, sav:  0]


# sdparm --set=RCD=1  /dev/sdaa
    /dev/sdaa: ST2000NM  0033-9ZM175       SN06
mode select(10): transport: Host_status=0x07 [DID_ERROR]
Driver_status=0x08 [DRIVER_SENSE]

ошибка полка недает сменить


рид кеш не отключить


еще раз комнанды

# sdparm --get=RCD /dev/sdu
    /dev/sdu: ST2000NM  0033-9ZM175       SN06
RCD           0  [cha: n, def:  0, sav:  0]


# sdparm --get=WCE /dev/sdu
    /dev/sdu: ST2000NM  0033-9ZM175       SN06
WCE           1  [cha: y, def:  1, sav:  1]


якобы это кеши именно прям самого диска.


вот эта хуйня
# cat /sys/block/sdi/queue/read_ahead_kb 
8192

это 8МБ
и она работает толко если мы читаем через пейдж кеш. а таккак у меня в fio стоит --direct=1
то пейдж кеш я неюзаю и этот мехагизм неисползуется


важный момент про полку altos 200F
я на компе вырубил на диске loook ahead и write cache пошел встаил в altos 200F
потом обратно вытащил притащил воткнул в комп и они опять ОКАЗАЛИСЬ ВКЛЮЧЕНЫ тоесть
полка реально включает на диске look ahead и write cache внезависимости как было на диске когда его вставляют
тесты это тоже подтверждают



#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((1024 *1024)) --iodepth=1  --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=92, BW=92.0MiB/s (96.5MB/s)(1406MiB/15281msec)


тоесть наша прога пуляет всего 1 иопс и потом ждет.
если бы лук ахед на диске был выключен то лин скорсть чтения была бы

    BW=72.8MiB/s

на основе того что я тестировал выше.

значит реально лукахед включен на диске и скорость должна бы быть BW=186MiB/s согласно тестам выше.
значит это ограничение интерпозера.
судя по всему он очереди понимает хотть както  поттому что вот я даю очередь

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((1024 *1024)) --iodepth=32  --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=94, BW=95.0MiB/s (99.6MB/s)(1470MiB/15475msec)

и результат возраастает. и он ворзрастает не из за диска.  на диске при включенном лукахед уже при иодепф=1
скорсть выдается BW=186MiB/s.  это чисто интепроер ограничение. ему нужно поток сата преобразовать в поток FC
где каждый кадр это 2КБ.  тоесть ему нужно разбить сата поток 1МБ на кучу 2К пактов FC.
тоесть сам диск выдает при такой нагузке 190MB\s в сторнону интепрзера а он уже внутрьполки может 
преобразовать только 95МБ/с


если я читаб из интерпозера кускаии по 128К иодепф=1

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((128 *1024)) --iodepth=1  --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=libaio, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=78.6MiB/s][r=628 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=14202: Tue Jan  6 19:27:59 2026
  read: IOPS=624, BW=78.1MiB/s (81.9MB/s)(1176MiB/15051msec)


то к сожалению макс скорсть интепозера 95МБ/с невыдается. хотя как видно из тестов выше при включенном лукахеде 
сам диск легко в сторону интепозера выдаст 190МБ/с. ему толко осаттся из кеша диска выгребать. но увы 95 он невыдает 
на этом режиме. но все таки с очердями интерпоезер худо безно умет рабтать. повышваем иодепф до 8

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((128 *1024)) --iodepth=8 --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=745, BW=93.2MiB/s (97.7MB/s)(1400MiB/15017msec)

и выгребаем максимум того что может из себя родить интепозер
тоест у него есть ограничеини по суммарному срупуту 95МБ/с
и величина очереди тоже имеет значение. если очередь маленькая то срурупут будет меньше
тоесть елси на интепрзер прилеает один запрос. он его осблсживает и обратно отпрваляет.
если придетает сразу неколкьо сзпсосов то он их сразу обслуивает и сразу отпарваляет. поэтому очеердь играет 
значение. 

далее я задал линейное чтение с датасет где рекорсдсайз 1М  привыкобченном зфс префетче
чтобы он не влияет

    # echo 1 > /sys/module/zfs/parameters/zfs_prefetch_disable


и запускаб чтение 1М кускамми в 1 поток.
должен поидее получить тоже самое что я имел на голом диске при 1 потоке тоесть 95МБ/с

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-1M/fio.dat  --bs=$((1024 *1024)) --iodepth=1 --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=85, BW=85.6MiB/s (89.8MB/s)(1342MiB/15678msec)

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdl             88.00  90112.00     0.00   0.00   10.60  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.93  92.90


на практике поменьше 86МБ/с 

если включить зфс претфет то коенчно уже выжммем все 95МБ/с

    # echo 0 > /sys/module/zfs/parameters/zfs_prefetch_disable


#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-1M/fio.dat  --bs=$((1024 *1024)) --iodepth=1 --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=94, BW=94.8MiB/s (99.4MB/s)(1470MiB/15504msec)



теперь прочитаем сбоевого пула
но прежде еще раз вспоимним как работат тут скорсоть на чтении с голых дисков
значит напоминаю что на самих дисках включен лукахед поэтому при заказе на чтение снаружи диск по любому читает 1-8МБ
и клоадет в кеш на чтение. поэтому при полследюущем обращении к диску данные уже есть в буфере.
и уже при чтении кускасмми по 128К и иодепф=1 сам диск при участии этого лук эхед механизма выдает в интерфейс 190МБ/с
тоесть свой максиум. а сверрху над диском сидит интепозер. который тормоз.
итак

bs=128К iodephth=1
#   fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((128 *1024)) --iodepth=1 --runtime=10  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=624, BW=78.1MiB/s (81.8MB/s)(784MiB/10041msec)

тоесть интерпозер может выбирать с буфера диска на 190МБ/с но он это не может.
он неможет даже на своем максмумме 95МБ/с


bs=128K iodepth=2 
#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((128 *1024)) --iodepth=2 --runtime=10  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=729, BW=91.2MiB/s (95.6MB/s)(920MiB/10087msec)

добавление двух запросов сразу наконец выбивает из интеропзера его максум

кстати в линуксе на iodepth>1 надо юзать движок io_uring потому что libaio каую то хуйню творит. ты ему даешь 
iodepth>1 а он на диски кидает непонянтую хуйню в плане очереди загрузки
проверяем это так. дали нагрузку на фио. и смотрим в iostat

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdk             96.00  98304.00     0.00   0.00   20.81  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.00  98.70

если aqu-sz равен нашему iodepth значит все работает как нам надо.


итак на bs=128K + iodepth=2 мы вишибаеим из интепозера его максмум.
посмтрим как дела с bs=1M


#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((1024 *1024)) --iodepth=1 --runtime=10  --readwrite=read  --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=92.1MiB/s][r=92 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=22973: Tue Jan  6 21:16:59 2026
  read: IOPS=91, BW=91.7MiB/s (96.1MB/s)(958MiB/10449msec)


нам хвтвает iodpeth=1 для bs=1M чтобы из интепозера выжать его максимум.

теперь кошда с голыми дсикаами я напомнил пеерходим к тестрованию чтения на датасете зфс.
рекодсайз 1МБ  на боевом пуле из страйпа на 8-ми миррорах (16 дисков)
первым делом отключакем зфс префетч чтобы он не влияел




и тут на вскидку полчаю нео чень понятную картину. 
делаю тест на иодепф=1
так как у нас пул это 8 мирроров то понятно что очредной ио запрос он лежит то на одном диске то на другом.
тоесть мы постянно скачем сдика на диск. ну у меня логика какая. вот мы прилетели на диск1. счттали оттуда медленно 1МБ. 
при этом диск считаел за этот прход на самом деле мегабайт 8. и при следующем залете на этот диск по идее следущий нужный
для зфс мегабайт должен лежать в буфрее. тоесть по мне загрузка дисков должна вылядеть как то что все диски спять
а с одного читается на скоростьи 95МБ/с если не сразу то со временем. 
а пофакту я имею вот такое

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sda              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdaa             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdab             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdb              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdc              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdd              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sde              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdf              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdg              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdh              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdi              6.00   6144.00     0.00   0.00   15.67  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.09   9.40
sdj              2.00   2048.00     0.00   0.00   57.50  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.11  11.50
sdk              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdl              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdm              4.00   4096.00     0.00   0.00   16.50  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.07   6.60
sdn              3.00   3072.00     0.00   0.00   39.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.12  11.80
sdo              5.00   5120.00     0.00   0.00   14.20  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.07   7.80
sdp              4.00   4096.00     0.00   0.00   14.25  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.06   4.70
sdq              4.00   4096.00     0.00   0.00   15.75  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.06   6.40
sdr              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sds              5.00   5120.00     0.00   0.00   15.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.07   7.60
sdt              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdu              2.00   2048.00     0.00   0.00   12.50  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.03   2.50
sdv              1.00   1024.00     0.00   0.00   11.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.01   1.10
sdw              2.00   2048.00     0.00   0.00   16.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.03   3.20
sdx              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdy              2.00   2048.00     0.00   0.00   52.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.10  10.50
sdz              4.00   4096.00     0.00   0.00   32.25  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.13  13.00



ну тоест окей. ладно. если дисков 16. то каждый из них в секуда рабоатет тлокьо 1/16 часть.
если интепорзер может забррать данные с диска на скрости 95МБ/с то 95/16 = 6 МБ/с ... ну окей похоже. тоесть
интепозер забрал данные с диска на скорости 95МБ/с а потом диск тип отдахыает поэтому мы видим 6МБ/с загрузку на диске.
нотогда неясно то что у нас тогда суммарная соерость чтения должна быть 95 МБ/с как минуиум. тоесть мы читаем с одного
диска в каддый момент времени на скрости 95МБ/с. просто сразных а пофвакту я виижу скороть чтения  52MB\s

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=1 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
  read: IOPS=51, BW=51.8MiB/s (54.3MB/s)(8318MiB/160480msec)



тоесть опять ккакя хуйня именнов интепорзере кроется.

выход какой - надо каждый диск нагрузить постоянной нагрузкой. тогда все будет окей. тоесть так как дисков 16 
то иодепф=16

идейстивтельно 


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=16 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=16
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=331MiB/s][r=331 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=23153: Tue Jan  6 21:28:44 2026
  read: IOPS=351, BW=351MiB/s (368MB/s)(10.0GiB/29146msec)


  на 16 иодепсах мы получаем нормалную скорость. просто она уже упералсь в скорость двух веревок FC 2GB\s

и самый смак когда iodepth=32 то есть у нас на каждм диске qlen=2 все время и тогда

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=32 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
  read: IOPS=385, BW=386MiB/s (405MB/s)(10.0GiB/26542msec)



теперь включаю зфс префетч


# echo 0 > /sys/module/zfs/parameters/zfs_prefetch_disable


он дает то что зфс видя что мы делаем линкйную нагрзку он на каждый диск в пуле
помимо нашего реквеста дает еще свои ио запросы да так умно что на каждом диске у нас постояно qlen=2


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sda              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdaa             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdab             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdb              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdc              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdd              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sde              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdf              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdg              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdh              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdi             26.00  26624.00     0.00   0.00  100.69  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.62  91.90
sdj             19.00  19456.00     0.00   0.00   81.68  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.55  63.50
sdk              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdl              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdm             22.00  22528.00     0.00   0.00   76.86  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.69  70.30
sdn             27.00  27648.00     0.00   0.00  105.59  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.85  97.10
sdo             27.00  27648.00     0.00   0.00   83.04  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.24  77.70
sdp             21.00  21504.00     0.00   0.00   87.38  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.83  70.10
sdq             24.00  24576.00     0.00   0.00   78.62  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.89  78.00
sdr             26.00  26624.00     0.00   0.00   93.12  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.42  93.10
sds             24.00  24576.00     0.00   0.00  106.62  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.56  88.30
sdt             21.00  21504.00     0.00   0.00   88.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.85  71.60
sdu             24.00  24576.00     0.00   0.00   70.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.68  66.50
sdv             25.00  25600.00     0.00   0.00   96.12  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.40  81.90
sdw             24.00  24576.00     0.00   0.00   86.71  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.08  72.60
sdx             25.00  25600.00     0.00   0.00   94.92  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.37  84.70
sdy             24.00  24576.00     0.00   0.00   76.04  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.83  67.90
sdz             28.00  28672.00     0.00   0.00   92.39  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.59  81.80


тоесть я на фио задаю всего iodepth=1

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=1 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=374MiB/s][r=374 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=23525: Tue Jan  6 21:34:24 2026
  read: IOPS=380, BW=381MiB/s (399MB/s)(10.0GiB/26879msec)



но зфс пуляет на пул помимо нашего запроса единсвтенного еще 31 свой запрос понимая как устроенн миррор. 
тоесть пофакту на пул летит iodepth=32  и ответ сохарянется в AB буфрее зфс. поэтому даже когда у меня 
на фио iodepth=1 мы получаем сумпашедший перфоманс такой какой мы получали только на иодепф=32 !!!! 
вот такую крутую вещь делает zfs prefettch!!!! просто мегаркутейшая вещь.! далеко накаждое прилжоение поррождает 
глубину запросов 32. поэтому зфс в этом случае здорово выручает. и наше прилжоение которое генирует 
лохвовской iodpeth=1 получает перфоманс как еси бы оно генирролвало иодепф=32
просто отлично!!!



длаее еще один иентеернсный момент.
создаем зфс датасет с  recordsize 128K
пишет туда файл
начинаем чиатть его линейно. причем  с разным iodepth
так вот интересно зфс будет с диска читать кусками по 128К или он будет агрегировать эти заросы в макрокуски
размером скажем 1МБ. посмтрим это и во фрибсл и в линусексе.

фрибсд.
вырубаем зфс префетч


  # sysctl  vfs.zfs.prefetch.disable=1
     0-> 1


запускаем чтение bs=128К  iodepth=1


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=read --numjobs=1  --group_reporting 

# iostat ...

 128   311 38.86  0.0     0  0.00 
 128  1181 147.62  0.0     0  0.00 
 128  1312 164.00  0.0     0  0.00 
 128  1210 151.25  0.0     0  0.00 
 128  1118 139.74  0.0     0  0.00 
 128  1280 160.00  0.0     0  0.00 
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
 128   910 113.75  0.0     0  0.00 
 128  1169 146.09  0.0     0  0.00 
 128  1243 155.42  0.0     0  0.00 
 128  1085 135.62  0.0     0  0.00 
 128  1302 162.77  0.0     0  0.00 
 128  1151 143.92  0.0     0  0.00 
 128    40  5.01  0.0     0  0.00 


все логично. в зфс поступает запрос. он сразу его пыатетс выполнить. запрос всего один. приложение больше 
нешлет пока неполучитответ. прфетч зфс выклюыен. поэтому чтение идет кускками по 128К


теперь я кидаю пачку 128К запроосов
bs=128K  iodepth=32

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=32 --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
bs: 1 (f=1): [R(1)][56.7%][r=168MiB/s][r=1344 IOPS][eta 00m:13s]


# iostat -d da2 da3 1
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
 400     1  0.41 12.4     0  0.00 
 607   285 168.63  0.0     0  0.00 
 462   360 162.27  0.0     0  0.00 
 386   418 157.41  0.0     0  0.00 
 621   278 168.39  0.0     0  0.00 
 468   373 170.56  0.0     0  0.00 
 944   189 174.41  0.0     0  0.00 
 431   408 171.73  0.0     0  0.00 
 499   349 170.18  0.0     0  0.00 
 678   265 175.32  0.0     0  0.00 
 510   338 168.37  0.0     0  0.00 
1024   184 183.73  0.0     0  0.00 
 623   282 171.77  0.0     0  0.00 
 807   224 176.73  0.0     0  0.00 
 491   346 165.80  0.0     0  0.00 


теерь зфс получает пачку запрсов сразу. аггрегирует их и на диск шлет уже макрозапрос размером 500-1024 КБ
тоесть зфс незаебывает диск милионом мелкеих запросов. и также видно что на диск идет нагрузка 280-400 иопс
крупных а приложение видит как бутто 1344 иопса на диск было послано. все это пдотвеидрждает агрегацию


тоест зфс умеет агрегировать заросы и из мелких в круаные нетолько при записи но и причтении.
теперь актвиирую зфс префетч и ioddepth ставлю 1


#  sysctl  vfs.zfs.prefetch.disable=0
    vfs.zfs.prefetch.disable: 1 -> 0

bs=128K iodepth=1
# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
^Cbs: 1 (f=1): [R(1)][50.0%][r=182MiB/s][r=1453 IOPS][eta 00m:15s]


# iostat -d da2 da3 1
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
 404     1  0.42 12.2     0  0.00 
1024   183 182.81  0.0     0  0.00 
1024   182 181.94  0.0     0  0.00 
1024   184 183.73  0.0     0  0.00 
 983   185 177.57  0.0     0  0.00 
1024   183 182.57  0.0     0  0.00 
1024   183 183.18  0.0     0  0.00 
1024   181 181.06  0.0     0  0.00 
1024   183 183.25  0.0     0  0.00 
1024   183 183.32  0.0     0  0.00 
1024   182 181.73  0.0     0  0.00 
1024   185 184.57  0.0     0  0.00 
1024   181 180.53  0.0     0  0.00 


и втидно насколку умно зфс делает префетч. 
она читает с диска кускками по 1МБ !! это при том что от приложениея прителел запрос всего на один иопс. 

вобщем круто. зфс умеет агрегировать мелкие запросы в крупные. тем более когда влкюачен зфс префетч. 
тоесть вот видно вместо 1453 запроса на диск зфс шлет на диск всего 181 запрос. в семь раз меньше.
и еще важный вывод - если префетч влючен и мы сидим на датасете 128К и читаем с bs=128K и iodepth=1
то  мы при этом полностью раскрываем весь срупут диска (!!)




тоже самое смотрим на линукс
отключаем пренфетч


 # echo 1 > /sys/module/zfs/parameters/zfs_prefetch_disable

bs=128K iodepth=1


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat     --bs=$((128 *1024)) --iodepth=1  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=183MiB/s][r=1467 IOPS][eta 00m:00s]


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc           1455.00 186240.00     0.00   0.00    0.57   128.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.83  82.50


также как на фрибсл здесь зфс читвает кускми по 128К. что полностью окей.


доабвляем iodepth=32 тесть закиывдаем в зфс сразу пачку запросов



# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat     --bs=$((128 *1024)) --iodepth=32  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=184MiB/s][r=1474 IOPS][eta 00m:00s]


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc           1010.00 187904.00     0.00   0.00    9.86   186.04    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    9.96  97.50



и вот тут видно что линукс агрегирует запросы но не так круто как фрибсд
тоесть вот он средний размер запроса на диск 186.04 и вот они иопсы на диск  1010.00
для сравния фрибсл на таоже тесте у него средний размер запроса на дитск 400-600-800 и иопсов на диск 200-400


теперь активруиуем префетч и возвращемся к iodepth=1


~# echo 0 > /sys/module/zfs/parameters/zfs_prefetch_disable
root@test-prox:~# 
root@test-prox:~# 
root@test-prox:~# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat     --bs=$((128 *1024)) --iodepth=1  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [R(1)][35.0%][r=189MiB/s][r=1512 IOPS][eta 00m:13s]



Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc            191.00 188416.00     0.00   0.00   16.24   986.47    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.10  98.60


ну нормал средний размер запроса 986.47 иногда даже его прбивает и он делает 1024КБ

тоесть ликустоже умеет агрегирвать запросы в вболее корпные при чтении. и улчше всего у него этополучсается
при включенном прфефетче.
сотвтесвенно также и ккак и на фроисбл.  надатасете 128К рекдрсзаайз при чтении bs=128K iodepth=1
при включенном пренфетче мы выжимаем полную линкейную скорость из диска 190 МБ/с 


резюме - фрибсл или линкус. если у нас есть  датасет с рекдсайзом 128К и мы читаем линейно кусками 128К 
при iodepth=1 и включенном зфс префетче то мы длжны получать полную сокрость которуб могут линейно разивать диски.
(при улосвии что файл неперемешан на диске в плане офффсетов то ест мы его только создали).
и также мы должны видеть что на сам диск запросы идут не в формет 128К а вболее крупных запросах типа 986-1024 KB
этотоже супер важно.

это я щас разбирал на примере когда наш диск подкючен к локальнму внутри компа ХБА контроллеру.
так вот я вовзращаюсь к нашей полке altos 
там тоже я создал датасет 128К
псомрим какая будет лин скорость чтения на нем



# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=356MiB/s][r=2846 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24320: Tue Jan  6 22:49:01 2026
  read: IOPS=3037, BW=380MiB/s (398MB/s)(10.0GiB/26966msec)



ну что отлично. ураган. 
тоесть датасет с рекорсайзом 128К  хватаает этого чтобы снять с ссисиетемы макс лин скорсть которубю она может 
из себя выжать на голых дисках. по краней мере пока файл в плане оффсетов непермешан на дисках.


про рандом риды. и фрибсл и лиукс я проверил они нормал паралелят иопсы на диски при iodepth>1
оба это умеют.

к нашему диску вернесся. столкнулся с неоченьпонятной штукой. если я тесриру на роандлом рид голвый диск то 76 иопс
а если файл на базе зфс то 127 иопс. как такое моежет быть непонимаю.
тестрирую диск когда он подключен к лоальнму hba котроллеру
тоесть узнаю его нативный перфоманс

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=1 --runtime=10  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=9600KiB/s][r=75 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=8129: Tue Jan  6 20:23:11 2026
  read: IOPS=76, BW=9767KiB/s (10.0MB/s)(95.8MiB/10039msec)



 fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
^Cbs: 1 (f=1): [r(1)][23.3%][r=16.9MiB/s][r=135 IOPS][eta 00m:23s]
fio: terminating on signal 2
test: (groupid=0, jobs=1): err= 0: pid=8140: Tue Jan  6 20:26:30 2026
  read: IOPS=125, BW=15.7MiB/s (16.5MB/s)(114MiB/7208msec)



делаю iodepth=32

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=32 --runtime=10  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=18.9MiB/s][r=151 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=8142: Tue Jan  6 20:29:14 2026
  read: IOPS=151, BW=18.9MiB/s (19.9MB/s)(200MiB/10542msec)



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=32 --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=32.4MiB/s][r=259 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=8144: Tue Jan  6 20:29:56 2026
  read: IOPS=259, BW=32.4MiB/s (34.0MB/s)(984MiB/30334msec)





теперь тестирую тоже самое но уже когда диск всталвен в полку

#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdk   --bs=$((128 *1024)) --iodepth=1 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=8960KiB/s][r=70 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24676: Tue Jan  6 23:28:05 2026
  read: IOPS=71, BW=9172KiB/s (9392kB/s)(136MiB/15156msec)

тут без потери скорости. 


#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=1 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=16.5MiB/s][r=132 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24710: Tue Jan  6 23:28:50 2026
  read: IOPS=125, BW=15.7MiB/s (16.5MB/s)(240MiB/15254msec)


тут тоже без потери скорсоти



теперь iodepth=32

#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdk   --bs=$((128 *1024)) --iodepth=32 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=14.3MiB/s][r=114 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24743: Tue Jan  6 23:30:00 2026
  read: IOPS=103, BW=12.9MiB/s (13.6MB/s)(200MiB/15440msec)

  полуичли 103 против 150 когда диск вставлен в локальный хба адаптер



#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=32 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=18.0MiB/s][r=144 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=25086: Tue Jan  6 23:37:04 2026
  read: IOPS=146, BW=18.3MiB/s (19.2MB/s)(280MiB/15288msec)

   получили 146 проотив 259 когшда диск ватсвлен в локальный хба адаптер


теперь проетсируем  на боевом пуле страйп из 8 мирроров
вначале iodpeth=16 это примерно по одному ио запросу на каждыцй диск. тоесть каждый диск будет под напгрузкой q=1
ранее мы полуили что для q=1 на пуле из одногодиска у нас 125 iops значит щас я ожидаю 125*16 = 2000 иопс

root@backup-02:/home/krivosheeva#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=16 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=16
  read: IOPS=1534, BW=192MiB/s (201MB/s)(2888MiB/15055msec)


    получил 1500 иопс неплохо (по 96 иопс на диск)


теперь iodepth=32 это по q=2 на каждый диск

  #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=32 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
  read: IOPS=1908, BW=239MiB/s (250MB/s)(3592MiB/15057msec)


   получил 1900 иопс неплохо. (по 119 иопс на диск)


ИТОГО с чтением будесм считать на этой полке разобралдись





ПЕРЕХОДИМ  к записи.


поехали разбираться с записью

отключаю writecache

   # smartctl  -s wcache,off /dev/da1


bs=1M iodepth=1

]#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((1024 *1024)) --iodepth=1  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=71, BW=71.9MiB/s (75.4MB/s)(1086MiB/15095msec); 0 zone resets

                   видим 71 иопс и 71.9 MB\s 
                   тоесть запись идет чисто на иопсах


bs=1M iodepth=32

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((1024 *1024)) --iodepth=32  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
  write: IOPS=179, BW=180MiB/s (188MB/s)(2750MiB/15307msec); 0 zone resets

                  здесь получается в диск влетает пачка запросов. он их агруегирует.
                  180 MB\s вобщем то мы достигли максмусма


посмтрим сможет ли раскрыть максимум bs=128K

bs=128K iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((128 *1024)) --iodepth=1  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
  write: IOPS=109, BW=13.7MiB/s (14.4MB/s)(208MiB/15120msec); 0 zone resets

                  13.7 MB\s иопс 109 
                  тоесть диск чисто на иопасх запиывает


кинем пачку таких запросов

bs=128K iodepth=32

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((128 *1024)) --iodepth=32  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=1419, BW=177MiB/s (186MB/s)(2680MiB/15101msec); 0 zone resets


              177MB\s видим что иопсы 1419
              конечно тут дело не в механике таких ипосов он не выдал
              просто запосы падают в рам память. а потом диск и макрокуском пишет на диск.
              максимум мы в значительной степени раскрыли


теерь врубаем кеш


   # smartctl  -s wcache,on /dev/da1



еще раз понмню как работает кеш. он невыелклдючается никогда запросы из компа всегда попадают в этот кеш.
просто если он выклоючен то диск сообщает косппу что запист прошла только когда запишет эти иопсы из 
кеша в блины.  а если кеш включен то диск сообщает компу что иопсы записаны на блины сразу кактолько 
данные упали в кеш. 


bs=1M iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((1024 *1024)) --iodepth=1  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
  write: IOPS=179, BW=180MiB/s (188MB/s)(2750MiB/15303msec); 0 zone resets


                получили 180 МБ/с
                тоесть уже достили макимума. 
                если раньше мы с компа послали запрос. он упал в буфер. 
                диск записал его наблины , послал ответ что запись прошла,
                комп шлет новый запрос. то щас работае так 
                комп послал запрос, он упал в буфер и диск тутже сообщает
                что якобы иопс записан уже на блины. тоесть компу
                только остается как можно быстрее совать новый запрос


bs=1M iodepth=32

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((1024 *1024)) --iodepth=32  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=181, BW=181MiB/s (190MB/s)(2814MiB/15513msec); 0 zone resets


            181 MB\s как видно особо нина что не повлияло. 
            тоесть если кеш включен bs=1M и иодепф даже всего равен 1 то мы 
            уже раскрываем макс скорост записи на этом диске. эого достанчно
            чтобы ее ракрыть



посмрим а что с bs=128K при каком условии его хватит чтобы выжать из диска макс лин запись

  bs=128K iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((128 *1024)) --iodepth=1  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=1424, BW=178MiB/s (187MB/s)(2672MiB/15006msec); 0 zone resets

          178 MB\s тоесть даже bs=128K при iodepth=1 уже
          достаточно чтобы из диска выжать макс лин запись 


bs=128K iodepth=32

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da1  --bs=$((128 *1024)) --iodepth=32  --runtime=15 --readwrite=write --numjobs=1  --group_reporting
  write: IOPS=1419, BW=177MiB/s (186MB/s)(2672MiB/15058msec); 0 zone resets


          вобщем то нианачто не повлияла iodepth=32




из чего я делаю основной вывод. чтобы из данного диска выжать максимум на линейной 
записи при включенном кеше на запись  нам уже достатточно bs=128K и iodepth=1
а если они еще болше то тем более



теперь напяливаем на этот диск ZFS и смотрим когда на нем можно из диска выжать максимум.
и еще будет интеерсовать момент - будет ли зфс делать агрегацию записей в макроблоки при подаче 
заявки на диск. будет ли она запросы по 128К оьбединять в 1МБ куски

вначалк тестикрую на freebds

#  fio --randrepeat=1 --ioengine=psync --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=1  --runtime=30 --readwrite=write --numjobs=1  --group_reporting

iostat
KB/t   tps  MB/s KB/t   tps  MB/s
32.0     2  0.06  991   182 176.19 
32.0     2  0.06  771   237 178.72 
32.0     4  0.13  768   239 178.89 
32.0     2  0.06  764   236 176.24 
 6.1    90  0.54  779   214 162.74 
32.0     4  0.12 1024   182 181.51 
32.0     2  0.06 1024   177 176.63 
32.0     4  0.12 1024   178 178.13 
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
32.0     2  0.06 1024   179 178.74 
32.0     2  0.06 1024   177 177.50 
32.0     4  0.13 1024   178 177.59 
32.0     2  0.06 1024   178 178.20 
32.0     4  0.12 1024   179 178.63 
32.0     2  0.06 1024   177 176.60 
32.0     2  0.06 1024   179 178.66 


нас интересует самая права колонка.  я считаю что мы уже доились макс лин скрости записи.
тоесть если мы одели зфс на диск то уже при bs=128K и iodepth=1 мы полностью ракываем макс вощмзмножную
динейную скрость запсиси этого диска

также видно из третеьй колоники справа что зфс пишет на диску макрокусками по 1МБ


теперь тестирую на зфс линукс




# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1  --size=15G --readwrite=write --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
  write: IOPS=1417, BW=177MiB/s (186MB/s)(15.0GiB/86660msec); 0 zone resets



Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc              0.00      0.00     0.00   0.00    0.00     0.00    2.00     64.00     0.00   0.00    2.00    32.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.20
sdd              0.00      0.00     0.00   0.00    0.00     0.00  185.00 189440.00     0.00   0.00   54.10  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   10.01  99.40



Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc              0.00      0.00     0.00   0.00    0.00     0.00   86.00    476.00     1.00   1.15    0.31     5.53    0.00      0.00     0.00   0.00    0.00     0.00    2.00    0.50    0.03   0.80
sdd              0.00      0.00     0.00   0.00    0.00     0.00  183.00 169616.00     0.00   0.00   45.77   926.86    0.00      0.00     0.00   0.00    0.00     0.00    2.00   77.00    8.53  98.70


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc              0.00      0.00     0.00   0.00    0.00     0.00    2.00     64.00     0.00   0.00    2.00    32.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.01   0.30
sdd              0.00      0.00     0.00   0.00    0.00     0.00  184.00 188416.00     0.00   0.00   54.43  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   10.02  99.20


я считаю что тут мы тоже раскрвыаем макс возмож лин скрост записи.  177 MB\s и как видно wareq-sz 1024 
тоесть и линукс зфс тоже пишет на диск макрокусками по 1МБ


таким образмом и на линуксе зфс чтобы выбжать из диска его  макс возмож лин скрости
нам домстаточно иметь датасет с recordsize 128K и писать блоками по 128К и iodepth=1
также видно что и линукс при записи агрегирует куски по 128К в макроскуски по 1МБ

и в линусе и в фрибсл я в массиве имел special vdev чтобы запись метаданных не мешала.



ТЕПЕРЬ переходим к полке acer altos 200F
смотрим сколько можно выжать из голго диска через интерпозер
полка осталвяет на диске включенным write cache


# lsblk -d -o NAME,SERIAL | grep 15HY
sdk  Z1X415HY

# sdparm --get=WCE /dev/sdk
    /dev/sdk: ST2000NM  0033-9ZM175       SN06
WCE           1  [cha: y, def:  1, sav:  1]



bs=128K iodepth=1

#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((128 *1024)) --iodepth=1 --size=30G  --readwrite=write  --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
test: (groupid=0, jobs=1): err= 0: pid=34678: Wed Jan  7 16:19:58 2026
  write: IOPS=43, BW=5617KiB/s (5751kB/s)(298MiB/54331msec); 0 zone resets


        как видно на том режиме на котором сам диске уже может выжимать свой максмисум
        интепозер вобще нихера неможет и дает только 5.5 МБ/с


добавим чтобы запросов была куча
bs=128K iodepth=32


#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((128 *1024)) --iodepth=32 --size=30G  --readwrite=write  --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [W(1)][3.7%][w=58.2MiB/s][w=465 IOPS][eta 08m:14s]
fio: terminating on signal 2
Jobs: 1 (f=1): [W(1)][3.9%][w=61.8MiB/s][w=494 IOPS][eta 08m:12s]
test: (groupid=0, jobs=1): err= 0: pid=34741: Wed Jan  7 16:21:47 2026
  write: IOPS=486, BW=60.8MiB/s (63.7MB/s)(1203MiB/19788msec); 0 zone resets


            60.8 МБ/с это уже лучше




увеличим блок до 1МБ


bs=1M  iodepth=1

backup-02 #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((1024 *1024)) --iodepth=1 --size=30G  --readwrite=write  --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
  write: IOPS=43, BW=43.5MiB/s (45.6MB/s)(998MiB/22924msec); 0 zone resets

      забавно стало даже хуже 43.5 MB\s


добавим запросов в кучу 

bs=1M  iodepth=32


 #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((1024 *1024)) --iodepth=32 --size=30G  --readwrite=write  --numjobs=1  --group_reporting
  write: IOPS=71, BW=71.3MiB/s (74.7MB/s)(1673MiB/23477msec); 0 zone resets

    вот это походу тот максмум который из себя может родить интерпозер 71 МБ/с



теперь сверху одеваем зфс
так как у нас на голом диске интеопзер дает макс скорсть только на bs=1M при iodepth=32
то значит надо смотреть зфс надатасете с bs=1M


#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15HY/test-ds-1M/fio.dat   --bs=$((1024 *1024)) --iodepth=1 --size=5G  --readwrite=write  --numjobs=1  --group_reporting


# iostat 
Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdk              0.00      0.00     0.00   0.00    0.00     0.00   69.00  70656.00     0.00   0.00   44.32  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.06  99.90

                  
                    видим 70 МБ/с . 
                    значит на датасете с рекордсайзом 1МБ мы раскрвыаем макс запись интепрзера
                    на режиме bs=1M iodepth=1
                    то что иоепф=1 уже достаточно это норм потому что мы пишем асинррронно и зфс пихает на диск
                    не один такой ио запрос а нсколкьо как видно три потому что aqu-sz=3



по приколу помтрисм датасет с рекорсдсайзом 128К
bs=128K iodepth=1


 #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15HY/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=1 --size=5G  --readwrite=write  --numjobs=1  --group_reporting

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdk              0.00      0.00     0.00   0.00    0.00     0.00   86.00  40704.00    16.00  15.69   35.55   473.30    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.06  99.80

                  видим 40М МБ/с  что нераскрвыает макс скорость записи интерпозера
                  также вижно что запись идет агрегировными кусками 473.3 КБ
                  скорей всего она идет кусками 1М просто у меня пул неиммеет спешл
                  поэтому туда вкрапливаются мелкие иопсы записи метаданных


добавим iodepth


 #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15HY/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=32 --size=5G  --readwrite=write  --numjobs=1  --group_reporting

# iostat
 Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdk              0.00      0.00     0.00   0.00    0.00     0.00   80.00  48512.00     6.00   6.98   41.89   606.40    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.35  99.80




                              как видим 48 МБ/с но все равно не 71 МБ/с


это все заебы интепозера. у него две задачи. ему нужно нарезать FC кадры (по 2КБ кажждый) и ему нужно 
обрабатвать заголовки SATA пакетов. поэому чем жирнее сата пакеты тоесть чем их меньше и чем они жирнее
тем ему нужно меньше обарабатать сата заголовки и болше осатется времени и сил на преобразовыание сата пейлоад
в кучу FC пакетов.
 



отсюда можно скзаать то что если я хочу макс выжимать лин скорость через интерпоезер то нужно на него кидать 
пакеты размером 1МБ. тоесть надо юзать зфс размером рекордсайз 1МБ.
но учитывая что  сам канал к полке всего 200МБ/с то получается если я хочу юзать рекорзсалсайз 128К то мне нужно 
будет 5 дисков чтобы его весь заполнить.  если же я буду юзать рекордсайз 1МБ то мне нужно будет три диска 
чтобы заплнить канал. 
у меня щас в боевом пуле 8 дисков. это значит что я должен выжать 200МБ/с и на датсете с рекорсдсайз 128К 
и на датасете с рекордсайз 1МБ. едиснвтенное что нельзя юзать иодепф=1 ведь надо чтобы каждый диск участтовтал в процесе
однноврменно поэтому иодепф должен быть равен 8 как минимум

тестирую 

bs=1M iodepth=8

#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat   --bs=$((1024 *1024)) --iodepth=8 --size=15G  --readwrite=write  --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=8
bs: 1 (f=1): [W(1)][34.5%][w=193MiB/s][w=193 IOPS][eta 00m:19s]


avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.08    0.00    0.75   61.99    0.00   37.18

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdi              0.00      0.00     0.00   0.00    0.00     0.00   23.00  23552.00     0.00   0.00  228.30  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.25  64.50
sdj              0.00      0.00     0.00   0.00    0.00     0.00   26.00  26624.00     0.00   0.00  190.35  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.95  51.00
sdm              0.00      0.00     0.00   0.00    0.00     0.00   23.00  23552.00     0.00   0.00  205.48  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.73  55.20
sdn              0.00      0.00     0.00   0.00    0.00     0.00   28.00  28672.00     0.00   0.00  203.36  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.69  89.90
sdo              0.00      0.00     0.00   0.00    0.00     0.00   20.00  20480.00     0.00   0.00  266.65  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.33  64.00
sdp              0.00      0.00     0.00   0.00    0.00     0.00   23.00  23552.00     0.00   0.00  219.09  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.04  63.50
sdq              0.00      0.00     0.00   0.00    0.00     0.00   28.00  28672.00     0.00   0.00  166.18  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.65  71.50
sdr              0.00      0.00     0.00   0.00    0.00     0.00   22.00  22528.00     0.00   0.00  238.73  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.25  59.80
sds              0.00      0.00     0.00   0.00    0.00     0.00   25.00  25600.00     0.00   0.00  217.52  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.44  78.30
sdt              0.00      0.00     0.00   0.00    0.00     0.00   24.00  24576.00     0.00   0.00  152.71  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.67  87.10
sdu              0.00      0.00     0.00   0.00    0.00     0.00   21.00  21504.00     0.00   0.00  245.57  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.16  65.00
sdv              0.00      0.00     0.00   0.00    0.00     0.00   25.00  25600.00     0.00   0.00  218.24  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.46  66.50
sdw              0.00      0.00     0.00   0.00    0.00     0.00   25.00  25600.00     0.00   0.00  236.12  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.90  63.60
sdx              0.00      0.00     0.00   0.00    0.00     0.00   26.00  26624.00     0.00   0.00  213.88  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.56  67.10
sdy              0.00      0.00     0.00   0.00    0.00     0.00   22.00  22528.00     0.00   0.00  228.55  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.03  69.50
sdz              0.00      0.00     0.00   0.00    0.00     0.00   22.00  22528.00     0.00   0.00  223.73  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.92  67.30



          и реально я выжал максмум из канала 193 МБ/с
          это легко и по другому проверить 24МБ/с с каждого диска умножить на 8 



теперь датасет с рекорссайзом 128К

bs=128K iodepth=8


backup-02 #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=8 --size=15G  --readwrite=write  --numjobs=1  --group_reporting
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=8
Cbs: 1 (f=1): [W(1)][38.5%][w=196MiB/s][w=1571 IOPS][eta 00m:24s]


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdi              0.00      0.00     0.00   0.00    0.00     0.00   38.00  21888.00     0.00   0.00  134.95   576.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.13  82.70
sdj              0.00      0.00     0.00   0.00    0.00     0.00   39.00  25344.00     0.00   0.00  131.87   649.85    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.14  82.20
sdm              0.00      0.00     0.00   0.00    0.00     0.00   34.00  26752.00     1.00   2.86  143.32   786.82    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.87  73.00
sdn              0.00      0.00     0.00   0.00    0.00     0.00   37.00  24576.00     1.00   2.63  145.14   664.22    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.37  69.10
sdo              0.00      0.00     0.00   0.00    0.00     0.00   46.00  24192.00     0.00   0.00  102.96   525.91    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.74  80.00
sdp              0.00      0.00     0.00   0.00    0.00     0.00   35.00  24832.00     1.00   2.78  127.00   709.49    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.45  70.90
sdq              0.00      0.00     0.00   0.00    0.00     0.00   39.00  25728.00     1.00   2.50  114.41   659.69    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.46  64.50
sdr              0.00      0.00     0.00   0.00    0.00     0.00   38.00  25984.00     0.00   0.00  137.95   683.79    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.24  78.90
sds              0.00      0.00     0.00   0.00    0.00     0.00   41.00  23424.00     0.00   0.00  125.93   571.32    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.16  78.10
sdt              0.00      0.00     0.00   0.00    0.00     0.00   37.00  23168.00     1.00   2.63  120.32   626.16    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.45  79.10
sdu              0.00      0.00     0.00   0.00    0.00     0.00   39.00  24832.00     1.00   2.50  144.69   636.72    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.64  65.50
sdv              0.00      0.00     0.00   0.00    0.00     0.00   38.00  24192.00     0.00   0.00  144.08   636.63    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.47  76.40
sdw              0.00      0.00     0.00   0.00    0.00     0.00   34.00  25088.00     0.00   0.00  171.47   737.88    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.83  63.90
sdx              0.00      0.00     0.00   0.00    0.00     0.00   36.00  24960.00     1.00   2.70  132.58   693.33    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.77  70.30
sdy              0.00      0.00     0.00   0.00    0.00     0.00   35.00  22144.00     0.00   0.00  148.31   632.69    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    5.19  76.10
sdz              0.00      0.00     0.00   0.00    0.00     0.00   39.00  23808.00     1.00   2.50  119.87   610.46    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    4.67  70.90



          как видно опять же я выжал 192 МБ/с
          на этом датасете, главное это помнить что обязательно нужно иметь iodepth=8 или более.
          в случае линейнрого чтения там за нам это делает зфс префетч. а тут нам нужно 
          самим об этом забоититться
          также видно что линукс зфс делает агрегация записей. тостесть он не пишет на диск кусками по 128К
          он пишет боеелее крупным кусками что хорошо. как видно это куски 500-700 КБ.
          на самом деело это 1М просто в эти иопсы вкрадываются и мелкие иопсы метаданных хотя пул боевой имеет
          спешл. ну по краеймей мере 500КБ на диск размер записи это не 128К

          




ГЛАВНЫЙ ВЫВОД:
исходя из вышесказанного с точки зреня получаения макс лин сокрости чтения и записи 
на этих полках можно узать датасеты как с recordsize=128K так и с 1МБ.
оба нам позволят при данном числе дисков ( а их будет еще болше) выжать 400МБ/с на лин чтении
и 200 МБ/с на лин записи











более старые опыты:
ЛИН ЧТЕНИЕ 

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=160MiB/s][r=160 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2645: Sun Jan  4 13:59:19 2026
  read: IOPS=183, BW=184MiB/s (193MB/s)(5566MiB/30261msec)


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=182MiB/s][r=182 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2660: Sun Jan  4 14:03:07 2026
  read: IOPS=185, BW=185MiB/s (194MB/s)(5566MiB/30056msec)



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((  128 *1024)) --iodepth=1  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=182MiB/s][r=1459 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2662: Sun Jan  4 14:04:06 2026
  read: IOPS=1481, BW=185MiB/s (194MB/s)(5560MiB/30027msec)



ЛИН ЗАПИСЬ

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=write  --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][0.3%][w=181MiB/s][w=181 IOPS][eta 02h:55m:29s]
test: (groupid=0, jobs=1): err= 0: pid=2667: Sun Jan  4 14:05:24 2026
  write: IOPS=186, BW=187MiB/s (196MB/s)(5694MiB/30472msec); 0 zone resets



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=write  --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][0.3%][w=183MiB/s][w=183 IOPS][eta 02h:57m:08s]


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((1*128 *1024)) --iodepth=1  --runtime=30  --readwrite=write  --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][0.3%][w=183MiB/s][w=1462 IOPS][eta 02h:57m:05s]
test: (groupid=0, jobs=1): err= 0: pid=2685: Sun Jan  4 14:07:06 2026
  write: IOPS=1479, BW=185MiB/s (194MB/s)(5552MiB/30017msec); 0 zone resets



иттого резюме лин чтение и запись 190 МБ/с





РАНДОМ РИД

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=672KiB/s][r=84 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2690: Sun Jan  4 14:08:41 2026
  read: IOPS=81, BW=654KiB/s (670kB/s)(19.5MiB/30498msec)

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=1496KiB/s][r=187 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2692: Sun Jan  4 14:09:19 2026
  read: IOPS=180, BW=1444KiB/s (1479kB/s)(43.0MiB/30480msec)



РАНДОМ ВРАЙТ


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [f(1)][100.0%][w=200KiB/s][w=25 IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2701: Sun Jan  4 14:10:54 2026
  write: IOPS=210, BW=1685KiB/s (1726kB/s)(49.5MiB/30064msec); 0 zone resets


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [f(1)][100.0%][w=752KiB/s][w=94 IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2716: Sun Jan  4 14:11:36 2026
  write: IOPS=207, BW=1658KiB/s (1698kB/s)(49.0MiB/30248msec); 0 zone resets


итого ранд рид врайты 80-200 иопс

