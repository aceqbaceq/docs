| disk
| HGST HUS724020AL A8B0
| ST2000NM0033-9ZM SN06



тест характеристик




HGST HUS724020AL A8B0
SN PN1181P5H6A19W
(скорсть его интфрейса 6Gb/s)

# geom disk list da3
Geom name: da3
Providers:
1. Name: da3
   Mediasize: 2000398934016 (1.8T)
   Sectorsize: 512
   Mode: r0w0e0
   descr: ATA HGST HUS724020AL
   lunid: 5000cca24ed0f76d
   ident: PN1181P5H6A19W
   rotationrate: 7200
   fwsectors: 63
   fwheads: 255


ЛИН ЧТЕНИЕ 

 fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=161MiB/s][r=161 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2588: Sun Jan  4 13:50:23 2026
  read: IOPS=162, BW=163MiB/s (171MB/s)(4926MiB/30234msec)


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=167MiB/s][r=167 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2586: Sun Jan  4 13:49:17 2026
  read: IOPS=162, BW=163MiB/s (171MB/s)(4926MiB/30239msec)


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((128 *1024)) --iodepth=1  --runtime=60  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=161MiB/s][r=1291 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2577: Sun Jan  4 13:48:07 2026
  read: IOPS=1302, BW=163MiB/s (171MB/s)(9776MiB/60051msec)





ЛИН ЗАПИСЬ

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=write --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=160MiB/s][w=160 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2595: Sun Jan  4 13:51:29 2026
  write: IOPS=163, BW=163MiB/s (171MB/s)(4926MiB/30197msec); 0 zone resets



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=write --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=166MiB/s][w=166 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2603: Sun Jan  4 13:52:20 2026
  write: IOPS=163, BW=163MiB/s (171MB/s)(4926MiB/30165msec); 0 zone resets




# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$((128 *1024)) --iodepth=1  --runtime=30  --readwrite=write --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=166MiB/s][w=1330 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2611: Sun Jan  4 13:53:07 2026
  write: IOPS=1304, BW=163MiB/s (171MB/s)(4896MiB/30029msec); 0 zone resets


резюме и лин чтение и лин запись по 160 МБ/с




РАНДОМ РИД

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=640KiB/s][r=80 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2616: Sun Jan  4 13:55:08 2026
  read: IOPS=84, BW=680KiB/s (696kB/s)(20.0MiB/30097msec)



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=1520KiB/s][r=190 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2627: Sun Jan  4 13:55:46 2026
  read: IOPS=180, BW=1445KiB/s (1479kB/s)(43.0MiB/30467msec)





РАНДОМ ВРАЙТ

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=1640KiB/s][w=205 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2632: Sun Jan  4 13:57:02 2026
  write: IOPS=197, BW=1577KiB/s (1615kB/s)(46.5MiB/30189msec); 0 zone resets


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/PN1181P5H6A19W  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [f(1)][100.0%][w=744KiB/s][w=93 IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2640: Sun Jan  4 13:57:42 2026
  write: IOPS=196, BW=1573KiB/s (1610kB/s)(46.5MiB/30267msec); 0 zone resets



резюме рандом рид врайт 80-200 иопс



========



ST2000NM0033-9ZM SN06
SN Z1X415HY
(его макс скорость интерфейса 6Gb/s)

# geom disk list da2
Geom name: da2
Providers:
1. Name: da2
   Mediasize: 2000398934016 (1.8T)
   Sectorsize: 512
   Mode: r0w0e0
   descr: ATA ST2000NM0033-9ZM
   lunid: 5000c5007a6890b2
   ident: Z1X415HY
   rotationrate: 7200
   fwsectors: 63
   fwheads: 255




вначале я вот что проверю.
смотрим фичи

# smartctl -g all /dev/da2

AAM feature is:   Unavailable
APM feature is:   Unavailable
Rd look-ahead is: Enabled
Write cache is:   Enabled
DSN feature is:   Unavailable
ATA Security is:  Disabled, NOT FROZEN [SEC1]



вырубаю лукахед
и смотрю как себя ведет диск

# smartctl  -s lookahead,off /dev/da2

 bs1=1M , iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=73.0MiB/s][r=73 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=6998: Tue Jan  6 12:10:49 2026
  read: IOPS=72, BW=72.8MiB/s (76.3MB/s)(766MiB/10521msec)

 bs1=1M , iodepth=2

 #  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=2  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=2
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=73.1MiB/s][r=73 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7007: Tue Jan  6 12:11:22 2026
  read: IOPS=72, BW=72.6MiB/s (76.1MB/s)(766MiB/10551msec)

удвительно iodepth=2 ничего недае как буттто там ncq неработает

 bs1=1M , iodepth=10

 iostat -x -d da2 1
device       r/s     w/s     kr/s     kw/s  ms/r  ms/w  ms/o  ms/t qlen  %b  
da2           97       0  99662.7      0.0   100     0     0   100   10 100 

видим что ос реально на диск сует по 10 команд за раз

заодно проверчею что ос читает рреально с диска 1МБ кусками

# iostat -d da2 1
             da2 
KB/t   tps  MB/s 
1021     0  0.06 
1024    98 97.94 
1024   110 109.73 
1024   110 109.83 
1024    97 96.64 



#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=10  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=10
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=113MiB/s][r=113 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7009: Tue Jan  6 12:12:02 2026
  read: IOPS=113, BW=113MiB/s (119MB/s)(1150MiB/10149msec)


обалдеьб. NCQ вобще работать нехочет



 bs1=1M , iodepth=32

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=32  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=101MiB/s][r=101 IOPS][eta 00m:00s]


тоесть без лукахед вобше конь непашет


еще рад интереса тест на чтение кусками по 128К ибо это тоже интересно

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=109, BW=13.7MiB/s (14.4MB/s)(144MiB/10455msec)

[root@dell-r720-freebsd ~]#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=10  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=380, BW=47.5MiB/s (49.8MB/s)(480MiB/10097msec)


[root@dell-r720-freebsd ~]#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=32  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=462, BW=57.8MiB/s (60.7MB/s)(592MiB/10230msec)




теперь врубаю лукахед


[root@dell-r720-freebsd ~]# smartctl  -s lookahead,on /dev/da2
Read look-ahead enabled

[root@dell-r720-freebsd ~]# smartctl  -g  all /dev/da2
Rd look-ahead is: Enabled
Write cache is:   Enabled
DSN feature is:   Unavailable
ATA Security is:  Disabled, NOT FROZEN [SEC1]


BS=1M iodepth=1


#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=179MiB/s][r=179 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7026: Tue Jan  6 12:17:54 2026
  read: IOPS=185, BW=186MiB/s (195MB/s)(1918MiB/10316msec)


шарманка сразу заработала

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((1024 *1024)) --iodepth=32  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=166MiB/s][r=166 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=7028: Tue Jan  6 12:18:28 2026
  read: IOPS=185, BW=185MiB/s (194MB/s)(1918MiB/10342msec)



и для bs=128K iodepth=1

#  fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=1  --runtime=10 --readwrite=read --numjobs=1  --group_reporting
  read: IOPS=1489, BW=186MiB/s (195MB/s)(1864MiB/10009msec)



тоесть даже на 128К idepth=1 он дожлен на лин нагрузке читать на полной скорости



ТЕПЕРЬ я плодключаюсь к этому диску когда он воткнут в Fc полку acer altos 200F


# smartctl -g all /dev/sdaa
smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.8.12-9-pve] (local build)
Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org

Read Cache is:        Enabled
Writeback Cache is:   Enabled


# smartctl -s  rcache,off /dev/sdaa
Read cache disable failed: RCD bit not changeable


# sdparm --get=RCD /dev/sdaa
    /dev/sdaa: ST2000NM  0033-9ZM175       SN06
RCD           0  [cha: n, def:  0, sav:  0]


# sdparm --set=RCD=1  /dev/sdaa
    /dev/sdaa: ST2000NM  0033-9ZM175       SN06
mode select(10): transport: Host_status=0x07 [DID_ERROR]
Driver_status=0x08 [DRIVER_SENSE]

ошибка полка недает сменить


рид кеш не отключить


еще раз комнанды

# sdparm --get=RCD /dev/sdu
    /dev/sdu: ST2000NM  0033-9ZM175       SN06
RCD           0  [cha: n, def:  0, sav:  0]


# sdparm --get=WCE /dev/sdu
    /dev/sdu: ST2000NM  0033-9ZM175       SN06
WCE           1  [cha: y, def:  1, sav:  1]


якобы это кеши именно прям самого диска.


вот эта хуйня
# cat /sys/block/sdi/queue/read_ahead_kb 
8192

это 8МБ
и она работает толко если мы читаем через пейдж кеш. а таккак у меня в fio стоит --direct=1
то пейдж кеш я неюзаю и этот мехагизм неисползуется


важный момент про полку altos 200F
я на компе вырубил на диске loook ahead и write cache пошел встаил в altos 200F
потом обратно вытащил притащил воткнул в комп и они опять ОКАЗАЛИСЬ ВКЛЮЧЕНЫ тоесть
полка реально включает на диске look ahead и write cache внезависимости как было на диске когда его вставляют
тесты это тоже подтверждают



#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((1024 *1024)) --iodepth=1  --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=92, BW=92.0MiB/s (96.5MB/s)(1406MiB/15281msec)


тоесть наша прога пуляет всего 1 иопс и потом ждет.
если бы лук ахед на диске был выключен то лин скорсть чтения была бы

    BW=72.8MiB/s

на основе того что я тестировал выше.

значит реально лукахед включен на диске и скорость должна бы быть BW=186MiB/s согласно тестам выше.
значит это ограничение интерпозера.
судя по всему он очереди понимает хотть както  поттому что вот я даю очередь

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((1024 *1024)) --iodepth=32  --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=94, BW=95.0MiB/s (99.6MB/s)(1470MiB/15475msec)

и результат возраастает. и он ворзрастает не из за диска.  на диске при включенном лукахед уже при иодепф=1
скорсть выдается BW=186MiB/s.  это чисто интепроер ограничение. ему нужно поток сата преобразовать в поток FC
где каждый кадр это 2КБ.  тоесть ему нужно разбить сата поток 1МБ на кучу 2К пактов FC.
тоесть сам диск выдает при такой нагузке 190MB\s в сторнону интепрзера а он уже внутрьполки может 
преобразовать только 95МБ/с


если я читаб из интерпозера кускаии по 128К иодепф=1

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((128 *1024)) --iodepth=1  --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=libaio, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=78.6MiB/s][r=628 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=14202: Tue Jan  6 19:27:59 2026
  read: IOPS=624, BW=78.1MiB/s (81.9MB/s)(1176MiB/15051msec)


то к сожалению макс скорсть интепозера 95МБ/с невыдается. хотя как видно из тестов выше при включенном лукахеде 
сам диск легко в сторону интепозера выдаст 190МБ/с. ему толко осаттся из кеша диска выгребать. но увы 95 он невыдает 
на этом режиме. но все таки с очердями интерпоезер худо безно умет рабтать. повышваем иодепф до 8

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY  --bs=$((128 *1024)) --iodepth=8 --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=745, BW=93.2MiB/s (97.7MB/s)(1400MiB/15017msec)

и выгребаем максимум того что может из себя родить интепозер
тоест у него есть ограничеини по суммарному срупуту 95МБ/с
и величина очереди тоже имеет значение. если очередь маленькая то срурупут будет меньше
тоесть елси на интепрзер прилеает один запрос. он его осблсживает и обратно отпрваляет.
если придетает сразу неколкьо сзпсосов то он их сразу обслуивает и сразу отпарваляет. поэтому очеердь играет 
значение. 

далее я задал линейное чтение с датасет где рекорсдсайз 1М  привыкобченном зфс префетче
чтобы он не влияет

    # echo 1 > /sys/module/zfs/parameters/zfs_prefetch_disable


и запускаб чтение 1М кускамми в 1 поток.
должен поидее получить тоже самое что я имел на голом диске при 1 потоке тоесть 95МБ/с

#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-1M/fio.dat  --bs=$((1024 *1024)) --iodepth=1 --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=85, BW=85.6MiB/s (89.8MB/s)(1342MiB/15678msec)

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdl             88.00  90112.00     0.00   0.00   10.60  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.93  92.90


на практике поменьше 86МБ/с 

если включить зфс претфет то коенчно уже выжммем все 95МБ/с

    # echo 0 > /sys/module/zfs/parameters/zfs_prefetch_disable


#  fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-1M/fio.dat  --bs=$((1024 *1024)) --iodepth=1 --runtime=15  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=94, BW=94.8MiB/s (99.4MB/s)(1470MiB/15504msec)



теперь прочитаем сбоевого пула
но прежде еще раз вспоимним как работат тут скорсоть на чтении с голых дисков
значит напоминаю что на самих дисках включен лукахед поэтому при заказе на чтение снаружи диск по любому читает 1-8МБ
и клоадет в кеш на чтение. поэтому при полследюущем обращении к диску данные уже есть в буфере.
и уже при чтении кускасмми по 128К и иодепф=1 сам диск при участии этого лук эхед механизма выдает в интерфейс 190МБ/с
тоесть свой максиум. а сверрху над диском сидит интепозер. который тормоз.
итак

bs=128К iodephth=1
#   fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((128 *1024)) --iodepth=1 --runtime=10  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=624, BW=78.1MiB/s (81.8MB/s)(784MiB/10041msec)

тоесть интерпозер может выбирать с буфера диска на 190МБ/с но он это не может.
он неможет даже на своем максмумме 95МБ/с


bs=128K iodepth=2 
#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((128 *1024)) --iodepth=2 --runtime=10  --readwrite=read  --numjobs=1  --group_reporting 
  read: IOPS=729, BW=91.2MiB/s (95.6MB/s)(920MiB/10087msec)

добавление двух запросов сразу наконец выбивает из интеропзера его максум

кстати в линуксе на iodepth>1 надо юзать движок io_uring потому что libaio каую то хуйню творит. ты ему даешь 
iodepth>1 а он на диски кидает непонянтую хуйню в плане очереди загрузки
проверяем это так. дали нагрузку на фио. и смотрим в iostat

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdk             96.00  98304.00     0.00   0.00   20.81  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.00  98.70

если aqu-sz равен нашему iodepth значит все работает как нам надо.


итак на bs=128K + iodepth=2 мы вишибаеим из интепозера его максмум.
посмтрим как дела с bs=1M


#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY   --bs=$((1024 *1024)) --iodepth=1 --runtime=10  --readwrite=read  --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=92.1MiB/s][r=92 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=22973: Tue Jan  6 21:16:59 2026
  read: IOPS=91, BW=91.7MiB/s (96.1MB/s)(958MiB/10449msec)


нам хвтвает iodpeth=1 для bs=1M чтобы из интепозера выжать его максимум.

теперь кошда с голыми дсикаами я напомнил пеерходим к тестрованию чтения на датасете зфс.
рекодсайз 1МБ  на боевом пуле из страйпа на 8-ми миррорах (16 дисков)
первым делом отключакем зфс префетч чтобы он не влияел




и тут на вскидку полчаю нео чень понятную картину. 
делаю тест на иодепф=1
так как у нас пул это 8 мирроров то понятно что очредной ио запрос он лежит то на одном диске то на другом.
тоесть мы постянно скачем сдика на диск. ну у меня логика какая. вот мы прилетели на диск1. счттали оттуда медленно 1МБ. 
при этом диск считаел за этот прход на самом деле мегабайт 8. и при следующем залете на этот диск по идее следущий нужный
для зфс мегабайт должен лежать в буфрее. тоесть по мне загрузка дисков должна вылядеть как то что все диски спять
а с одного читается на скоростьи 95МБ/с если не сразу то со временем. 
а пофакту я имею вот такое

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sda              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdaa             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdab             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdb              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdc              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdd              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sde              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdf              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdg              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdh              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdi              6.00   6144.00     0.00   0.00   15.67  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.09   9.40
sdj              2.00   2048.00     0.00   0.00   57.50  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.11  11.50
sdk              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdl              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdm              4.00   4096.00     0.00   0.00   16.50  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.07   6.60
sdn              3.00   3072.00     0.00   0.00   39.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.12  11.80
sdo              5.00   5120.00     0.00   0.00   14.20  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.07   7.80
sdp              4.00   4096.00     0.00   0.00   14.25  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.06   4.70
sdq              4.00   4096.00     0.00   0.00   15.75  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.06   6.40
sdr              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sds              5.00   5120.00     0.00   0.00   15.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.07   7.60
sdt              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdu              2.00   2048.00     0.00   0.00   12.50  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.03   2.50
sdv              1.00   1024.00     0.00   0.00   11.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.01   1.10
sdw              2.00   2048.00     0.00   0.00   16.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.03   3.20
sdx              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdy              2.00   2048.00     0.00   0.00   52.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.10  10.50
sdz              4.00   4096.00     0.00   0.00   32.25  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.13  13.00



ну тоест окей. ладно. если дисков 16. то каждый из них в секуда рабоатет тлокьо 1/16 часть.
если интепорзер может забррать данные с диска на скрости 95МБ/с то 95/16 = 6 МБ/с ... ну окей похоже. тоесть
интепозер забрал данные с диска на скорости 95МБ/с а потом диск тип отдахыает поэтому мы видим 6МБ/с загрузку на диске.
нотогда неясно то что у нас тогда суммарная соерость чтения должна быть 95 МБ/с как минуиум. тоесть мы читаем с одного
диска в каддый момент времени на скрости 95МБ/с. просто сразных а пофвакту я виижу скороть чтения  52MB\s

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=1 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
  read: IOPS=51, BW=51.8MiB/s (54.3MB/s)(8318MiB/160480msec)



тоесть опять ккакя хуйня именнов интепорзере кроется.

выход какой - надо каждый диск нагрузить постоянной нагрузкой. тогда все будет окей. тоесть так как дисков 16 
то иодепф=16

идейстивтельно 


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=16 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=16
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=331MiB/s][r=331 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=23153: Tue Jan  6 21:28:44 2026
  read: IOPS=351, BW=351MiB/s (368MB/s)(10.0GiB/29146msec)


  на 16 иодепсах мы получаем нормалную скорость. просто она уже упералсь в скорость двух веревок FC 2GB\s

и самый смак когда iodepth=32 то есть у нас на каждм диске qlen=2 все время и тогда

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=32 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
  read: IOPS=385, BW=386MiB/s (405MB/s)(10.0GiB/26542msec)



теперь включаю зфс префетч


# echo 0 > /sys/module/zfs/parameters/zfs_prefetch_disable


он дает то что зфс видя что мы делаем линкйную нагрзку он на каждый диск в пуле
помимо нашего реквеста дает еще свои ио запросы да так умно что на каждом диске у нас постояно qlen=2


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sda              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdaa             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdab             0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdb              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdc              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdd              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sde              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdf              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdg              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdh              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdi             26.00  26624.00     0.00   0.00  100.69  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.62  91.90
sdj             19.00  19456.00     0.00   0.00   81.68  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.55  63.50
sdk              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdl              0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.00
sdm             22.00  22528.00     0.00   0.00   76.86  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.69  70.30
sdn             27.00  27648.00     0.00   0.00  105.59  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.85  97.10
sdo             27.00  27648.00     0.00   0.00   83.04  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.24  77.70
sdp             21.00  21504.00     0.00   0.00   87.38  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.83  70.10
sdq             24.00  24576.00     0.00   0.00   78.62  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.89  78.00
sdr             26.00  26624.00     0.00   0.00   93.12  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.42  93.10
sds             24.00  24576.00     0.00   0.00  106.62  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.56  88.30
sdt             21.00  21504.00     0.00   0.00   88.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.85  71.60
sdu             24.00  24576.00     0.00   0.00   70.00  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.68  66.50
sdv             25.00  25600.00     0.00   0.00   96.12  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.40  81.90
sdw             24.00  24576.00     0.00   0.00   86.71  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.08  72.60
sdx             25.00  25600.00     0.00   0.00   94.92  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.37  84.70
sdy             24.00  24576.00     0.00   0.00   76.04  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.83  67.90
sdz             28.00  28672.00     0.00   0.00   92.39  1024.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    2.59  81.80


тоесть я на фио задаю всего iodepth=1

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-1M/fio.dat    --bs=$((1024 *1024)) --iodepth=1 --runtime=160  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=374MiB/s][r=374 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=23525: Tue Jan  6 21:34:24 2026
  read: IOPS=380, BW=381MiB/s (399MB/s)(10.0GiB/26879msec)



но зфс пуляет на пул помимо нашего запроса единсвтенного еще 31 свой запрос понимая как устроенн миррор. 
тоесть пофакту на пул летит iodepth=32  и ответ сохарянется в AB буфрее зфс. поэтому даже когда у меня 
на фио iodepth=1 мы получаем сумпашедший перфоманс такой какой мы получали только на иодепф=32 !!!! 
вот такую крутую вещь делает zfs prefettch!!!! просто мегаркутейшая вещь.! далеко накаждое прилжоение поррождает 
глубину запросов 32. поэтому зфс в этом случае здорово выручает. и наше прилжоение которое генирует 
лохвовской iodpeth=1 получает перфоманс как еси бы оно генирролвало иодепф=32
просто отлично!!!



длаее еще один иентеернсный момент.
создаем зфс датасет с  recordsize 128K
пишет туда файл
начинаем чиатть его линейно. причем  с разным iodepth
так вот интересно зфс будет с диска читать кусками по 128К или он будет агрегировать эти заросы в макрокуски
размером скажем 1МБ. посмтрим это и во фрибсл и в линусексе.

фрибсд.
вырубаем зфс префетч


  # sysctl  vfs.zfs.prefetch.disable=1
     0-> 1


запускаем чтение bs=128К  iodepth=1


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=read --numjobs=1  --group_reporting 

# iostat ...

 128   311 38.86  0.0     0  0.00 
 128  1181 147.62  0.0     0  0.00 
 128  1312 164.00  0.0     0  0.00 
 128  1210 151.25  0.0     0  0.00 
 128  1118 139.74  0.0     0  0.00 
 128  1280 160.00  0.0     0  0.00 
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
 128   910 113.75  0.0     0  0.00 
 128  1169 146.09  0.0     0  0.00 
 128  1243 155.42  0.0     0  0.00 
 128  1085 135.62  0.0     0  0.00 
 128  1302 162.77  0.0     0  0.00 
 128  1151 143.92  0.0     0  0.00 
 128    40  5.01  0.0     0  0.00 


все логично. в зфс поступает запрос. он сразу его пыатетс выполнить. запрос всего один. приложение больше 
нешлет пока неполучитответ. прфетч зфс выклюыен. поэтому чтение идет кускками по 128К


теперь я кидаю пачку 128К запроосов
bs=128K  iodepth=32

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=32 --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
bs: 1 (f=1): [R(1)][56.7%][r=168MiB/s][r=1344 IOPS][eta 00m:13s]


# iostat -d da2 da3 1
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
 400     1  0.41 12.4     0  0.00 
 607   285 168.63  0.0     0  0.00 
 462   360 162.27  0.0     0  0.00 
 386   418 157.41  0.0     0  0.00 
 621   278 168.39  0.0     0  0.00 
 468   373 170.56  0.0     0  0.00 
 944   189 174.41  0.0     0  0.00 
 431   408 171.73  0.0     0  0.00 
 499   349 170.18  0.0     0  0.00 
 678   265 175.32  0.0     0  0.00 
 510   338 168.37  0.0     0  0.00 
1024   184 183.73  0.0     0  0.00 
 623   282 171.77  0.0     0  0.00 
 807   224 176.73  0.0     0  0.00 
 491   346 165.80  0.0     0  0.00 


теерь зфс получает пачку запрсов сразу. аггрегирует их и на диск шлет уже макрозапрос размером 500-1024 КБ
тоесть зфс незаебывает диск милионом мелкеих запросов. и также видно что на диск идет нагрузка 280-400 иопс
крупных а приложение видит как бутто 1344 иопса на диск было послано. все это пдотвеидрждает агрегацию


тоест зфс умеет агрегировать заросы и из мелких в круаные нетолько при записи но и причтении.
теперь актвиирую зфс префетч и ioddepth ставлю 1


#  sysctl  vfs.zfs.prefetch.disable=0
    vfs.zfs.prefetch.disable: 1 -> 0

bs=128K iodepth=1
# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
^Cbs: 1 (f=1): [R(1)][50.0%][r=182MiB/s][r=1453 IOPS][eta 00m:15s]


# iostat -d da2 da3 1
             da2              da3 
KB/t   tps  MB/s KB/t   tps  MB/s 
 404     1  0.42 12.2     0  0.00 
1024   183 182.81  0.0     0  0.00 
1024   182 181.94  0.0     0  0.00 
1024   184 183.73  0.0     0  0.00 
 983   185 177.57  0.0     0  0.00 
1024   183 182.57  0.0     0  0.00 
1024   183 183.18  0.0     0  0.00 
1024   181 181.06  0.0     0  0.00 
1024   183 183.25  0.0     0  0.00 
1024   183 183.32  0.0     0  0.00 
1024   182 181.73  0.0     0  0.00 
1024   185 184.57  0.0     0  0.00 
1024   181 180.53  0.0     0  0.00 


и втидно насколку умно зфс делает префетч. 
она читает с диска кускками по 1МБ !! это при том что от приложениея прителел запрос всего на один иопс. 

вобщем круто. зфс умеет агрегировать мелкие запросы в крупные. тем более когда влкюачен зфс префетч. 
тоесть вот видно вместо 1453 запроса на диск зфс шлет на диск всего 181 запрос. в семь раз меньше.
и еще важный вывод - если префетч влючен и мы сидим на датасете 128К и читаем с bs=128K и iodepth=1
то  мы при этом полностью раскрываем весь срупут диска (!!)




тоже самое смотрим на линукс
отключаем пренфетч


 # echo 1 > /sys/module/zfs/parameters/zfs_prefetch_disable

bs=128K iodepth=1


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat     --bs=$((128 *1024)) --iodepth=1  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=183MiB/s][r=1467 IOPS][eta 00m:00s]


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc           1455.00 186240.00     0.00   0.00    0.57   128.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.83  82.50


также как на фрибсл здесь зфс читвает кускми по 128К. что полностью окей.


доабвляем iodepth=32 тесть закиывдаем в зфс сразу пачку запросов



# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat     --bs=$((128 *1024)) --iodepth=32  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=184MiB/s][r=1474 IOPS][eta 00m:00s]


Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc           1010.00 187904.00     0.00   0.00    9.86   186.04    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    9.96  97.50



и вот тут видно что линукс агрегирует запросы но не так круто как фрибсд
тоесть вот он средний размер запроса на диск 186.04 и вот они иопсы на диск  1010.00
для сравния фрибсл на таоже тесте у него средний размер запроса на дитск 400-600-800 и иопсов на диск 200-400


теперь активруиуем префетч и возвращемся к iodepth=1


~# echo 0 > /sys/module/zfs/parameters/zfs_prefetch_disable
root@test-prox:~# 
root@test-prox:~# 
root@test-prox:~# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-03/test-ds-128K/fio.dat     --bs=$((128 *1024)) --iodepth=1  --runtime=20  --readwrite=read --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
^Cbs: 1 (f=1): [R(1)][35.0%][r=189MiB/s][r=1512 IOPS][eta 00m:13s]



Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
sdc            191.00 188416.00     0.00   0.00   16.24   986.47    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    3.10  98.60


ну нормал средний размер запроса 986.47 иногда даже его прбивает и он делает 1024КБ

тоесть ликустоже умеет агрегирвать запросы в вболее корпные при чтении. и улчше всего у него этополучсается
при включенном прфефетче.
сотвтесвенно также и ккак и на фроисбл.  надатасете 128К рекдрсзаайз при чтении bs=128K iodepth=1
при включенном пренфетче мы выжимаем полную линкейную скорость из диска 190 МБ/с 


резюме - фрибсл или линкус. если у нас есть  датасет с рекдсайзом 128К и мы читаем линейно кусками 128К 
при iodepth=1 и включенном зфс префетче то мы длжны получать полную сокрость которуб могут линейно разивать диски.
(при улосвии что файл неперемешан на диске в плане офффсетов то ест мы его только создали).
и также мы должны видеть что на сам диск запросы идут не в формет 128К а вболее крупных запросах типа 986-1024 KB
этотоже супер важно.

это я щас разбирал на примере когда наш диск подкючен к локальнму внутри компа ХБА контроллеру.
так вот я вовзращаюсь к нашей полке altos 
там тоже я создал датасет 128К
псомрим какая будет лин скорость чтения на нем



# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=read  --numjobs=1  --group_reporting
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=356MiB/s][r=2846 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24320: Tue Jan  6 22:49:01 2026
  read: IOPS=3037, BW=380MiB/s (398MB/s)(10.0GiB/26966msec)



ну что отлично. ураган. 
тоесть датасет с рекорсайзом 128К  хватаает этого чтобы снять с ссисиетемы макс лин скорсть которубю она может 
из себя выжать на голых дисках. по краней мере пока файл в плане оффсетов непермешан на дисках.


про рандом риды. и фрибсл и лиукс я проверил они нормал паралелят иопсы на диски при iodepth>1
оба это умеют.

к нашему диску вернесся. столкнулся с неоченьпонятной штукой. если я тесриру на роандлом рид голвый диск то 76 иопс
а если файл на базе зфс то 127 иопс. как такое моежет быть непонимаю.
тестрирую диск когда он подключен к лоальнму hba котроллеру
тоесть узнаю его нативный перфоманс

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=1 --runtime=10  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=9600KiB/s][r=75 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=8129: Tue Jan  6 20:23:11 2026
  read: IOPS=76, BW=9767KiB/s (10.0MB/s)(95.8MiB/10039msec)



 fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=1 --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
^Cbs: 1 (f=1): [r(1)][23.3%][r=16.9MiB/s][r=135 IOPS][eta 00m:23s]
fio: terminating on signal 2
test: (groupid=0, jobs=1): err= 0: pid=8140: Tue Jan  6 20:26:30 2026
  read: IOPS=125, BW=15.7MiB/s (16.5MB/s)(114MiB/7208msec)



делаю iodepth=32

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/da2  --bs=$((128 *1024)) --iodepth=32 --runtime=10  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=18.9MiB/s][r=151 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=8142: Tue Jan  6 20:29:14 2026
  read: IOPS=151, BW=18.9MiB/s (19.9MB/s)(200MiB/10542msec)



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/ST2000/test-ds-128K/fio.dat  --bs=$((128 *1024)) --iodepth=32 --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=32.4MiB/s][r=259 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=8144: Tue Jan  6 20:29:56 2026
  read: IOPS=259, BW=32.4MiB/s (34.0MB/s)(984MiB/30334msec)





теперь тестирую тоже самое но уже когда диск всталвен в полку

#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdk   --bs=$((128 *1024)) --iodepth=1 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=8960KiB/s][r=70 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24676: Tue Jan  6 23:28:05 2026
  read: IOPS=71, BW=9172KiB/s (9392kB/s)(136MiB/15156msec)

тут без потери скорости. 


#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=1 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=16.5MiB/s][r=132 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24710: Tue Jan  6 23:28:50 2026
  read: IOPS=125, BW=15.7MiB/s (16.5MB/s)(240MiB/15254msec)


тут тоже без потери скорсоти



теперь iodepth=32

#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/sdk   --bs=$((128 *1024)) --iodepth=32 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=14.3MiB/s][r=114 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=24743: Tue Jan  6 23:30:00 2026
  read: IOPS=103, BW=12.9MiB/s (13.6MB/s)(200MiB/15440msec)

  полуичли 103 против 150 когда диск вставлен в локальный хба адаптер



#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-15A4/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=32 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=18.0MiB/s][r=144 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=25086: Tue Jan  6 23:37:04 2026
  read: IOPS=146, BW=18.3MiB/s (19.2MB/s)(280MiB/15288msec)

   получили 146 проотив 259 когшда диск ватсвлен в локальный хба адаптер


теперь проетсируем  на боевом пуле страйп из 8 мирроров
вначале iodpeth=16 это примерно по одному ио запросу на каждыцй диск. тоесть каждый диск будет под напгрузкой q=1
ранее мы полуили что для q=1 на пуле из одногодиска у нас 125 iops значит щас я ожидаю 125*16 = 2000 иопс

root@backup-02:/home/krivosheeva#  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=16 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=16
  read: IOPS=1534, BW=192MiB/s (201MB/s)(2888MiB/15055msec)


    получил 1500 иопс неплохо (по 96 иопс на диск)


теперь iodepth=32 это по q=2 на каждый диск

  #  fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/POOL-02/test-ds-128K/fio.dat   --bs=$((128 *1024)) --iodepth=32 --runtime=15  --readwrite=randread  --numjobs=1  --group_reporting
test: (g=0): rw=randread, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=io_uring, iodepth=32
fio-3.33
  read: IOPS=1908, BW=239MiB/s (250MB/s)(3592MiB/15057msec)


   получил 1900 иопс неплохо. (по 119 иопс на диск)


ИТОГО с чтением будесм считать на этой полке разобралдись





ПЕРЕХОДИМ  к записи.

уже известно что сам диск может писать на скорости 190МБ/с








более старые опыты:


ЛИН ЧТЕНИЕ 

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=160MiB/s][r=160 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2645: Sun Jan  4 13:59:19 2026
  read: IOPS=183, BW=184MiB/s (193MB/s)(5566MiB/30261msec)


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=182MiB/s][r=182 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2660: Sun Jan  4 14:03:07 2026
  read: IOPS=185, BW=185MiB/s (194MB/s)(5566MiB/30056msec)



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((  128 *1024)) --iodepth=1  --runtime=30  --readwrite=read --numjobs=1  --group_reporting 
test: (g=0): rw=read, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=182MiB/s][r=1459 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2662: Sun Jan  4 14:04:06 2026
  read: IOPS=1481, BW=185MiB/s (194MB/s)(5560MiB/30027msec)



ЛИН ЗАПИСЬ

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=32  --runtime=30  --readwrite=write  --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][0.3%][w=181MiB/s][w=181 IOPS][eta 02h:55m:29s]
test: (groupid=0, jobs=1): err= 0: pid=2667: Sun Jan  4 14:05:24 2026
  write: IOPS=186, BW=187MiB/s (196MB/s)(5694MiB/30472msec); 0 zone resets



# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((8*128 *1024)) --iodepth=1  --runtime=30  --readwrite=write  --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][0.3%][w=183MiB/s][w=183 IOPS][eta 02h:57m:08s]


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$((1*128 *1024)) --iodepth=1  --runtime=30  --readwrite=write  --numjobs=1  --group_reporting 
test: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [W(1)][0.3%][w=183MiB/s][w=1462 IOPS][eta 02h:57m:05s]
test: (groupid=0, jobs=1): err= 0: pid=2685: Sun Jan  4 14:07:06 2026
  write: IOPS=1479, BW=185MiB/s (194MB/s)(5552MiB/30017msec); 0 zone resets



иттого резюме лин чтение и запись 190 МБ/с





РАНДОМ РИД

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=672KiB/s][r=84 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2690: Sun Jan  4 14:08:41 2026
  read: IOPS=81, BW=654KiB/s (670kB/s)(19.5MiB/30498msec)

# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randread --numjobs=1  --group_reporting 
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=1496KiB/s][r=187 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2692: Sun Jan  4 14:09:19 2026
  read: IOPS=180, BW=1444KiB/s (1479kB/s)(43.0MiB/30480msec)



РАНДОМ ВРАЙТ


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=1  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [f(1)][100.0%][w=200KiB/s][w=25 IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2701: Sun Jan  4 14:10:54 2026
  write: IOPS=210, BW=1685KiB/s (1726kB/s)(49.5MiB/30064msec); 0 zone resets


# fio --randrepeat=1 --ioengine=posixaio --direct=1 --gtod_reduce=1 --name=test --filename=/dev/gpt/Z1X415HY  --bs=$(( 8 *1024 )) --iodepth=32  --runtime=30  --readwrite=randwrite --numjobs=1  --group_reporting 
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=32
fio-3.40
Starting 1 process
Jobs: 1 (f=1): [f(1)][100.0%][w=752KiB/s][w=94 IOPS][eta 00m:00s]  
test: (groupid=0, jobs=1): err= 0: pid=2716: Sun Jan  4 14:11:36 2026
  write: IOPS=207, BW=1658KiB/s (1698kB/s)(49.0MiB/30248msec); 0 zone resets


итого ранд рид врайты 80-200 иопс

