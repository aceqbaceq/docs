lxd
====
как его поставить из snap
потому что там самый послдний

# обновит инфо о снапах
   29  snap refresh

# обвноить наш lxd
   35  snap refresh lxd --channel=latest/stable

# проеверить что наш снап обновился
   40  snap list

=====

как искать имаджи на удаленной системе

смотрим список имаджей

# lxc remote list

+-----------------+------------------------------------------+---------------+-------------+--------+--------+--------+
|      NAME       |                   URL                    |   PROTOCOL    |  AUTH TYPE  | PUBLIC | STATIC | GLOBAL |
+-----------------+------------------------------------------+---------------+-------------+--------+--------+--------+
| images          | https://images.linuxcontainers.org       | simplestreams | none        | YES    | NO     | NO     |
+-----------------+------------------------------------------+---------------+-------------+--------+--------+--------+
| local (current) | unix://                                  | lxd           | file access | NO     | YES    | NO     |
+-----------------+------------------------------------------+---------------+-------------+--------+--------+--------+
| ubuntu          | https://cloud-images.ubuntu.com/releases | simplestreams | none        | YES    | YES    | NO     |
+-----------------+------------------------------------------+---------------+-------------+--------+--------+--------+
| ubuntu-daily    | https://cloud-images.ubuntu.com/daily    | simplestreams | none        | YES    | YES    | NO     |
+-----------------+------------------------------------------+---------------+-------------+--------+--------+--------+


если хотим искать в источнике ubuntu, ( из него удобно искать образы убунты)

# lxc image list ubuntu: | grep ubuntu | grep 86_64 | grep 22.04| grep 2023 | grep Apr | grep CONTAINER



если хотим искать в источике images (напрмиер оттуда можно вытянуть образ centos), то 
# lxc image list images: | grep centos  |  grep x86_64 | grep CONTAINER



скопироать имадж с удаленного серверра на свой локальный комп

# lxc image copy ubuntu:b75bb602cfee   local:

самая полезня ссылка = https://ubuntu.com/blog/nested-containers-in-lxd
чтообы запустить первый контейнер надо
взять два файла


# ls -1 /etc/sub*id
/etc/subgid
/etc/subuid


и вставить в них
# cat /etc/sub*id
root:500000:196608
root:500000:196608

далее надо убедиться что /etc/lxc/ default.conf отсутсвтует


теперь мы можем запустить кнтйенер от рута

# lxc launch  local:ed7509d7e83f  first

---
после того как запустили контейнер как в него войти

# lxc exec NAME -- bash

если видим ошибку
$ lxc exec minikube -- bash
Error: LXD VM agent isn't currently running

то надо еще малек подождать система еще не просралась.

---
а вот как сделтаь так чтобы внутриэтого контйенера можно было заупстить докер

https://discuss.linuxcontainers.org/t/what-does-security-nesting-true/7156/2

прверть какие контйенеры у меня идут с настройкой позволяюзей заупскать докер внутри наших lxc контйенеров

# lxc list security.privileged=true


роверить эту настройку для конкретного котнйерера

$ lxc config get your-container-name security.privileged





---



как проерить что контйенер работает в unprivilged mode

заходим в контйенер

# lxc exec  cont_name -- /bin/bash

и далее


It's also possible to check if a container is unprivileged from inside the LXD container by checking:

/proc/self/uid_map
/proc/self/gid_map
where it will show something like (root 0 mapped to user 1000000):

root@first:~# cat /proc/self/gid_map
         0     500000     196608
root@first:~# cat /proc/self/uid_map
         0     500000     196608


а теперь покызваю содержимое файлов уже на хостовой ОС моей

# cat /etc/sub*id
root:500000:196608
root:500000:196608

все совпадает

----

как с хоста засунуть в виртуалку файл

# lxc file push ~/temp/1/gt_erp.sql.gz first/mnt/

first = название вм

---
лимит памяти на контейнер

#  lxc config set first  limits.memory 400MB

first = название контейнера

прчием можно менять налету!

есть еще такое

# lxc config set my-container limits.memory.enforce 2GB
незнаю в чем разница
---

как быстро забить оперативку

mount -t tmpfs none /new/path/for/temp -o size=32m
---
lxc start --all
lxc stop --all
lxc restart --all
----
скопировать контенер в другой контенер
# lxc copy  src_cont   dest_cont
----
как защиттить контенер от случайного удаления

# lxc config set my-container security.protection.delete true

как снять азщиту

# lxc config set my-container security.protection.delete false





----
пробрсто порта

# lxc config device add имя_контейнера  myport80 proxy listen=tcp:0.0.0.0:9898  connect=tcp:127.0.0.1:80   (добавить)
# lxc config device remove first myport80 (удалить)


как выглядит это в netstat 
видно что слушатель это lxd
# netstat -tnlp | grep -E "Proto|lxd"
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp6       0      0 :::1200                 :::*                    LISTEN      13469/lxd           


---
 в предеелах хоста одна виртуалка может достучаться до другой по доменому имени.
 в виде имя_виртуалки.lxd

 если имя вируталки second то  с другой виртуалки можно достучаться как

$ ping -c 3  second.lxd

---


конфиг жинкса (так чисто можт пригодистя)

upstream apache-server {
    server apache-server.lxd:80;
}

server {
    listen 80 proxy_protocol;
    listen [::]:80 proxy_protocol;
    server_name apache.server.test; #< Your domain goes here

    location / {
        proxy_pass http://apache-server;

        proxy_redirect off;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

---

создать снэпшот

# lxc snapshot {container} {snapshot-name}

это будет stateless снэпшот.

еще можно снть statefull - пока незнаю как.
---
вотстановить снэпшот
# lxc restore {container} {snapshot-name}

---
удалить снэпшот
lxc delete {container}/snapshot-name}
---

перенести контейнер с машины на машину
https://openschoolsolutions.org/how-to-backup-lxd-containers/

---
lxd + lvm

https://www.pither.com/simon/blog/2018/09/28/lxd-lvm-thinpool-setup

а от себя добавлю вот что:
значит инициируем lxd

# lxd init

он  там спрашивает хотим ли мы создаь новый thin pool
или будем юзать уже созданный.

я значит создал руками thin pool вот так
тоесть можно создать thin pool 
уже на сущетвующем vgs надо лищь чтоб было какоето колиество свободных блоков на нем. у меня было.
итак создал thin pool

# lvcreate -L 10G -T vg-ssd-Heewu1aa/lxd-thinpool

после этого он никак его не мог увидеть


# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: stor-pool-1
Invalid input, try again.

Do you want to configure a new storage pool? (yes/no) [default=yes]: yes
Name of the new storage pool [default=default]: stor-pool-1
Name of the storage backend to use (btrfs, dir, lvm) [default=btrfs]: lvm
Create a new LVM pool? (yes/no) [default=yes]: no
Name of the existing LVM pool or dataset: lxd-thinpool

The LVM thin provisioning tools couldn't be found. LVM can still be used
without thin provisioning but this will disable over-provisioning,
increase the space requirements and creation time of images, containers
and snapshots.

If you wish to use thin provisioning, abort now, install the tools from
your Linux distribution and run "lxd init" again afterwards.

Do you want to continue without thin provisioning? (yes/no) [default=yes]: ^C



я проблва указывать имя и как vg-ssd-Heewu1aa/lxd-thinpool
и как lxd-thinpool


выяснилось что надо поставить пакет
# apt-get -y install thin-provisioning-tools

после этого стадия lxd init  проходит дальше

но все равно в итоге как бы пошел нахуй

# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: 
Name of the new storage pool [default=default]: st-pool-1
Name of the storage backend to use (btrfs, dir, lvm) [default=btrfs]: lvm
Create a new LVM pool? (yes/no) [default=yes]: no
Name of the existing LVM pool or dataset: vg-ssd-Heewu1aa/lxd-thinpool
Would you like to connect to a MAAS server? (yes/no) [default=no]: no
Would you like to create a new local network bridge? (yes/no) [default=yes]: yes
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: auto
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: none
Would you like LXD to be available over the network? (yes/no) [default=no]: no
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] no
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]: no
Error: Failed to create storage pool 'st-pool-1': Failed to run: vgs --noheadings -o lv_count vg-ssd-Heewu1aa/lxd-thinpool: Invalid volume group name vg-ssd-Heewu1aa/lxd-thinpool.
  Run `vgs --help' for more information.


тогда создал предварително руками lxd storage
# lxc storage create st-pool-1  lvm source=vg-ssd-Heewu1aa lvm.thinpool_name=lxd-thinpool
Storage pool st-pool-1 created

# lxc storage list
+-----------+-------------+--------+-----------------+---------+
|   NAME    | DESCRIPTION | DRIVER |     SOURCE      | USED BY |
+-----------+-------------+--------+-----------------+---------+
| st-pool-1 |             | lvm    | vg-ssd-Heewu1aa | 0       |
+-----------+-------------+--------+-----------------+---------+


после этого инициировал lxd
указав что нехочу создавать сторадж

# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: no
Would you like to connect to a MAAS server? (yes/no) [default=no]: no
Would you like to create a new local network bridge? (yes/no) [default=yes]: 
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: none
Would you like LXD to be available over the network? (yes/no) [default=no]: no
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] no
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]: no


после этого надо обяснить lxd какой сторадж использовать по умполчанию

# lxc profile device add default root disk path=/ pool=st-pool-1
где st-pool-1 это то что показыает команда

# lxc storage list
+-----------+-------------+--------+-----------------+---------+
|   NAME    | DESCRIPTION | DRIVER |     SOURCE      | USED BY |
+-----------+-------------+--------+-----------------+---------+
| st-pool-1 |             | lvm    | vg-ssd-Heewu1aa | 0       |
+-----------+-------------+--------+-----------------+---------+




после этого успешный запуско контейнера 

# lxc launch ubuntu:18.04 sphinx2 

либо если хочется то имя сторджа можно укзаать всегда руками


# lxc launch ubuntu:18.04 sphinx2 -s st-pool-1


еще раз ползной окзаалось ссылка 
- https://askubuntu.com/questions/1222407/setup-lxd-storage-thin-pool-on-an-existing-lvm-volume-group-of-the-host



---
есть эфемерные кнтейнеры которые будут удалены при стопе  автоматом
----
как получить конфиг lxd

lxc config show
lxc cluster show mycluster
lxc network show lxdbr0
lxc storage show default
lxc profile show default


из вывода можно состраяпать один файл
и скормить его новой установке

# lxd init --presees < file.yaml
-----
постмреть конфиг контейнера 

# lxc config show  имя_контейнера --expanded

----
автостарт контйеров при старте хоста
# lxc config set {container-name} boot.autostart true

следущая настройка задает таймаут 10 секунд после того как котейнер стартанул
до старта ледущего котейнтера
#  lxc config set mysql  boot.autostart.delay 10

cледующая насторкйка высталвяет приортет. какй когтейнер сртартует первым
чем больше число тем контенер стартует первее
# lxc config set db_vm boot.autostart.priority 100

---
как зайти в контейнер не под root

# lxc exec some-ubuntu -- sudo --login --user ubuntu

----
можно смотреть сколько жрет цпу каждый lxd контейнер через 

# systemd-cgtop
----
прикольный сайт по lxd

https://blog.simos.info/how-to-add-multi-line-raw-lxc-configuration-to-lxd/
-----
это кусок связан с тем как внести raw.lxc параметр в конфиг lxd контейнера

printf 'lxc.cgroup.devices.allow = a\nlxc.mount.auto=proc:rw sys:rw\nlxc.cap.drop=\nlxc.apparmor.profile=unconfined' | lxc config set mycontainer raw.lxc -

------
?????

как сробрать котнейнер с hdd+ssd стораджами
как перенести файловый бэкап внутрь контейнера
как при старте контейнера указат на ккаком сторадже ему сидеть? (если сторадже несколько )
---

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
какойто кусок команд про lxd
начало блока
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# lxd init
Would you like to use LXD clustering? (yes/no) [default=no]: 
Do you want to configure a new storage pool? (yes/no) [default=yes]: no
Would you like to connect to a MAAS server? (yes/no) [default=no]: 
Would you like to create a new local network bridge? (yes/no) [default=yes]: 
What should the new bridge be called? [default=lxdbr0]: 
What IPv4 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
What IPv6 address should be used? (CIDR subnet notation, “auto” or “none”) [default=auto]: 
Would you like LXD to be available over the network? (yes/no) [default=no]: 
Would you like stale cached images to be updated automatically? (yes/no) [default=yes] 
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]: yes
config: {}
cluster: null
networks:
- config:
    ipv4.address: auto
    ipv6.address: auto
  description: ""
  managed: false
  name: lxdbr0
  type: ""
storage_pools: []
profiles:
- config: {}
  description: ""
  devices:
    eth0:
      name: eth0
      nictype: bridged
      parent: lxdbr0
      type: nic
  name: default


  ==


#  lxc storage create st-pool-1-ussd  lvm source=vg01-ussd lvm.thinpool_name=LXDPool

при этом можем получить ошибки:
Error: Volume group is not empty
или 
Error: Volume group "vg01-ussd" is already used by LXD

первая ошибка вылезет если внутри этгого thinpool недолжно быть ничего. иначе он напишет что пошел нахер volume group непустой.
на самом деле он хочет сказать что thin volume внури должен быть пустой.

вторая хер знает почему. 

в итоге если такое вылезло то запускаем вот так:

# lxc storage create st-pool-1-ussd lvm source=vg01-ussd lvm.vg.force_reuse=true lvm.use_thinpool=true lvm.thinpool_name=LXDPool



# lxc storage list
+----------------+--------+-----------+-------------+---------+---------+
|      NAME      | DRIVER |  SOURCE   | DESCRIPTION | USED BY |  STATE  |
+----------------+--------+-----------+-------------+---------+---------+
| st-pool-1-ussd | lvm    | vg01-ussd |             | 0       | CREATED |
+----------------+--------+-----------+-------------+---------+---------+


# lxc profile device add  default root disk path=/ pool=st-pool-1-ussd





root@lxd-01:/home/krivosheev# 
root@lxd-01:/home/krivosheev# 
root@lxd-01:/home/krivosheev# 
root@lxd-01:/home/krivosheev# lxd init --dump
config:
  images.auto_update_interval: "0"
networks:
- config:
    ipv4.address: 10.228.56.1/24
    ipv4.nat: "true"
    ipv6.address: none
  description: ""
  name: lxdbr0
  type: bridge
  project: default
storage_pools:
- config:
    lvm.thinpool_name: LXDPool
    lvm.vg_name: vg01-ussd
    source: vg01-ussd
    volatile.initial_source: vg01-ussd
  description: ""
  name: st-pool-1-ussd
  driver: lvm
profiles:
- config: {}
  description: Default LXD profile
  devices:
    eth0:
      name: eth0
      network: lxdbr0
      type: nic
    root:
      path: /
      pool: st-pool-1-ussd
      type: disk
  name: default
projects:
- config:
    features.images: "true"
    features.networks: "true"
    features.networks.zones: "true"
    features.profiles: "true"
    features.storage.buckets: "true"
    features.storage.volumes: "true"
  description: Default LXD project
  name: default

root@lxd-01:/home/krivosheev# 



===
походу этой командой можно укзаывать какие папки в контйнере монтирвовать в какой пул

# lxc profile device add  default root disk path=/ pool=st-pool-1-ussd

в данном случае указывается что "/" в контйенере надо монтроватв  в пул st-pool-1-ussd

==
ошибка


# lxc start vm_name
error:newuidmap binary is missing


это значит что в контейнере нехатает нужным пакетов
ршение

# root@lxd-01:/home/krivosheev# apt-get install uidmap

===
вобщем приколп полнкнейий. 

вот мы поставили себе lxd из snap
в состав пакетв входит утилита lxd.migrate
но она нерабоатет пишет

=> Connecting to source server
error: Unable to connect to the source LXD: Get "http://unix.socket/1.0": dial unix /var/lib/lxd/unix.socket: connect: no such file or directory
[root@lenovo ~]# systemctl list-units | grep lxd


рещенние такое. смотри версию нашего lxd

# snap list

lxd                   5.13-cea5ee2     24758  latest/stable    canonical✓

потом идем на страницу релизов
https://github.com/lxc/lxd/releases

и оттуда руками качаем эту же утилиту.
wget https://github.com/lxc/lxd/releases/download/lxd-5.13/bin.linux.lxd-migrate

запускаем ее и о чудов все заработало.
я так понимаю что пробем в том что  lxd.migrate из снапа она там заизолррованан инеимеет доступа к сокету 
на локальном хосте.

вот такоп прикол.


также чтобы зараотало надо прерставит lxd чтобы в настройках иницлизации было укзаано что он 
дложено быть доступен по сети!!!!
===
migrate
миграция

нужно на сервере lxd куда будем импортиррова контейнер создать токен 
чрез который можно на клиенте аутентифицироваься перед ним

# lxc config trust add 


и тут я нактнулся на очень важный момент. 
он спросит в какой папке на хосте лежит будущий "/"
так вот он будет копировать только эту точку монтрования а если у нее есть там
суботчки монтиорвания то он их нетронет. поэтому для копронрования субточек монтрования 
нужно их указать 
напрмиер у нас вот так подмонтрированы папаки на хосте:
 /mnt/mysql
 /mnt/mysql/var/lib/mysql
 /mnt/mysql/boot

 мы прям так и указыаем а он сам пойму куда их нужно скопррвать в контейнер. это очень важный момент.
 иначе у нас небудет хватать папок на фс контйенера



когда импортровали в контенер и запустили его то надо 
- переимновать /etc/ftsab в fstab.bak
- удалить cloud-init
   # apt-get purge cloud*
- устанвоить hostame
   # hostnamectl hostname vasya123
- проермть что сервисы при стопе\старте контейнера автоматом запускаются сами


после переноса вм с мускулом в контйенер он нехотел запускаться 
и выдвалва в логах ошиюку

[ERROR] Unknown/unsupported storage engine: InnoDB

решение - нужно удалить лог файл базы

# rm /var/lib/mysql/ib_logfile0
# rm /var/lib/mysql/ib_logfile1 

нашел это ренеие тут - https://serverfault.com/questions/379714/unknown-unsupported-storage-engine-innodb-mysql-ubuntu


==
microceph


# microceph.ceph status
  cluster:
    id:     d64966ad-7849-4b7b-a2f9-79461128965c
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum lxd-02,lxd-03,lxd-04 (age 7m)
    mgr: lxd-02(active, since 8m), standbys: lxd-03, lxd-04
    osd: 3 osds: 3 up (since 6m), 3 in (since 6m)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 577 KiB
    usage:   70 MiB used, 279 GiB / 279 GiB avail
    pgs:     1 active+clean




# microceph status
MicroCeph deployment summary:
- lxd-02 (10.103.1.27)
  Services: mds, mgr, mon, osd
  Disks: 1
- lxd-03 (10.103.1.24)
  Services: mds, mgr, mon, osd
  Disks: 1
- lxd-04 (10.103.1.13)
  Services: mds, mgr, mon, osd
  Disks: 1




# microceph.ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.27237  root default                              
-2         0.09079      host lxd-02                           
 0         0.09079          osd.0        up   1.00000  1.00000
-3         0.09079      host lxd-03                           
 1         0.09079          osd.1        up   1.00000  1.00000
-4         0.09079      host lxd-04                           
 2         0.09079          osd.2        up   1.00000  1.00000



# lxc cluster list


# lxc storage list
+--------+--------+-------------+---------+---------+
|  NAME  | DRIVER | DESCRIPTION | USED BY |  STATE  |
+--------+--------+-------------+---------+---------+
| remote | ceph   |             | 1       | CREATED |
+--------+--------+-------------+---------+---------+

#  lxc launch ubuntu:22.04 ub22-1 --storage remote



# lxc list
+------+---------+---------------------+------+-----------+-----------+----------------+
| NAME |  STATE  |        IPV4         | IPV6 |   TYPE    | SNAPSHOTS |    LOCATION    |
+------+---------+---------------------+------+-----------+-----------+----------------+
| c1   | RUNNING | 240.27.0.241 (eth0) |      | CONTAINER | 0         | lxd-cluster-01 |
+------+---------+---------------------+------+-----------+-----------+----------------+
| c2   | RUNNING | 240.24.0.124 (eth0) |      | CONTAINER | 0         | lxd-03         |
+------+---------+---------------------+------+-----------+-----------+----------------+
| c3   | RUNNING | 240.13.0.30 (eth0)  |      | CONTAINER | 0         | lxd-04         |
+------+---------+---------------------+------+-----------+-----------+----------------+

--
как перенсти вм с одного сервера на другой

https://www.cyberciti.biz/faq/how-to-movemigrate-lxd-vm-to-another-host-on-linux/

но бываюи  ип роблемы что хочты не видят друг доргуа тогда

# lxc copy php/snap0 lxd-02:php --verbose  --mode relay
---
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
какойто кусок команд про lxd
конец блока
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


| docker

как сделать так чтобы можно было внуиири lxd конетенра запускат докер

скажем есть контйенер openconnect

тогда


# lxc config set openconnect  security.nesting=true security.syscalls.intercept.mknod=true  security.syscalls.intercept.setxattr=true

# lxc restart openconnect

===
| clone

у нас есть хост на нем контйенер.
мы хотим его склонировать на этт же хост. и запустить

# lxc copy --stateless cont1 cont2

====
| iptables

мы хотим чтобы контейнеры lxd могли ходит в интернет. тогда надо в iptables 
всавтиь правила


*filter
# LXD
-A FORWARD -o lxdbr0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i lxdbr0  -j ACCEPT
COMMIT

*nat
-A POSTROUTING -s 10.199.160.0/24 ! -o  lxdbr0 -j MASQUERADE
COMMIT


lxdbr0 = наш lxd бридж
 10.199.160.0/24 = диапазон ip адресов бриджа

 чтобы правила сохранились при перезашузке надо 


ставим пакет 

$ sudo apt-get install -y iptables-persistent

он спросит при устанвоке куда надо сохранить текущие правила
и ооткуда потом их читать при загрузке
по дефолту  это 

# ls -1 /etc/iptables/rules.v4 
/etc/iptables/rules.v4

вот туда наши правила и добавлявем

проверяем что служба стоит в автозагрузке

# systemctl is-enabled netfilter-persistent.service
enabled


насколько я понял эта служба толкьо читает правила из файла при загрузке.
но чего она неделает она не сохраняет автоматом правила при ребуте. а жаль


===

| size

охуенная важная вещь

если мы юзаем lxd и сторадж у него на базе lvm 
то при создании контейнера он на thin pool создает lvX и у него есть размер. и этот размер будет виден 
внутри контейнера как размер root partition. обычно по  дефолту lxd создает lv размером 10GB.

если мы хотим увелчитьть размер диска под контейнер то ни в коем случае нельзя лазить на lvm руками с хоста
напримем потому что если у нас lxd работает через snap то у него там исползуются свои встроенные верссиии lvm 
итд и тп. а надо оппросить сам lxd чтобы он измнил размер lv вот таким образом


# lxc config device set CONTAINER_ANME  root size 60GB


вот так выгдядит lv в котором крутится контейнер

# lvs
  LV                                                                      VG      Attr       LSize  Pool         Origin Data%  Meta%  Move Log Cpy%Sync Convert
  containers_pec--mult                                                    vg02    Vwi-aotz-k 55.88g lxd-thinpool        17.79                                  
  lxd-thinpool                                                            vg02    twi-aotz-- 92.80g                     12.08  14.51                           



или вот так чтобы глаза не ломать

# lvs vg02/containers_pec--mult
  LV                   VG   Attr       LSize  Pool         Origin Data%  Meta%  Move Log Cpy%Sync Convert
  containers_pec--mult vg02 Vwi-aotz-k 55.88g lxd-thinpool        30.60


lv называется  containers_pec--mult имеет размер 55.88g из которго занято 30.60%




====
| profile
| storage

задача такая - унас есть сторадж. и мы хотим его удалить. 
прблема в том что если на нем чото лежит то хер мы его удалим.
надо все что на нем либр удалить либо перенести.

вот срисок стораджей

# lxc storage list
+-----------+--------+--------------------------------------------------+-------------+---------+---------+
|   NAME    | DRIVER |                      SOURCE                      | DESCRIPTION | USED BY |  STATE  |
+-----------+--------+--------------------------------------------------+-------------+---------+---------+
| default   | dir    | /var/snap/lxd/common/lxd/storage-pools/default   |             | 1       | CREATED |
+-----------+--------+--------------------------------------------------+-------------+---------+---------+
| st-pool-3 | dir    | /var/snap/lxd/common/lxd/storage-pools/st-pool-3 |             | 16      | CREATED |
+-----------+--------+--------------------------------------------------+-------------+---------+---------+


смотрим что лежит на одном из стораджей

# lxc storage show st-pool-3
config:
  source: /var/snap/lxd/common/lxd/storage-pools/st-pool-3
description: ""
name: st-pool-3
driver: dir
used_by:
- /1.0/instances/cakephp7-2
- /1.0/instances/cent7
- /1.0/instances/cent8
- /1.0/instances/cent9
- /1.0/instances/first
- /1.0/instances/local-kub-01
- /1.0/instances/local-kub-02
- /1.0/instances/local-kub-03
- /1.0/instances/mar-1
- /1.0/instances/mar-1/snapshots/snap0
- /1.0/instances/mar-2
- /1.0/instances/mar-2/snapshots/snap0
- /1.0/instances/mar-single
- /1.0/instances/phpcake
- /1.0/instances/phpcake2
- /1.0/profiles/default
status: Created
locations:
- none

видно что на нем лежит несолкько инстансов - то бищь контейнеров
и один профиль.

инстаны надо либо удплить либо перенести на другой страдж командй

# lxc move CONTAINER_NAME -s новый_сторадж

с профилем потруднее. особенно с дефолтовым профилем. 
его нельяз удалить. непонятно как пренести. едиснтенное что его можно как бы зачистить

# printf 'config: {}\ndevices: {}' | lxc profile edit default

после этого lxc storage list в графе "used by" должен показать ноль. 
только после этого мжно удалить сторадж

====
| remove 
| purge

как правильно удалять пакет

толкьо вот так

$ snap remove --purge lxd

нужен флаг --purge 
иначе эта пдала будет чтототам сохранять в какйо снэпшот.и это будет ужасно долго!

====
| specific version

как поствть конкретную версию 

ванчале смотрим какие весрии для пакета есть

# snap info lxd

channels:
  latest/stable:    5.14-7072c7b  2023-06-01 (24918) 178MB -
  latest/candidate: 5.14-7072c7b  2023-05-29 (24918) 178MB -
  latest/beta:      ↑                                      
  latest/edge:      git-6bdccb9   2023-06-20 (25013) 181MB -
  5.14/stable:      –                                      
  5.14/candidate:   5.14-7072c7b  2023-05-31 (24918) 178MB -
  5.14/beta:        ↑                                      
  5.14/edge:        ↑                                      
  5.13/stable:      5.13-8e2d7eb  2023-05-31 (24846) 174MB -
  5.13/candidate:   ↑                                      
  5.13/beta:        ↑                                      
  5.13/edge:        ↑                                      
  5.0/stable:       5.0.2-838e1b2 2023-01-25 (24322) 117MB -
  5.0/candidate:    5.0.2-838e1b2 2023-01-18 (24322) 117MB -


во второй колонке стоит знак "-" это значит что 5.14/stable:  еще неготов.

в итоге ставим что есть

# snap install lxd --channel=5.14/candidate

===
| nfs 
| nfs server

ставим nfs сервер внутрь lxd  контейнера


/// как поставит nfs сервер ///
/// начало блока            ///
значит если ставим его внутрь lxd то контейнер надо запускать с особыми опциями. иначе 
незаработает

# lxc launch local:dcfe2a671f1d nfs-02   -c security.privileged=true -c raw.apparmor="mount fstype=rpc_pipefs, mount fstype=nfsd,"

(параметры подсмотрел вот здесь = https://github.com/HSBawa/nfs-server-on-linux-container)

супер важна вот эта вот запятаая на конце. без нее выдаст ошибку:
raw.apparmor="mount fstype=rpc_pipefs, mount fstype=nfsd,"


либо если контейнер уже запуще без спец параметров надо его оставоить и зайти в редактор конфига
контйнера и в раздел config: добавить опции

# lxc config edit имя_контейнера

config:
  raw.apparmor: mount fstype=rpc_pipefs, mount fstype=nfsd,
  security.privileged: "true"

опять же повторю супер важна запятая в конце


# mkdir /var/nfs
# chown nobody:nogroup /var/nfs
# apt-get install -y nfs-kernel-server

# cat << EOF >> /etc/exports
/var/nfs    172.16.102.0/24(rw,sync,no_subtree_check,no_root_squash)
EOF


/var/nfs = папка на сервере которую мы расшариваем
172.16.102.0/24 = клиенты которым можно подключаться
no_root_squash = эта опция нужна потому что при копировании файлов 
клиент также и устанавливает пермишнс на копируемый файл. так вот 
по дефолту на nfs сервере запрещено в расшаренной папке уставливать
владельцем файла пользователя root. и когда мы под рутом копируем файла
на nfs папку то клиентский линукс неможет установить владельцем файла рута.
и вылезает ошибка. вот чтобы рут мог стаовиться владельцем файлов и папок
в nfs папке испольщуется опция no_root_squash


# exportfs -a
если lxd контейнер запущен без спец параметров то на этом этапе вылезет ошибка
exportfs does not support NFS export


# systemctl enable nfs-server
# systemctl start nfs-server

как посмотреть какие шары наш nfs сервер опубликовал. 

первый способ

# exportfs -v
/var/nfs        172.16.102.0/24(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)
/var/nfs2       172.16.102.0/24(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)

второй способ

# showmount -e localhost
Export list for localhost:
/var/nfs2 172.16.102.0/24
/var/nfs  172.16.102.0/24

кстати в случае nfs шары называются экспортами


как подключаться к nfs серверу тоесть как проверить как примаунтить на другом сервере 
эти NFS шары:

ставим nfs-client
# apt-get install nfs-common
после этого nfs папку можно монтировать через mount 

# mount -t nfs  test-nfs-01.mk.local:/var/nfs /mnt/nfs
или
# mount IP_nfs_сервера:/var/nfs/general /nfs/general

где 
/var/nfs/general = в точности путь к папке шары на nfs сервере


/// как поставит nfs сервер ///
/// конец блока             ///

====

lxd
vm

во первых : 

если не удается запустить вм на основе lxd 
значит надо с компа удалить vmware workstatiom.
а вот virtualbox немешает

запустить виртулку 

# lxc launch ubuntu:22.04 vm1 --vm

как наложить лимиты на виртуалку
# lxc config set     vm1  limits.cpu 2
# lxc config set     vm1  limits.memory 360MB



# lxc config  show vm1
architecture: x86_64
config:
  limits.cpu: "2"
  limits.memory: 360MB


как добавить к виртуалке дополнительное блочное устройство
тоесть до диск.
для этого у нас уже должен быть какойто сторадж (который еще зовется пул)
на нем создаем volume тоесть блочное устройство

# lxc storage list
+-----------+--------+--------------------------------------------------+-------------+---------+---------+
|   NAME    | DRIVER |                      SOURCE                      | DESCRIPTION | USED BY |  STATE  |
+-----------+--------+--------------------------------------------------+-------------+---------+---------+
| default   | dir    | /var/snap/lxd/common/lxd/storage-pools/default   |             | 1       | CREATED |
+-----------+--------+--------------------------------------------------+-------------+---------+---------+
| st-pool-3 | dir    | /var/snap/lxd/common/lxd/storage-pools/st-pool-3 |             | 19      | CREATED |
+-----------+--------+--------------------------------------------------+-------------+---------+---------+

# lxc storage volume create st-pool-3  vol2  --type block size=2GiB
Storage volume vol2 created

мы на сторадже st-pool-3 создали volume "vol2" который будет блочное
устройство внутри виртуалки.


как проверить что этот вольюм появился

# lxc storage volume list st-pool-3 | grep vol2
| custom               | vol2         |             | block        | 0       |

добавляем это блоч устройство к вирталке vm1

# lxc config device add vm1  disk0  disk pool=st-pool-3  source=vol2
Device disk0 added to vm1

vm1 = назавние виртуалки
disk0 = это название блочноого устройства в конфиге lxd 
для этой виртуалки а также это имя будет участоввать в идетификации 
эттго диска внутри вм (об этом ниже)
st-pool-3 = это  сторадж из которого мы вырезали это блоч устройство
vol2 = это названия вольюма создданного на сторадже

заходим в вм vm1

# lsblk -f
NAME    FSTYPE   FSVER LABEL           UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
sdb                                                                                        

новый диск это sdb


вот доказательство что sdb это наш тот самый disk0
# ll /dev/disk/by-id | grep disk0
lrwxrwxrwx 1 root root   9 Jul  5 12:56 scsi-0QEMU_QEMU_HARDDISK_lxd_disk0 -> ../../sdb
lrwxrwxrwx 1 root root  10 Jul  5 12:56 scsi-0QEMU_QEMU_HARDDISK_lxd_disk0-part1 -> ../../sdb1
lrwxrwxrwx 1 root root   9 Jul  5 12:56 scsi-SQEMU_QEMU_HARDDISK_lxd_disk0 -> ../../sdb
lrwxrwxrwx 1 root root  10 Jul  5 12:56 scsi-SQEMU_QEMU_HARDDISK_lxd_disk0-part1 -> ../../sdb1


ура мы научились добавлять +1 блочное устройство к вируалке lxd
на оснвое этого можно созать три vm и создать там microceph

или создать три vm туда накатить к8 который может подключаться к цеф 
как блочный rbd клиент 



посморим где в конфиге теперь записана инфо об новом диске
# lxc config show vm1
...
devices:
  disk0:
    pool: st-pool-3
    source: vol2
    type: disk

или 
# lxc config device show vm1
disk0:
  pool: st-pool-3
  source: vol2
  type: disk




===
| миграция
| перенос
| migration 

https://www.cyberciti.biz/faq/how-to-movemigrate-lxd-vm-to-another-host-on-linux/



настрваием что нужно чтобы скопировать контенер на другой хост
на dest сервеврее  выполняем
   # lxc config set core.https_address 10.110.2.2:8443
где 10.110.2.2 это IP адрес сет карточки vswitchif
   # lxc config set core.trust_password fa5cieTh0nah6zooy3maichooch6la

на source серревер выплняем
   # lxc remote add  ovh-vin1-hw-nodes-11  10.110.2.2
   где ovh-vin1-hw-nodes-11 это dest сервер

контеенер у нас остановелен.
запускаем копрование
   # lxc copy  ovh-waw1-lxc-btc-03      ovh-vin1-hw-nodes-11:ovh-waw1-lxc-btc-03   --verbose  --mode=push  --storage default --allow-inconsistent --instance-only --stateless
флаг --mode=push иначе копироание просто неработает. поылает с отлупом
флаг --storage default тоже важен. иначе можно пролететь. надо указать конкретный storage pool 
на удаленном сервере
флаг --instance-only   чтобы не копироать снэпшоты
флаг --stateless  = Copy a stateful instance stateless  => хрен знает что значит

на счет push vs pull vs relay
Transfer modes (--mode):
- pull: Target server pulls the data from the source server (source must listen on network)
- push: Source server pushes the data to the target server (target must listen on network)
- relay: The CLI connects to both source and server and proxies the data (both source and target must listen on network)


вот эта команда у меня 100% отработала успешно
# lxc copy     ovh-hil1-lxc-proxy-08/ovh-hil1-lxc-proxy-08-snap1      ovh-vin1-hw-nodes-01:ovh-hil1-lxc-proxy-08   --verbose  --mode=push  --storage default --allow-inconsistent  --stateless



===

| libgtk3-nocsd.so.0

была совершенно ебанутая проблема

при запуске команды вылезала ошибка
$ lxc list
ERROR: ld.so: object 'libgtk3-nocsd.so.0' from LD_PRELOAD cannot be preloaded (failed to map segment from shared object): ignored.


единсвтеное решение которое сработал это https://stackoverflow.com/a/58931222/4274460


$ sudo grep -r LD_PRELOAD /etc

будет найдена вот такая хрень

/etc/X11/Xsession.d/51gtk3-nocsd-detect: export LD_PRELOAD="libgtk3-nocsd.so.0${LD_PRELOAD:+:$LD_PRELOAD}"


так вот надо нахер удалить пакет

$ sudo apt-get purge gtk3-nocsd


но на самом дееле этого оказалось мало. по каойто приинчне в кажом терминале была переменная LD_PRELOAD

$ set | grep LD_PRELOAD

LD_PRELOAD = ...libgtk3-nocsd.so.0

я так и непонял откуда эта сука еберся в переменных поэтму
 я просто в ~/.bashrc вствавил

 unset LD_PRELOAD

 ибо она просто забелала

 тоесть эта пременнея предпиыала при запуске каждого терминала подгуражить  libgtk3-nocsd.so.0
 а ее уже небыло . поэтому lxc list выдававл ошибку

 

==
