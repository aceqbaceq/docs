gluster


установка на основе : https://medium.com/running-a-software-factory/setup-3-node-high-availability-cluster-with-glusterfs-and-docker-swarm-b4ff80c6b5c3

с чем я столкнулся когда устаналивал glsuterfs наоснове статьи
ненадо в /etc/ftsab юзать точки монтирования через /dev/sdb
у меняли линукс путался  в точках моонтирования вмето этого надо монтировать 
через uuid вот таким образом:

# cat /etc/fstab

/dev/disk/by-uuid/9e04e1b0-2416-450d-9b8a-0550d151690 /gluster/bricks/2 xfs defaults 0 0


--

# cat /etc/fstab

/dev/sdb   /gluster/bricks/1 xfs defaults 0 0

localhost:/gfs /mnt/gluster glusterfs   defaults,acl,_netdev,backupvolfile-server=localhost,x-systemd.requires-mounts-for=/gluster/bricks/1 0 0


здесь важная хуня. во первых зависимость однойточки 
монтирования от другой.
вот вторых вклчена поддержка acl для glusterfs иначе setfacl небудет работать!



docker swarm - запуском реолокацией  контейнеров в сворме занимается исключетельно сука 
нода с рольью "менеджер". если сдох менеджео то сворму пизда. ни один контейнер не будет релоцирован
если нода с ролью "воркер" сломалась. поэтому надо иметь много менеджеров!!!
ноду можно повысить до роли менеджера командой

# docker node promote nl-test-02

выполнять эту команду можно только с работающего менеджера. сука

как найти спсоок менеджеров в сворме. ответ 
# docker info


и еще

v# docker node ls
ID                            HOSTNAME     STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
mhka9bbzhl8aw54evitmqenj3 *   nl-test-01   Ready     Active         Leader           23.0.6
jzt8nty0ccei9ipe3r6v16afz     nl-test-02   Ready     Active         Reachable        23.0.6
svkb5cc2qf3cz982cwbwwknt6     nl-test-03   Ready     Active         Reachable        23.0.6




 с регистри прикол. 

вот так запускать ненадо. потому что он сука будет хранить имадж локально на ноде.
если нода сдохла то он сука стартанет пустой на другой ноде.тоесть от него толка нахуй ноль.
# docker service create --name registry --publish published=5000,target=5000 registry:2

надо запускать обязательно с вольюмом сетевым. например на гластерфс.
# docker service create --name registry \
                        --mount type=bind,source=/mnt/gluster/docker/local-registry/var,target=/var/lib/registry \
                        --publish published=5000,target=5000 registry:2


как вывести ноду и обрано ввветисти в кластер

   98  docker node update --availability drain  nl-test-01
   99  docker node ls
  100  docker node update --availability active  nl-test-01



скорсть разворота базы мукусула из дамапа - на локальный фс = 3м а на гластерфс раздел 4м
ну нормал по скорости

---
добавляем новый brick и получаем ошибку

GlusterFS Error volume add-brick: failed: Pre Validation failed on BRICK is already part of a volume

рещение = https://www.jamescoyle.net/how-to/2234-glusterfs-error-volume-add-brick-failed-pre-validation-failed-on-brick-is-already-part-of-a-volume


а именно

Solution!
The solution is to use setfattr to clear the hidden filesystem attributes containing GlusterFS information about the bricks previous life. Run the following commands on the server that has the drive that you’re trying to add as a new brick.

# setfattr -x trusted.glusterfs.volume-id /mnt/brick1/data
# setfattr -x trusted.gfid /mnt/brick1/data
Of course, you should also ensure that the filesystem you’re adding is cleared, especially the .glusterfs hidden directory.

# rm -rf /mnt/brick1/data/.glusterfs
And that’s it! Try running the add-brick command again, and you should be in business.

# gluster v add-brick data-volume replica 3 gluster3:/mnt/brick1/data


---
firewall порты которые надо открыть


TCP and UDP ports 24007 and 24008 

также в конфиге  настраиваются доп порты. 
там указывается диапазон  доп портов.
# cat /etc/glusterfs/glusterd.vol  | grep "\-port"
    option transport.socket.listen-port 24007
    option base-port 49152
    option max-port  60999


чтоб поменьше диапазон открывать на файрволле
то надо в конфиге поменьше указывать диапазон
==
benchmark

кллстер на яндексе из трех нод( на каждоый ноде гластер папка основана на диске д unreplcated-ssd размер диска 93GB ).
импорт некоего эталонного дампа на gluster раздел занимает 2м 27с
импорт на локальный такой же диск заняло 2м
тоесть glsuetr раздел всего на  20% медленее!
причем zcat вобще никакой роли на скроость импорта невлияет.


импорт на гластер папку на другом кластере из трех нод (неяндекс серверы) заняло 4m20s
импорт на этот же диск но без гластера заняло real	2m50с

общий вывод: диски на яндексе нормуль по скорости под гластер.

===
