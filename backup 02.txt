backup 02 


загрузочные  4 диска в четвером мирроре подключены к контроллеру 3ware (lsi)
так вот - если хотя бы один диск отваливается и мы перезагрузили сервер то этот контроллер 
своебразно глючит. он пишет при загрузке на черном экране во время загрузки что такогото диска нет. типа

    
    unknown disk ....

и далее прикол в том что uefi неможет прочитать FAT32 EFI разделы с оставшихся дисков этого контроллера.
решение - нужно еще раз перезагрузить сервер через ctrl+alt+del и убедиться что на этот раз 3ware
не будет никак ругаться на то что какието диски отсуствуют.  тогда uefi уже успешно прочитает FAT32 разделы 
с оставшихся загрузочных живых дисков и загрузка пройдет успешно.
иттго- если один из загрузочных дисков отвалилися и мы перезагрузили сервер и 3ware руагется что один из дисков 
остуствтует и сервер перестал грузится через uefi - то нужно еще раз сделать ctrl+alt+del и сервер должен обязан 
успешно загрузьиттся с оставшихся загрузчоных дисков.
я проверил - сервер точно успешно грузится с любого из четырех загрузочных дисков. с любого. достаточно чтобы выжил
хотя бы один

сервер создан на базе прокскс инслаятора потому что он позволяет постсвить линукс на зфс.
после этгого прокскс был удален. но при загрузке он все равно пишет в uefi что это прокскс сервер. 


загрузка в uefi идет не через grub а через systemd-boot
вот как выглдяит загрузчный конфиг uefi



root@backup-02:/home/krivosheeva# efibootmgr  -v
BootCurrent: 0000
Timeout: 10 seconds
BootOrder: 0000,0014,0013,0012,0009,000A,000B,000C,0004,0005,0006,0001
Boot0000* BOOT S/N EPAS HD(2,GPT,7b1e6c62-158e-4560-b268-ce0eafa1090e,0x800,0x200000)/File(\EFI\BOOT\BOOTX64.EFI)
Boot0012* BOOT S/N 2FPM HD(2,GPT,f7d7bfa2-f129-46ec-ad76-97de9895195b,0x800,0x200000)/File(\EFI\BOOT\BOOTX64.EFI)
Boot0013* BOOT S/N EPB0 HD(2,GPT,51b115ab-3eca-4ce2-90cd-633314c9a29a,0x800,0x200000)/File(\EFI\BOOT\BOOTX64.EFI)
Boot0014* BOOT S/N EQ7L HD(2,GPT,003f3bc4-f490-4495-b89c-5ee8c63a7170,0x800,0x200000)/File(\EFI\BOOT\BOOTX64.EFI)


соосвттенно это четыре загрузочных диска
здесь видны uuid загрузочных партиций
и путь где лежит загрузчик \EFI\BOOT\BOOTX64.EFI и как его нужно прописвать в uefi nvram







у меня в серервер три карты

одна заведет половиной дисков на морде лица сервера
вторая заведует второй полвоиной дисков на морле лица сервера
и третья карта это Qlogic FC которая подклюается к двум FC полкам


индикация через лампочки FC полок нератботает. 

сеть




# cat /etc/network/interfaces
auto lo
iface lo inet loopback



iface enp65s0f0 inet manual
    mtu 9000
iface enp65s0f1 inet manual
    mtu 9000


auto bond0
iface bond0 inet static
        bond-slaves  enp65s0f0 enp65s0f1 
        bond-mode 802.3ad
        bond-miimon 100
        bond-lacp-rate fast
        bond-xmit-hash-policy layer2+3
        mtu 9000

auto bond0.55
iface bond0.55 inet static
        vlan-raw-devices bond0
        address 100.69.55.24/24
        gateway 100.69.55.254
        mtu 1500


auto bond0.78
iface bond0.78 inet static
        vlan-raw-devices bond0
        address 100.69.78.24/24
        mtu 9000




iface eno1 inet manual

iface eno2 inet manual

iface eno3 inet manual

iface eno4 inet manual



source /etc/network/interfaces.d/*






выклчил ipv6

выключил idrac сет карту внутри ос (idrac порт os passtghrough.txt)




=====
deb http://deb.debian.org/debian/ bookworm main non-free-firmware
deb-src http://deb.debian.org/debian/ bookworm main non-free-firmware

deb http://security.debian.org/debian-security bookworm-security main non-free-firmware
deb-src http://security.debian.org/debian-security bookworm-security main non-free-firmware

deb http://deb.debian.org/debian/ bookworm-updates main non-free-firmware
deb-src http://deb.debian.org/debian/ bookworm-updates main non-free-firmware

===
прверратить проксккс в дебиан (ищи такой txt файл)
===
apt install smartmontools

включаем на всех диска кеш на запись
for i in $( seq 0 13); do echo "da=da$i"; smartctl --set=wcache,on /dev/da$i ; done

Проверка что кеш на запись у дисков включен
for i in $( seq 0 13); do echo "da=da$i"; smartctl -x /dev/da$i | grep -i Cache | grep -i disable ; done

=====
apt install firmware-qlogic

====

обнволение прощшивки контрролера хба

я имею карточку 

		3ware 9650SE-4LPML  (внутри lsi)


чтоб ею управлять нужна утилиа tw_cli
9650SE_9690SA_firmware_9-5-5-1codeset_fw4-10-00-027.zip
CLI_linux-from_the_10-2-2-1_9-5-5-1_codesets.zip




root@backup-02:/opt/3ware# ./tw_cli /c0 show all
/c0 Driver Version = 2.26.02.014
/c0 Model = 9650SE-4LPML
/c0 Available Memory = 224MB
/c0 Firmware Version = FE9X 4.10.00.021
/c0 Bios Version = BE9X 4.08.00.003
/c0 Boot Loader Version = BL9X 3.08.00.001
/c0 Serial Number = L326010B0460209
/c0 PCB Version = Rev 035
/c0 PCHIP Version = 2.00
/c0 ACHIP Version = 1.90
/c0 Number of Ports = 4
/c0 Number of Drives = 4
/c0 Number of Units = 4
/c0 Total Optimal Units = 4
/c0 Not Optimal Units = 0 
/c0 JBOD Export Policy = on
/c0 Disk Spinup Policy = 1
/c0 Spinup Stagger Time Policy (sec) = 1
/c0 Auto-Carving Policy = on
/c0 Auto-Carving Size = 2048 GB
/c0 Auto-Rebuild Policy = on
/c0 Rebuild Mode = Adaptive
/c0 Rebuild Rate = 1
/c0 Verify Mode = Adaptive
/c0 Verify Rate = 1
/c0 Controller Bus Type = PCIe
/c0 Controller Bus Width = 4 lanes
/c0 Controller Bus Speed = 2.5 Gbps/lane

Unit  UnitType  Status         %RCmpl  %V/I/M  Stripe  Size(GB)  Cache  AVrfy
------------------------------------------------------------------------------
u0    JBOD      OK             -       -       -       232.883   RiW    OFF    
u1    JBOD      OK             -       -       -       232.883   RiW    OFF    
u2    JBOD      OK             -       -       -       232.883   RiW    OFF    
u3    JBOD      OK             -       -       -       232.883   RiW    OFF    

VPort Status         Unit Size      Type  Phy Encl-Slot    Model
------------------------------------------------------------------------------
p0    OK             u0   232.88 GB SATA  0   -            SEAGATE ST32500NSSU 
p1    OK             u1   232.88 GB SATA  1   -            SEAGATE ST32500NSSU 
p2    OK             u2   232.88 GB SATA  2   -            SEAGATE ST32500NSSU 
p3    OK             u3   232.88 GB SATA  3   -            SEAGATE ST32500NSSU 



активиурем кэш  назапись на дисках



#### /opt/3ware/tw_cli /c0/u0 set cache=on
Setting Write Cache Policy on /c0/u0 to [on] ... Done.

#### /opt/3ware/tw_cli /c0/u1 set cache=on
Setting Write Cache Policy on /c0/u1 to [on] ... Done.

#### /opt/3ware/tw_cli /c0/u2 set cache=on
Setting Write Cache Policy on /c0/u2 to [on] ... Done.

#### /opt/3ware/tw_cli /c0/u3 set cache=on
Setting Write Cache Policy on /c0/u3 to [on] ... Done.




root@backup-02:/opt/3ware# ./tw_cli /c0 show

Unit  UnitType  Status         %RCmpl  %V/I/M  Stripe  Size(GB)  Cache  AVrfy
------------------------------------------------------------------------------
u0    JBOD      OK             -       -       -       232.883   RiW    OFF    
u1    JBOD      OK             -       -       -       232.883   RiW    OFF    
u2    JBOD      OK             -       -       -       232.883   RiW    OFF    
u3    JBOD      OK             -       -       -       232.883   RiW    OFF    





перешиваем прошивку на новую



root@backup-02:/opt/3ware# /opt/3ware/tw_cli /c0 show all | grep -i firmware
/c0 Firmware Version = FE9X 4.10.00.021
root@backup-02:/opt/3ware# ls
tw_cli
root@backup-02:/opt/3ware# cd ~
root@backup-02:~# ls
prom0006.img
root@backup-02:~# 
root@backup-02:~# 
root@backup-02:~# 
root@backup-02:~# ls
prom0006.img
root@backup-02:~# /opt/3ware/tw_cli /c0 update fw=/root/prom0006.img 

Warning: Updating the firmware can render the device driver and/or
management tools incompatible. Before you update the firmware,
it is recommended that you:

1) Back up your data.

2) Make sure you have a copy of the current firmware image so that
you can roll back, if necessary.

3) Close all applications.

Examining compatibility data from firmware image and /c0 ... Done.

New-Firmware        Current-Firmware    Current-Driver      Current-API 
------------------------------------------------------------------------
FE9X 4.10.00.027    FE9X 4.10.00.021    2.26.02.014         2.08.00.027       

Both API and Driver are compatible with the new firmware.
Recommendation: proceed to update.

Given the above recommendation...
Do you want to continue ? Y|N [N]: Y
Downloading the firmware from file /root/prom0006.img ... oDone. 
The new image will take effect after reboot.

root@backup-02:~# reboot

Broadcast message from root@backup-02 on pts/1 (Fri 2026-01-02 19:39:20 MSK):

The system will reboot now!


после обнолвния

#### /opt/3ware/tw_cli  /c0 show all
/c0 Driver Version = 2.26.02.014
/c0 Model = 9650SE-4LPML
/c0 Available Memory = 224MB
/c0 Firmware Version = FE9X 4.10.00.027
/c0 Bios Version = BE9X 4.08.00.004
/c0 Boot Loader Version = BL9X 3.08.00.001
/c0 Serial Number = L326010B0460209
/c0 PCB Version = Rev 035
/c0 PCHIP Version = 2.00
/c0 ACHIP Version = 1.90
/c0 Number of Ports = 4
/c0 Number of Drives = 4
/c0 Number of Units = 4
/c0 Total Optimal Units = 4
/c0 Not Optimal Units = 0 
/c0 JBOD Export Policy = on
/c0 Disk Spinup Policy = 1
/c0 Spinup Stagger Time Policy (sec) = 1
/c0 Auto-Carving Policy = on
/c0 Auto-Carving Size = 2048 GB
/c0 Auto-Rebuild Policy = on
/c0 Rebuild Mode = Adaptive
/c0 Rebuild Rate = 1
/c0 Verify Mode = Adaptive
/c0 Verify Rate = 1
/c0 Controller Bus Type = PCIe
/c0 Controller Bus Width = 4 lanes
/c0 Controller Bus Speed = 2.5 Gbps/lane




#### /opt/3ware/tw_cli  /c0 show

Unit  UnitType  Status         %RCmpl  %V/I/M  Stripe  Size(GB)  Cache  AVrfy
------------------------------------------------------------------------------
u0    JBOD      OK             -       -       -       232.883   Ri     OFF    
u1    JBOD      OK             -       -       -       232.883   Ri     OFF    
u2    JBOD      OK             -       -       -       232.883   Ri     OFF    
u3    JBOD      OK             -       -       -       232.883   Ri     OFF    

VPort Status         Unit Size      Type  Phy Encl-Slot    Model
------------------------------------------------------------------------------
p0    OK             u0   232.88 GB SATA  0   -            SEAGATE ST32500NSSU 
p1    OK             u1   232.88 GB SATA  1   -            SEAGATE ST32500NSSU 
p2    OK             u2   232.88 GB SATA  2   -            SEAGATE ST32500NSSU 
p3    OK             u3   232.88 GB SATA  3   -            SEAGATE ST32500NSSU 


p0 это физ диски
а u0 это то что презентует для ос . типа как луны. 

запрос через смарт  к дискам они хотьт и JBOD но к ним хрен напрямую пролезешь через sdN
	# smartctl -x  -d 3ware,0 /dev/twa0 



==================================
install lsscsi  screen parted
================================
создать EFI разделы на +2 двух дисках
создать разделы по зфс на этих дисках
расширть зфс бут миррор до 4-ех дисков
заменить в пуле scsi метки на S\N метки



загрузка с 4-ех дисков (4-вей миррор зфс) через uefi
далее мудеж по разбивке новых двух дисков также как первые два диска
и поклюенчеие их к зфс и изменние меток через которые диск плкюлючены к зфс.
(прикол в том что фрибсл автмоатом создает метки дисков на базе серийноного номера а лринукс 
этого недлает .  линукс создает автмоатом метки на базе wwn)


в целом можно перенсти разбивка сдика на диск вот так


# Копируем разметку с sda на sdc
	sfdisk -d /dev/sda | sfdisk /dev/sdc
это очень крутая удобная команда

если потом при добавлении диска/раздела к зфс он напишет что добавляемый диск походу
был частью зфс это значит что на добавляемом диске реаьно когда ты был зфс и остались его следы.
и нужнопросто добавить ключ -f и все будет хорошо тоесть 

  zpool attach -f POOL_NAME  old_disk     new_disk


но можно разбить новый диск и руками
ссмотрим разбивку старого диска


GNU Parted 3.5
Using /dev/sdc
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) unit s                                                           
(parted) p                                                                
Model: AMCC 9650SE-4LP DISK (scsi)
Disk /dev/sdc: 488390625s
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags: 

Number  Start     End         Size        File system  Name  Flags
 1      34s       2047s       2014s                          bios_grub
 2      2048s     2099199s    2097152s    fat32              boot, esp
 3      2099200s  138412032s  136312833s  zfs

(parted) ^C                                                               


первый раздел на нем нет фс. там просто граб хранит часть своего загрузчика для случая когда комп грузится
в режиме BIOS (тоесть не в режиме уефи). сам диск как видно имеет разметку гпт. тоесть с него можно потенциально
грузится через уефи. но если мы хотим грузится через биос при такой разметке то нужен это первый раздел размером 1М
где граб засунет часть своего загрузчкика.
второй раздел это уже FAT32 + флаг EFI. этот тот раздел который умеет анализирвать UEFI и грузится с него.
таким макром этот диск позволит грузиться как через биос так и через уефи.
третий разлдел это уже зфс раздел.


создаю первый раздел на двух новых дисках (sde sdf)
	# parted -s /dev/sde mkpart primary 34s 2047s
Warning: The resulting partition is not properly aligned for best performance: 34s % 2048s != 0s
	# parted -s /dev/sdf mkpart primary 34s 2047s
Warning: The resulting partition is not properly aligned for best performance: 34s % 2048s != 0s
	# parted -s /dev/sde set 1 bios_grub on
	# parted -s /dev/sdf set 1 bios_grub on


создаем EFI раздел
#### parted -s /dev/sde mkpart primary fat32 2048s 2099199s
#### parted -s /dev/sdf mkpart primary fat32 2048s 2099199s
#### parted -s /dev/sde set 2 esp on
#### parted -s /dev/sdf set 2 esp on

кстати плоезная команда 
blockdev --getsz /dev/sdc

создаем тртий раздел под ZFS
####  parted -s /dev/sde mkpart primary zfs 2099200s 138412032s
####  parted -s /dev/sdf mkpart primary zfs 2099200s 138412032s


проверяю выравниванеим для третьего раздела
#### parted /dev/sde align-check optimal 3
3 aligned
#### parted /dev/sdf align-check optimal 3
3 aligned


провреяб результат
#### parted /dev/sde
GNU Parted 3.5
Using /dev/sde
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) unit s                                                           
(parted) p                                                                
Model: AMCC 9650SE-4LP DISK (scsi)
Disk /dev/sde: 488390625s
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags: 

Number  Start     End         Size        File system  Name     Flags
 1      34s       2047s       2014s                    primary  bios_grub
 2      2048s     2099199s    2097152s                 primary  boot, esp
 3      2099200s  138412032s  136312833s  zfs          primary

(parted) ^C                                                               

#### parted /dev/sdf
GNU Parted 3.5
Using /dev/sdf
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) unit s                                                           
(parted) p                                                                
Model: AMCC 9650SE-4LP DISK (scsi)
Disk /dev/sdf: 488390625s
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags: 

Number  Start     End         Size        File system  Name     Flags
 1      34s       2047s       2014s                    primary  bios_grub
 2      2048s     2099199s    2097152s                 primary  boot, esp
 3      2099200s  138412032s  136312833s  zfs          primary

(parted) ^C                                                               


щас буду дбавялять раздел зфс на новых дисках  к текущему пулу
итакже создам GPT метку для этих разделов и на основе этих меток поменяю то чтерез что зфс цепляет эти диски


#### zpool status -v
  pool: rpool
 state: ONLINE
config:

	NAME                                       STATE     READ WRITE CKSUM
	rpool                                      ONLINE       0     0     0
	  mirror-0                                 ONLINE       0     0     0
	    scsi-1AMCC_5QE4EPAS000000000000-part3  ONLINE       0     0     0
	    scsi-1AMCC_9QE82FPM000000000000-part3  ONLINE       0     0     0

errors: No known data errors

#### zpool attach -f  rpool scsi-1AMCC_5QE4EPAS000000000000-part3  /dev/disk/by-id/scsi-1AMCC_5QE4EPB0000000000000-part3
#### zpool attach -f  rpool scsi-1AMCC_9QE82FPM000000000000-part3  /dev/disk/by-id/scsi-1AMCC_5QE4EQ7L000000000000-part3



#### zpool status -v
  pool: rpool
 state: ONLINE
  scan: resilvered 1.27G in 00:01:54 with 0 errors on Fri Jan  2 20:53:53 2026
config:

	NAME                                       STATE     READ WRITE CKSUM
	rpool                                      ONLINE       0     0     0
	  mirror-0                                 ONLINE       0     0     0
	    scsi-1AMCC_5QE4EPAS000000000000-part3  ONLINE       0     0     0
	    scsi-1AMCC_9QE82FPM000000000000-part3  ONLINE       0     0     0
	    scsi-1AMCC_5QE4EPB0000000000000-part3  ONLINE       0     0     0
	    scsi-1AMCC_5QE4EQ7L000000000000-part3  ONLINE       0     0     0

errors: No known data errors




создаю GPT метки на раздел
#### sgdisk -c 3:"SN-EPAS"  /dev/sdc

это охуенно полезная команда. онп позволяет поменять или сощдать GPT метку для раздела.
как видно аргумент это диск а раздел указан через '3:'

#### sgdisk -c 3:"SN-2FPM"  /dev/sdd
#### partprobe /dev/sdd (засвтавляю ядро пеерчитать таблицу разделов  с диска)

#### sgdisk -c 3:"SN-EPB0"  /dev/sde
#### partprobe /dev/sde


#### sgdisk -c 3:"SN-EQ7L"  /dev/sdf
#### partprobe /dev/sdf


#### zpool detach scsi-1AMCC_5QE4EQ7L000000000000-part3
missing <device> specification
usage:
	detach <pool> <device>
#### zpool detach rpool scsi-1AMCC_5QE4EQ7L000000000000-part3
#### zpool attach  rpool scsi-1AMCC_5QE4EPAS000000000000-part3   /dev/disk/by-partlabel/SN-EQ7L 
#### 
#### 
#### zpool detach rpool    scsi-1AMCC_5QE4EPB0000000000000-part3
#### zpool attach rpool   scsi-1AMCC_5QE4EPAS000000000000-part3     /dev/disk/by-partlabel/SN-EPB0 
#### zpool detach rpool      scsi-1AMCC_9QE82FPM000000000000-part3
#### 
#### 
#### zpool attach rpool   scsi-1AMCC_5QE4EPAS000000000000-part3     /dev/disk/by-partlabel/SN-2FPM 
#### 

#### zpool detach rpool       scsi-1AMCC_5QE4EPAS000000000000-part3
#### zpool attach rpool    SN-EQ7L   /dev/disk/by-partlabel/SN-EPAS 




 backup-02 # zpool status -v
  pool: rpool
 state: ONLINE
  scan: resilvered 1.27G in 00:01:55 with 0 errors on Fri Jan  2 21:23:53 2026
config:

	NAME         STATE     READ WRITE CKSUM
	rpool        ONLINE       0     0     0
	  mirror-0   ONLINE       0     0     0
	    SN-EQ7L  ONLINE       0     0     0
	    SN-EPB0  ONLINE       0     0     0
	    SN-2FPM  ONLINE       0     0     0
	    SN-EPAS  ONLINE       0     0     0

errors: No known data errors


форматирруем в FAT32 наши новые EFI разделы

sdv     65:80   0   1.8T  0 disk 
 backup-02 # mkfs.vfat -F 32 -n EFI /dev/sde2
mkfs.fat 4.2 (2021-01-31)
 backup-02 # mkfs.vfat -F 32 -n EFI /dev/sdf2
mkfs.fat 4.2 (2021-01-31)



 добваоляю новые ентри в UEFI NVRAM


добавляю новые диски в меню


	# udevadm info /dev/sde |  grep by-id
S: disk/by-id/scsi-1AMCC_5QE4EPB0000000000000
E: DEVLINKS=/dev/disk/by-id/scsi-1AMCC_5QE4EPB0000000000000 /dev/disk/by-diskseq/13 /dev/disk/by-path/pci-0000:43:00.0-scsi-0:0:2:0


	# efibootmgr -c -d /dev/sde -p 2 -L "BOOT EPB0" -l '\EFI\systemd\systemd-bootx64.efi'


чтобы эта команда сработал охуенно важно указать диск /dev/sde к которому она отсноиттся и  раздел -p 2
потому что эта команда она с помиощю этого ищет какой у раздела UUID иименно его она вставит в ентри.
и нужно указать путь к файлу.
кстти это оличется от фрибслд и его комнанды
	#efibootmgr -c -l /mnt/01/EFI/BOOT/BOOTX64.EFI -L "BOOT WWN cc384"
во фрибадс нужно смонтрвать ефи раздел. и фотбсл проеряет наличие файла по указаному пути. а раздел и диск
оно определить автоатом на основе точки монтрвания. вот такой прикол!




сразу проверяем 


 backup-02 # efibootmgr  -v
BootCurrent: 0011
Timeout: 10 seconds
BootOrder: 0000,0002,000E,000F,0010,0011
Boot0000* BOOT EPB0	HD(2,GPT,37c89389-97b3-4cd4-af4f-96fb82c7d7b7,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)


тоесть селси чтото сделат ьневерно то вот этот партишн GUID 37c89389-97b3-4cd4-af4f-96fb82c7d7b7
будет состоятьть из одних нулей


 backup-02 # efibootmgr -c -d /dev/sdf  -p 2 -L "BOOT EQ7L" -l '\EFI\systemd\systemd-bootx64.efi'
 backup-02 # efibootmgr  -v
BootCurrent: 0011
Timeout: 10 seconds
BootOrder: 0001,0000,0002,000E,000F,0010,0011
Boot0000* BOOT EPB0	HD(2,GPT,37c89389-97b3-4cd4-af4f-96fb82c7d7b7,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)
Boot0001* BOOT EQ7L	HD(2,GPT,a4824f5a-d4a6-47f4-af99-6ede28ca993e,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)





удаляю старые NVRAM ENTRY

BootCurrent: 0007
Timeout: 0 seconds
BootOrder: 000B,000A,0007,0006,0000,0001,0002,0003,0004,0005,0008,0009
Boot0000  EFI Network 1
Boot0001  EFI Network 2
Boot0002  EFI Network 3
Boot0003  EFI Network 4
Boot0004* Virtual Floppy  
Boot0005* Virtual CD      
Boot0006* Linux Boot Manager
Boot0007* Linux Boot Manager
Boot0008* EFI Fixed Disk Boot Device 1
Boot0009* EFI Fixed Disk Boot Device 2
Boot000A* BOOT EPB0
Boot000B* BOOT EQ7L


  250  efibootmgr -B -b 0001
  251  efibootmgr -B -b 0002
  252  efibootmgr -B -b 0003
  253  efibootmgr -B -b 0004
  254  efibootmgr -B -b 0005
  255  efibootmgr -B -b 0008
  256  efibootmgr -B -b 0009
  257  efibootmgr -B -b 0006




добавляю старые диски 



 backup-02 # udevadm info /dev/sdc |  grep by-id
S: disk/by-id/scsi-1AMCC_5QE4EPAS000000000000
E: DEVLINKS=/dev/disk/by-path/pci-0000:43:00.0-scsi-0:0:0:0 /dev/disk/by-diskseq/11 /dev/disk/by-id/scsi-1AMCC_5QE4EPAS000000000000

 backup-02 # efibootmgr -c -d /dev/sdc  -p 2 -L "BOOT EPAS" -l '\EFI\systemd\systemd-bootx64.efi'

 backup-02 # udevadm info /dev/sdd |  grep by-id
S: disk/by-id/scsi-1AMCC_9QE82FPM000000000000
E: DEVLINKS=/dev/disk/by-id/scsi-1AMCC_9QE82FPM000000000000 /dev/disk/by-diskseq/12 /dev/disk/by-path/pci-0000:43:00.0-scsi-0:0:1:0

 backup-02 # efibootmgr -c -d /dev/sdd  -p 2 -L "BOOT 2FPM" -l '\EFI\systemd\systemd-bootx64.efi'


 backup-02 # efibootmgr  -v
BootCurrent: 0011
Timeout: 10 seconds
BootOrder: 0004,0003,0001,0000,0002,000E,000F,0010,0011
Boot0000* BOOT EPB0	HD(2,GPT,37c89389-97b3-4cd4-af4f-96fb82c7d7b7,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)
Boot0001* BOOT EQ7L	HD(2,GPT,a4824f5a-d4a6-47f4-af99-6ede28ca993e,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)
Boot0003* BOOT EPAS	HD(2,GPT,decf9046-f77e-45c3-a1b9-3ac9d13d2ead,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)
Boot0004* BOOT 2FPM	HD(2,GPT,0905f26d-b049-47c8-a3fd-6b3fecb721a7,0x800,0x200000)/File(\EFI\systemd\systemd-bootx64.efi)


все 4 ентри есть в меню. все имеют GUID партишена с ефи. и все со звезлдами значит участвуют впереборе вариантов.
(если что ссмотри текст файл freebsd uefi настйрока .txt)


определяю порядок загрузки по этим ентри в каком порядке нужно перебирать

 backup-02 # efibootmgr -o 0000,0001,0003,0004


еще нужно записать на диски ту часть граба которая на сулчай если мы будем гоурузится через БИОС

	# /usr/sbin/grub-install.real /dev/sdc
Installing for x86_64-efi platform.
/usr/sbin/grub-install.real: error: cannot find EFI directory.

	# /usr/sbin/grub-install.real /dev/sdd
Installing for x86_64-efi platform.
/usr/sbin/grub-install.real: error: cannot find EFI directory.

	# /usr/sbin/grub-install.real /dev/sde
Installing for x86_64-efi platform.
/usr/sbin/grub-install.real: error: cannot find EFI directory.

	# /usr/sbin/grub-install.real /dev/sdf
Installing for x86_64-efi platform.
/usr/sbin/grub-install.real: error: cannot find EFI directory.


можно забить хрен на ефи ошибку. с ефи все уже настроено. нам лищь нужно было тиснуть граб на первый
раздел диска про который я ранее говорил.


     efibootmgr -A -b 0007  (это дезактивирует лишние ентри в меню что делл пихает сам автмоатмо от души)
     efibootmgr -A -b 0008
     efibootmgr -A -b 0009
     efibootmgr -A -b 000C
     efibootmgr -A -b 000D
     efibootmgr -o 0000,0001,000A,000B  (задаем порядок перебора)
     efibootmgr -t 10   (это устанвлиывает таймаут)
  


по идее все готово.
ос должна грузиьтся через UEFI с любого из четрых дисков





типа поезный совет . когда импорртирую между линукс/фрибсл чуждой пул и наипример я знаю что 
он помтроен на базе GPT меток то ОС нужно подскзаать в какой папке искать метки 

	# zpool import -d /dev/gpt [имя_пула]

как сказал ИИ сам ипорт у зфс происодит на основе GUID меток котореые он запиывает на сами диски.
а уже потом зфс начинаетискать в ос те идентфикатторы дисков и партиций котоыре зфс толкко что импортировала.
и вот так как унас в ос для дисков и партииций если разная куча развознанчых идентификаторов то зфс 
подставить хер знает какие но не те что надо. тоесть условно зфс подсваить /dev/disk/by-id/scsi* идентификаторы
хотя нам нужны /dev/disk/by-id/partlabel/* 
именно плтому зфс приимпорте нужно посдкзаать какие метки нужно подтянуть к пулу при его импорте




про диски.
у нас диски бОльшая часть которые составят пул они покдлчаются к хосту через Qlogic FC HBA
а та ведет на FC полки. так вот в итоге каждый диск для линукса видет в разрезе устройств идентицикации 
в папке 

     /dev/disk/

только как 

/dev/disk/by-id/wwn-0x2208000a33ffef06
/dev/disk/by-id/wwn-0x2208000a33ffef07
/dev/disk/by-id/wwn-0x2208000a33ffef08


но это не wwn диска самого а это wwn которйы ему назначила полка.
поэтому прежде чем собирать зфс пул нужно назначить диску какуюто неубивамему метку чтобы 

  1) по этой метке его можно было бы идентиыицировать 
  2) чтобы если я вытащу его из полки и вставлю напрямую в ос через обычный HBA карту чтобы для зфс метка эта
     никуда не исчезла а ОС бы ее автоматом создала

поэтому решено на каждом диске который вставлен в полку да и вобще который будет вставлен в пул нарезать GPT партицию
и нанее навесить gpt label в форме серийника этого диска.
тогда мы поэтому гпт метке доавбить диск в зфс пул. человек в статусе пула будет вилеть серийник. этотже сериник 
наклеиь на морду салазок.и если этот диск пеереставить в другой комп то уж ГПТ метку на диске другая ос должна увидеть 
тоесть пул собеертся ровно на таких же идентификаторах
для этого я юзаю скрипт
скрипт создает на диске gpt label и потом gpt раздел 
и этому разделу дает gpt метку с серийником диска
подходит только для linux

script 
#!bash

for disk in {i..x} ; do
  dev="/dev/sd$disk"
  # Достаем серийник именно так, как его видит система
  sn=$(lsblk -dno SERIAL $dev)
  
  echo "Processing $dev (SN: $sn)..."
  
  # 1. Создаем таблицу GPT
  parted -s $dev mklabel gpt
  
  # 2. Создаем раздел (отступ 2048s для выравнивания)
  # Используем 100% диска
  parted -s $dev mkpart primary 2048s 100%
  
  # 3. Присваиваем имя разделу (Partlabel) равное серийнику
  parted -s $dev name 1 $sn
done




для freebsd

#!/usr/local/bin/bin/bash

# Список дисков (укажите свои, например da0 da1 da2...)
DISKS="da1 da2 da3"

for dev_name in $DISKS; do
    dev="/dev/$dev_name"
    
    # 1. Достаем серийный номер через camcontrol
    # Очищаем от лишних пробелов
    sn=$(camcontrol identify $dev_name | grep "serial number" | awk '{print $3}')
    
    if [ -z "$sn" ]; then
        echo "Ошибка: Не удалось получить серийник для $dev"
        continue
    fi

    echo "Обработка $dev (SN: $sn)..."

    # 2. Уничтожаем старую разметку (если есть), чтобы не было ошибок
    gpart destroy -F $dev_name 2>/dev/null

    # 3. Создаем таблицу GPT
    gpart create -s gpt $dev_name

    # 4. Создаем раздел с выравниванием (автоматически 1MB в gpart)
    # Ключ -l задает метку (GTP Label), которую вы хотите (серийник)
    gpart add -t freebsd-zfs -l "$sn" $dev_name

    echo "Готово: $dev теперь имеет раздел /dev/gpt/$sn"
done


или втоой вариант  когда сеинийики уже лежат в папке


#!/usr/local/bin/bash

# Проходим по всем дискам, которые начинаются на DISK-PN
for fullpath in /dev/diskid/DISK-PN*; do
    
    # 1. Получаем имя устройства (например, DISK-PN1181P5H5889W)
    dev_id=$(basename "$fullpath")
    
    # 2. Вырезаем чистый серийник (все, что после префикса DISK-)
    # Если префикс всегда DISK-, то удаляем первые 5 символов
    sn=${dev_id#DISK-}

    echo "Processing $fullpath (Label: $sn)..."

    # 3. Очищаем старую разметку (важно, чтобы gpart не ругался)
    # Используем само устройство из /dev/diskid/
    gpart destroy -F "$fullpath" 2>/dev/null

    # 4. Создаем таблицу GPT
    gpart create -s gpt "$fullpath"

    # 5. Создаем раздел ZFS и присваиваем ему метку (GTP Label) равную серийнику
    # Это создаст устройство /dev/gpt/<серийник>
    gpart add -t freebsd-zfs -l "$sn" "$fullpath"

    echo "Done: Created /dev/gpt/$sn"
done


./sn.bash 
Processing /dev/sdi (SN: Z1X414R4)...
Processing /dev/sdj (SN: Z1X41591)...
Processing /dev/sdk (SN: Z1X4159P)...
Processing /dev/sdl (SN: Z1X414XP)...
Processing /dev/sdm (SN: Z1X415N4)...
Processing /dev/sdn (SN: Z1X415AD)...
Processing /dev/sdo (SN: Z1X415E4)...
Processing /dev/sdp (SN: Z1X415R1)...
Processing /dev/sdq (SN: Z1X415JP)...
Processing /dev/sdr (SN: Z1X415M0)...
Processing /dev/sds (SN: Z1X4159A)...
Processing /dev/sdt (SN: Z1X4158J)...
Processing /dev/sdu (SN: Z1X415HP)...
Processing /dev/sdv (SN: Z1X414PY)...
Processing /dev/sdw (SN: Z1X415C4)...
Processing /dev/sdx (SN: Z1X4151F)...



витоге получаю

# ls -1 /dev/disk/by-partlabel/Z*
/dev/disk/by-partlabel/Z1X414PY
/dev/disk/by-partlabel/Z1X414R4
/dev/disk/by-partlabel/Z1X414XP
/dev/disk/by-partlabel/Z1X4151F
/dev/disk/by-partlabel/Z1X4158J
/dev/disk/by-partlabel/Z1X41591
/dev/disk/by-partlabel/Z1X4159A
/dev/disk/by-partlabel/Z1X4159P
/dev/disk/by-partlabel/Z1X415AD
/dev/disk/by-partlabel/Z1X415C4
/dev/disk/by-partlabel/Z1X415E4
/dev/disk/by-partlabel/Z1X415HP
/dev/disk/by-partlabel/Z1X415JP
/dev/disk/by-partlabel/Z1X415M0
/dev/disk/by-partlabel/Z1X415N4
/dev/disk/by-partlabel/Z1X415R1




так как у данной полки есть очень слабое место это в салазахках стоят интепозеры. это такие переходники
которые берут SATA потоки прверщают его в FC поток. 
как ниже покажу тесты они могут на чтение давать всего 95МБ/с сдиска. на запись всего 72МБ/с с диска.
так вот чтобы по дефолут запись еще ниже 44МБ/с чобы ее поднять до 72МБ/с вот такие правила вставолюятся в udev

	# cat  /etc/udev/rules.d/99-qlogic-all.rules

# Правило для блочного уровня (диски на карте QLogic 2532)
ACTION=="add|change", SUBSYSTEM=="block", ENV{ID_BUS}=="scsi", ATTRS{device}=="0x2532", ATTR{queue/max_sectors_kb}="1024", ATTR{queue/scheduler}="mq-deadline", ATTR{queue/read_ahead_kb}="4096"

# Правило для SCSI уровня (глубина очереди для дисков на карте QLogic 2532)
ACTION=="add|change", SUBSYSTEM=="scsi", ENV{DEVTYPE}=="scsi_device", ATTRS{device}=="0x2532", ATTR{queue_depth}="32"


это приавло привзяыается к тому что диски у них родитель это карта qlogic с поеределынным ID

ATTRS{vendor}=="0x1077" (это ID компании QLogic)
ATTRS{device}=="0x2532" (это ID твоей карты)


накатываем эти правила

# udevadm control --reload-rules
# udevadm trigger


главным образом эту штука подркручиитывает вот этот прараметр у дисков

cat /sys/block/sdX/queue/max_sectors_kb


s# lsscsi 
[7:0:0:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdi 
[7:0:1:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdj 
[7:0:2:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdk 
[7:0:3:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdl 
[7:0:4:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sds 
[7:0:5:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdt 
[7:0:6:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdu 
[7:0:7:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdv 
[7:0:8:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdy 
[8:0:0:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdm 
[8:0:1:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdn 
[8:0:2:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdo 
[8:0:3:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdp 
[8:0:4:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdq 
[8:0:5:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdr 
[8:0:6:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdw 
[8:0:7:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdx 
[8:0:8:0]    disk    ST2000NM 0033-9ZM175      SN06  /dev/sdaa

# cat /sys/block/sdm/queue/max_sectors_kb
1024








тест перфоманса полки на голых дисках.
дело в том что салазки имеют интепрозер которы преобразует SATA поток в FC поток.и этот интппозер он 
слабый.
беру диск  ST2000NM0033-9ZM 
который отлично протестировал просто на обычно хосте тоесть его скорости  я знаю (ищи в txt файлах этот тест)
и тестирую сколько из него выжмет полка

ЛИН ЧТЕНИЕ

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY --bs=$((8*128  *1024)) --iodepth=32  --runtime=15  --readwrite=read   --numjobs=1  --group_reporting  
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=96.1MiB/s][r=96 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=85593: Sun Jan  4 20:34:37 2026
  read: IOPS=95, BW=95.3MiB/s (99.9MB/s)(1470MiB/15429msec)


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY --bs=$((8*128  *1024)) --iodepth=1  --runtime=30  --readwrite=read   --numjobs=1  --group_reporting  
test: (g=0): rw=read, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=93.0MiB/s][r=93 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=85478: Sun Jan  4 20:29:59 2026
  read: IOPS=91, BW=91.5MiB/s (96.0MB/s)(2750MiB/30043msec)


тоесть 90-95 МБ/с против 190-200МБ/с которые может выдать сам диск. тоесть это все этот интерпозер так слабо работает

ЛИН ЗАПИСЬ

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415HY --bs=$((8*128  *1024)) --iodepth=1  --runtime=20  --readwrite=write   --numjobs=1  --group_reporting  
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=43.0MiB/s][w=43 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=85639: Sun Jan  4 20:36:07 2026
  write: IOPS=43, BW=43.6MiB/s (45.7MB/s)(894MiB/20516msec); 0 zone resets

  43.6 MB\s против 190МБ/с которые выжиаем сам диск. опять же это интепозер такой слабый


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415A4 --bs=$((8*128  *1024)) --iodepth=32  --runtime=20  --readwrite=write   --numjobs=1  --group_reporting  
test: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [W(1)][2.2%][w=65.1MiB/s][w=65 IOPS][eta 15m:50s]  
test: (groupid=0, jobs=1): err= 0: pid=85730: Sun Jan  4 20:38:33 2026
  write: IOPS=69, BW=69.8MiB/s (73.2MB/s)(1470MiB/21054msec); 0 zone resets


на многоптоке повеселее 69.8 МБ/с против 190МБ/с котоыре выжиаамает сам диск



РАНД РИД

# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/PN1181P5H6A19W --bs=$((8  *1024)) --iodepth=1 --runtime=20  --readwrite=randread   --numjobs=1  --group_reporting  
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=608KiB/s][r=76 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=85885: Sun Jan  4 20:42:24 2026
  read: IOPS=83, BW=664KiB/s (680kB/s)(13.0MiB/20016msec)


    83 иопса столько же сколко и сам диск может из себя выжать


 # fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415A4 --bs=$((8  *1024)) --iodepth=32 --runtime=20  --readwrite=randread   --numjobs=1  --group_reporting  
test: (g=0): rw=randread, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [r(1)][100.0%][r=1328KiB/s][r=166 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=86036: Sun Jan  4 20:46:13 2026
  read: IOPS=162, BW=1301KiB/s (1333kB/s)(26.0MiB/20445msec)

на многоптоке разные представители этого диска выдавали с разбросом. этот выдал 162 а другой такойже 124
а два других диска от другого проиизвоодиетля выдали 132 и 133 иопс.



РАНД ВРАЙТ

 fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415A4 --bs=$((8  *1024)) --iodepth=1 --runtime=20  --readwrite=randwrite   --numjobs=1  --group_reporting  
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=1
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=352KiB/s][w=44 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=86165: Sun Jan  4 20:50:51 2026
  write: IOPS=43, BW=350KiB/s (358kB/s)(7152KiB/20447msec); 0 zone resets

   43 иопса против 210 которые выжимает из себя диск


# fio --randrepeat=1 --ioengine=io_uring --direct=1 --gtod_reduce=1 --name=test --filename=/dev/disk/by-partlabel/Z1X415A4 --bs=$((8  *1024)) --iodepth=32 --runtime=20  --readwrite=randwrite   --numjobs=1  --group_reporting  
test: (g=0): rw=randwrite, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=io_uring, iodepth=32
fio-3.33
Starting 1 process
Jobs: 1 (f=1): [w(1)][100.0%][w=1273KiB/s][w=159 IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=86243: Sun Jan  4 20:52:18 2026
  write: IOPS=158, BW=1264KiB/s (1294kB/s)(25.5MiB/20644msec); 0 zone resets


тут один выдал 158 второй 117, а два диска от дргугого мануфакрутера выдали по 123.
все это против 207 которвые выдает сам диск из себя


тоесть данная полка очень круто теряет скорость за счет того что у нее медленные интепоозеры в салазках.
и поэтому далее когда мы будем щас зфс на эти диски накатывать то зфс будет тут ни причем.
хотя это все становится неважно так как дисков много а  к кадой полке идет канал FC всего 2Gb/s
но все равно важно понимпть почему с одного диска так мало снимается. это не зфс. это интеопзеры(SATA-FC) в салаказах!








создание главного пула



zpool create -f -o ashift=12 \
    -o autoreplace=on  \
    -o failmode=continue \
    -o autotrim=on \
    -O compression=lz4 \
    -O xattr=sa \
    -O atime=off \
    -O recordsize=128k \
    -O normalization=formD \
    -O utf8only=on \
    -O canmount=off \
    POOL-02 mirror /dev/disk/by-partlabel/Z1X415R1   /dev/disk/by-partlabel/Z1X4158J



# zpool add   POOL-02 mirror /dev/disk/by-partlabel/Z1X415M0  /dev/disk/by-partlabel/Z1X4159A
# zpool add   POOL-02 mirror    /dev/disk/by-partlabel/Z1X4151F      /dev/disk/by-partlabel/Z1X415HP
# zpool add   POOL-02 mirror    /dev/disk/by-partlabel/Z1X415C4     /dev/disk/by-partlabel/Z1X414PY
# zpool add   POOL-02 mirror    /dev/disk/by-partlabel/Z1X415N4     /dev/disk/by-partlabel/Z1X414R4
# zpool add   POOL-02 mirror      /dev/disk/by-partlabel/Z1X415AD      /dev/disk/by-partlabel/Z1X4159P
# zpool add   POOL-02 mirror     /dev/disk/by-partlabel/Z1X415E4      /dev/disk/by-partlabel/Z1X41591
# zpool add   POOL-02 mirror     /dev/disk/by-partlabel/Z1X415JP     /dev/disk/by-partlabel/Z1X414XP



# zpool status -v POOL-02
  pool: POOL-02
 state: ONLINE
config:

        NAME          STATE     READ WRITE CKSUM
        POOL-02       ONLINE       0     0     0
          mirror-0    ONLINE       0     0     0
            Z1X415R1  ONLINE       0     0     0
            Z1X4158J  ONLINE       0     0     0
          mirror-1    ONLINE       0     0     0
            Z1X415M0  ONLINE       0     0     0
            Z1X4159A  ONLINE       0     0     0
          mirror-2    ONLINE       0     0     0
            Z1X4151F  ONLINE       0     0     0
            Z1X415HP  ONLINE       0     0     0
          mirror-3    ONLINE       0     0     0
            Z1X415C4  ONLINE       0     0     0
            Z1X414PY  ONLINE       0     0     0
          mirror-4    ONLINE       0     0     0
            Z1X415N4  ONLINE       0     0     0
            Z1X414R4  ONLINE       0     0     0
          mirror-5    ONLINE       0     0     0
            Z1X415AD  ONLINE       0     0     0
            Z1X4159P  ONLINE       0     0     0
          mirror-6    ONLINE       0     0     0
            Z1X415E4  ONLINE       0     0     0
            Z1X41591  ONLINE       0     0     0
          mirror-7    ONLINE       0     0     0
            Z1X415JP  ONLINE       0     0     0
            Z1X414XP  ONLINE       0     0     0




теперь нужно в остав пула включить SSD
для начала их нужно полностью освободить изунтри . чтоб он был как с магазина. все ячейки пустые досутпные
для работы

# blkdiscard /dev/sda
# blkdiscard /dev/sdb
# blkdiscard /dev/sdc
# blkdiscard /dev/sdd



создаю два диска под SLOG размером 24ГБ


 backup-02 # lsblk -dno WWN /dev/sda
0x55cd2e4155d59e7e
 backup-02 # parted -s  /dev/sda  mklabel gpt
 backup-02 # parted -s  /dev/sda  mkpart primary 2048s 24GiB
 backup-02 # parted -s /dev/sda  name 1 wwn-55cd2e4155d59e7e


  backup-02 # lsblk -dno WWN /dev/sdb
0x55cd2e4155d5cd47
 backup-02 # parted -s  /dev/sdb  mklabel gpt
 backup-02 # parted -s  /dev/sdb  mkpart primary 2048s 24GiB
 backup-02 # parted -s /dev/sdb  name 1 wwn-55cd2e4155d5cd47


# blkid | grep -E "sda1|sdb1" 
/dev/sdb1: PARTLABEL="wwn-55cd2e4155d5cd47" PARTUUID="4894c8e4-67d0-445f-8807-c2405ba3aa73"
/dev/sda1: PARTLABEL="wwn-55cd2e4155d59e7e" PARTUUID="f1e24650-3052-4a35-ae3a-199202eacf32"


добавляю SLOG в пул

# zpool add POOL-02 log mirror /dev/disk/by-partlabel/wwn-55cd2e4155d5cd47    /dev/disk/by-partlabel/wwn-55cd2e4155d59e7e


почему я навесил на ссд диски гпт метку не в виде серийника а виде его истинного wwn номера который напкечатан на 
этикетке диска. потому что у интел ссд дисков у них серийники на конце одинаковые.  аменяются они толко в середине.
вот пример 

 backup-02 # lsblk -o NAME,SERIAL /dev/sda
NAME   SERIAL
sda    BTYI224201MG480BGN
└─sda1 
 backup-02 # lsblk -o NAME,SERIAL /dev/sdb
NAME   SERIAL
sdb    BTYI22420644480BGN
└─sdb1 

а ихние wwn они на  конце разные. а так как размер под наклейку на салазках очень маленткий то я не смогу туда всунуть
серийник целиком. зато я могу туда всунуть оконцовку wwn. поэтому вот так.



 

 # zpool status -v POOL-02 | tail -n 10
            Z1X41591              ONLINE       0     0     0
          mirror-7                ONLINE       0     0     0
            Z1X415JP              ONLINE       0     0     0
            Z1X414XP              ONLINE       0     0     0
        logs
          mirror-8                ONLINE       0     0     0
            wwn-55cd2e4155d5cd47  ONLINE       0     0     0
            wwn-55cd2e4155d59e7e  ONLINE       0     0     0



добаляем ссд под спешл.


 backup-02 # lsblk -dno WWN /dev/sdc
0x55cd2e4155d5d4f9
 backup-02 # 
 backup-02 # parted -s  /dev/sdc  mklabel gpt
 backup-02 # parted -s  /dev/sdc  mkpart primary 2048s 100%
 backup-02 # parted -s /dev/sdc  name 1 wwn-55cd2e4155d5d4f9
 backup-02 # 
 backup-02 # 
 backup-02 # lsblk -dno WWN /dev/sdd
0x55cd2e4155d59908
 backup-02 # parted -s  /dev/sdd  mklabel gpt
 backup-02 # parted -s  /dev/sdd  mkpart primary 2048s 100%
 backup-02 # parted -s /dev/sdd  name 1 wwn-55cd2e4155d59908
 backup-02 # 
 backup-02 # zpool add POOL-02 special     mirror         /dev/disk/by-partlabel/wwn-55cd2e4155d5d4f9    /dev/disk/by-partlabel/wwn-55cd2e4155d59908
 backup-02 # 
 backup-02 # 




 # zpool status -v POOL-02 | tail -n 15
            Z1X415E4              ONLINE       0     0     0
            Z1X41591              ONLINE       0     0     0
          mirror-7                ONLINE       0     0     0
            Z1X415JP              ONLINE       0     0     0
            Z1X414XP              ONLINE       0     0     0
        special 
          mirror-9                ONLINE       0     0     0
            wwn-55cd2e4155d5d4f9  ONLINE       0     0     0
            wwn-55cd2e4155d59908  ONLINE       0     0     0
        logs
          mirror-8                ONLINE       0     0     0
            wwn-55cd2e4155d5cd47  ONLINE       0     0     0
            wwn-55cd2e4155d59e7e  ONLINE       0     0     0



все пул собран.



теперь создаем датасет под proxmox backup server



 backup-02 # zfs create -o recordsize=1M \
           -o xattr=sa \
           -o atime=off \
           -o compression=lz4 \
           POOL-02/proxmox_backup_server


 backup-02 # zfs set special_small_blocks=128K POOL-02/proxmox_backup_server


теперь на special будут храниться нетлоко метаданые но и мелкие файлы размером до 128К коих 
будет много у пбс. его индексы и чтото там еще.


# zfs set quota=11T POOL-02  (осталяяем резервиурем  место под COW zfs)



# zpool list POOL-02
NAME      SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
POOL-02  14.9T  1.44M  14.9T        -         -     0%     0%  1.00x    ONLINE  -


 # zfs list -r POOL-02
NAME                            USED  AVAIL  REFER  MOUNTPOINT
POOL-02                        1.44M  11.0T    96K  /POOL-02
POOL-02/proxmox_backup_server    96K  11.0T    96K  /POOL-02/proxmox_backup_server
POOL-02/test-ds-128K             96K  11.0T    96K  /POOL-02/test-ds-128K







теперь я создаю два тестовых датасета с 128К и 1М рекаордсайзом.
что померять сколько можно выжать с этих датасетов. тоесть уже из зфс
напомню что наданный момент пул ссостоит из 16 дисков. это 8 мирроров в страйпе.

recordzie128K
	лин чтение
		bs=1MB
			iodepth=32  386 MB/s
  			iodepth=1   379MB\s


    выше мы получили для голого диска что на этой полке диск выдает 90-95МБ/с с одной штуки на лин чтении.
    по идее на iodepth=1 у нас должны бы была быть скорость 90-95 МБ/с
    она больше засчет того что зфс делает префетч. это нас спаасает на iodepth=1 (см файл *prefetech*.txt)
    на iodepth=32 по идее все должно быть отлично. у нас 8 мирроров в страйпе так что 8*95 легко забиывает 2*2Gb\s
    что мы и видим. тоесть результат ожидаемый. так то оно так но по другой причине.
    это было бы так если бы линукс умел брать пачку ио и пихать их однроврмннона зфс. но он этого не умеет
    а вот фрибсд умеет (смортри об этом txt файл *iodepth*.txt ), поэтому нас тут спасает опять все тот же 
    префет зфс. тоесть сам линкус работает на этом режиме точно также как iodepth=1 но нас спасает зфс
    с его префетчем. толко пэтому мы видим эту отличную скорость.
    итак линкейное чтение отличное в конечном итоге. 




    лин запись
		bs=1MB
			iodepth=32  130 MB/s
  			iodepth=1   130 MB\s


  остановился ттут

  дальнейше капитальный анализ полки Fc и зфс я сделал тут 

  				'disk - HGST HUS724020AL - ST2000NM0033-9ZM тест характеристик.txt'




еще раз проеряверю что у меня на спешл будет валитьс не только метаданные но и 
малые блоки датасет преданазнчнного для PBS

#  zfs get special_small_blocks POOL-02/proxmox_backup_server
NAME                           PROPERTY              VALUE                 SOURCE
POOL-02/proxmox_backup_server  special_small_blocks  128K                  local


все в порядке



далее тест на то что одна из полок отвалилась а пул все равно живой
я отключал оба порта FC поочееди.  (FC выключить порт.txt)
пул сразу реагирует



root@backup-02:/# zpool status -v  POOL-02 
  pool: POOL-02
 state: DEGRADED
status: One or more devices has been removed by the administrator.
        Sufficient replicas exist for the pool to continue functioning in a
        degraded state.
action: Online the device using zpool online' or replace the device with
        'zpool replace'.
  scan: resilvered 732M in 00:00:15 with 0 errors on Wed Jan  7 18:02:04 2026
config:

        NAME                      STATE     READ WRITE CKSUM
        POOL-02                   DEGRADED     0     0     0
          mirror-0                DEGRADED     0     0     0
            Z1X415R1              REMOVED      0     0     0
            Z1X4158J              ONLINE       0     0     0
          mirror-1                DEGRADED     0     0     0
            Z1X415M0              REMOVED      0     0     0
            Z1X4159A              ONLINE       0     0     0
          mirror-2                DEGRADED     0     0     0
            Z1X4151F              REMOVED      0     0     0
            Z1X415HP              ONLINE       0     0     0
          mirror-3                DEGRADED     0     0     0
            Z1X415C4              REMOVED      0     0     0
            Z1X414PY              ONLINE       0     0     0
          mirror-4                DEGRADED     0     0     0
            Z1X415N4              REMOVED      0     0     0
            Z1X414R4              ONLINE       0     0     0
          mirror-5                DEGRADED     0     0     0
            Z1X415AD              REMOVED      0     0     0
            Z1X4159P              ONLINE       0     0     0
          mirror-6                DEGRADED     0     0     0
            Z1X415E4              REMOVED      0     0     0
            Z1X41591              ONLINE       0     0     0
          mirror-7                DEGRADED     0     0     0
            Z1X415JP              REMOVED      0     0     0
            Z1X414XP              ONLINE       0     0     0
          mirror-10               DEGRADED     0     0     0
            spare-0               DEGRADED     0     0     0
              Z1X415HY            REMOVED      0     0     0
              Z1X415DT            ONLINE       0     0     0
            Z1X415A4              ONLINE       0     0     0
        special 
          mirror-9                ONLINE       0     0     0
            wwn-55cd2e4155d5d4f9  ONLINE       0     0     0
            wwn-55cd2e4155d59908  ONLINE       0     0     0
        logs
          mirror-8                ONLINE       0     0     0
            wwn-55cd2e4155d5cd47  ONLINE       0     0     0
            wwn-55cd2e4155d59e7e  ONLINE       0     0     0
        spares
          Z1X415B9                AVAIL   
          Z1X415DT                INUSE     currently in use




и как видно пул DEGRADED но четко в кждом мирроре нет только одного диска.
тоесть отвалилась полвинка всех мироров. и пул работает. я проверял. 
отвечает на запрсы иопосов
также видно что сразу в бой вступают спейры. автоматом. 


когда полка обратно подключается то сами иски обратно автоматом вставляются
и спейры обратно откатываются
и мы получаем исходное состояния


root@backup-02:/# zpool status -v  POOL-02 
  pool: POOL-02
 state: ONLINE
  scan: resilvered 1.11G in 00:00:13 with 0 errors on Wed Jan  7 18:06:25 2026
config:

        NAME                      STATE     READ WRITE CKSUM
        POOL-02                   ONLINE       0     0     0
          mirror-0                ONLINE       0     0     0
            Z1X415R1              ONLINE       0     0     0
            Z1X4158J              ONLINE       0     0     0
          mirror-1                ONLINE       0     0     0
            Z1X415M0              ONLINE       0     0     0
            Z1X4159A              ONLINE       0     0     0
          mirror-2                ONLINE       0     0     0
            Z1X4151F              ONLINE       0     0     0
            Z1X415HP              ONLINE       0     0     0
          mirror-3                ONLINE       0     0     0
            Z1X415C4              ONLINE       0     0     0
            Z1X414PY              ONLINE       0     0     0
          mirror-4                ONLINE       0     0     0
            Z1X415N4              ONLINE       0     0     0
            Z1X414R4              ONLINE       0     0     0
          mirror-5                ONLINE       0     0     0
            Z1X415AD              ONLINE       0     0     0
            Z1X4159P              ONLINE       0     0     0
          mirror-6                ONLINE       0     0     0
            Z1X415E4              ONLINE       0     0     0
            Z1X41591              ONLINE       0     0     0
          mirror-7                ONLINE       0     0     0
            Z1X415JP              ONLINE       0     0     0
            Z1X414XP              ONLINE       0     0     0
          mirror-10               ONLINE       0     0     0
            Z1X415HY              ONLINE       0     0     0
            Z1X415A4              ONLINE       0     0     0
        special 
          mirror-9                ONLINE       0     0     0
            wwn-55cd2e4155d5d4f9  ONLINE       0     0     0
            wwn-55cd2e4155d59908  ONLINE       0     0     0
        logs
          mirror-8                ONLINE       0     0     0
            wwn-55cd2e4155d5cd47  ONLINE       0     0     0
            wwn-55cd2e4155d59e7e  ONLINE       0     0     0
        spares
          Z1X415B9                AVAIL   
          Z1X415DT                AVAIL   




я решил устанвоить PBS сервер (ищи PBS *устанвока*.txt)
а потом выыяснилоас что он хуйня





root@backup-02:/# echo 107374182400 >  /sys/module/zfs/parameters/zfs_arc_max
root@backup-02:/# cat /etc/modprobe.d/zfs.conf 
options zfs zfs_arc_max=107374182400
root@backup-02:/# 
root@backup-02:/# cat /etc/modprobe.d/qla2xxx.conf 
options qla2xxx ql2xmaxqdepth=64 ql2xiidmaenable=0 qlport_down_retry=30


в итоге делаю NFS шару.
здесь важно оговрить кучку параметрров.
если мы говорим про скрость по нфс нужно четко 
скзаать как настроана шара на сервре у нее там sync или async
у меня он вседа sync

следущий момент как настроен маунтинг на клиенте(прокскмкске) он sync или async.

следущий параметр как мы бэапис сжатием или без сжатия.

еще важно какими кусками bs и какой iodepth мы делаем при остылке на нфс сервер.

вначале я рассматриваю вариант когда на клиенте включен nfs sync
и вначале я говорб чему равна скорсть если послыать потко чеерз fio
также важно скзаать что один SLOG диск может принимта синнхронно на скрости 225 МБ/с
тоесть если в слог стоит два INTEL то из сети можно принимать на скрости 225 МБ/с
но такая сорость работает только если слать с PVE кусками по 1МБ и iodepth=32 ( при условии что и на сервере NFS sync
и на клиенте nfs sync)
мне это фио и показал.

если длбавить в SLOG вторую пару интелов то при правилтной нагрзуке в fio(сервер sync клиент sync 1МБ и iodepth=32)
наш нфс сервер может принимаь на сроксти 450 МБ/с !!


итак с тем что можно выжать из нфс при правильной нгрузке мы поняли.
теерь что из этого выжимает vzdump от pve.
итак у нас стоит в слоге 4 диска интел. которые могут засасывть из сети на скрости 450МБ/с

если на клиенте стоит sync и если мы бэкапим без сжатия то vzdump выжимает 
			208-230 МБ/с (тоесть далеко не 450 МБ/с)
если я бэкаплю с сжатием zstd (и неважно насколько быстрое сжатие) то скрорость падает до 
			70-90 МБ/с  (сука)
виноват именно ебнуты zstd дело не в цпу ни в чем. просто у него такой обмена инфомаций как я понимаю.

итак режим sync на клиенте на проксксксе на не подходит вобще нихуя.



тогда на клиенте я выставляю nfs async в /etc/pve/storage.conf


nfs: BACKUP-02-NFS
	export /POOL-02/NFS_PROXMOX-2
	path /mnt/pve/BACKUP-02-NFS
	server 100.69.78.2
	content backup
	options vers=4.2,async,nconnect=16,noatime
	prune-backups keep-all=1



 если мы бэкапим без сжатия то vzdump пуляет данные в кеш ядра и скорость
			1 GB\s

далее ядро агрергрует эти блоки и сует их всеть причем как положено сует. я реально вижу что на SLOG нагрузка
450 МБ/с. все отлично кроме одного - нам нахуй ненужен бэкап без сжатия.

так вот если я бэкаплю с сжатием zstd то скрорость падает до 
			220 МБ/с  (сука)
виноват именно ебнуты zstd дело не в цпу ни в чем. просто у него такой обмена инфомаций как я понимаю.
zstd с такой скростью кладет данные кэш ядра сука. а потом уже ядро в асинхронном режиме шлет их посети.
и получается такой вывод - нам нахуй ненежуен NFS серевер соскостью 450МБ/с потому что zstd неумеет рабатть на такой
скрости. и дело не в цпу ни вчем. просто он ска такой тупой.


есит и еще момент.  слог у нас быстрый. а вот дата диски у нас  работают на скрости 200МБ/с.
поэтому если мы пишем постоянно по сети на скрости 450МБ/с то зфс какоето время сгладит эут разницу за счет
оперативки но рано или поздно а точнее если файл бэкапа больше 15ГБ зфс остановит прием новых данных пока
данные что в памяти не будут записаны на дата диски. при этом скросость записи по нфс падает до 10МБ/с на
секунд 15. а потом обратно выраатстсет до 450МБ/с
собсвтеннно дилема. толи оставить эту возможность бурста. тооли убрать из слог два диска и тогда как
раз все совпдаает. слог раотает на скрости 225 МБ/с и дата диски на крости 200МБ/с и запист будет все время
ровная. без бурста но и без провалов.
учитыа что zstd все равно быстрее чем 200МБ/с данные не поставляет вроде как и смысла на 450 нет.
но это если в нфс пишет один прокскскс. а если у нас их два то смысл уже вроде как есть.
в любом случае
вот что нужно при этом покрутиьт на зфс


налету

  583  echo 1048576 > /sys/module/zfs/parameters/zfs_vdev_aggregation_limit_non_rotating (тут неясно стоит или нет)
  606  echo 8589934592 >  /sys/module/zfs/parameters/zfs_dirty_data_max
  608  echo 80 >  /sys/module/zfs/parameters/zfs_dirty_data_sync_percent


в файле

# Увеличиваем объем данных, которые могут висеть в памяти перед сбросом на диски (в байтах)
# Для 128ГБ памяти можно смело поставить 8-16 ГБ (например, 8589934592)
options zfs zfs_dirty_data_max=8589934592

# Увеличиваем лимит, при котором ZFS начинает агрессивно тормозить клиента
options zfs zfs_dirty_data_sync_percent=80

options zfs_vdev_aggregation_limit_non_rotating=1048576


итговй фвайл 

# cat zfs.conf 
options zfs zfs_arc_max=107374182400
options zfs zfs_dirty_data_max=8589934592
options zfs zfs_dirty_data_sync_percent=80
options zfs_vdev_aggregation_limit_non_rotating=1048576


тоесть можно выбрать. либо слог состоитоит из двух интел и они дают 230 МБ/с что совдпатает с скростью 
дата дисков 200МБ/с
то ли добавить +2 диска в слог и получить бурст для файлов порядка 15ГБ


в любом слуачае нуэно на клиенте  делать async при монтировании. вот так


nfs: BACKUP-02-NFS
	export /POOL-02/NFS_PROXMOX-2
	path /mnt/pve/BACKUP-02-NFS
	server 100.69.78.2
	content backup
	options vers=4.2,async,nconnect=16,noatime
	prune-backups keep-all=1



настрокий на среререре для нфс


# zfs create -o recordsize=1M  -o xattr=sa  -o atime=off  -o compression=lz4   POOL-02/NFS_PROXMOX-2
# zfs create -o recordsize=1M  -o xattr=sa  -o atime=off  -o compression=lz4   POOL-02/NFS_PROXMOX-1
# zfs mount -a

# ls -1al /POOL-02/NFS_PROXMOX-2
total 34
drwxrwxrwx 4 nobody nogroup  4 Jan  8 16:55 .
# chown nobody:nogroup /POOL-02/NFS_PROXMOX-2

# ls -1al /POOL-02/NFS_PROXMOX-1
total 34
drwxrwxrwx 4 nobody nogroup  4 Jan  8 16:55 .
# chown nobody:nogroup /POOL-02/NFS_PROXMOX-1



# cat /etc/exports  | grep -v '#'
/POOL-02/NFS_PROXMOX-2  100.69.78.2/24(rw,sync,no_subtree_check,no_root_squash,no_all_squash)
/POOL-02/NFS_PROXMOX-1  100.69.78.2/24(rw,sync,no_subtree_check,no_root_squash,no_all_squash)



# cat /etc/sysctl.d/99-sysctl.conf  | grep -v '#'


net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1


net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.core.netdev_max_backlog = 5000



 cat /etc/nfs.conf 
...
[nfsd]
threads=128
max_block_size=1048576
vers2=n
vers4=n
vers4.0=n
vers4.1=n
vers4.2=y


заствить нфс сервер пеерчиатт конфиг
		# exportfs -ra


на PVE проксксксксах нужно  обязательно прописать праамтеры нфс в том числе async
прям можно добавлять не через веб морду а сразу вбиваем в /etc/pve/stoarge.cfg

nfs: BACKUP-02-NFS
	export /POOL-02/NFS_PROXMOX-2
	path /mnt/pve/BACKUP-02-NFS
	server 100.69.78.2
	content backup
	options vers=4.2,async,nconnect=16,noatime
	prune-backups keep-all=1


если все верно то на PVE будет 
  # mount | grep nfs

100.69.78.2:/POOL-02/NFS_PROXMOX-1 on /mnt/pve/BACKUP-02-NFS type nfs4 (rw,noatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,nconnect=16,timeo=600,retrans=2,sec=sys,clientaddr=100.69.78.11,local_lock=none,addr=100.69.78.2)


на прокскмс PVE 
также нужно чтобы

	# cat /etc/vzdump.conf  | grep -v '#'
compress: zstd
performance: max-workers=16
zstd: 0



весь вопрос в том  какой слог оставить на  backup-02 из двух дисков или из четырех.




состав пула

# zpool list
NAME      SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
POOL-02  21.8T   466G  21.3T        -         -     0%     2%  1.00x    ONLINE  -
rpool    64.5G  1.43G  63.1G        -         -     1%     2%  1.00x    ONLINE  -
root@backup-02:/home/krivosheeva/scritps# zpool status -v
  pool: POOL-02
 state: ONLINE
  scan: resilvered 1.11G in 00:00:13 with 0 errors on Wed Jan  7 18:06:25 2026
remove: Removal of vdev 9 copied 13.2M in 0h0m, completed on Thu Jan  8 16:52:56 2026
        16.0K memory used for removed device mappings
config:

        NAME                      STATE     READ WRITE CKSUM
        POOL-02                   ONLINE       0     0     0
          mirror-0                ONLINE       0     0     0
            Z1X415R1              ONLINE       0     0     0
            Z1X4158J              ONLINE       0     0     0
          mirror-1                ONLINE       0     0     0
            Z1X415M0              ONLINE       0     0     0
            Z1X4159A              ONLINE       0     0     0
          mirror-2                ONLINE       0     0     0
            Z1X4151F              ONLINE       0     0     0
            Z1X415HP              ONLINE       0     0     0
          mirror-3                ONLINE       0     0     0
            Z1X415C4              ONLINE       0     0     0
            Z1X414PY              ONLINE       0     0     0
          mirror-4                ONLINE       0     0     0
            Z1X415N4              ONLINE       0     0     0
            Z1X414R4              ONLINE       0     0     0
          mirror-5                ONLINE       0     0     0
            Z1X415AD              ONLINE       0     0     0
            Z1X4159P              ONLINE       0     0     0
          mirror-6                ONLINE       0     0     0
            Z1X415E4              ONLINE       0     0     0
            Z1X41591              ONLINE       0     0     0
          mirror-7                ONLINE       0     0     0
            Z1X415JP              ONLINE       0     0     0
            Z1X414XP              ONLINE       0     0     0
          mirror-10               ONLINE       0     0     0
            Z1X415HY              ONLINE       0     0     0
            Z1X415A4              ONLINE       0     0     0
          mirror-12               ONLINE       0     0     0
            Z1X4152Q              ONLINE       0     0     0
            Z1X4157Y              ONLINE       0     0     0
          mirror-13               ONLINE       0     0     0
            Z1X4151E              ONLINE       0     0     0
            Z1X415K9              ONLINE       0     0     0
          mirror-14               ONLINE       0     0     0
            Z1X4159C              ONLINE       0     0     0
            Z1X415FM              ONLINE       0     0     0
        logs
          mirror-8                ONLINE       0     0     0
            wwn-55cd2e4155d5cd47  ONLINE       0     0     0
            wwn-55cd2e4155d59e7e  ONLINE       0     0     0
          mirror-11               ONLINE       0     0     0
            wwn-55cd2e4155d5d4f9  ONLINE       0     0     0
            wwn-55cd2e4155d59908  ONLINE       0     0     0
        spares
          Z1X415B9                AVAIL   
          Z1X415DT                AVAIL   
          Z1X415PG                AVAIL   
          Z1X4153N                AVAIL   



если одна из полок временно отвалилась то диски этой полки 
передйту в пуле с статус REMOVED
потом когда полка обратно подключться то можно запстить скрипт rescan-scsi-bus.sh
из пакета sg3_tools  хотя по идее они станут видны линусу и так 
а вот зфс эти диски неочень хочет обратно запускать в пуле. они булудут виесть в статусе REMOVED
инадо будет пройтись по кажму диску и обьявить его как ONLINE

     # zpool online POOL-02 Z1X4159C
     # zpool online POOL-02 Z1X415JP


когда резилвенириног закончится то зфс спейры обратно в пул спейров возвращать небудет.
убеждаемся что в кажом мирроре есть свои два диска и полсе этого нужно руками спейры
вернуть в пул спейров через detach



  # zpool status -v POOL-02
  # zpool detach POOL-02 Z1X415B9
  # zpool status -v POOL-02
  # zpool detach POOL-02 Z1X415DT
  # zpool detach POOL-02 Z1X415PG   Z1X4153N
  # zpool status -v POOL-02
  # zpool detach POOL-02 Z1X4153N
  # zpool status -v POOL-02






$$$$$$$
END
$$$$$$$