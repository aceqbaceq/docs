linux
общее

--
ubuntu 20 
сука в два раза жирнее по диску чем ubuntu16
гораздо мнеее отзывчивый тяжелый

стал как и весь другой софт сразу занчительно неповоротливый тяжелый.
нахуй он такой нужен
тяжело натужно грузится
миллион служб. все как с другим софтом чем дальше тем хуевей
убунту 16 просто ракета по сравению

что нужно допилить напильиником в ubuntu 20:

	иодитский netplan 
	как его удалить - https://disnetern.ru/disable-netplan-ubuntu/
	причем netplan я удалидл. ifupdown пакет поставил но при этом
	настройки про dns из /etc/networkj/interfaces нихера не подсатыватся.
	в итоге нужно еще поставить # apt install resolvconf
	тогда настройки dns надо прорисывать в /etc/resolv.conf про dns
	как и когда то одавно!
		# cat resolv.conf
		nameserver 172.16.101.2
		nameserver 172.16.101.7
		domain mk.local
		search mk.local

	нужно пояс часовой сделать москва
	 sudo timedatectl set-timezone Europe/Moscow

	удалит floppy из системы.
	$ sudo rmmod floppy
				$ echo "blacklist floppy" | sudo tee /etc/modprobe.d/blacklist-floppy.conf
				$ sudo dpkg-reconfigure initramfs-tools
				
	
--


делаем чтобы можно было к нектиться к ФС

ceph auth get-or-create client.cephfs mon 'allow r' osd 'allow rwx pool=cephfs_metadata,allow rwx pool=cephfs_data' -o /etc/ceph/client.cephfs.keyring

ceph-authtool -p -n client.cephfs /etc/ceph/client.cephfs.keyring > /etc/ceph/client.cephfs

--

iperf

задача сделать тест скорости сети 
из между офисами.

далем на основе iperf

ставим на обоих компах iperf3
нужен именно он

# sudo apt-get install iperf3

стандартный iprf работает так, один конец запускаем в
виде сервера

(a)# iperf -p 49001 -s

-p 490001 = это порт на который сервер принимает запросы.
надо на файрволле открыть этот порт для входящих

второй конец работает клиентом

(b)# iperf -c 123.12.12.12 -p 49001 -R -i 5

-c 123.12.12.12 = адрес сервера
-i 5 = через сколько секунд выводить статистику на экран

еще можно указать ключ -t 10 = это значит что тест будет идти 10 секунд
если -t вобще неуказывать то тест будет идти бесконечно что удобно

самый главный ключ это -R и он есть только в iperf3
он дает то что клиент будет скачивать данные с сервера.
потому что по дефолту клиент подклчается к серверу и начинает UPLOAD 
данных на сервер. 
поэтому если мы хотим чтобы клиент DOWNLOAD данные с сервера нужен ключ -R

---

LINUX REMAP KEYS

купил мелкую клаву.
а на ней эти уроды убрали insert 
pgup pgdown end

как сделат ьтак чтобы жмешь наодну конопку а унее функция другой кнлопки


редактикрую файл

/usr/share/X11/xkb/keycodes/evdev

а если пром еще хочется узнать какой код у какой кнопки то 
хзапусти прогу xev

=============
ssh

known_hosts

что в этом файле на компе клиента.

когда мы конектимся к серверу 
то он нам присылыает свой публичный ключ.
так вот в known_hosts наш хост сохраняет хэш от этого публичного ключа + IP\hostname от этого сервера.
зачем мы это делаем = чтобы в следущий раз если мы стучим на этот сервер то заранее знать какой нам публичеый 
ключ ожидать. потому что злолдей может заглущить ориганлный сервер поставить себе его ip 
создать свой приватный \пубичный ключ  и совать их нам выдавая себя за оригинаьный сервер.

строчка в этом файле прдставляет собой вначале идет hostname\ip смотря что мы вбивали в команде ssh
причем оно указано не в открытом виде а виде тоже хэша. а потом идет хэш от публичного ключа того сервера

поскульку это хэши то дешифровать никак нельзя.

зато можно провнрить есть и в known_hosts запись о том или ином хосте.

$ ssh-keygen -F mail.ru

еще можно удлаить строчку из known_hosts

$ ssh-keygen -R mail.ru

если удалить весь файл то ничго страшного.
просто при стучании на серверы начнет выскакивать надпись дескать 
удаденный хост имеет такой то хэш пуличного ключа мол вы согласный что удалеенный хост это удаленный хост
вот и все последствия

=============
tty

про эту тему tty можно писать много и долго.
про эту тему уже разбросано несолько кусков по моей документации. 
я позже их соберу. а щас рассмотрим такую тему. вот если нажать ps aux

то в выводе идет список процессов и у каких то процессов указан знак вопроса "?"  в столбце tty а 
у каких то указано tty1 или tty7 или pts/0. pts небудем трогать пока считаем что это некая разновидость tty. так вот что же это значит что процесс имеет некий tty или не имеет его. на что
это вляет как это понять почуствовать. в чем разница и все такое.

если процесс в выводе ps имеет в свойствах tty это значит что у данного процесса в свойствах 
открытых файлов имеется устройство tty. пример

смотрим на ps для процесса 1180

$ ps aux | grep -E "TTY|1180"
USER       PID %CPU %MEM    VSZ   RSS   TTY      STAT   START   TIME   COMMAND
root      1180  0.6  0.5 1227504 89308  tty7     Rsl+   апр18   116:29  /usr/lib/xorg/Xorg

видим что ps показывает для 1180 что tty7

смотрим какие файловые дескрипторы открыты для процесса 1180 на proc
$ sudo ls -1al /proc/1180/fd/ | grep tty
lrwx------ 1 root root 64 мая  1 01:10 12 -> /dev/tty7

и видим что среди файловых дескрипторов есть /dev/tty7

файловый дескриптор это поток обмена информации между процессом и внешним миром. в него 
можно читать и писать. в данном случае у процесса поток номер 12 имеет 
бекендом  устройство /dev/tty7 тоесть у процесса есть канал обмена информацией между процессом
и устройством /dev/tty7 

таким образом если в ps мы видим что процесс имеет в своих свойствах tty устройство 
это значит что у процесса открыт канал связи с tty устройством.

что такое tty устройство - для начала  /dev/ устройства в целом что это такое. есть реальное физическое устройство железка. данные от железки поступают в ядро. а /dev/ устройства это кранчик
который торчит из ядра который позволяет программе из user space получит данные от железного устройства из ядра через этот кранчик /dev/ файла.  какая железка является бекендом для /dev/ttyX 
файла. в одном случае это COM порт. тоесть запись в /dev/ttyX файл пишет в COM порт и чтение из 
/dev/ttyX файла это принятие данных из COM порта.  
таким образом если у процесса есть файловый дескриптор который ведет в /dev/tty устройство это значит данный процесс может писать в COM порт или читать данные из COM порта.
цепочка тогда выглядтит так:

процесс -> файлоый дескриптор -> /dev/tty устройство -> ядро -> COM порт

и наоборот.

тоесть если у процесса есть tty устройство это значит что у процесса открыт канал связи с внешним миром через COM порт в обе стороны. тоесть с точки зрения процесса у него открыт файл на чтение 
и запись. бекендом данного файла явяляется tty файл бекендом которого является COM железка.

далее. чуть сложнее. раньше клавы и мониторы были вынесены за системный блок. то есть схема 
выглядела так. системный блок у него COM порт. в него идет провод от коробки под названием 
терминал. терминал это коробка у которой есть клава и экран вот сигнал от клавы на терминале 
поступал в системный блок через COM порт получается через /dev/tty файл. и также от системного блока
инфо которая была предназначена для вывода на экран она летела в /dev/tty файл от процесса в системном блоке потом в ядро и в COM порт оттуа по проводу в терминал а он(терминал) поступившую инфо  преобразовавл и выводил буквы на экран терминала.  таким образом COM порт это был порт 
через который инфо от клавы и монитора попадала в системный блок в обе стороны.
таким образом COM порт это был порт через который в процесс могл прилетать инфо от клавы и 
и от процесса инфо могла попадать на экран монитора. тоесть COM порт и tty устройство это устройство
через которое через который процесс имеет связь с живым юзером.

далее прикол в том что в ядро линукс встроен виртуальный терминал выглядит это так. процесс 
открывает файл /dev/ttyX устройство и начинает туда писать. данные летят в ядро. ядро направляет поток в драйвер виртуального терминала и он направляет его на экран монитора. и про клаву. поток из клавы летит в ядро. ядро направляет поток в драйвер вирт терминала. он обрабатывает поток и шлет
поток в /dev/tty файл и процесс чеерз этот файл получает поток от клавы внутрь себя. таким образом
если у процесса открыт на чтение запись файл /dev/tty{1..7} то это значит что даные с клавы летят в данный процесс и данные от процесса выводятся на экран.

таким образом отвечаем на изначальный вопрос в чем разница между процессом у которого открыт 
поток чтения записи в tty устройство от процесса у которого такого потока неоткрыто.
разница в том что процесс у которого отркыт поток обмена данным с tty устройством дает 
возможность чтобы инфо с клавы компа летели внутрь процесса и поток вывода от процесса в tty 
приводит к тому что инфо поступает на экран компа. таким образом tty поток дает воможность 
взаимодействовать процессу с живым юзером через экран и клаву.

во времена терминалов за каждый ttyX  портом скрвыался на том конце терминал со своей клавой 
и своим экраном. тоесть если было семь tty1-tty7 портов то это значит что было на том конце
семь терминалов, семь клавиатур и семь мониторов. поэтому если процесс 1 имел открытым tty1 то 
инфо с клавы 1 летело в процесс1 а если процесс2 имел открытым tty2 то инфо с клавы2 летело в процесс2. а какая же ситуация сейчас когда у нас на компе есть tty1-tty7 но при этом клава 
физически у компа одна. как же ядро понимает в какой tty посылать поток идущий с клавы.
я понимаю что дело выгляди так - вот мы тыкнули Ctrl-Alt-F1 значит ядро понимает 
что на данный момент мы "активировали" первый виртуальный терминал в ядре.значит поток букв от клавы
надо посылать в /dev/tty1 и значит тот процесс который читает с этого устройтва и будет получать
поток букв с клавы.  теперь про экран. пусть у нас семь процессов и каждый из них открыл свой tty
тоесть

pid 1 -> tty1
pid 2 -> tty2
...
pid 7 -> tty7

далее каждый из этих процессов постоянно чтото шлет записывает в свой tty. 
возникает вопрос какой из этих потоков выводить на экран. думаю что рабтает также.
что поток сохраняется в некий буфер в памяти ядра. для каждого tty свой буфер. далее
мы жмем Ctrl-Alt-F1 и ядро понимает что мы активировали первый виртальный терминал. 
тода ядро выводит на физический экран вывод из буфера памяти от tty1 устройства.
далее мы тыкаем Ctrl-Alt-F5 и ядро понимает что мы перключились на пятй виртуальный терминал
и ядро выводми на экран вывод из буфера памяти который хранит накопленный поток от tty5.
это была логика когда у нас линукс работает в текстовом init 3 режиме.
когда у нас графический режим то вместо tty используются pts устройства. и вместо Ctral-Alt-Fx
комбинаий исполщуется то что мы мышкой активируем то или иное окно.

таким образом есл вернтся в текстовый режим то полчается что ядро линукса оно биндит
виртальные терминалы(монитор и клава) в физическую клаву и физический монитор.

итак еще о чем нам говорит если у процесса  в списке открытых файлов есть tty устройство.
это нам говорит о том что данный проецесс подразумевает может контактировать с живым юзером
через клаву и монитор. через клаву процесс может получать от юзера нажатие кнопок а через монитор процесс может выводить инфо. если у процесса нет открытых tty файлов то работа данного процесса
не подразумевает контакт с живым юзером. с клавы он инфо принимать неможет и на экран выводиь 
инфо он тоже неможет. назовем это так : есть юзер интерактивные процессы и неинтерактивные.

вопрос а что если несколько процессов одновременно имеют откртытым один и тот же tty файл например tty2.
вот мы жмем кнопки на клаве в какой процесс летит поток от клавы. ответ такой что от того что 
процессом открыт файл еще незначит что он из него постоянно читает. 
если они оба постоянно пытаются из него читать то будет идти конкурентная борьба и часть букв 
будет попадать в один процесс а часть в другой. 

а что если они оба активно пишут в tty2. значит на экране будет идти смесь то одного процесса то 
от другого. тоесть. скажем первый шлет "aaaaaaaaaaaa" а второй шлет "bbbbbbbbbb"
на экране юзер буездет видеть чтото типа того "ababbbaabbabababbaab" тоесть будет тоже идти
конкуретная борьба.

но как я уже скаал от тооо что два процесса имеют открытым один tty2 это незначит что они оба
из него сейчас активно читают или пишут. пример откроем терминал в граф оболочке. и введем команду
$ tty
она покажет к какому устройству вирт терминала прикреплен данный виртуальный терминал.

$ tty
/dev/pts/21

pts это упрошенно говоря тоже некий аналог вирт терминала.

теперь вот что запустим

$ ps aux | grep 'pts/21'
vasya    15543  0.1  0.0  34112  5448 pts/21   Ss   02:53   0:00 bash
vasya    15557  0.0  0.0  48852  3772 pts/21   R+   02:54   0:00 ps aux


тут видно что два процесса имеют открытым один и тот же файл вирт терминала pts/21
но между ними конфликта нет. мы имели запущенный bash и в нем мы запустили команду ps aux
и баш что сделал. он породил новый процесс ps aux и при этом процесс bash 15543 ушел в состояние 
interruptible sleep. конечно статус в ps не является прям точной инфо потому что процесс может 
чтото делать по быстрому и потом уходить в состояние sleep тоесть это незначит что процесс прям
все время в этом состяонии. он был в том состоянии только на момент вывода ps. одномоментно.
но с высокой долей вероятности можно полагачть что 
  хотя bash 15543 и имеет открытым pts устройство но он из него ничего нечитает и в него 
ничего не выводит. поэтому на момент выполнения ps aux вывод на pts\21 был подчинен только 
процессу ps aux. поэтом конфликат никакого нет. поэтому нет никакой чехарды с потоком от клавы
 и потоком на монитор. но в целом его легко устроить если запустить какойто цикл который чтот выводит экран. а потом открыть другой терминал и начать слать мусор в /dev/pts/21 и тогда на экране
 первого терминала будет идти смешение вывода от цикла и от нашего ручного мусорного потока.


а вот интеренсый вопрос что происходит когда в bash мы запускаем команду  в
background режиме ( это такая фишка баша). например

$  (while true; do sleep 10; echo "111"; done)&

при этом будем наблдюдать интересуную картину с одной стороны мы имеем доступ
к консоли но в ней периодически будет появляться "111"

получаеся по факту у нас два процесса у которых один pts/22 и наш первый процесс он периодически
шлет в pts/22 инфо и она выскакивает на окне терминала. в тоже время мы тоже сидя за клавой
может чтото писать на терминале. тоесть мы имеем два процесса которые одновременно вывоводят в 
один терминал. насколько я понимаю что просиходит при запуске в баш команды в бекграунд режиме 
то форкается процесс в нем запускается команда но исходный баш процесс не помещается в режим sleep
а продолжает свою работу. 

 а вот еще пример

 $  ( while true; do     sleep 10; read var1; done )&

 тут получается что процесс должен считать с клавы. так вот у нас получится что есть 
 два процесса с одним pts и оба процесса в какойто момент времени будут читать с клавы
 так вот незнаю как но баш делает так что если бекграундный процесс лезет читать в то время как 
 исходный баш процесс читает ведь с клавы так вот бекграуден процесс тут же будет остановлен 
 башем.  с точки зрения статуса данный процесс будет переведен в статус "T" когда он попытается
 счиатть с клавы наряду с исходным баш процессом.


====
каналы 1 ,2 

по поводу пернаправления вывода

find / 2>/dev/null

что там происходит по капотом в этом случае. что за загадочные цифра 2.
на самом деле все просто. создаетя процесс который обслуживает find
и у него есть файловые дескприоры 

/proc/$$/fd/{0,1,2}

так вот мы башу закаываем чтобы он когда будет создавать процесс под команду find
чтобы он дескпритор 2 перенарпавил (тоесть что у него будет бекендом ) не на экран ( тоесть не на /dev/pts/17) а в /dev/null вот и все

$ sudo ls -1al /proc/20920/fd
lrwx------ 1 root root 64 мая 17 23:42 0 -> /dev/pts/32
l-wx------ 1 root root 64 мая 17 23:42 1 -> 'pipe:[1662807]'
l-wx------ 1 root root 64 мая 17 23:42 2 -> /dev/null


вот четко видно что 

2 -> /dev/null

что файловый дескриптор 2 ( в который процесс кидает поток с инфо о ошибках) совать в /dev/null

вот как работает под капотом вся эта хрень с перенапраавлением ввода вывода!















===
processes 'R' 'D' status
+ kernel mode

It is not 100% beatifull answer but it can give you a flavour. If you have some process that constantly in 'R' state - you can start to monitor two fields from procfs:

$ awk '{print $14, $15}' /proc/$$/stat 
you will see something like: 0 3915

The first number shows 'Amount of time that this process has been scheduled in user mode, measured in clock ticks' , the second one shows 'Amount of time that this process has been scheduled in kernel mode, measured in clock ticks'. (please have a look at man proc for the details).

However the point is if 3915 is growing fast and 0 is not growing it means the process is running under kernel mode right now. The more fast 3915 is growing the more we can be sure that the process is running under kerhel mode.

an example:

$ sudo dd if=/dev/nvme0n1p2 of=/dev/null bs=30M count=1000

    $ top

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                   
16691 root      20   0   45352  32712   2112 R  50,5  0,2   0:06.00 dd

  
$ awk '{print $14, $15}' /proc/16691/stat 
0 467

$ awk '{print $14, $15}' /proc/16691/stat 
0 512

$ awk '{print $14, $15}' /proc/16691/stat 
0 557

$ awk '{print $14, $15}' /proc/16691/stat 
0 594

$ awk '{print $14, $15}' /proc/16691/stat 
0 630

$ awk '{print $14, $15}' /proc/16691/stat 
0 666

$ awk '{print $14, $15}' /proc/16691/stat 
0 699
So we can say - yes, the process is running under kernel mode.

As for 'D' state:

As for 'D' state (correct me if i'm wrong) - it means the process in a "sleep" state. It is 'uninterruptible sleep' state anyway it is a sleep state , it means the code of the process (user space) or invoked kernel code via syscall IS NOT scheduled on cpu until some necessary data\structure is available. So i suppose the processes in 'D' state should be excluded from the review. Why? Because they just not executed on cpu at all. However there is a subtle moment. The process can switch between 'D' and 'R' states quickly so we can think the process is in 'D' state however it goes from time to time to 'R' state.

Let me give explanation in details: Very often people say that if a process in 'D' state it means it waits some I\O. It is not necessarily so.

Simple program in C.

$ cat 30.c
#include <sys/types.h>
#include <stdio.h>
#include <stdlib.h>
#include <errno.h>
#include <unistd.h>
#include <sys/wait.h>

int main() {

    pid_t pid = vfork();

    if (pid == 0) {
    sleep(180);
    return 0;
    }

      printf("parent: I am exiting\n");

     return(0);
}
Compile.

$ gcc -o 30.exe 30.c
Launch.

$ ./30.exe
It uses vfork that creates a child process. The parent process will be blocked until child exits. Also the state of parent process will be 'D'.

$ ps  aux | grep 30.exe
vasya     6495  0.0  0.0  10700   964 pts/66   D+   03:30   0:00 ./30.exe
vasya     6496  0.0  0.0  10700   964 pts/66   S+   03:30   0:00 ./30.exe
So the parent doesnt do any i\o operations but have 'D' status.

Next - Lets have a look if process with 'D' uses cpu. So lets check if it really sleepls.

$ while true; do cat /proc/6495/stat | awk '{print $3, $14, $15}'; done
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
D 0 0
As we can see it is.

Next. Very often we can see a process is in 'D' state however "top" shows it consumes CPU. How is it possible? The quick answer - the process switches between 'D' and 'R' states. It can happen very quickly. Remember "top" reads all the information from procfs. By default "top" refreshes all the data every 3 seconds so if the process switched very often to 'D' state and not so often to 'R' state it will seem to us as the process lives all the time in 'D' state. However it is false assumption.

The next important point is that the state of the process is a property of instant moment. That is when we are talking about the process has "D" state we mean it is in this state at this particular time. However when we are talking about CPU consumption - it is not about property of instant moment of time. It is average value FOR SOME PERIOD OF TIME. Pls have a look at the picture:

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+   COMMAND
13416 vasya     20   0   24024   5308   2132 D  62,9  0,0   0:05.02    dd  

                                                                                                                  
We see that the state = 'D' %CPU = 62,9

It means that at the moment (right now) the state = 'D' It means right now process does not consume cpu cycles , however some time before the process was NOT in 'D' state and it was consuming cpu cycles. So for the simplicity "top" calculates the average for the last three seconds. It can be like so:

Now - 0%
1 sec ago - 62,9%
2 sec ago - 62,9%
3 sec ago - 62,9%

the average = (62,9%+62,9%+62,9%+0%)/(1+1+1) = 62,9%
So thats why "top" shows 62.9% of cpu usage in spite of the state of the process is 'D'.

To proof that 'dd' switches between 'D' and 'R' states:

$ while true; do cat /proc/13416/stat | awk '{print $3, $14, $15}'; done
R 0 745
R 0 745
D 0 746
D 0 746
D 0 746
D 0 746
D 0 746
D 0 746
R 0 746
R 0 746
R 0 746
R 0 747
R 0 747
R 0 747
R 0 748
D 0 748
D 0 748
D 0 748
D 0 748
R 0 748
R 0 748
R 0 749
D 0 749
R 0 749
R 0 749
D 0 750
D 0 750
D 0 750
D 0 750
D 0 750
D 0 750
R 0 750
R 0 751
R 0 751
R 0 752
R 0 752
R 0 752
D 0 752
D 0 752
D 0 752
D 0 752
D 0 752
R 0 753
R 0 753
D 0 753
D 0 753
D 0 753
D 0 753
R 0 754
R 0 754
R 0 755
R 0 755
D 0 756
D 0 756
About the meaning of $14 and %15 fields: As for "man proc":

$14 = Amount of time that this process has been scheduled in user mode, measured in clock ticks (divide by sysconf(_SC_CLK_TCK)).
$15 = Amount of time that this process has been scheduled in kernel mode, measured in clock ticks (divide by sysconf(_SC_CLK_TCK)).

As you can see "dd" switches between 'R' and 'D' states. Thats why average cpu consumption is not 0%.

Also you can see that while the process really in 'D' state it does not consume cpu cycles neither for user mode nor for kernel mode.

As the final suggestion: if you have a process and want to know if it is running right now in user or kernel space --> start monitoring

$ cat /proc/13416/stat | awk '{print $3, $14, $15}';
If $14 is changing - it means the process in user space, if %15 is changing - it means the process is in kernel space

Hope it helps

Еще добавок про S и D состояния процесса.
Если у процесса состояние S то это означает две вещи: 
1. код программы (непосредственный код программы) шедулером больше неразмещается на цпу. Он физически больше непопадает на цпу.тоесть код процесса реально больше непопдает и неисполняется на цпу. а если процесс сделал сисколл тоесть вошел в ринг0 и код ядра. то тоже самое. и ядерный код неразмещен шедулером на цпу. таким образом ни юзерский
код процесса ни вызыванный код ядра через сисколл - ничего из этого на данный момент неразмещено на цпу. ( на данный
момент потому что статус это характеристика типа instant тоесть вот оно как прям щас)  

2. Плюс статус S означает что процесс принимает сигналы. таким образом например работу
процесса можно прервать прям щас.
что значит процесс принимает сигналы.значит что ядру разрешено
в таблицу процессов данному процессу записать свойство сигнал. а шедулеру разрешено тут же взять 
и запустить обработчик этого сигнала. тоесть по факту это нето что там код процесса чтото решает принимать ему или нет. это один кусок кода ядра разрешает другому куску кода ядра сделать 
изменнеие в таблице процессов для данного процесса. а шедулер увидев что в свойствах процесса
появилось своства сигнал запустити обрабочик сигнала. сам процеесс ничего нерешает.

Сразу скажу про статус 'D' он такой же самый только нет пункта два. тоесть 
нельзя процессу доставлять сигналы. запрещено. таким образом пока процесс имеет статус 'D' никак неповлиять на него через сигналы. (кроме ребута). Когда процесс выйдет из статуса 'D' то сигналы что ему слали (они сохраняются) будут ему наконец "доставлены".


далее. вверху я уже написал  на английском  но еще раз скажу:
если у процесса статус S или D то согласно пункту один код процесса шедулером неразмещается
на цпу и также если проецесс сисколл вызывал то его код тоже неразмещается на цпу. дело в том 
что да если процесс вызывал сисколл то как это работает в паре с шедулером? отображается ли 
работа ядерного кода в статусе процесса или нет. ответ да. когда вызывано ядро через сисколл 
то условно гооворя запускается функция просто она физически размещается не в коде программы
а в отдельных других файлах в которых код ядра (условно говоря либо в вбиблиотеке libc либо в ядре vmlinuz) так вот что там происходит. а ничего особого. цпу перекчается в ринг0. да там немного 
меняютсяс пара регистров , условно говоря ссылка на стек меняется но в целом ядерный код начинает
работать в окружении регистров и вирт памяти которая осталась от процесса. да ядерный код помимо 
вирт памяти процесса также может лазить в память ядра но в целом как пишут книжки ядерный код
работает в окружении процесса. и вот далее важно. один момент это то что вот на цпу работает ядерный код. далее происходит таймер интеррапт и цпу прерывает работу этого ядерного кода 
без проблем ему похер ядерный код или юзерский. далее появляется шедулер. и он как то понимает 
что этот ядерный код он относится к процессу и он во первых выдавливает этот код с цпу, сохраняет
все что нужно в свойствах процесса гдето в памяти причем ссылка на команду идет именно на 
ядернй код там где была остановка из за интерапта и шедулер обновляет статистику толтко что заюзанного  тайм слайса со стороны ядерного кода в статистику работы именно процесса.
так что мы имеем два момента , один это то что цпу когда ядерный код выполняет то его можно прервать
и второе что шедулер то время которое ядерный код работал по вызыву процесса зачисляет в статистику
самого процесса в procfs так что мы четко можем быть уверены что время работы ядерного кода на цпу
будет отражено именно в статиске процесса а значит мы увидим в top работу ядерного кода для процесса
в статистике процесса в плане cpu usage. иначе получалась дурацкая ситуация ядерный код по запросу
нашего процесса работает но мы невидим эту статистику в свойствах процесса и думали бы что процесс
прохлаждается . неважно в рамках нашего процесса код из нашей программы крутится на цпу и внешняя функция из ядра - по факту это все ресурсы запользованные нашим процессом так что статистика
должны отражаться в свойствах процесса. потмоу что было непонятно вот заустил процесс сисколл 
цпу перешел в режим ядра и где искать сколько cpu usage сейчас жрет этот сисколл. оказывается 
искать ненадо. его cpu usage засунут в свойства процесса как бутто это щас код из процесса 
и крутится на цпу. получается если мы в top видим что у процесса есть какйото cpu usage то 
мы неможем навскидку сказать это время когда цпу работал в юзер моде или в кернел моде 
потому что там указана их сумма. ( конечно надо понимать что в рамках процесса цпу в каждый момент времени работает либо в юзер моде либо в кернел моде но неодновременно).

также еще ращ скажу важно понимать что характеристика статус процесса типа "S"\"R"\"D"
это характиристика в конкретной точке времени. тоесть скажем прям щас. 
втоже время cpu usage в рамках команды top (потому что в procfs там нет этого параметра в чистом виде) это характеристика не про момент времени а за некоторый пропмежуток времени средняя величина.
Поэтому если мы видим что статус процесса S\D но в тоже время его cpu usage неравен нулю тут 
нет никакого противоречия потому что S\D статус относится к тому как это щас у процесса а его
cpu usage относится к промежутку времени между три секунды назад и щас. (три секунды это дефолтовое
время усреднения со стороны top). тоесть это значит что в моменты времени до нашего текущего
у процесса было переключение на статус R (это 100% и процесс жрал цпу. жрал это значт что он занимал кванты времени находясь на цпу своим кодом). вот поэтому и нет противорречия. да если щас статус S то прям щас процесс использует цпу на 0%. но еще раз top непоказывает на сколько % процесс
исполует цпу прям щас . потому что на самом деле нет такой характеристики на сколько % процесс занимает цпу прям щас. потому что прям щас процесс либо выполняется на цпу тоесть он его занял
либо он невыполняется. но процентов никаких нет в этом плане в природе. процент получается исходя
из совершенно другого если унас есть три промеждутка времени. ( это то как это работает со стороны шедулера) и скажем в первый промежуток у нас код находился на цпу на втором он ненаходился и на третьем он анаходидся то это значит что с точки зрения времени сколько процесс находился на цпу это будет 100%*(1+0+1)/3 =67% тоесть показатель cpu usage это показатель сколько времени процесс находился за заданный проемежуток времени на цпу ане то что прям сейчас цпу работат на 67% своей мощности. у цпу нет никакой мощности он либо работает щас на 100% либо нет. просто кажому процессу
дается небольшой квант времени. и cpu usage это отношение сколько таких квантов процесс находился на
цпу к общему числу квантов времени. поэтому никакого парадокса в top нет. вот как в этом примере

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+   COMMAND
 2185 vasya     20   0 3181856 282088  50380 S  25,9  1,7   377:25.95 cinnamon 

статус S это то как щас а 25.9%CPU это в среднем за последние 3 секунды.
почуствуй разницу между "щас" и "за последние три секунды"

еще подчеркну что cpu usage в точке щас такого понятия вобщемто нет. точнее оно есть
но оно неудобное. процессор щас либо занят CPU usage его 100% либо незанят и тогда 0%.
это нам ничего недает.

далее я еще раз  обсуждаю часть то что сказал выше  а часть нового:
также суперванжно добавить что процесс себя сам выдавить из цпу неможет. самс себя убрать из цпу
процесс неможет. нет ассемблерной команды которая бы выдавила процесс из цпу. вообще что значит
выдавить процесс из цпу. это значит что на какое то время код процесса небудет размещаться на цпу. а ведь только в этом случае процесс будет иметь статус S\D.
это может сделать толко шедулер ибо он в очередной квант времени сует код код процесса на цпу
а код других процессов несует.в юзерспейсе нет такой команды асемблера которая бы обратилась
к шедулеру и сказала мол все я закончила давай убирай меня из цпу. (этот факт что программа сама добровольно обратилась  к шедулеру и сказала мол я закончила раньше времени либо я буду ожидать наступление некоего события поэтому можешь пока выдавливать меня называется voluntary_ctxt_switches
тоесть это процесс сообщил добровольно шедулеру что он может выдавливать этот проецсс из цпу на какоето время. так вот я утверждаю что из кода юзерской программы это невозможно сделать, тоесть 
самому себя выдавить из цпу невозможно а даже если бы это было возможно то об этом бы незнал шедулер , шедулер бы считал что процесс отработал на цпу полный квант времени и статистика по процессу в procfs была все равно такая что как бутто он отработал полный квант в любом случае
сам код программы это сделать неможет работая в юзер спейсе он это может сделать только вызвав
ядерный код через сисколл. я несмотрел код сисколлов но я точно уверен что добровольная отдача обратно цпу а другими словами запрос к шедулеру о том чтобы он выдавил процесс из цпу и вставил другой находится в сисколлах. самый простой сисколл который может это сделать это nanosleep(), 
если в баше то это /bin/sleep, если через асемблер запускать то это сискол номер 35, если пишем на
C то там это вызывается через libc обертку sleep(), остальные сисколлы они там сами решают
когда они хотят сделать запрос к шедулеру чтобы он выдавил процесс из цпу.  еще раз напомню 
что добровольная отдача цпу что по факту никакая неотдача а запрос к шедулеру чтобы он выдавил
процесс из цпу назвыается voluntary_ctxt_switches. так вот я еще раз скажу что я считаю что 
это может сделать только сисколл. делает он это неспрашивая нас когда мы его вызываем например
наша програма  ждет нажатия кнопки для этого мы вызываем сисколл который этим занимается, сисколл
выставляет условие в ядро о том когда процесс надо будить и сисколл просит у шедулера выдавить
процесс из цпу и небудить до момента наступления нажатия кнопки. так и только так происходит voluntary_ctxt_switches . из кода программы юзерского мы так сделать неможем никак а может только 
вызывав сисколл. причем сисколл nanosleep() через него мы в явном виде заказываем выдаволение процесса с цпу на заданное время , другие же сисколлы сами определяют когда на какое время или до наступления какого условия выдавить процесс из цпу и делают запрос к шедулеру тоесть вызывася эти
сисколлы мы понятия неимеем как когда зачем и почему данный сисколл закажет выдавливание ( ака добровольный конекст свич) потому что мы вызываем эти сисколлы для других целей например прими нажатие от кнопки. цель этого длинного куска была втом чтобы обьяснить как на практике и откуда
получается добровльный контекст свич. оценить как часто сисколлы нашей программы заказывали
добровольное выдавливание можно через:

	$ cat /proc/$$/status | grep switch
	voluntary_ctxt_switches:	172
	nonvoluntary_ctxt_switches:	5

соответвенно nonvoluntary_ctxt_switches - это недобровольное выдавливание процесса из цпу 
со стороны шедулера. когда это происходит - тогда когда программа работает но ее тайм слайс 
так уж вышло закончился. тогда шедулер принудительно выдавилвает процесс из цпу. я бы еще добавил 
так что если процесс все время нахрится в состоянии 'R' то у него конечно будут только сплощные
nonvoluntary_ctxt_switches потому что его нужно будет постоянно выдавиливать. а если процесс
слабо нагруженный который часто в "S" то у него буду преимушественно voluntary_ctxt_switches.
оно ипонятно процесс сам по себе в состояние S перейти никак неможет, для этого он обязательнро 
должен вызывать сисколл в котоом наверняка будет выдавилвака процесса из цпу , слабонагруженность
это прямое следствие статуса S и выдавиливалки в сисколле том или ином.

посмотрим для примера статистику по процессам:

$ cat /proc/1/status | grep switch
voluntary_ctxt_switches:	  34780
nonvoluntary_ctxt_switches:	 1194

логично.

а тепер вот такую программу возьмем

$ cat 24.c
#include <stdio.h>
#include<unistd.h> 


int main () {

   /* local variable definition */
   pid_t pid;
   pid = getpid();
   printf ("pid = %d\n",pid);

   int a = 10;
   /* while loop execution */
   while( a < 20 ) {
   }

   return 0;
}

тоесть по факту это просто пустой бесконечный цикл.

компирлируем
$ gcc -o 24.exe 24.c

и сразу для интереса дизасемблиуриуем:
$ objdump -b elf64-x86-64 -M intel  -d 24.exe

000000000000068a <main>:
 68a:	55                   	push   rbp
 68b:	48 89 e5             	mov    rbp,rsp
 68e:	48 83 ec 10          	sub    rsp,0x10
 692:	e8 b9 fe ff ff       	call   550 <getpid@plt>
 697:	89 45 f8             	mov    DWORD PTR [rbp-0x8],eax
 69a:	8b 45 f8             	mov    eax,DWORD PTR [rbp-0x8]
 69d:	89 c6                	mov    esi,eax
 69f:	48 8d 3d ae 00 00 00 	lea    rdi,[rip+0xae]        # 754 <_IO_stdin_used+0x4>
 6a6:	b8 00 00 00 00       	mov    eax,0x0
 6ab:	e8 b0 fe ff ff       	call   560 <printf@plt>
 6b0:	c7 45 fc 0a 00 00 00 	mov    DWORD PTR [rbp-0x4],0xa
 6b7:	83 7d fc 13          	cmp    DWORD PTR [rbp-0x4],0x13
 6bb:	7e fa                	jle    6b7 <main+0x2d>
 6bd:	b8 00 00 00 00       	mov    eax,0x0
 6c2:	c9                   	leave  
 6c3:	c3                   	ret    
 6c4:	66 2e 0f 1f 84 00 00 	nop    WORD PTR cs:[rax+rax*1+0x0]
 6cb:	00 00 00 
 6ce:	66 90                	xchg   ax,ax


на самом деле цикл while 
это вот эти две команды которые будут крутится вечно

 6b7:	83 7d fc 13          	cmp    DWORD PTR [rbp-0x4],0x13
 6bb:	7e fa                	jle    6b7 <main+0x2d>

ну это чисто так для интереса мы дизасмлировали чтобы посмотрть как С програма будет
выглядит на асемблере.

так вот такая программа неимеет в своем цикле никаких сисколлов так что
эта программа хочет крутится на цпу все время (потому что без вызоыва сисколла программа 
претендует вечно занимать цпу ровно за этим и нужен шедулер), соттвенвенно статус у процесса будет R
все время и загрузка по цпу 100% птому что программа будет стремиться занимать все таймслоты что ей даст шедулер а шедулер будет вынужден выдалавить процесс хотя бы порой. вот щас это и проверим

запускаем
$ ./24.exe
pid = 20260


  PID USER      PR  NI    VIRT    RES    SHR     S  %CPU     %MEM     TIME+ COMMAND                
20260 vasya     20   0   10832    968    868     R  100,0     0,0   0:23.64 24.exe  


$ while true ; do cat /proc/20260/status | grep switch; sleep 1; done
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	73
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	73
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	74
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	74
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	74
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	74
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	76
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	76
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	76
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	76
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	77
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	77
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	77
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	77
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	79

видно что у программы нет ниодного добровольного отдачи цпу потому что да его и нет 
в коде програмы потому что у нас в цикле нет ниодного сисколла. так что щедулер 
исключтельно принудительно nonvoluntary выдавливает процесс из цпу.
цикл снятия статистики выдает ее раз в секунду ( а мы помним что щедулер срабаывает примерно
100 раз в секунду) так что из статистики видно что шедулер далеко некаждый раз выдавливает 
нащ процесс из цпу (иначе бы за секунду он это делал 100 раз а мы видим что за две секунды бывает
что и ни разу), так что программа резвится на цпу очень даже хорошо , лишь иногда шедулер
все таки выдавивет ее из цпу. отсюда мы делаем три вывода: 1) шедулер далеко не каждый тайм слайс (10мс) выдавливает процес из цпу, 2) если в коде программы нет сисколла ( у нас конечно есть в программе сисколлы но вне цикла while тоесть до них очередь недошла) то ни о каком добровольной 
отдаче цпу и речи неидет для этого обязательно нужно вызывать сисколл в коде программы. 3) даже если в системе есть свободные ресурсы то все равно время от времени шедулер выкидывает процесс
из цпу это значит что система недаст процессу 100% времени на цпу иногда всеже она будет
его на немного выдавливать. опять же незабываем что интеррапты приходят на цпу и немало и 
их обработка она же тоже жрет время цпу так что даже на многопроцессорной системе часть мощности
цпу будет вседа уходит по крайней мере на обработку интераптов( я так посмотрел в top цифра si и она доходит порой до 4%). так что будем ссчитать что в лучшем случае процесс может сидеть на цпу на уровне 96%.




пример с nanosleep
$ cat 37.c
#include<stdio.h>
#include<time.h>


int main()
{

     const struct timespec kuku = {300, 100};
     struct timespec kuku2 = {3, 100};

     nanosleep(&kuku, &kuku2);

    return 0;
}


через этот сисколл мы просим шедулер выдавить из цпу наш процесс на 300 секунд и еще 100мс
компилируем запускаем:
$ gcc -o 37.exe 37.c
$ ./37.exe

дальше интерсно я подождал скажем минуту а дальше смотрю статистику
$ cat /proc/24455/stat | awk '{print $3, $14, $15}'
S 0 0

про $14 и $15  я уточню интересное это сколько цифры . первая про юзер спейс вторая про кернел спейс. сколько времени процесс работал в юзер спейсе и в кернел спейсе тоесть сколько времени
процесс выполнял юзерский код а сколько времени проецесс выполнял сисколлы. так вот интересно тут
то что в каких единицах времени это меряется. в man proc написано что цифры обозначают clock tics
сразу скажу что это не цпу клоки и все такое. 1 clock tic это время между вызовами шедулера это 
один тайм слот. вот $14 и $15 показывают число тайм слотов в штуках которые процесс провел на цпу.
далее они предлагают число этих штук этих слотов поделить на sysconf(_SC_CLK_TCK) по дефолту
оно равно 100. тоесть 100 раз в сеунду срабывает таймер интеррапт и вызвается шедулер .таким 
образом если мы поделим тайм слоты на 100 то узнаем в секундах ( а в не в штуках слотов) процесс
прокрутился на цпу. 

  на мой взгляд поэтому немного странно получаенные цифры S 0 0. что как бутто процесс
  непровел на цпу ни единого тайм слота. хотя.. может быть %14 и %15 округляет до целых ведь процесс может провести на цпу толко часть тайм слота а потом попросить щедулер его выдавить.
  тайм слот очень большой целых 10мс. за это время туча кода может отработать. а наша программа
  мелкая и вся заточена под скорейшее выдавливание из цпу. так что может 0+0 это и реальная цифра 
  тоесть программа провел на цпу какоето время но существенно меньшее чем даже 1 тайм слот 10мс.


 теперь посмотрим на 

 $ cat /proc/24455/status | grep switch
voluntary_ctxt_switches:	1
nonvoluntary_ctxt_switches:	0

это было снято через минуту уже как программа была запущена.
тут мы видим что процесс ощутил на себе всего одно добровольное выдавливание из цпу.
и больше ни одного выдавливания небыло. это показывает то что цисло эти switch оно обозначает
сколько раз реально из цпу код процессы был выкинут. это значит что такое минимальное количество
раз код точно побывал на цпу и его выкинули. это незначит что ровно такое число процесс был
на цпу. число может быть гораздо больше скажем для постоянно работющего процесса. как я показал
выше шедулер постоянно рабоатающий проецесс при налиичии ресурсов выдавливает далеко некаждый 
цикл. так что сколько секунд или циклов (которые можно пересчитать в циклы умножив на sysconf(_SC_CLK_TCK) который обычно равен 100) процесс провел на цпу можно посчитать заглянув в 
$14 и $15. а в  voluntary_ctxt_switches и nonvoluntary_ctxt_switches указано реально 
сколько раз код был выкинут,выдавлен из цпу.

вот в нашем конкретном примере четко можно увидеть что код залетел на цпу был выкинут 
и больше код на цпу недопускался.

получается возьмум pid=1
$ cat /proc/1/stat  | awk '{print $14, $15}'
999 1390

получается на цпу он отработал всего (999+1390).100~24c при том что аптайм 12 дней. но тут нет 
парадокса потому что $14 и $15 это же то суммарное время когда код процесса реально работал на
цпу а процесс init он же постоянно спит 'S' так что все нормально. указано чистое время
сколько код процесса реально крутился на цпу. аптайм здесь ни при чем.

а вот если мы возьмем процесс который постоянно раотает статус R у него $14+$15 должны очень
точно совпдаать с временем как мы его запустили (при наличии ресурсов у компа то есть процесс
дейсвительно должен потоянго быть R )


запустим такую программу которая просто цикл
#include <stdio.h>
#include<unistd.h> 


int main () {

   /* local variable definition */
   pid_t pid;
   pid = getpid();
   printf ("pid = %d\n",pid);

   int a = 10;
   /* while loop execution */
   while( a < 20 ) {
   }

   return 0;
}


я запустил а далее

$ cat /proc/25506/stat | awk '{print $14, $15}'
1462 0

получаем что программа крутится на цпу ~15c и да это похоже на правду (засекал с секундомером).

а вот еще чуть позже  я посмотрел статистику для этого процесса
$ cat /proc/25506/stat | awk '{print $14, $15}'
10882 2
$ cat /proc/25506/status | grep swi
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	1246

получается что процесс отработал на цпу ~11c
из них он недобровольно (оно и понятно почему ) был выдавлен с цпу 1246 раз.

это значит что он неработал после этого хотя бы один цикл цпу прежде чем быть назначен обратно
это значит 1246\100 = 1.2с 

ну тоесть процесс отработал на цпу (10882+2) циклов а его выдавили из цпу 1246 раз. тут
конено возникает вопрос сколько времени занимает процесс выдавливания процесса из цпу
потом сколько времени процесс ждал пока его обратно засунуть на цпу потом время которое нужно
чтобы его всякие там регистры засунуть в цпу  это все потеря времени. ну если например предпололо
жить что что каждый раз когда его выдавили из цпу то весь процесс обратного его ожидания и сования
на цпу составил всего один цикл (хотя может и быстрее а может и медленее) тогда получается что 
с того момента как процесс запустили он простаивал 10% времени.

так в /proc/$$/stat  в поле $39  показывается номер ядра на котором раобтает процесс.
я запустил процесс и статистику снова. получил такое. 

~$ while true; do cat /proc/25919/stat | awk '{print $14, $15, $39}'; sleep 1; done
3301 0 1
3401 0 1
3502 0 1
3602 0 1
3703 0 1
3803 0 1
...
8730 0 1
8830 0 1

$ cat /proc/25919/status | grep swi
voluntary_ctxt_switches:	0
nonvoluntary_ctxt_switches:	28


тоест процесс выдавливался шедулером 28 раз недобровольно.
но небыло смены цпу все время процесс работал на ядре номер 1.
получется процесс отработал на цпу ~8.8с и был выдавлен 28 раз.
если процесс выдавливания и обратгного размещеия считать занимал 1 цикл (что нефакт что это так по времени может меньше может больше) то получается что (28\8830) ~= 3% процесс простаивал. 
интересно.


далее будет частичное повторение уже сказанного и часть нового. 
про ассемблерную команду nop. которая ничего недалает. влияет ли ее применение на уменьшение  
cpu usage в выводе top. короткий ответ - нет нихрена невлияет вообще.
 если мы в программе разместим цикл с асемблерной
командой NOP которая по факту внутри цпу ничего неделает это нам ничего недаст потому что 
по факту цпу ничего неделает но с точки зрения шедулера код процесса все равно исполняется на цпу
формально команда nop это тоже команда. код неисполняется значит то что на цпу работает код от 
другого процесса а нето что на цпу выполняется NOP от данного процесса. 
как шедулер высчиывает цпу юзэж процесса он считает циклы цпу в течение которого код выполняется
на цпу (можноу ивдеть в procfs) , далее как top высиытает cpu uxsage  - он берет некий интервал. 
смотрит солько это в тиках цпу. а потом делит сколько тиков процесс сидел на цпу на число тиков на 
инетрвале. получем cpu usage %. здесь важно быол понять что из юзерсепейса процесс сам себя в состояние S перевести неможет так как для этого он должен удалить сам себя на какоето время 
из цпу.  насколько я понимаю дажебы если бы процесс мог такое сделать то об этом бы незнало ядро.
тоест скажем процесс некоей конмандй остановил свое выоплолнение на какое то время до момента
насутлпения таймер интеррапт на цпу. но при этом процесс его регистрвы все равно занимают
цпу а сам процесс неможет себя вычистить из цпу. но тем неменее на каком то интервале до настулпния
таймер интеррапт он бы невыполнялся но фишка в том что ядро и щедулер об этом бы незнали 
и  с их точки зрения процесс сидел на цпу все кванты времени поэтому перменые статистики в /pric/pid/ все равно бы были такие что как бутт процесс выполнылся на цпу. поэтмоу такое невозоможно 
через юзер код. а вопзомоно только если попрпоситьоб этом шедулер. тоесть нужно передать управление
шедулеру с прсобой вычистить регистры от этого процесса и загрузить туда другой и обновить
параметры статистикии  в процфс вот только тоггда для ядра процесс рельно неисоплнется на цпу
и его статус будет S. поэтому процесс состоящий из одиеих команд NOP будет выглядеть в 
top как процесс который на 100% жрет цпу. хотя казалось бы он ничего неделаеи. но это он 
неделает по факту. но с точки зреня шедулера на цпу лежат регистры именно нашео процесса,
выполняется порядок команда нашего процесса и другого процесса на цпу нележит в этот квант времени прэтому с тчоки зерения шедулера цпу занят нащим процессом. важно было разсянить этот экспримент.



двигаем дальше перехоим к рассмотрению переменной /proc/21944/syscall.
так как уже было сказано что если статус процесса R тоесть это означает 
что прям щас на цпу выполняется либо код программы (user space) либо код ядра вызыванный
через сисколл (кернел спейс) то нам непонятно нам хочется знать так щас что выполняется
код программы или кернел код. как это узнать. 

узнать это можно если посмотреть что покажет proc/$$/syscall
$ sudo cat /proc/21944/syscall

если там будет слово
running

это значит что на данный момент выполняется юзерский код программы.

strace при этом покажет вот это:
$ sudo strace -p 19356
strace: Process 19356 attached
strace: [ Process PID=19356 runs in x32 mode. ]


а если там будет вот чтото такое
270 0x1 0x7ffce82706c0 0x0 0x0 0x0 0x7ffce82705f0 0x7ffce82705b0 0x7f24978ceec9

тоэто значит что сейчас на цпу выполняется ядерный код вызыванный через сисколл

270 это номер сисколла котоый щас выполняется.
270 это pselect6 (согласно /usr/src/linux-headers-5.4.0-91-generic/arch/x86/include/generated/uapi/asm/unistd_64.h) 

также убедиться в этом же можно если присодиниться к процессу через strace

$ sudo strace -p $PID
pselect6(1, [0], NULL, NULL, NULL, {[], 8}

единственное что мы можем неуспеть то есть пока мы подкючимся процесс будет уже выполнять какото другой код.


а что покажет /proc/21944/syscall если у нас процесс спящий тоесть его статус 'S'\'D'.
отвечаю что он покажет номер сисколл который значит то что до того как процесс впал в спячку
цпу был занят выполнением этого сисколла. более того я утрвеждаю что именно этот сисколл и был
тем кодом который попросил шедулер вогнать процесс в спячку. я там далее ниже показал это 
на примере запущенной команды $ sleep 180

еще хочу добавить про /proc/pid/wchan вроде бы он вцелом показыает тоже самое что и /proc/pid/syscall но я читал  в инете что этот wchan порой показыает хрень так что на него лучше неполагаться.

итак подвеем итоги по /proc/pid/syscall.
если статус процесса "R" и цпу выполняет юзерский код то /proc/pid/syscall покажет "running"
если статус процесса "R" и цпу выполняет кернел код то /proc/pid/syscall покажет номер сискола
который щас исполняется
если статус процесса "S"/"D" то /proc/pid/syscall покажет номер сисколла который выполнялся на цпу
до того как процесс начал спать, более того это имено тот сисколл который попросил шедулер вогнать
процесс в сон, и конечно на данный момент этот сисколл невыполняется на цпу.


далее повтор того что уже сказано и возможно часть нового.
интересенйшая вещь. команда asm nop хотя она по факту ничего неделает на цпу но процес в цикле 
с этой командой будет показывать статуст R 100%. потому что cpu usage в top\proc происходит с той
точки зрения выполнялся ли код на цпу. а такак nop это все равно команда то с точки зрения шедулера
или другими словами ядра - код выполнялся, поэтому небудет нкиких 0% cpu usage. 0% cpu usage возможно толко 
тогда когда код процесса физиечески шедулером неразмещается на цпу. поэтому 
я скажу так что переменстить процсс в состояние S невомозможно без вызовы хоть какого то
сисколла в проетсейшем случае это sleep. нужн какойто ядерный код который соощит шедулеру о том 
что код этого роцеса ненадо пихать на цпу столко то времени либо до настулпнеия такого то события.
опа!!! поэтому можно быть 100% увреным что у всех проецссов в состоянии S в его /proc/$$/syscall 
стоит имя какого нибуль сисколла! именно этот сисколл и сунул процесс в состояние sleep говоря другими словами попросил щедулер исключить процесс из цпу либо на промежуток времени либо до наступления какогто события.

сделаем пример. команда sleep

$ sleep 180
процесс будет в состоянии S в течение 180 секунд.
посмотрим через strace какой сисколл щас она покажет
$ strace ...
...
nanosleep({tv_sec=180, tv_nsec=0}, 

посмотрим какой сисколл указан в /proc/$pid/syscall 
$ sudo cat /proc/29719/syscall
35 0x7fff0baec2f0 0x0 0x0 0x0 0x0 0x0 0x7fff0baec2e8 0x7fba62d9e774

номер 35. 
найдем имя этого сисколла
$ cat /usr/src/linux-headers-5.4.0-91-generic/arch/x86/include/generated/uapi/asm/unistd_64.h | grep 35
#define __NR_nanosleep 35

совпадает с тем что показывает strace.
Получается что имя сисколла который показывает strace и который показывает /proc/$pid/syscall
если статус процесса 'S'\'D' то это не имя сисколла котрый щас выполняется на цпу . нет. это 
имя сисколла который выполнялся последним до того как процесс вошел в состояние 'S'\'D'.
по факту я утверждаю что именно этот сисколл и вогнял процесс в состояние 'S'\'D'.
существенно тут то что сисколл щас невыполняется. нет. он выполнялся до того как процесс вошел в
спящее состояние но щас он невыпоняется. также можно смело утверждать что именно выполнялся сисколл
а не чтото иное до того как процесс вошел в спящее состояние. и еще раз именно указанный сисколл
и вогнал процесс в спящее состояние. естственно не только сисколл 35 умеет вгонять процесс в спящее
состояние. таких сисколлов вагон. важно тут было понять то что значит имя сисколла который мы видим
в /proc/$pid/syscall если статус процесса 'S'\'D'. еще раз смысл найденного сисколла в том что это 
было последнее что выполнял цпу для данного процесса прежде чем процеесс вошел в спящее состояние.
сейчас код процесса ни юзерский ни сисколловый на цпу невыполняется , оно все вытеснено. и то что
именно этот сискоолл и вогнал процесс в спящее состояние.



далее опять повторение того что выше и может быть чуть нового.
nop только на powerconsumption влияет а на статус S и на цпу юзадж
с точки щрения шедулера никак! ведь ядро как определяет cpu usage - по числу таймслайсов в теччение которых код выполнялся\размещался (что одно и тоже) на цпу. пусть даже эта команды nop. nop невозвращает управление шедулеру ядру ОС. это такая же обычная команда как и любая другая. 
просто по факту цпу в это время ничего неделает. но с точки зрения шедулера это абсолютно неважно! вот это я хочу донести. шедулер меряет произвиодительность cpu usage для процессора исходя неиз того какие команды запускает процесс. на это шедулеру насрасть. шедулер меряет количество времени
в течение которого на цпу размещается любой код процесса. вот шедулер поместил код процесса на цпу
на 1 тайм слайс. процесс это время отработал. шедулер вытесняет код процесса (иногда оставляет но это щас неважно для простоты щас считаем что он каждый раз код вытесняет) и ставит себе галочку в 
/proc/pid/stat о том что процесс отработал на цпу +1 тайм слайс. а команда top читает это количество и делит его на три секунды (дефолтовый промежуток обнволения экрана у top). и таким 
макаром в top появляется cpu usage. тоесть в cpu usage играют рояль только тайм слайсы в течение
которых код процесса крутился на цпу. и шедулеру и top абсолютно насрать какой код крутился в те моменты будь это mov или nop или любое другое. поэтому применение циклов с nop никоим образом
неуменьшаем cpu usage для процесса. оно только уменьшает powerconsumption у цпу на которйы нам полностью насрать. единсвенйы вариант уменьшить цпу юзадж это когда наш процесс запустил команду
syscall и начад исполняться ядерный код и этот код сам решает на момент работы в таймслайсе что
уже ему сисколлу ненужен больше цпу либо на какоето время либо до момента наступлениея какогто
события и тогда сисколл обращается к шедулеру и просит его выдавить процесс из остатка таймслайса.
и вот только тогда и только по такой причине процесс на скольо то таймслайсов небудет размещаться 
шедулером на цпу. получется что на какоето время на какоето количество таймслайсов процесс больше
не размещается на цпу. он физически больше неисполняется. и это дает то что в ячейке отображения
статистики в /proc/pid/stat останавливаются поля $14 и $15 и только тогда у нас с точки зрения
top падает cpu usage. потому что если за 3 секунды у нас ненаступило приращение полей 14 и 15
то значит что 
100*[ 0/(100*3) ] = 0%  cpu usage за последние 3 секунды
где 
0 = дельта изменеия $14+$15
100 = число таймслайсов в секунду (сколько раз в секунду вызывается шедулер)
3 = интервал в секундах обновления экрана в top по дефолту

то есть мы делим количество таймслайсов которые процесс отработал за последнеи три секунды
и делим на общее число таймслайсов которые прошли за это время. это и есть как top высчитывает 
колонку %cpu usage. если процесс многотредовый то наверное в /proc/pid/stat поля $14 и $15
отображают суммарную статистику по таймслайсам для всех тредов. отсюда и получает в top цифры такие
как 200% , 400% , 1000% итд.


то как я щас себе это вижу. как процсс может попать на S состтояние.  через юзер спейс команды никак. никак сам себя процесс неможет выдавить из очереди на выполпнение шедулера. чобы это сделать
надо вызывать какойто сисколл который сформирует условие для шедулера\ядра о том что мол пока что 
меня ты с очереди выкинь то наступления некоего события. и только тошгда процесс будет шедудером выкинут из очереди на выполеннеи!

разница S и D тока в том что S разрешает прилет сигнала к процессу. а D нераазрешеает.
а так это оба процесса код которых неразмещен на цпу.

еще раз про voluntary_ctxt_switches, nonvoluntary_ctxt_switches.
оно показывает сколько прям раз код процесса находясь на цпу был выдавлен из цпу , либо добровольно
тоесть когда сисколл сам попросил об шедулер либо принудительно когда у процесса истек таймслайс
сработал таймер интеррапт запустился щедулер на этом цпу и он выдавил этот процесс из цпу.
важно здесь еще раз отметить что это не цисло сколько раз цпу проходясь по очереди runqueue доходил
до этого процесса и проверял мол ставить его на цпу или нет. это совсем нетак. во первых процесс
удаляется из runqueu а во вторых как уже сказал это неколичетсво проверок а реально число случаев
когда код процесса сидел на цпу и он был выгнан выдавлен вычищен из цпу.

на счет шедулера важнйы момент важный вопрос а как он работает в случае когда у нас много цпу в 
системе. вот на одном цпу сработал таймер интеррапт значит на этом цпу будет загружен шедулер 
и запущен. и понятно что на этом цпу шедулер сможет выдавить текущий процесс а что в это время
с другими ядрами? туда тоже шедулер в этот момент загружен или нет или щедулер на этом цпу 
сидя выдавливает процессы с других ядер . нет походу это все нетак. как я щас понимаю
когда сработал таймер интеррапт на текущем ядре и был загружен шедулер на текущее ядро 
то этот шедулер выдавливает процесс с текущего ядра только а другие ядра нетрогаются да и как
он их сможет тронуть если на тех ядрах произошел такойже таймер интеррапт. а это нет!
так что шедулер загружается на данное ядра. выдавливает процесс на данном ядре а на остальных 
ядрах работа в это время идет без изменений. потом на другом ядре происходит таймер 
интеррапт и на том другом ядре происходит индивидуальное выдавливание процесса. и так для каждого
ядра индививидуально согласно его индивидуальному срабатывания таймер интеррапт. других ядер 
это некасается. вот так я думаю работает система шедулинга в случае много ядерной машины.

==

bash, strace, 
fork, clone


поповоду того что баш если запускает команду из файла то он вначале себя 
форкает точнее испольуется сисколл clone() потом запускает execve.
убедимся на примере:

во первых если мы пишем

$ strace ls

то мы неувидим как баш делает fork ( при том что щас как я прочитал линукс неделает форк а
делает вместо него clone) но в любом случае мы это неувидим потому что как я понял strace создает 
из себя грубо говоря клон и в нем запускает уже коману и уже смотрит какие сисколы эта комнда будет
вызывать. а нам же нужно затрейсить наш баш в котором мы сидим  а не сам ls. поэтому нам надо 
отркыть второе окно и в нем запустить strace самого баша тот который  в первом окне через команду

$ sudo strace -f -p 26294   (где 26294 это pid баша из первого окна)
также можно сразу указать какие сисколлы мы хотим отслеживать

$ sudo strace -e clone,fork,execve -f -p 26294

так вот что меня удивляет.
у баш есть встроенные команды тоесть которые у него в его бинарнике /bin/bash зашиты
получается что если мы их будем запускать то башу форкать ничего ненадо. проверяем на while ибо он 
являетс builtin для баша. он встроен в бинарник баша.

(окно1) $ while :;do :; done
(окно2) $ здесь увидим пару ioctl и пару rt_sigaction

окей. все понятно все красиво. все совпадает.

а теперь пробуем команду echo

с одной стороны баш пишет что это его встроеный builtin

$ type echo
echo is a shell builtin

с другой стороны 

$ which echo
/bin/echo

и также через strace видно что запускается некий внешний файл /bin/echo :

$ strace echo $$
clone(..
execve("/bin/echo", ["echo", "26294"], 0x7ffcd2aa8898 /* 70 vars */) = 0

тоесть получается по дефолту баш при команде echo испольщует несвой билтин а внешнюю команду.
потом я приказал башу в явном использовать встроенный echo

(окно1) $ builtin echo $$
и тогда уже все отработало нормльно без clone и execve.

как заставить баш по дфеолту для команды echo исполтзовать свой билтин непонятно.

а вот уже как выглядит запуск ls который уже точно невстроенный

(окно1) $ ls

(окно2) 
$ clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fb85ece9490) = 28795
strace: Process 28795 attached
[pid 28795] execve("/bin/ls", ["ls", "--color=auto"], 0x558216669510 /* 71 vars */) = 0
[pid 28795] +++ exited with 0 +++

здесь мы четко видим что при старте внешней команды баш иполтьзует clone+execve

=======

cpu 
power consumption

задача. надо получить сколько ватт жрет цпу.
поможет прога turbostat из пакета linux-common-tools.

$ sudo turbostat --quiet --show PkgWatt
PkgWatt
1.23
1.22

==========
blocked process
наконец нашел что же такое blocked process оказывается = A runnable process is one
that is not blocked; a blocked process is one that is sleeping, waiting for I/O from the
kernel. 
тоесть blocked процесс это процесс в состоянии 'D'

=========
OR и biwise OR (Bitwise inclusive or)

обыкновенный OR (обозначается || ) этот тот у которого на выходые true или false
пример:

2<5 || 5<1 = true

bitwise or (обозначается | ) = это логический OR только для битов, когда мы берем
один байт и второй байт и делаем побитовый OR. резулттатом является набор битов.

9 | 5 =  15

1010
0101
-----
1111
========
processes kill parent child

что было интересно.

вот есть процесс. этот процесс порождает другой процесс.
первый это парент второй это чайлд.

далее что будет если послать kill -15 паренту?  умрет ли автоматом при этом чайлд?

что я высснил на данный момент.
если написать самопимную программу на C которая делает fork() 
то kill -15 парента убивает парент но чайлд спокойно остатется жить дальше.

а вот если в качестве подопытной программы взять bash то тут (видимо в нем прописан
кастомный обработчик сигналов) все идет подругому. посылка kill -15 к паренту неприводит
ни к чему. парент продолжает спокойно жить. я так думаю что в баш прописан кастомный хендлер
для сигнала 15 в котором написано что если у баш есть чилдрены то тогда сигнал 15 парент должен
игнорировать. а вот если паренту послать сигнал -9 то тогда умирает и парен и чайлд.

вот так интересные результаты пока из эксперимнтов
=========
scheduler
run vs runnable

у нас в ps есть статус 'R'
про него написано что это состояние обозначает сразу два типа процессов - run и runnable.
тоесть это значит что либо процесс реально испольняется на цпу либо неисполняется а стоит в
очеред runqueue и ждет когда для него освободится цпу.


вопрос - а можно както узнать так процесс щас реаьно исполняется или он в очереди стоит ?
поиска в инете ответ я увидел что люди такой вопрос задают но ответа нет. так что я пришел к 
выводу что простого решения как это увидеть нет. едиснвенное решение что я видел это наисать ядерный модуль который будет сканировать что-то в ядре и на основе деталей статуса в таблице процессов или еще где то там (тоесть в ядре безусловно есть точная галочка о том что прроцесс щас реально исполняется на цпу или всеже в очереди стоит) получать такую инфо.
итак короткий ответ  - простого способа это узнать нет.

========
ps
FLAGS 1,4

у команды ps есть флаг -l
и в нем есть колонка F

$ ps -l
F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD
0 S  1000 13908  3866  0  80   0 -  8528 wait   pts/148  00:00:00 bash
4 R  1000 14966 13908  0  80   0 -  9525 -      pts/148  00:00:00 ps

о чем эта колонка? в ней могут быть числа 0,1,4,5.

по факту они описывают два свойства процесса.

флаг 1 =  в мане описывается так "fork but didnt exec". по факту это значит что процесс был получен путем запуска fork() на его родителе (или clone() о чем нет в 
man ps) но потом в этом процессе (который дочка для парента) небыл запущен execve

тоесть еще раз о чем цифра 1 в этой колонке. обычно как в системе получается новый процесс.
есть процесс А он хочет создать новый процесс. как он это делает. он берет и делает fork()
или clone() получаем дочку B. Далее в дочке B запускается execve(). таким образом мы получили новый процесс B в системе.  так вот флаг 1 нам сообщает что процесс B был создан через fork\clone
НО после этого execve небыл использован. когда на практике это может быть. очень просто. когда 
у нас был  к примеру запущен bash и мы в нем запускаем еще один bash. поскольку бинарник у нас
такой же /bin/bash то смысла выполнять execve нет смысла. execve нам нужен если у нового процесса
другой файл бинарника. например парент процесс у нас /bin/bash а из него мы запускаем /bin/ls

флаг 4 = он говорит о том что процесс запущен с правами root. пример

$ sudo bash
$ echo $$
$ ps -Al | grep $$
# ps -Al | grep -E "$$|PID"
F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD
4 S     0 15102 15101  0  80   0 -  7464 wait   pts/148  00:00:00 bash


флаг 5 = это когда одновременно и флаг 1 и флаг 4.

прикол еще раз скажу в том что ( я проверил на практике)  что флаг 1 у процесса нетолько 
когда был использован fork но и когда clone тоже. тоесть я бы сказал про флаг 1 по другому
неважно форк там или клон или еще что было примеенено у парента. главное что в текущем процессе
небыл применен execve. тоесть это флаг об отсуствии применнеия execve.


флаг 0 = гораздо хитрее что значит 0. 0 значит отсуствие обоих флагов обоих условий но что это значит на практике?
разве это значит что  какойто процесс был получен без применнеия fork\clone ? как такое возмжно? нет!! это значит что процесс был получаем с примененеием fork\clone и далее с помощью execve !!!!!! вот что !!! таким образом можно воббще забыть про эти упоминания форк хуерк он тут неиграет
никакой роли. флаг 0 означает что в текущем процессе был использован execve при его создании
а флаг 1 означает что execve небыл использован . вот и все! это обьяснение гораздо лучше мудацкого
обьяснения из man ps.


что еще интересно: чтобы вызывать clone() (чтобы успешно откомпилирвать файл  на *.c) 
нужно в C программе использовать следующие макросы:

#define _GNU_SOURCE
#include<sched.h> 


где define _GNU_SOURCE совершенно особая штука. (https://stackoverflow.com/questions/5582211/what-does-define-gnu-source-imply)


что еще паралельно интересно. что еси в C программе мы вызываем fork()
то пофакту вызывается сисколл clone() а не форк!!! и strace это показыавет. а дело вот в чем!: дело в том что в C прогармме
кода мы пишем fork() clone() и прочие типа сисколлы на самом деле мы вызываем не сисколлы а функции библиотки
LIBC. а вот уже те функции вызывают чистые сисколлы. так вот  
libc fork() он по факту вызывает  clone() сисколл.
загадка разгадана.  в ядре естественно есть сисколл fork() но чтобы его заюзать 
надо тогда писать программу на асемблере а не на C. еще раз важно понять что в C коде мы вызываем функции библиотек libC
(функции обертки) а не реальные сисколлы ядра. поэтому это еще вопрос какой сисколл ядра юзает 
функция обертка libc.

итак еще раз когда мы в программе на C пишем вызов fork() то fork в данном случае
это не сисколл fork ядра а функция fork LIBC которая в свою очередь обращается к некоторому сисколлу ядра. в случае функции fork LIBC она обращается к сисколлу ядра clone а не к сисколлу fork как это можно было бы ожидать. 

в целом я бы сказал что столбик F имеет мало какого важного значения мало что дает 
интересного полезного. 

=================
zombie

еще раз поговорить про зомби процесс.

как он получается.

вообще это статус процесса. в целом это нормальный статус процесса если он длится недолго.
и ненормально если длится долго.


как он получается. когда в ядро поступает запрос ( неважно откуда от самого процесса, через Ctrl+c, от другого процесса вобщем неважно) на уничтожение этого процесса из системы то 
система вычищает все кишки процесса но остается небольшой хвостик в таблице процессов в ядре
и в частности сохраняется код возврата говорящий о том нормально процесс завершил работу или с ошибкой и ядро далее шлет сигнал парент процессу SIGCHLD который сообщает процессу о том что
состояние дочернего процесса изменилось ( кстати необязательно в сторону что дочерний процесс склеил ласты , также SIGCHLD посылаетя ядром к парент процессу если напримр процесс был останолвен изза сигнала SIGSTP либо продолжил свою работу после сигнала  SIGCONT). Далее ядро вот как подразумевает: в коде программы может быть прописан хендлер обработчик сигнала SIGCHLD если так есть то при старте парент процесса ядро это учтет и зарегистрирует этот обработчик. так вот при в этом случае при поступлении в парент процесс SIGCHLD сигнала шедулер при следующем таймслайсе
запустить этот кастомный обработчик этого сигнала. в целом ос ожидает что в этом обработчике в итоге будет использован libc функция wait (которая в итоге юзает кернел сисколл wait4) и данная функция через сисколл считает код возврата дочернего процесса. как только ядро увидит этот факт 
то ядро считает что вот теперь то дочерний процесс можно полностью уничтожить. значит как только в ядро поступил запрос на удаление процесса и до момента когда парент процесс считает код возврата
дочернего процесса все это время дочерний процесс будет висеть в списке процессов имея статус Z.
так вот когда могут пойти проблемы - кастомный хендлер в парент процессе для сигнала SIGCHLD его нужно написать програмисту тоесть это обязанность создателя программы парент процесса. а если он этого несделает? тогда в строй вступает дефолтовый ядерный обработчик данного сигнала и фишка в том что дефолтовый обработчик по дефолту игнорирует данный сигнал. Поэтому получается что если парент программа неимеет своего кастомного обработчика да еще и правильно написанного ( с использованием libc wait() ) то тогда получается что код возврата от дочернего процесса небудет
прочитан парент процессом никогда. и значит что дочерний процесс будет висеть в статусе Z вечно.
уничтожить его через сигналы невозможно ибо он уже почти уничтожен и болльше его неуничтожить.
итак причина появления Z процессов это остутствие кастом хендлера сигнала SIGCHLD в парент процессе. тоесть нужно парент программу переписывать. правда есть еще такой выход - нужно 
остановить уничтожить парент процесс тогда у дочернего процесса поменяется номер парент процесса 
в лоховской литературе пишут что этим процессом 100% станет сразу процесс с pid=1. но это брехня
как написано в более правльно литература новым парентом станет некий ближайший к дочернему процессу процесс но совсем необязательно что это будет процесс с pid=1
по моей практике парентом станет совсем другой процесс.

пример.

вот у меня есть процесс 22.exe
и вот его дерево от дочернего процесса до его парента а далее парента того парента итп.

~$ pstree  -A  -s  -p 20834
systemd(1)---systemd(1718)---gnome-terminal-(3866)---bash(20614)---mc(20742)---bash(20744)---22.exe(20833)---22.exe(20834)

парентом для 20834 является pid=20833

еще раз в этом убедимся

$ ps -o user,pid,ppid,pgrp,sess,stat,cmd -p 20834
USER       PID  PPID  PGRP  SESS STAT CMD
vasya    20834 20833 20833 20744 S+   ./22.exe

уничтожаем парента 20833

$ kill -9 20833

$ ps -o user,pid,ppid,pgrp,sess,stat,cmd -p 20834
USER       PID  PPID  PGRP  SESS STAT CMD
vasya    20834  1718 20833 20744 S    ./22.exe

и мы видим интереснейшую картину что новым парентом стал далеко непроцесс с pid=1 нет.
новым парентом стал pid=1718

об этом редко где пишут.


если мы еще раз посмотрим на исходное дерево родительских отношений для исходного процесса

systemd(1)---systemd(1718)---gnome-terminal-(3866)---bash(20614)---mc(20742)---bash(20744)---22.exe(20833)---22.exe(20834)

то мы увидим что новый парент это вверх по цепочке , второй от начала.

вот это очень интересно что новый парент это не pid=1 как пишут во многих книжках.

почему в системе несколько systemd процессов это другой разговор. скажу только что 
systemd(1718) это systemd который запущен в системе вот с таким ключом

/lib/systemd/systemd --user

почему именно на этот процесс из всей цепочки пал выбор для нового парента пока тоже непонятно.

в любом двигаем дальше что это нам теперь дает. дает оно то что если у нового парента есть 
кастомный обработчик сигнала SIGCHLD а у  systemd он однозначно есть то тогда (как я понимаю) 
ядро еще раз направляет сигнал SIGCHLD уже к systemd(1718) и он считывает код возврата дочернего 
процесса и вот вуаля ядро наконец удаляет дочерний процесс (20834) из системы полностью и целиком
и списке процессов исчезает наш Z процесс.

в чем вообще прооблема с этим Z процессом кроме того что непорядок что он виисит в системе.
проблема в общемто только в том что он пока висит в статусе Z то он занимает номер в таблице процессов. на 32-битных ядрах всего был 32768 доступных номеров для процессов так что если 
наразмножится много Z процессов то система исчерпает возможность создавать новые процессы. незнаю
что при этом будет. кернел паник или что.

еще раз скажу что процес в статусе Z он уже непринимает сигналы так что его никаким $ kill -9 невырубить. 

через $ ps такой процесс будет иметь вид как <defunct> например

22.exe <defunct>

насколько я понял самый реальный случай откуда могут вылезат на практике Z процессы
это скажем апач который запускает типа скрипт баша чтоли а в этом скрипте есть скажем строка с grep
типа 

...

cat /....  | grep 

и далее там было написано где я этот пример встречал что апач может по таймауту грохнуть процесс
скрипта а как он его грохает ну конечно же  некоей командой килл и аналогичным сисколлом в итоге
ядро пошлет сигнал -15 процессу со скриптом по хорошему этот процесс со скриптом должен иметь 
хеендлер кастомный для сигнала -15 который убивает все дочерние процессы причем он должен вначале убить все дочерние процессы считать их код возврата и потом только сам закончить свое существование.
на практике наверное этого часто нет. и что в итоге. в итоге процесс со скриптом будет уничтожен
но в системе останется дочерний процесс тот который grep. далее что получается что раньше для 
grep праентом был процесс со скриптом а теперь ( кстати вопрос) парентом станет (наверное) процесс 
с апачем и он ( неужели? ) неиммет обработчика катомного для SIGCHLD сигналов и поэтому процесс с 
grep будет висеть как Z. помне этот пример какойто бред потому что мне кажется что новым парентом будет systemd и он грохнет этот grep без проблем. но даже если новым парентом будет апач то я думаю у него 100% есть свой хендлер для SIGCHLD процессов потому что он же сам порождает чайлдов так что у него просто обязан быть этот хендлер. вобщем вопрос как на живой практике люди ловят
стада Z процессов надо еще выяснить.

================
proc

это файловая система которая позволяет из юзерского пространства получить доступ ( а procfs 
выствпает как шлюз как интерфейс) к kernel data structures.  тоесть можно полазить в ядре
из юзер пространства

===

