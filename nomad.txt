nomad

ставим номад на убунту
на основе этого https://www.linode.com/docs/guides/using-nomad-for-orchestration/

# sudo apt install wget gpg coreutils

# wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg

# echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list

# sudo apt update
# sudo apt install nomad

# nomad --version


далее юзаем вот это 
https://storiesfromtheherd.com/just-in-time-nomad-80f57cd403ca

для запуска номада на девеолперской машине (тестоввая инстляция)

# nomad agent -dev -bind 0.0.0.0 -log-level INFO

следующую команду можно не запускать. ососбенно на вирт машине
# nomad ui
потому что по факту она просто нарпсто запускает бразуер с ссылкой 127.0.0.1:4646
а если у нас вирт машина не имеет граф режима то просто вылезет ошибка

Opening URL "http://127.0.0.1:4646"
Error opening URL: exec: "xdg-open": executable file not found in $PATH

нужно просто пробросить сокет 127.0.0.1:4646 наружу от вирт машины и запускать
уже с другго хоста просто в браузере. 

так вот тут два  варианта - если мы устанвоили номад в lxd контейнере то 
надо выполнить 


# lxc config device add имя_контейнера  myport4646 proxy listen=tcp:0.0.0.0:4646  connect=tcp:127.0.0.1:4646


а если мы устанвоили номад внутри виртуальной машины то нужно поебаться:
 для того чтобы пробрость 127.0.0.1:4646 наружу
из виртуальной машины то надо открыть файл iptables.txt и в нем найти секцию 

 | port forward
 | loopback
 | lo "



итак мы пробррсили 4646 наружу на наш лэптоп, тогда идем в бразуер
и набираем http://IP:4646
где IP это адрес виртуалки.
и мы попадем в веб мооду номада.

итак на порту 4646 номад сервер имеет веб морду


пока не двинули дальш хочу разобрать эту строку

# nomad agent -dev -bind 0.0.0.0 -log-level INFO

nomad agent - запускает бинарь номада. они называют это "запустить агент номада". приэтом 
надо понимать что слово агент тут имеет ебанутый смысл. агент это незначит клиент. нет. в данном случае
это незначит нихуя. точнее это значит что мы запукаем бинарь номада. а вот следущая настройка говооит в каком режиме
запущен бинарь номада - в режиме сервера, в режиме клиентая, в девелоперском режиме.
режиме сервера -server это когда номад работает в режиме control plane
режим клиента -client это когда он ищет номад серверы конектится к ним и готов на этом хосте запускать джобы
режим девелоперский -dev это когда номад работает в комбинированном режиме и клиента и сервера одноверменно.
цитирую из хелпа


  -server
    Enable server mode for the agent. Agents in server mode are
    clustered together and handle the additional responsibility of
    leader election, data replication, and scheduling work onto
    eligible client nodes.


  -client
    Enable client mode for the agent. Client mode enables a given node to be
    evaluated for allocations. If client mode is not enabled, no work will be
    scheduled to the agent.


-dev
    Start the agent in development mode. This enables a pre-configured
    dual-role agent (client + server) which is useful for developing
    or testing Nomad. No other configuration is required to start the
    agent in this mode, but you may pass an optional comma-separated
    list of mode configurations:


есть еще один интресный режим

 -dev-connect
    Start the agent in development mode, but bind to a public network
    interface rather than localhost for using Consul Connect. This
    mode is supported only on Linux as root.



таким макамро вот эта строчка
# nomad agent -dev

означает что мы запустили бинарник номаа в режиме сервер+клиент . тоесть он и  за контрол плейн отвечает(сервер режим)
и готов запускать на этом же хосте джобы (режим клиента)



отсстальное уже понятно
-bind 0.0.0.0 -log-level INFO

означает куда ему биндится и какого уорвня логи срать на экран





далее.
номад проодукт как я понял состоит из одного файла (бинарь)
и этот файл в себе содержит и cli и серверную часть и клиентскую часть(это не cli 
это другое клиентская часть означает что это как агент у заббикса или как kuebelet у к8. тоесть этот клиент принимает 
команды от сервера и запускает джобы на том хосте на котором этот клиент запущен). и это смешно . потому что это мегамонолит. щас мода разбивать монолит.
а тут что у к8 (фраза состоит из 5 бинарей)и что номад сосоттоит весь функционал
из одного бинаря. 

роль "сервер" как я понял это роль control plane как у к8. как они пишут что эти типа аналог
мастер ноды у к8.

серверы оббразуют кластер (по ихнему регион). один из серверов явялется мастером
 а осталные пассивны. кластер стейт это "база"  в которой сохраняются всякие 
 там кластерные данные хранится у номада мастера в папке. у к8 переменные 
 кластера хранятся в etcd. а у номада стейт хранится в папке.  
далее написано чтото непнятное - написано что в эту папку данные записыаются по  алгоритму RAFT (как я помню это упрощенная веосия
 алгоритма PAXOS по которому мониторы в цефе приходят к конесунсу об изменении какойто переменной в кластере )
RAFT - Reliable, Replicated, Redundant, And Fault-Tolerant.
пока этот вопрос остатвляем про детали как рабтает RAFT. 
итак servver mode это такая хрень которая аналогична мастер ноде у к8.
на хосте где круитится номад в режиме мастера джобы запускаться не могут.

есть client mode работы у номада это аналог воркер ноду у к8.

если к8 оперирует подами то номад помимо контейнеров можно еще оперировать виртуалками
как я понял.

еще раз у к8 это control plane сосотоящий из master nodes.
у номада это server nodes

у к8 это woker nodes
у номада это client nodes
в веб морде номала client ноды называются "clients"


то что мы будем запускать в номаде описывется в job file. он внутри распиыывается через 
язык HCL (hashicorp configuration language) (  у к8 это yaml)

у к8 это под. у номада это таск. 

task group это группа тасков. группа тасков описыатся внутри джоба.

task --> task group ---> job  (от меньшего к болшему)


When defining a task, you specify: number of instances, task driver (e.g. docker vs Java vs ?), image, CPU resources (in MHz), and memory resources

task driver это некая хрень которая позволяет абстрагиррваться номаду от конкретики
таска (тоесть таск это либо контейнер либо виртуалка и между номадом и конейнером
стоит rask driver как слой абстрации )

если под у к8 это кучка контейнеров то таск номада это контейнер

номад таск группа это аналог пода у к8 . потому что под это кучка контйенеров.


далее берем job

job "2048-game" {
  datacenters = ["dc1"]  
  type = "service"
  group "game" {
    count = 1 # number of instances

    network {
      port "http" {
        static = 80
      }
    }
 
    task "2048" {
      driver = "docker"
 
      config {
        image = "alexwhen/docker-2048"

        ports = [
          "http"
        ]

      }

      resources {
        cpu    = 500 # 500 MHz
        memory = 256 # 256MB
      }
    }
  }
}


далее запускаем его
# nomad job run 2048-game.nomad

при этом у нас на хосте предварительно должен быть установлен docker !!
сам номад за нас докер ставить не будет

еще - еслиу нас  номад запущен внутри lxd контейнена то чтобы там докер можно было запускать 
надо 

# lxc config set nm1  security.nesting=true security.syscalls.intercept.mknod=true  security.syscalls.intercept.setxattr=true
# lxc restart nm1


проверяем что джоб запустился

# nomad job  status
ID         Type     Priority  Status   Submit Date
2048-game  service  50        running  2023-09-03T10:15:16Z


идем в веб морду
жмем jobs- у нащего джоба должен быть статус RUNNING

на вкладке overview 
видим
Name   Count Allocation Status  Volume  Reserved CPU   Reserved Memory   Reserved Disk  
game    1                                   500 MHz           256 MiB     300 MiB


это макс ресурсов сколько разрещшено сожрать контейнеру. ксати номад может еще с подманом работать

на вкладке "Definitions" будет показан наш джоб манифест что очень удобно.

номад в режиме -dev жрет в памяти 100МБ. всего лишь.

итак датацентр в терминах номада это кластер в терминах к8. (идиотизм)



еще вот так смотрим
# nomad job status 2048-game
ID            = 2048-game
Name          = 2048-game
Submit Date   = 2023-09-03T10:15:16Z
Type          = service
Priority      = 50
Datacenters   = dc1
Namespace     = default
Node Pool     = default
Status        = running
Periodic      = false
Parameterized = false

Summary
Task Group  Queued  Starting  Running  Failed  Complete  Lost  Unknown
game        0       0         1        0       0         0     0

Latest Deployment
ID          = 7acce2ef
Status      = successful
Description = Deployment completed successfully

Deployed
Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
game        1        1       1        0          2023-09-03T10:28:23Z

Allocations
ID        Node ID   Task Group  Version  Desired  Status   Created     Modified
32a8659a  2fc12930  game        0        run      running  10m42s ago  10m25s ago


жалко тут лимиты не указаны.





далее.
если мы хотим юзать номад кластер в проде то нужно устанвливать нетолько номад
но и consul и vault это все хашикорп  проги

vault это прога для  работы  с secrets (секретами)

Consul is used as a service mesh — i.e. service discovery, DNS, and basic load-balancing (similar to Istio and Linkerd).  (пока тут мутное понимние)


consul это  видимо кластер или группа консул серверов и плюс его агенты устаолвенные
на nomad client машины.

консул это вроде как тоже один бинарник (щас все срут от этого копиятком) который может
работать либо в режиме клиента лиоб в режиме сервера


вот здесь вот есть картинка - https://storiesfromtheherd.com/just-in-time-nomad-80f57cd403ca
там нарсовано как увяывзаывается номад и консул.
а имено есть консул кластер. это отдельные машины.
есть набор машин с номад мастерами (они их зовут номад серверы) и есть ряд машин 
с номад воркерами (номад клиенты). на номад мастерах крутится контрол плейн номада.
на номал клиентах запускаются джобы(таски).  так вот консул клиенты (агенты консула) ставятся
на всех номал машинах тоесть и на номад мастерах и на номад клиентах.

как я понял в к8 его кластерные настройки(переменные) хранятся в etcd 
у номада это хранится в папке. папка хранится на мастер ноде. и реплицируется
на слейв ноды.

Consule - это service mesh. что это такое еще надо изучать

HCL - на нем пищутся файлов джобов. насколько я помню этот же HCL 
юзается в тераформе

то что у к8 обеспечивается чеерез kube-proxy (а что он делает?) у номада это делает консул
видимо через консул агенты

вот еще интересно - ищется аналогии между к8 и номадом

|Kubernetes                    |Nomad                                  |
|------------------------------|---------------------------------------|
|Control Plane (Master Node)   |Server                                 |
|Worker Node                   |Client                                 |
|Cluster                       |Region                                 |
|Kubelet                       |Client agent                           |
|Container Runtime             |Task Driver (not limited to containers)|
|Pod (smallest deployable unit)|Task Group                             |
|Container (in a Pod)          |Task (smallest deployable unit)        |
|Deployment manifest           |Job file                               |
|Kubernetes Deployment         |Nomad Job (for containers)             |
|Kubevirt (separate install)   |Nomad VM Task Driver                   |
|Service                       |Service                                |


из этой таблицы я не понял на счет соотвестия между cluster у к8 и region у номада.
ранее было наисано что в терминах номдада кластер это datacenter но не region.


номад типа может запускаь и контролировать - контейнеры, виртуалки, просто бинарные elf
файлы и селедить за ними упали неупали.  такая типа контрол панеь слежениея за докерами (джобами)


двигаем дальше.
вот здесь https://mykidong.medium.com/install-nomad-cluster-d9a40d2206f5
я читаю что к8 типа хороший для стейтлесс приложения и херовый для стейтфулл приложений.
а типа номад лучше подходит для стейтвфулл приложений.
говоится что консул сервер ноды  лучше ставит на туже виртуалку куда и номад сервер роли.
а консул клиенты(агенты) ставить на номад клиенты.

номад сервер роли прдлагается разворачитвать на 3-5 нодах. (макс 7 штук поддреживается)

чтобы установить номад кластер надо вначале иметь консул кластер.
итак ставим консул кластер
для начала поставим его прям руками, делаю по этой статье
https://computingforgeeks.com/how-to-install-consul-cluster-18-04-lts/?expand_article=1

написано что если наша прога обратиться к агенту коснула то он автоматом
перенаправит (непонятно как это конкретно) этот запрос на сервер консула.
прикольно.




# mkdir  ~/downloads
 cd ~/downloads
 wget https://releases.hashicorp.com/consul/1.16.0/consul_1.16.0_linux_amd64.zip
 sudo apt-get update && apt-get -y install unzip
 unzip ./consul_1.16.0_linux_amd64.zip
 chmod +x consul
 mv ./consul /usr/local/bin/


# consul version
Consul v1.16.0


# sudo groupadd --system consul
 sudo useradd -s /sbin/nologin --system -g consul consul
 sudo mkdir -p /var/lib/consul
 sudo chown -R consul:consul /var/lib/consul
 sudo chmod -R 775 /var/lib/consul
 mkdir /etc/consul.d
 chown -R consul:consul /etc/consul.d


# echo "10.113.151.191   cnsl1" >> /etc/hosts
  echo "10.113.151.42    cnsl2" >> /etc/hosts
  echo "10.113.151.159   cnsl3" >> /etc/hosts


# echo "10.113.151.232  consul1-dc2" >> /etc/hosts
  echo "10.113.151.208  consul2-dc2" >> /etc/hosts
  echo "10.113.151.183  consul3-dc2" >> /etc/hosts


# cat<<EOF >/etc/systemd/system/consul.service 
[Unit]
Description=Consul Service Discovery Agent
Documentation=https://www.consul.io/
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=consul
Group=consul
ExecStart=/usr/local/bin/consul agent \
    -node=cnsl1 \
    -config-dir=/etc/consul.d

ExecReload=/bin/kill -HUP $MAINPID
KillSignal=SIGINT
TimeoutStopSec=5
Restart=on-failure
SyslogIdentifier=consul

[Install]
WantedBy=multi-user.target
EOF


apt-get -y install mc


# consul keygen
EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=


# cat<<EOF > /etc/consul.d/consul.json
{
    "advertise_addr": "10.113.151.191",
    "bind_addr": "10.113.151.191",


    "bootstrap_expect": 3,
    "client_addr": "0.0.0.0",
    "datacenter": "DC1",
    "data_dir": "/var/lib/consul",
    "domain": "consul",
    "enable_script_checks": true,
    "dns_config": {
        "enable_truncate": true,
        "only_passing": true
    },
    "enable_syslog": true,
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",
    "leave_on_terminate": true,
    "log_level": "INFO",
    "rejoin_after_leave": true,
    "retry_join": [
     "cnsl1",
     "cnsl2",
     "cnsl3"
    ],
    "server": true,
    "ui_config": {
          "enabled": true
    }
}

EOF


где
    "advertise_addr": "10.113.151.191",  <=== под каким адресом эта нода видна другим (если за NAT)
    "bind_addr": "10.113.151.191",       <=== на какую карту садиться с биндингом сервису
    "client_addr": "0.0.0.0",            <--- откуда принимать запросы от клиентов
    "datacenter": "DC1",                 <--- ????
    "domain": "consul",                  <--- ????
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",   <---- пароль через который ноды друг с другом шифрутся
 
    "retry_join": [
     "cnsl1",          <---   DNS имена нод по которым они ищут друг друга
     "cnsl2",
     "cnsl3"
    ],
 
    "ui": true     <--- активировать веб морду


проверяем конфигурацию

# consul validate /etc/consul.d/consul.json 
The 'start_join' field is deprecated. Use the 'retry_join' field instead.
The 'ui' field is deprecated. Use the 'ui_config.enabled' field instead.
bootstrap_expect > 0: expecting 3 servers
using enable-script-checks without ACLs and without allow_write_http_from is DANGEROUS, use enable-local-script-checks instead, see https://www.hashicorp.com/blog/protecting-consul-from-rce-risk-in-specific-configurations/
Configuration is valid!


# systemctl deamon-reload
systemctl start consul
systemctl status consul
journalctl -f -u consul

тут я хочу важное сказать - кластер не соберется до тех пор пока мы не запустим все три ноды.
если мы развернем одну или две ноды то в логах консула будет написано : 
   agent.anti_entropy: failed to sync remote state: error="No cluster leader"


тоесть даже имея две ноды лидер выбираться НЕБУДЕТ.
и только когда будет развернуто все три ноды выберется лидер. 
и тольлко после этого можно гасить одну ноду и все будет оставаться окей. но чтбы лидер изначально выбрался нужно все 
три ноды. 
вот такой прикол. возможно из за того чтобы не получилось что у нас выборалось два лидера на каждой из двух нод.

веб морда порт :8500


до того как лезть в веб морду оценю вот что  в systemd юните указано
ExecStart=/usr/local/bin/consul agent \
    -node=cnsl1 \
    -config-dir=/etc/consul.d


мы видим что указано "consul agent   -node=cnsl1  -config-dir=/etc/consul.d"  мы видим что бинарник консула 
запускается но не указано в каком режиме - то ли  врежиме клиента то ли в режиме сервера. однако разгадака
состоит в том что остальная часть параметров запуска указана в /etc/consul.d/consul.json

"server": true,

тоесть консул запусксется на этих хостах в режиме сервера. тоесть получается часть параметров запуска косула
прописана в systemd юните ExecStart=  а другая часть параметров запуска прописана в другом месте просто в consul.json
вот в чем разгадка

мне конечно непонятен ряд настроек в consul.json например -bootstrap-expect
# consul  agent --help
...
...
  -bootstrap-expect=<value>
     Sets server to expect bootstrap mode.




в нашем случае в consul.json установлена

 "bootstrap_expect": 3,

 как я понимаю именно поэтому для начального взлета кластера нужно именно 3 сервера.


 что такое federation при раскатке кластера?

 еще раз напомню nomad web морда это порт 4646
 а консул web морда это поорт 8500


 далее.
 устанвоит autocomplete для консула

# consul -autocomplete-install
# complete -C /usr/local/bin/consul consul


consul datacenter ? nomad dataceter? nomad region ? nomad zone?



нашел что такое boostrap правда на примере номада а не коснула. но суть одна:
Q: What is "bootstrapping" a Nomad cluster?
Bootstrapping is the process when a Nomad cluster elects its first leader and writes the initial cluster state to that leader's state store. Bootstrapping will not occur until at least a given number of servers, defined by bootstrap_expect, have connected to each other. Once this process has completed, the cluster is said to be bootstrapped and is ready to use.

Certain configuration options are only used to influence the creation of the initial cluster state during bootstrapping and are not consulted again so long as the state data remains intact. These typically are values that must be consistent across server members. For example, the default_scheduler_config option allows an operator to set the SchedulerConfig to non-default values during this bootstrap process rather than requiring an immediate call to the API once the cluster is up and running.

If the state is completely destroyed, whether intentionally or accidentally, on all of the Nomad servers in the same outage, the cluster will re-bootstrap based on the Nomad defaults and any configuration present that impacts the bootstrap process.
( https://developer.hashicorp.com/nomad/docs/faq )


nomad 
ставим его в форме кластера. 
это можно сделать и без консула. но лучще делать на основе консула.

$ export NOMAD_VERSION="1.6.1"
  curl --silent --remote-name https://releases.hashicorp.com/nomad/${NOMAD_VERSION}/nomad_${NOMAD_VERSION}_linux_amd64.zip
  apt-get -y install unzip
 unzip nomad_${NOMAD_VERSION}_linux_amd64.zip
 chown root:root nomad
chmod +x nomad
 mv nomad /usr/local/bin/
 rm nomad_${NOMAD_VERSION}_linux_amd64.zip
 nomad -autocomplete-install
 complete -C /usr/local/bin/nomad nomad

 mkdir --parents /opt/nomad
 mkdir --parents /etc/nomad.d
 chmod 700 /etc/nomad.d
 touch /etc/nomad.d/nomad.hcl



 cat<<EOF>/etc/nomad.d/main.json 
{

    "acl": {
        "enabled": true
    },



    "bind_addr": "0.0.0.0",



    "client": {
        "enabled": false,
        "host_volume": {},
        "template": {
            "disable_file_sandbox": true
        }
    },


    "consul": {
        "address": "127.0.0.1:8500",
        "auto_advertise": true,
        "client_auto_join": true,
        "client_service_name": "nomad-client",
        "server_auto_join": true,
        "server_service_name": "nomad-master"
    },


    "data_dir": "/var/lib/nomad",
    "datacenter": "dc1",
    "region": "ovh",



    "server": {
        "bootstrap_expect": 3,
        "enabled": true,
        "heartbeat_grace": "60s",
        "max_heartbeats_per_second": 10.0,
        "min_heartbeat_ttl": "30s"
    },



    "telemetry": {
        "collection_interval": "1s",
        "disable_hostname": false,
        "prometheus_metrics": true,
        "publish_allocation_metrics": true,
        "publish_node_metrics": true
    }


}
EOF







cat<<EOF>/etc/systemd/system/nomad.service 
[Unit]
Description=Nomad
Documentation=https://www.nomadproject.io/docs
Wants=network-online.target
After=network-online.target

[Service]
ExecReload=/bin/kill -HUP $MAINPID
ExecStart=/usr/local/bin/nomad agent -config /etc/nomad.d
KillMode=process
KillSignal=SIGINT
LimitNOFILE=infinity
LimitNPROC=infinity
Restart=on-failure
RestartSec=2
StartLimitBurst=3
#StartLimitIntervalSec=10
TasksMax=infinity

[Install]
WantedBy=multi-user.target
EOF



также для этого надо поствить клиент консула тоесть как я понял номад обратиться к клиенту консула а тот обратттся 
к серверу консула. и будет шоколад

$ export CONSUL_VERSION="1.16.1"
export CONSUL_URL="https://releases.hashicorp.com/consul"

curl --silent --remote-name ${CONSUL_URL}/${CONSUL_VERSION}/consul_${CONSUL_VERSION}_linux_amd64.zip
 unzip consul_${CONSUL_VERSION}_linux_amd64.zip
 chown root:root consul
chmod +x consul
 mv consul /usr/local/bin/
 rm consul_${CONSUL_VERSION}_linux_amd64.zip

consul -autocomplete-install
complete -C /usr/local/bin/consul consul

useradd --system --home /etc/consul.d --shell /bin/false consul
 mkdir --parents /opt/consul
 chown --recursive consul:consul /opt/consul


mkdir --parents /etc/consul.d
touch /etc/consul.d/consul.hcl
 chown --recursive consul:consul /etc/consul.d
 chmod 640 /etc/consul.d/consul.hcl



cat<<EOF>/etc/consul.d/consul.json 
{
    "server": false,
    "datacenter": "dc1",
    "data_dir": "/var/consul",
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",
    "log_level": "INFO",
    "enable_syslog": true,
    "leave_on_terminate": true,
    "start_join": [
        "10.113.151.191",
        "10.113.151.42",
        "10.113.151.159"
    ]
}
EOF



cat<<EOF> /etc/systemd/system/consul.service 
[Unit]
Description=Consul Startup process
After=network.target
 
[Service]
Type=simple
ExecStart=/usr/bin/bash -c '/usr/local/bin/consul agent -config-dir /etc/consul.d/'
TimeoutStartSec=0
 
[Install]
WantedBy=default.target
EOF



systemctl daemon-reload
systemctl start consul
systemctl status consul
consul members

apt-get update
apt-get -y install mc


systemctl daemon-reload
systemctl start nomad
systemctl status nomad



по идее номады должны успешно соединиться  в кластер.

остается на него зайти. для этого надо аутентфицироваться. аутентфикция идет через токен.
его полуить можно вот так

# nomad acl bootstrap
Accessor ID  = 51983480-e2cf-b13f-3ee4-17fae91babb9
Secret ID    = b1e55cc6-8289-6b6e-c779-9f0b0a30e96b
Name         = Bootstrap Token
Type         = management
Global       = true
Create Time  = 2023-09-03 19:18:48.944872154 +0000 UTC
Expiry Time  = <none>
Create Index = 9
Modify Index = 9
Policies     = n/a
Roles        = n/a


для региона "us" другой токен у меня
# nomad acl bootstrap
Accessor ID  = 9d82a086-9771-ee84-1093-a845cf43ae12
Secret ID    = c4a04b33-85dd-bd17-8137-d26c4be0f428
Name         = Bootstrap Token
Type         = management
Global       = true
Create Time  = 2023-09-06 20:35:51.221105416 +0000 UTC
Expiry Time  = <none>
Create Index = 8
Modify Index = 8
Policies     = n/a
Roles        = n/a





идем в веб морду номада IP(номада):4646
там тыкаем "Sign in" и вводим Secret ID    = b1e55cc6-8289-6b6e-c779-9f0b0a30e96b

также можно аутентфицивраться и в cli
для этого
export NOMAD_TOKEN="b1e55cc6-8289-6b6e-c779-9f0b0a30e96b"

что интересно. если регион прописано в конфиге номад сервера как "ovh" то имя сервера в номаде будет 
заканчиваться на .ovh тоесть

# nomad server members
Name        Address         Port  Status  Leader  Raft Version  Build  Datacenter  Region
nm2-cl.ovh  10.113.151.192  4648  alive   false   3             1.6.1  dc2         ovh
nm3-cl.ovh  10.113.151.174  4648  alive   true    3             1.6.1  dc3         ovh
nm4-cl.ovh  10.113.151.231  4648  alive   false   3             1.6.1  dc4         ovh


а если регион в конфиге номад сервера указано как "us" то автоматом имена серверов будет 
заканчиваться на .us

# nomad server members
Name           Address         Port  Status  Leader  Raft Version  Build  Datacenter  Region
nomad1-dc2.us  10.113.151.224  4648  alive   true    3             1.6.1  dc2         us
nomad2-dc2.us  10.113.151.233  4648  alive   false   3             1.6.1  dc2         us
nomad3-dc2.us  10.113.151.90   4648  alive   false   3             1.6.1  dc2         us







#
# теперь ставим номад клиент
для начала ставим consul клиент


$ export CONSUL_VERSION="1.16.1"
export CONSUL_URL="https://releases.hashicorp.com/consul"

curl --silent --remote-name ${CONSUL_URL}/${CONSUL_VERSION}/consul_${CONSUL_VERSION}_linux_amd64.zip
 unzip consul_${CONSUL_VERSION}_linux_amd64.zip
 chown root:root consul
chmod +x consul
 mv consul /usr/local/bin/
 rm consul_${CONSUL_VERSION}_linux_amd64.zip

consul -autocomplete-install
complete -C /usr/local/bin/consul consul

useradd --system --home /etc/consul.d --shell /bin/false consul
 mkdir --parents /opt/consul
 chown --recursive consul:consul /opt/consul


mkdir --parents /etc/consul.d
touch /etc/consul.d/consul.hcl
 chown --recursive consul:consul /etc/consul.d
 chmod 640 /etc/consul.d/consul.hcl



cat<<EOF>/etc/consul.d/consul.json 
{
    "server": false,
    "datacenter": "dc1",
    "data_dir": "/var/consul",
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",
    "log_level": "INFO",
    "enable_syslog": true,
    "leave_on_terminate": true,
    "start_join": [
        "10.113.151.191",
        "10.113.151.42",
        "10.113.151.159"
    ]
}
EOF



cat<<EOF> /etc/systemd/system/consul.service 
[Unit]
Description=Consul Startup process
After=network.target
 
[Service]
Type=simple
ExecStart=/usr/bin/bash -c '/usr/local/bin/consul agent -config-dir /etc/consul.d/'
TimeoutStartSec=0
 
[Install]
WantedBy=default.target
EOF



systemctl daemon-reload
systemctl start consul
systemctl status consul
consul members

apt-get update
apt-get -y install mc





# ставим номад как клиент

$ export NOMAD_VERSION="1.6.1"
  curl --silent --remote-name https://releases.hashicorp.com/nomad/${NOMAD_VERSION}/nomad_${NOMAD_VERSION}_linux_amd64.zip
  apt-get -y install unzip
 unzip nomad_${NOMAD_VERSION}_linux_amd64.zip
 chown root:root nomad
chmod +x nomad
 mv nomad /usr/local/bin/
 rm nomad_${NOMAD_VERSION}_linux_amd64.zip
 nomad -autocomplete-install
 complete -C /usr/local/bin/nomad nomad

 mkdir --parents /opt/nomad
 mkdir --parents /etc/nomad.d
 chmod 700 /etc/nomad.d
 touch /etc/nomad.d/nomad.hcl



 cat<<EOF> /etc/nomad.d/main.json 
{
    "acl": {
        "enabled": true
    },
    "bind_addr": "0.0.0.0",
    "client": {
        "enabled": true,
        "host_volume": {},
        "meta": {},
        "template": {
            "disable_file_sandbox": true
        }
    },
    "consul": {
        "address": "127.0.0.1:8500",
        "auto_advertise": true,
        "client_auto_join": true,
        "client_service_name": "nomad-client",
        "server_auto_join": true,
        "server_service_name": "nomad-master"
    },
    "data_dir": "/var/lib/nomad",
    "datacenter": "dc1",
    "region": "ovh",
    "server": {
        "bootstrap_expect": 1,
        "enabled": false,
        "heartbeat_grace": "60s",
        "max_heartbeats_per_second": 10.0,
        "min_heartbeat_ttl": "30s"
    },
    "telemetry": {
        "collection_interval": "1s",
        "disable_hostname": false,
        "prometheus_metrics": true,
        "publish_allocation_metrics": true,
        "publish_node_metrics": true
    }
 
}
EOF




cat<<EOF>/etc/systemd/system/nomad.service 
[Unit]
Description=Nomad
Documentation=https://www.nomadproject.io/docs
Wants=network-online.target
After=network-online.target

[Service]
ExecReload=/bin/kill -HUP $MAINPID
ExecStart=/usr/local/bin/nomad agent -config /etc/nomad.d
KillMode=process
KillSignal=SIGINT
LimitNOFILE=infinity
LimitNPROC=infinity
Restart=on-failure
RestartSec=2
StartLimitBurst=3
#StartLimitIntervalSec=10
TasksMax=infinity

[Install]
WantedBy=multi-user.target
EOF



systemctl daemon-reload
systemctl start nomad
systemctl status nomad


ставим докер на хосте

для начала меняем lxc свойства контенера
# lxc config set nm5-cl  security.nesting=true security.syscalls.intercept.mknod=true  security.syscalls.intercept.setxattr=true
 lxc restart nm5-cl

systemctl enable consul
systemctl enable nomad
systemctl start consul
systemctl start nomad


sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo docker run hello-world


возвращаемся обратно на мастер номада

создаем джоб


cd ~
cat<<EOF>2048-game.nomad 
job "2048-game" {
  datacenters = ["dc1"]  
  type = "service"
  group "game" {
    count = 1 # number of instances

    network {
      port "http" {
        static = 80
      }
    }
 
    task "2048" {
      driver = "docker"
 
      config {
        image = "alexwhen/docker-2048"

        ports = [
          "http"
        ]

      }

      resources {
        cpu    = 500 # 500 MHz
        memory = 256 # 256MB
      }
    }
  }
}
EOF

делаем экспорт токена для атуентификации
export NOMAD_TOKEN="b1e55cc6-8289-6b6e-c779-9f0b0a30e96b"

далее деплоим джоб
# nomad job run 2048-game.nomad
3T19:52:43Z: Monitoring evaluation "7fda11b6"
    2023-09-03T19:52:43Z: Evaluation triggered by job "2048-game"
    2023-09-03T19:52:44Z: Evaluation within deployment: "4ca38329"
    2023-09-03T19:52:44Z: Allocation "451903cc" created: node "107e6c5b", group "game"
    2023-09-03T19:52:44Z: Evaluation status changed: "pending" -> "complete"
==> 2023-09-03T19:52:44Z: Evaluation "7fda11b6" finished with status "complete"
==> 2023-09-03T19:52:44Z: Monitoring deployment "4ca38329"
  ✓ Deployment "4ca38329" successful
    
    2023-09-03T19:53:01Z
    ID          = 4ca38329
    Job ID      = 2048-game
    Job Version = 0
    Status      = successful
    Description = Deployment completed successfully
    
    Deployed
    Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
    game        1        1       1        0          2023-09-03T20:02:59Z


  непонятно номад он overlay сети поддреживает? или он так чисто за контейенрами тольо приглядыать умеет?


  попробйе развенуть жинкс на номаде


cat<<EOF>nginx.nomad
 # cat nginx.nomad 
   job "nginx" {
  datacenters = ["dc1"]
  type = "service"
  group "nginx" {
    count = 1
    task "nginx" {
      driver = "docker"
      config {
        image = "nginx"
        port_map {
          http = 8080
        }
        port_map {
          https = 443
        }
        volumes = [
          "custom/default.conf:/etc/nginx/conf.d/default.conf"
        ]
      }
      template {
        data = <<EOH
          server {
           listen 8080;
            server_name nginx.service.consul;
            location /nginx {
              root /local/data;
            }
          }
        EOH
        destination = "custom/default.conf"
      }
      # consul kv put features/demo 'Consul Rocks!'
     template {
        data = <<EOH
        Nomad Template example (Consul value)
        <br />
        <br />
        {{ if keyExists "features/demo" }}
        Consul Key Value:  {{ key "features/demo" }}
        {{ else }}
          Good morning.
        {{ end }}
        <br />
        <br />
        Node Environment Information:  <br />
        node_id:     {{ env "node.unique.id" }} <br/>
        datacenter:  {{ env "NOMAD_DC" }}
        EOH
        destination = "local/data/nginx/index.html"
      }
      resources {
        cpu    = 100 # 100 MHz
        memory = 128 # 128 MB
        network {
          mbits = 10
          port "http" {
          }
          port "https" {
          }
        }
      }
      service {
        name = "nginx"
        tags = [ "nginx", "web", "urlprefix-/nginx" ]
        port = "http"
        check {
          type     = "tcp"
          interval = "10s"
          timeout  = "2s"
        }
      }
    }
  }
}

EOF


деплоим
# nomad job run nginx.nomad


кстти как удалить job
# nomad job status
ID         Type     Priority  Status   Submit Date
2048-game  service  50        running  2023-09-03T19:52:43Z
nginx      service  50        running  2023-09-03T19:58:13Z

# nomad job stop -purge  nginx



еще один job

# cat graphql.nomad 

job "website" {

  datacenters = ["dc1"]

  type = "service"


  update {
    max_parallel = 1

    min_healthy_time = "10s"

    healthy_deadline = "3m"

    progress_deadline = "10m"

    auto_revert = false

    canary = 0
  }

  migrate {
    max_parallel = 1

    health_check = "checks"

    min_healthy_time = "10s"

    healthy_deadline = "5m"
  }

  group "website" {
    count = 1

    restart {
      attempts = 2
      interval = "30m"

      delay = "15s"

      mode = "fail"
    }

    ephemeral_disk {

      size = 300
    }

    task "graphql" {
      driver = "docker"

      config {
        image = "dalongrong/mygraphql"
        port_map {
          website = 80
        }
      }



      resources {
        network {
          mbits = 10
          port "website" {}
        }
      }

      service {
        name = "graphql-website"
        tags = ["global", "website","graphql","urlprefix-/"]
        port = "website"
        check {
          name     = "alive"
          type     = "http"
          interval = "10s"
          path   = "/"
          timeout  = "2s"
        }
      }




    }
  }
}

==

| consul 
| token

как подсунуть токен для авторизации
$ consul members -token 6e2aff60-f047-3158-58ea-ae2b1afc24a6

| consul
| leader

как найти лидера

$ curl 10.113.151.191:8500/v1/status/leader
"10.113.151.42:8300"


| полезняшки

$ consul members

$  curl localhost:8500/v1/catalog/nodes | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2215    0  2215    0     0  2229k      0 --:--:-- --:--:-- --:--:-- 2163k
[
  {
    "ID": "3e7865d0-5327-4f8d-87c2-15a1cb271401",
    "Node": "cnsl1",
    "Address": "10.113.151.191",
    "Datacenter": "dc1",
    "TaggedAddresses": {
      "lan": "10.113.151.191",
      "lan_ipv4": "10.113.151.191",
      "wan": "10.113.151.191",
      "wan_ipv4": "10.113.151.191"
      ....



Consul also provides a DNS interface to query nodes. It serves DNS on 8600 port by default. That port is configurable.
тесть что интесно - что консул отвечаает нетолько по http в формате json но и по DNS.
так вот если мы зайдем в веб морду у нас там есть закладка "Nodes".
так вот все что в этой закладке можно запросить через DNS в домене *.nodes.consul
тоесть

# dig @127.0.0.1 -p 8600   cnsl1.node.consul +short
10.113.151.191
# dig @127.0.0.1 -p 8600   cnsl2.node.consul +short
10.113.151.42
# dig @127.0.0.1 -p 8600   cnsl3.node.consul +short
10.113.151.159


то что в веб морде консула закладка "services"  можно зарпосит по DNS в домене *.service.consul
примеры
# dig @127.0.0.1 -p 8600   nomad-master.service.consul +short
10.113.151.192
10.113.151.174
10.113.151.231


# dig @127.0.0.1 -p 8600   consul.service.consul +short
10.113.151.191
10.113.151.42
10.113.151.159

а вот так можно запросить порты на которых слущаю хосты в сервисах

# dig @127.0.0.1 -p 8600   nomad-master.service.consul +short  SRV
1 1 4646 0a7197c0.addr.dc1.consul.
1 1 4647 0a7197c0.addr.dc1.consul.
1 1 4648 0a7197ae.addr.dc1.consul.
1 1 4647 0a7197ae.addr.dc1.consul.
1 1 4648 0a7197e7.addr.dc1.consul.
1 1 4648 0a7197c0.addr.dc1.consul.
1 1 4646 0a7197e7.addr.dc1.consul.
1 1 4647 0a7197e7.addr.dc1.consul.
1 1 4646 0a7197ae.addr.dc1.consul.

~# dig @127.0.0.1 -p 8600   consul.service.consul +short  SRV
1 1 8300 cnsl2.node.dc1.consul.
1 1 8300 cnsl3.node.dc1.consul.
1 1 8300 cnsl1.node.dc1.consul.


можно сервисы запросить и через curl
# curl http://localhost:8500/v1/catalog/service/nomad-master | jq
[
  {
    "ID": "ea7d24e8-b271-4618-2ec8-da9db3423041",
    "Node": "nm2-cl",
    "Address": "10.113.151.192",
    "Datacenter": "dc1",
    "TaggedAddresses": {
      "lan": "10.113.151.192",
      "lan_ipv4": "10.113.151.192",
      "wan": "10.113.151.192",
      "wan_ipv4": "10.113.151.192"
    },
    "NodeMeta": {
      "consul-network-segment": "",
      "consul-version": "1.16.1"
    },
    "ServiceKind": "",
    "ServiceID": "_nomad-server-gsfiflv55mwear6o6rp2cdybbq4jfeco",
    "ServiceName": "nomad-master",
    "ServiceTags": [
      "serf"
    ],
    "ServiceAddress": "10.113.151.192",
    "ServiceTaggedAddresses": {
      "lan_ipv4": {
        "Address": "10.113.151.192",
        "Port": 4648
...


можно запросит сервис у которого все хорошо со здоровьем 
добавляем ?passing
# curl http://localhost:8500/v1/catalog/service/nomad-master?passing | jq


далее
If you wish to update the service definition on a running Consul agent, it is very simple.
There are three ways to achieve this. You can send a SIGHUP signal to the process, reload Consul which internally sends SIGHUP on the node or you can call HTTP API dedicated to service definition updates that will internally reload the agent configuration.

засталвяю консул переичитать конфиг
# kill -SIGHUP 2004

при этом в логах будет инфо
# journalctl -f -u consul
Sep 04 19:55:04 cnsl2 consul[20045]: 2023-09-04T19:55:04.694Z [WARN]  agent.server.rpc-rate-limit: UpdateConfig called but configuration has not changed.  Skipping updating the server rate limiter configuration.
Sep 04 19:55:04 cnsl2 consul[20045]: agent.server.rpc-rate-limit: UpdateConfig called but configuration has not changed.  Skipping updating the server rate limiter configuration.

а вот еще как это в логах
Sep 04 20:17:15 cnsl2 consul[20045]: 2023-09-04T20:17:15.023Z [INFO]  agent: Caught: signal=hangup
Sep 04 20:17:15 cnsl2 consul[20045]: agent: Caught: signal=hangup
Sep 04 20:17:15 cnsl2 consul[20045]: 2023-09-04T20:17:15.026Z [WARN]  agent.auto_config: bootstrap_expect > 0: expecting 3 servers
Sep 04 20:17:15 cnsl2 consul[20045]: agent.auto_config: bootstrap_expect > 0: expecting 3 servers
Sep 04 20:17:15 cnsl2 consul[20045]: agent.server.rpc-rate-limit: UpdateConfig called but configuration has not changed.  Skipping updating the server rate limiter configuration.

тоесть он пишет что мол получил sighup.
перечитываю конфиг. 

| config

повреить коректность конфига

# consul validate /etc/consul.d/
bootstrap_expect > 0: expecting 3 servers
Configuration is valid!

причем фишка в том что мы указыем папку. где может лежать куча конфигов конула. он все поймет и все проверит.

| заставит консул перерчитать конфиг

# kill -SIGHUP $(pgrep consul)

тоесть прцесс у нас остается тот же мы просто просим перечиать свой конфиг.

так вот прикол в том что эта падла по крайней мере в одном случае пререитвать перечитывает но сука
новые настройки не применяем. например коогда я активировал ACL.
чтобы новые настройки ACL вступлииои в силу нужно обязательно РЕСТАРТОВАТЬ процесс. 
так что если новые натроки из когфига не вступают в силу значит попробуй рестартовать процесс.


| ACL

для начала его надо актививровать.
перед началом можно проверить может он уже активирван

# consul info | grep -i acl
  acl = disabled


тут реальный прикол. вот мы защли на машину с коснулом неважно это сервер или агент.
мы делаем запрос
# consul info | grep -i acl
  acl = disabled

и эта падла показыает активиован ли ACL конкретно на нем. а не то как это есть на лидере серверов консула.
поэтому как там на остальные клиентах или серверах мы незнаем. мы тлоько знаем как это есть на этом хосте на 
этом консуле(клиенте или сервере)

далее
активиовать на этом консуле можно вот так. создаем файл 

cat<<EOF >/etc/consul/acl.hcl
acl = { 
    enabled = true
    default_policy = "allow"
    enable_token_persistence = true
}
EOF

systemctl restart consul

здесь очен вжано сказать что чтобы настройка встуали в силу недостаточно сделать 
# systemctl reload consul
или 
# kill -SIGHUP $(pgrep consul)
это нихуя не прокатит. нужно обязательно перезапустить процесс.
но это мы сделаем только на одном хосте . дока от консула говорит что так надо сдеать на каждом 
сервере консула и на кажом клиенте. насколко я поимаю в реальносити так надо сделать на кажом серврее консула.
после того как мы перезапутили консул надо проверить что на этомхосте детйсвтено атктививовался ACL

# consul info | grep acl
  acl = enabled

  когда так будет на каждом сервере консула у нас успещно сработает следущая команда


# consul acl bootstrap
AccessorID:       f735707c-266b-cdb1-c689-9abf1fef13c6
SecretID:         b3cd3f8d-be92-94b4-f964-b8a8a9b1c9d4
Description:      Bootstrap Token (Global Management)
Local:            false
Create Time:      2023-09-04 21:17:06.027702728 +0000 UTC
Policies:
   00000000-0000-0000-0000-000000000001 - global-management


и нам выдадут мегатокен  с наивысшими правами.
вот эта строчка
SecretID:         b3cd3f8d-be92-94b4-f964-b8a8a9b1c9d4
исползуется как пароль прилогине в веб морде.
далее прикол в том что повторно если команду вызывать то она тебя пошлет нахер.

# consul acl bootstrap
Failed ACL bootstrapping: Unexpected response code: 403 (Permission denied: rpc error making call: ACL bootstrap no longer allowed (reset index: 9347))

таким обоаом эту команду можно запустить только один раз. и этот токен надо сочно записать на бумажку.

вот эта хрень (reset index: 9347) исполуется если мы забыли\потеряли наш главный супер токен и хотим его
восставноить тоесть заресеиить.

вот еще интересный вопсро - у нас есть три коснул сервера. мы открываем все три веб морды. вопрос - они данные
берут с мастера или с себя самого. моя практика показала что работае странно  - част данных еберется с локальной
ноды а часть с мастера.

несолько теории про ACL. в консуле исполщуется токены для того чтобы агент кога обращался к серверу то 
сервер его через токен как чере паспорт аутентифицирует и потом определяет что ему агенту можно делать а что нет.
когда токен серверу предьявлен то сервер смотррит куда этот токен на сервере привязан - токен может быть привязан 
к policy . в ней написано куда с этим токеном можено лазить. но также токен может  быть привязан к role.
роль это группа полиси. удобство роли в том что в ролль можно удалять и добавлять полиси. насколко я навскикду понял
что если токен привязали к полиси то хрень потом их разъединишь. пэтому если мы завели другую полиси то клиенту 
надо выдывать новый токен. а это остой. вместо этого лучше привязать токен к роли. и уже на сервере в эту роль
пихать либо исключать разные полиси.

далее там естьтакие хрени как SeriviceIdebtity и NodeIdentity они как то позволяют на основе шаблона налету
генерирвать полиси эт дает то что для одинаковых сервисов ненадо создавать руками однотпные полиси . вобщем это
пока  в тумане.

ксаттив доке от консула https://developer.hashicorp.com/consul/tutorials/security/access-control-setup-production
они реально коркетно пишут что после того как мы отредактирвали конфиг на консуле

cat<<EOF >/etc/consul/acl.hcl
acl = { 
    enabled = true
    default_policy = "allow"
    enable_token_persistence = true
}
EOF

надо его обяазтельно РЕСТАРТОВАТЬ.

разберем 
default_policy = "allow" == задает дефолтовую полиси. котоая раздрешает всем подряд конектится к консуд серверу. я так понял
enable_token_persistence = true  = это якобы сохраняет токен на диск. я так понял
правда нахернужен токен если и так можно конектится непонятно.


далее вопрос как аутентфициоваться через cli. ответ
# export CONSUL_HTTP_TOKEN=b3cd3f8d-be92-94b4-f964-b8a8a9b1c9d4

теперь можно создать токен для все серверов консула(без аутентфииакации через супер токен он пошлет нахер)
# consul acl token create -description "cnsl1 agent token" -node-identity "cnsl1:dc1"

AccessorID:       28fc61cb-236f-1407-0529-6c55a02f14b7
SecretID:         67e21c7f-e260-63d8-6406-a4cbf6befd04
Description:      cnsl1 agent token
Local:            false
Create Time:      2023-09-04 22:02:43.574033566 +0000 UTC
Node Identities:
   cnsl1 (Datacenter: dc1)

где cnsl1 = это dns имя сервера консула
dc1 = это датацентр консула


далее добавляем его в конфиг вот так

вот так было 
# cat acl.hcl 
acl = { 
    enabled = true
    default_policy = "deny"
    enable_token_persistence = true

}


вот так стало
# cat acl.hcl 
acl = { 
    enabled = true
    default_policy = "deny"
    enable_token_persistence = true

  tokens {
    agent  = "67e21c7f-e260-63d8-6406-a4cbf6befd04"
  }

}


далее я походу понял вот что - у нас есть мастер сервер. и насторйки в консул кластере оределяет он.
поэтому когда мы меняем конфиг на слейв ноде сервера консула то это нихрена не влияет ни на что. потому что все 
играют по правилам мастер ноды. поэтому то что я сменил default_policy на "deny"
и прописал токен ни на что нехрена не влияет. потому что я преезагрузил слейв ноду. поэтому надо обнволяеть вначале
все слейв ноды а мастер ноду уже вконце. когда я перезагружу мастер ноду и мастером станет какаято друая нода
то на всех нодах уже будет единый новый конфиг.




 # consul acl token create -description "cnsl2 agent token" -node-identity "cnsl2:dc1"
AccessorID:       f9455bc1-71e7-277f-6638-e28b8c0618a0
SecretID:         58d35c8a-c2bd-3579-4ed6-ec2bb44385b2
Description:      cnsl2 agent token
Local:            false
Create Time:      2023-09-04 22:19:05.408993456 +0000 UTC
Node Identities:
   cnsl2 (Datacenter: dc1)


суем этот токен на вторую ноду (мастером у нас третья)

 
 дальше дока пишет что созданный таким макаром токены для всех трех серверо консула
 они только дают право нодам консула делать свои дела серверные. а токены для пользовательских сервисов
 нужно создавать отдельно.


такиже пишут что такой ручной механизм нужен толко для сервров коснула. а для консулов с ролью клиента
там есть какито механизмы массового деплоя токенов. и ссылка дана . (https://developer.hashicorp.com/consul/tutorials/security-operations/docker-compose-auto-config)  но пока туда не лезу.

еще пишут что лучше токены хранить не в конфиге консула а в Vault. я тока не пойму а какая разница. 
в конфиге консула будет стоять некая хрень которая позволяет залезть в волт и получить этот токен. смысл?

что я еще непонял. ну установил я на серверах консула опцию default_policy = "deny"
так у меня все равно ранее устаовленные сервисы (скажем номад кластер) так и продолжают успешно работать. 
я чото тогда непонял толк отэтого ACL. 

в доке (https://developer.hashicorp.com/consul/tutorials/security/access-control-setup-production) написано что 
Service tokens are necessary for managing the service's information in the catalog, including registration and health checks.
ну окей - сервис который уже есть ему регистрировать ненужно. но тогда типа хелсчек недожен раотать. а он работает.
так что я непонял как тоогда ACL работает. походу же он нерабоает


далее.
про service discovery. вот толковая статья - https://www.baeldung.com/cs/service-discovery-microservices
в ней сказано вот что. в классической системе у нас жестко задано зарнее в конфигах компонентов о том 
как им друг с другом связываться. так вот в современном мире компоннты типа создаются и убиваются в докер контейнерах
и у них постоянно мняюются IP итд. и пэтому заранее адреса компонентов неизвестны. и они менюся. поэтому нужен механизм
чтоы один компоенет(сервис) мог найти другой сервис. тут приходит на помощь механизм Service Discovery.и есть 
два механизма service discovery : Client-Side Discovery и Server-Side Discovery. по мне названия идиотские. не отражают суть.
тем не менее. 
Client-Side Discovery выгляди так : наш сервис А который хочет связаться с сервисом Б делает запрос к Service Registry (прога в которой записан адрес сервиса Б). получает список адресов на которых сидит сервис Б. далее сервис А выбирает один из адресов
и делает туда запрос. выбор адресов называется лоад балансинг. и вот в стьте пишут о том что якобы это очень усложняет
код сервиса А о том как делать лоад балансинг. по мне это булшит. лоад балансинг это элементарная вещь. что такого сложного
выбрать один из десяти IP адресов из списка. тем не менее. они так говорят.

Server-Side Discovery выгляди так: сервис А обращается на лоад балансер. лоад балансер обращается на Service Registry.
лоаад балансер получается список IP адресов сервиса Б. выбирает адрес и далее непонятно - то ли проксирует запрос
то ли роутит запрос туда. таким макаром по сравеннию с предыдущим способом у нас всего навсего функция лоад балансера
вынесена на отдельный компоеннент . и только то. 

далее в саттье рассмаривается еще одна интеренсная вещь. кто и как должен регситриррвать сервис Б в service Registry.
по одной модели это должен делать сам сервис Б. и он же должен себя там дерегистрировать. они пишут что это усложняет 
код сервиса. другой способ что есть некий внешний комоенент который каким то неведоомым образом узнает что где то там
возник сервис  Б и он регистриует его на Service Registry и он же его дерегистриурует. 

в терминах консула Service Registry называется Service Catalog
консул в себе включает лоад балансер. то есть он дает вдомжность выполнить Server-Side Discovery.  ну окей.
как пишут в докумегатции консул непроксирует а роутит реквест. я тотка неочень понимаю у нас запрос идет на dest IP консула
а ответ сервису А прилетит от src IP сервиса Б. как это так может работать.

в сттатье от консула пишет что якобы классические лоад балансеры неимеют тех фишек что ест у него а именно
что они не предназначены для динамической регистрации сервисов. кхм. лоад балансры вобще  не про сервисы. они про 
IP:port . еще лоад балансеры типа не имеют high avalability. в целом да. сейчас не знаю как а раньше было так.
а консул типа имеет и то и другое.


как я понял есть две схемы работы сервиса А и консула. либо вот так
сервис А --> клиент консула ---> сервер консула
либо вот так
сервис А --> envoy proxy --> клиент консула ---> сервер консула

вариант с энвоем называется service mesh.


в консуле control plane-ом называется коснул сервера + консул клиенты. 
дата плейт это сервис А либо сервис А + энвой

в коснуле есть центровой термин - датацентр. они хитрят и юлят. но наконец то я узнал что же эта херня
значит по факту. значит консул состоит из набора агентов которые запущены в режиме сервер (далее просто сервер консула)
и набора агентов запушенных в режиме клиент (далее просто клиент консула). клиенты консула просто проксируют запрос от сервиса 
юзера которому надо узнать где нахрится другой сервис юзера до сервера консула. тоесть клиент консула знает где искать
серверы консула и как к ним делать запрос. сервис юзера только знает где искать клиент консула. 
на севрерах консула хранится база обо всех сервисах юзеров. северы консула выбирают среди себя лидера. он один. остальные
серверы консула включаются в режиме слейвой (в терминах консула в режим фолловеров). если реквест прилетел на слейв сервер
консула то как я понял он проксируется на лидер сервер консула. он считает данные со своей базы и оборатно отдает ответ.
также как я понял данные из базы лидера реплицируеются на базы фоловеров на случай если лидер сломается чтобы любой фолловер
мог сразу стать лидером. итак все пляше вокруг базы в которой лежит вся инфо о всех сервисах юзеров. слоов база фигуральная.
потому что консуле баз нету. есть просто каталог с файлами котоырй реплицируется. так вот набор серверов консула на одном из котрых есть база а на других есть реплика этой базы называется кластером. иногда в состав кластера формально также включают
и клиенты консула. итак еще раз кластер консула это набор серверов консула и опцилнально клиентов консула. 
сервера в кластере обладают той общностью что на одном из них хранися база кластера. а на других ее реплика. это и есть 
кластер консула.  так вот датацентр консула и кластер консула это одно и тоже. вот ответ на вопрос что такое блядь 
датацентр когнсула. напрактике. значит если у нас есть куча серверов консула которые входят в один датацентр. значит они входят 
в один кластер. значит что на них на всех хранися одна и таже база. в этом суть.
по мне термина датацентр в данном случае применем категорически неудачно. каждый сервер когсула может быть развернуть в
отдельном геограяическом датацентре. поэтому терминп применен категориечески неудачно.

как происходит регистрация юзерского сервиса в service registry\catalog registry консула. вот у нас есть хост. на нем крутится
юзерский сервис. он должен уметь общаться с клиентом консула. в конфиге сервиса пропиывается адрес клиента косула.
юзер сервис звонит на клиент консула и говорит я такой то сервис . зареистриуй меня в консуле. в клиенте консула
указаны ip адреса сеерврров консула . причем понятно что эти сервера должны входит в один кластер . или что тоже самое в
один датацентр. такого не может быть что там будет указана часть серверов одного кластера и часть из другого нет. 
клиент конектится только к одному кластеру. как я понял. такикм макаром клиент консула передает данные на один из серврео консула
и если этот сервер не лидер то он предает эти данные на лидер консула а тот запиывает их в базу. 
таким образом все юзер сервисы через клиенты консула регситриуется в одном кластере консула. и могут через него найти
друг  друг друга. 

 вдоках консула написано что можно связвать друг с другом кластеры консула. или как они гвоорят консулы из разных датацентров.
 пока я незнаю как это происходит. но я полагаю что на клиентах коснула ничего не проысывается про это дело. это должно 
 прориываться только на серверах консула.  далее как я понимаю шарманка рабоатет так. юзер клиент у котоого прописан 
 консул клиент который смотрит на консул сервера котоыре принадждержать кластеру А послывет запрос своему клиенту 
 с вопросом где сервис Б. клиент передает запросы свереру консула. тот ненаходит этот сервис и передает запрос
 серверам консула из другого кластера. я полагаю так это рабоатет. также я дмаю что юзер сервис не может себя зарегистрирвать
 в чужом консул кластере. толко в своем может. то есть в том кластере котоырй прописан в консул клиенте в который смотрит
 юзер сервис. ну и соовтевтенно если связь с другим кластером пропадает то нам это похер для доступа на сервисы 
 нашего кластера

 далее я наконец понял что консул дока исползует слово forward request а на самом деле по факту это значит прокси реквест.
  я подозреваю что они и слово роут использует там где надо было исползовать слово прокси. то есть некоректный тех нический
  язык.


далее. 
вот так можно зарегистрировать сервис в консуле
$ consul services register -name=web-test
при этом он пояивтся в разделе Serices в веб морде консула. и будет указано что этот сервис сидит
на хосте откуда мы эту команду ввели. ( ясен банан чтобы это все сработал надо чтобы на хосте был настроенный клиент
консула).

вот как проверить что сервис и где он сидит через DNS запрос
# dig @127.0.0.1 -p 8600   web-test.service.consul +short
10.113.151.192

тот же запрос через curl
$ curl http://localhost:8500/v1/catalog/services/web-test | jq


а вот запрос просмотреть список сервисов
# curl -s http://localhost:8500/v1/catalog/services | jq
{
  "consul": [],
  "nomad-client": [
    "http"
  ],
  "nomad-master": [
    "serf",
    "http",
    "rpc"
  ],
  "web-test": []
}

а еще можно вот так псмотреть список сервисов

# consul catalog services
consul
web-test-dc2
причем эту команду можно запускать как на сервере консула и на хосте где запущен клиент консула



далее.
задача - поднять второй консул кластер с другим значением параметра "датацентр". 
и потом надо связать первый кластер со вторым кластером.

поднимаем второй кластер с датацентр = dc2 согласно инструкции выше.
теперь нам надо их связать. 
для простоты на обоих кластерах выключен ACL.
так вот оказыается интересная вещь: у нас есть пароль который заранее прописан на каждом серверер когнсула
которым он шифрует свое общение с другими своими соседями. мы задаем его в конфиге строчкой

# cat /etc/consul.d/consul.json | grep encrypt
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",

так вот он хранится вот тут 

# cat /var/lib/consul/serf/local.keyring 
["EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY="]

# cat /var/lib/consul/serf/remote.keyring 
["EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY="]


так вот у консула есть два gossip pool. один это LAN gossip pool и wan gossip pool.
в  lan gossip pool входят все сервера и клиенты которые входят в конкретный датацентр.  все эти клиенты и серверы
котоыре входят в один gossip ppool они обмениваются друг с другом мессагами шифруя их одним и тем же ключом симметричного
шифрования который записан у них в конфиге. например вот как он записан в конфиге клиента

# cat /etc/consul.d/consul.json  | grep encrypt
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",

    тоесть рвно также как в конфиге сервера когсула.

кстати скаать если в конфиге консула не прописано в какой папке харнить данные то он их хранит в папке /var/consul
так вот в файле local.keyring  что сервер что клиент хранят пароль с помощью которого они шфируют данные которые 
они посылают друг другу в рамках своего датацентра. в рамках своего LAN GOSSIP POOL.


теперь мы пеерходим к файлу remote.keyring  и WAN GOSSIP POOL.  дело вот в чем. можно взять несколько (по крайней)
меере два кластера (два датацентра) консула и соединить их вместе. это называется федерация. точнее "Federate multiple datacenters with WAN gossip".  и тогда это дает вот что - что мы можем с клиента который сиидит в одном датацентре сделать
запрос сервиса и указать название друого датацентра. этот запрс улетить на сервер нашего датацентра. наш сервер свяжется
с сервером другого датацентра передаст запрос. получить ответ и прищлет нам.  так вот WAN GOSSIP POOL это все сервера
разных датацентров которые обьединены вместе в пул. все эти сервера могут посылать друг другу запросы. это и есть WAN POOL. это и есть федерация. так вот remote.keyring  хранить пароль симметрчиного шифрования с помощью которого все эти серверра из пула 
со всех этих разных датацентров шифруют друг другу сообщения. вот что это такое. этот файл есть только на серверах консула.
на клиентах его нет.  так вот когда у нас на сервреер задана в конфиге вот такая стрчка 

cat /etc/consul.d/consul.json  | grep encrypt
    "encrypt": "EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY=",

то один и тот же пароль записан и в  local.keyring  и в remote.keyring
я так и ненашел какой опцией в конфиге можно задать один пароль для lAN gossip pool 
а второй пароль для wan gossup pool. едиснвтенное как я придумал это идем в /var/lib/consul/serf/keyring.remote
и там руками вставляем другой пароль. так вот ! - чтобы мы могли обеьединить в федерацию консул сервререрв с разных
кластеров(разных датацентров) надо чтобы у них у всех был один и тот же пароль  в keyring.remote! иначе пошлет нахер.

само обьединенеие делается командой
# consul join -wan   cnsl1 cnsl2 cnsl3 consul1-dc2 consul2-dc2 consul3-dc2

в ней мы должны укзаать все серверы в обоих кластерах.
если все окей то оно напишет 
  Successfully joined cluster by contacting 6 nodes

посмотреть мемберов WAN GOSSIP pool можно вот так
# consul members -wan
Node             Address              Status  Type    Build   Protocol  DC   Partition  Segment
cnsl1.dc1        10.113.151.191:8302  alive   server  1.16.0  2         dc1  default    <all>
cnsl2.dc1        10.113.151.42:8302   alive   server  1.16.0  2         dc1  default    <all>
cnsl3.dc1        10.113.151.159:8302  alive   server  1.16.0  2         dc1  default    <all>
consul1-dc2.dc2  10.113.151.232:8302  alive   server  1.16.0  2         dc2  default    <all>
consul2-dc2.dc2  10.113.151.208:8302  alive   server  1.16.0  2         dc2  default    <all>
consul3-dc2.dc2  10.113.151.183:8302  alive   server  1.16.0  2         dc2  default    <all>

вот видно что у нас часть серверов из датацентра "dc1" а часть из "dc2"

теперь отвечаю на вопрос как нам сидя на клиенте в одном датацентра сделать запрос сервиса 
котоырй сиодит в другом датацентар. отвечаю:

$ curl -s http://localhost:8500/v1/catalog/services?datacenter=dc2 | jq
  
фишка в том что мы должны в запросе указать в каком датацентра мы ищем сервис. 
в данном случае "?datacenter=dc2" мы указывам что мы запршиваем сервис который в "dc2"
по дефолту если мы этот парметр опускаем то поиск идем в текущем датацентре.
этот параметр я нашел здесь = https://developer.hashicorp.com/consul/api-docs/catalog

можно запросить по другому

# consul catalog services -datacenter "dc1"
consul
nomad-client
nomad-master
web-test

# consul catalog services -datacenter "dc2"
consul
web-test-dc2


как указать что мы ищем сервис в другом датацентре через DNS запрос (через dig ) я не знаю.

вот интересная команда как нам прсмотреть ключи шифрования для gossip пулов
# consul keyring -list
WAN:
  EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY= [6/6]
dc1 (LAN):
  EneGuc0QfyfqtBuXafqMd5C/dui62f0BlZMMkCk3ypY= [7/7]
dc2 (LAN):
  4+Uua8iBoxc915+hTXosC8evoTTAIjKCVYTjn/ojGDM= [3/3]


здесь у нас показано два LAN gossip пула. и один WAN gossip пул. ихние ключи.

ужас состоит в том что в оф доке о том как обтединить два кластера в федерацию (https://developer.hashicorp.com/consul/tutorials/networking/federation-gossip-wan) эти суки ни словом не обмолвливаются о том что чтобы два клестра можно было обьединить
в одну федерацию у них в remote.keyring должен быть один и тот же пароль.
если будут стоять разные пароли то при поаытке выполнить команду 
#  consul join -wan ...
у нас вылезет ошибка 
    consul Failed to join No installed keys could decrypt the message

также . если у нас консул кластер входит в федерацию то тогда если мы зайдем в веб морду то там слева вверу
где написано название датацентра там мы если тыкнем то откроетмя список всех датацентров входящих в федерацию.


| nomad
порты которые ему нужны
4646- на нем сидит его веб морда
4647- через него контачат серверы и клиенты друг с другом
4648 - через него серверы контачат друг с другом
20,000-32,000 - через него как я понял номад пробрасыает наружу порты из контейнеров.


| nomad
| region

насколько я понял регион в номаде это ровно тоже самое что датацентр в консуле. тоесть эта такая виртуальная хрень
означающая что если у нас два сервера в номаде принадлежат одному region значит они входят в один кластер. 
также как и в консуле кластер номад серверов с одного региона можно обьединиит в фередацию с кластером из друого региона. 
это дает то что у нас пояялется единая веб морда панель управления этими двумя кластерами. сами кластера остаются
полностью незавиисмыми друг от друга. просто едина панель управления.

из видео я понял то что в рамках одного калстера серверп номада должны быть в плане сетевой доступности близко не далее
чем в 10 мс друг от друга иначе их протокол совместной деятеальности RAFT несможет их вместо номрально держать  и кластер 
будет разваливаться.

тоогда у меня вопрос что такое datacenter в терминах номада.
из чтения  и эксперимнта стало понятно что опция datacenter никакой функдаментальной роли в номаде не имеет.
тоесть изменение этого парамтера на сервере номдаа ни кчему не ведет. он также споокойно входит в состав 
кластера после перезапуска сервера. я единственное что полагаю возможно это может влиять както на то как реплицируется
состояние кластера (та самая папка) от лидера кластера на его фолловеров. возможно если два сервера вхоят в один 
датацентр то копия преедается от одного другому потому что так быстрее чем передвать ее от лидера который сидит
в другом датацентре (имеет в настройках другой датацентр).
таким образом на настройку datcenter в ноомаде в приицпе похеру. главное что они должны входить в один регион. тогда
они принадлежать одному кластеру. тоесть сервереры в которых указан один регион входят в один кластер.
как и в консуле разные кластеры номаа можно обьединять в одну федерацию. это не означает что кластеры сливаются
в один мегакластер. нихера. просто у них в веб морде появялется одна панель для управления. 


| nomad
mTLS (mutual tls)

не исползовать публичный CA (типа letsencrypt)
юзать только приватный CA
полезное видео - https://www.youtube.com/watch?v=31rvngI7vUk&ab_channel=HashiCorp

создавать сертификат для сервера в виде  server.region.nomad

хорошо бы наладит mTLS между серверами номада и клиентами номада. 
однако это означает что надо выпустить тонну сертификатов.


| nomad
| federation


обьдеинение двух кластеров номада в федерацию


федерация не делает из двух кластеров один.
она толко позволяет в веб морде удобно переключаться между одним и вторым.
и еще она позволяет ходить по двум кластерам в той же веб морде на основе одного токена. 
а без федерации нам нужно открывать две веб морды и  в каждой вводить свой индиивидуальный токен.
вот что дает федерация. а так по факту кластеры остаются независимымми.



само обтединение выполняется вот так
# nomad server join 10.113.151.192:4648 10.113.151.174:4648 10.113.151.231:4648 10.113.151.224:4648  10.113.151.233:4648  10.113.151.90:4648
Joined 6 servers successfully

в строке указаны все номад серверы из обоих кластеров.

я только непонял. это что можно любые сервера соединить номада? главное чтобы у них был доступ к порту 4648?
дело в том что в /var/nomad/serf уже никакого ключа шифрования нет который бы нужно было бы чтобы он совпадал.



# nomad server members
Name           Address         Port  Status  Leader  Raft Version  Build  Datacenter  Region
nm2-cl.ovh     10.113.151.192  4648  alive   false   3             1.6.1  dc2         ovh
nm3-cl.ovh     10.113.151.174  4648  alive   true    3             1.6.1  dc3         ovh
nm4-cl.ovh     10.113.151.231  4648  alive   false   3             1.6.1  dc4         ovh
nomad1-dc2.us  10.113.151.224  4648  alive   true    3             1.6.1  dc5         us
nomad2-dc2.us  10.113.151.233  4648  alive   false   3             1.6.1  dc5         us
nomad3-dc2.us  10.113.151.90   4648  alive   false   3             1.6.1  dc5         us

видно что два региона: 
 - ovh
 - us

 значит мы обьединли два кластера. (регион и кластер это синомнимы)


но дальше жопа.
да у нас в веб морде видно два региона.
но чтобы зайти в каждый надо вводить для кажодго свой токен.

так вот далее надо сделать еще вот что,
нам надо выбрать какой регион\кластер у нас будет типа главным. главным в том смысле что мы будем в веб морде вводить
токен который был создан на сервере этого главного региона а остальные неглавные регионы будут тоже принимать этот токен.
тоесть у нас в кажом регионе свой authentication authority. а нам надо чтобы он как бы был один. надо какойто из двух
сделать главным.

у меня два региона : ovh и us.
я выбираю что у меня главным регионом будет ovh регион. 
тогда я иду на сервер номада в ovh регионе и создаю специальный токен


(ovh регион) # nomad acl token create -type="management" -global=true \
  -name="Cluster A Replication Token" \
  -token="b1e55cc6-8289-6b6e-c779-9f0b0a30e96b"
Accessor ID  = d74642e7-2b8b-25c1-d4e9-169e18548788
Secret ID    = 6ca5eae5-adb9-1687-f2f3-214ab5c4f8c5
Name         = Cluster A Replication Token
Type         = management
Global       = true
Create Time  = 2023-09-06 21:07:53.77551344 +0000 UTC
Expiry Time  = <none>
Create Index = 2462
Modify Index = 2462
Policies     = n/a
Roles        = n/a

соотвесвтенно "-token="b1e55cc6-8289-6b6e-c779-9f0b0a30e96b" это bootstrap токен ovh региона (тот который с макс 
правами).


теперь идем на каждый сервер номада в "us" регионе и меняем конфиг в двух местах


  "acl": {
        ...
         "replication_token": "6ca5eae5-adb9-1687-f2f3-214ab5c4f8c5"
    }


"server": {
        ...
        "authoritative_region": "ovh",   <====================
        ...
    }


тоесть мы в разделе "acl" добавляем опцию "replication_token" (тот токен котоырй мы тлоько что создали)
и в разделе  "server" добавляем опцию "authoritative_region"

перезапускаем все номад сервера в регионе us.
в регионе ovh конфиги менять ненадо!

все! теперь идем в веб морду номада. ввводим там boostrap токен ovh региона и без проблем можем из одной
веб морды ходить и по региону ovh и по региону us !! вот что мы сделали! (регион можно выбирать из выпаадающего списка вверху слева). причем почемуто прикол такой что когда мы выбрали регион и тыкаем на "Servers" он почемуто покаызвает серверы 
обоих регионов. этого  я непонял прикола.


если же в конфиг не добавить "replication_token" то в логах номадов серверов региона us будет 
лезть вот такая ошибка

[ERROR] nomad: failed to fetch ACL binding rules from authoritative region: error="rpc error: Permission denied"
[ERROR] nomad: failed to fetch tokens from authoritative region: error="rpc error: Permission denied"

я немог настроить изанчально .потом этот пост (  https://stackoverflow.com/questions/65992236/unable-to-bootstrap-nomad-cluster-with-multi-region-setup-error-bootstrapping/77055474#77055474)

ГЕНИАЛЬНО!!!!


| nomad
как проверить коректность синтакисима манифеста джоба.

# nomad validate ./redis.hcl 
Driver configuration not validated since connection to Nomad agent couldn't be established.
Job validation successful

чтобы убрать ошибку "Driver configuration not validated since connection to Nomad agent couldn't be established."
надо перед эти вызвать команду
# nomad eval list
No evals found

тоесть вот так в итоге
# nomad eval list
No evals found
root@nm2-cl:~/nomad-jobs# nomad validate ./redis.hcl 
Job validation successful


| nomad
| redis


job "redis" {
  datacenters = ["dc1"]
  type        = "service"

  group "redis" {
    count = 1

    network {
      mode = "host"
      port "redis" {
        to = 6379
      }
    }

    task "redis" {
      driver = "docker"

      config {
        image = "redis:7"
        ports = ["redis"]
      }

      resources {
        cores  = 1
        memory = 256
      }
    }
  }
}



# nomad eval list
# nomad validate ./redis.hcl 
# nomad run redis.hcl



| consul
| service
| checks


вот у нас есть хост. на хосте есть клиент консула.
как на через клиент консула зарегистрировать на консул сервере новый сервис.
для этого создаем файл

# cat /etc/consul.d/web-2.json 
{
  "service": {
    "name": "web-2",
    "tags": ["frontend"],
    "port": 81

  }

}


# consul validate  /etc/consul.d/
Configuration is valid!
# systemctl restart consul
# journalctl -f -u consul


если не видим ошибок то идем в веб морду в раздел "services"
и видим наш новый сервис "web-2" зарегистрирован в консуле. причем статус у него зеленый.
ура. 
при том что на компе где консул клиент реально никакого сервиса не крутится. но он зеленый
только отому что у нас нет никаких проверок "checks" для того чтобы косул клиент мог проереврить
жив наш сервис или нет. тогда делаем вот как. поднимаем на компе реально новый сервис
а именно веб сервис


$ docker run --rm -d -v $(pwd)/index.html:/usr/share/nginx/html/index.html -p 81:80 nginx
$ cat index.html 
web-2

я заменил дефолтовую жинкс страницу с тем чтобы она была короткая и чтобы из нее было видно какой
конктено жинкс мы запросили.

итак у нас есть сервис на хосте . веб сервис. который откликается по 81 порту.
теперь меяем манифест консул сервиса


а именно я добавляю health check

# cat /etc/consul.d/web-2.json 
{
  "service": {
    "name": "web-2",
    "tags": ["frontend"],
    "port": 81,

    "check": {
      "args": [ "bash", "-c", "curl -s -I localhost:81  | grep HTTP | awk '{print $2}'" ],
      "interval": "10s",
      "timeout": "2s"
    }


  }

}


также надо установить еще одну опцию иначе консул агенту по дефолту запрещено запускать проги на хосте.
# cat consul.json 
{
    ...
    "enable_local_script_checks": true,
    ...

без этой опции у нас влогах будет ругань

agent: Error starting agent: error="Failed to register service \"web-2\": Scripts are disabled on this agent; to enable, configure 'enable_script_checks' or 'enable_local_script_checks' to true"

проверяем верность конфигов. перезпускаем консул

# consul validate  /etc/consul.d/
Configuration is valid!
# systemctl restart consul
# journalctl -f -u consul

идем в веб морду.
там  у нас в сервисе web-1 теперь есть хелс чеки. просто прекрасно!


также хочу обратить внимание на эту строчку
      "args": [ "bash", "-c", "curl -s -I localhost:81  | grep HTTP | awk '{print $2}'" ],
так как у нас есть pipe то работает только такая строка. с привлечением "bash -c"
вариант типа такого 
      "args": [ "curl", "-s", "-I", "localhost:81  | grep HTTP | awk '{print $2}'" ],
не прокатит
если есть pipe то обязательно нужен "bash -c"

также хочу показать вариант хелс чека когда мы знаем что мы опрашиваем именно HTTP сервис. и когда у него 
ответ короткий на выводе. (чтобы не перегружать засососом мусора консул агент)

# cat web-3.json 
{
  "service": {
    "name": "web-3",
    "tags": ["frontend"],
    "port": 82,

    "check": {
      "http": "http://localhost:82",
      "method": "GET",
      "interval": "10s",
      "timeout": "2s"
    }


  }

}


теость если в прошлом примере у нас типа хелсчека это "args"
то тут  у нас "http"

типы хесл чеков тут = https://developer.hashicorp.com/consul/docs/services/usage/checks
и тут https://developer.hashicorp.com/consul/docs/services/usage/checks
и тут https://developer.hashicorp.com/consul/docs/services/configuration/checks-configuration-reference


иитак в этом куске мы научились создавать манифесты сервисови регистрировать сервис в консуле.
также научились прилеплять к сервиса хелс чеки.

| consul
| mesh

я начал городить mesh. оказалось что это та еще запарка.
оказался это конкретный пиздец.


во первых надо активировать mesh на серврерах консула

# cat /etc/consul.d/mesh.hcl 
connect {
  enabled = true
}



во вторых надо активировать gRPC на клиенте

# cat /etc/consul.d/consul.json 
{
    ....
    "ports": {
         "grpc": 8502
     },
     ....


втреитьих надо в сервисе прописать что мы хотим юзать mesh



d# cat /etc/consul.d/web-4.json 
{


  "service": {
    "name": "web-4",
    "tags": ["frontend"],
    "port": 82,


######### секция про mesh
"connect": {
   "sidecar_service": {
       "port": 21000
        }
   },
#################



    "check": {
      "http": "http://localhost:82",
      "method": "GET",
      "interval": "10s",
      "timeout": "2s"
    }


  }



}





где
"port": 21000, = эат настройка как я понимаю говорит консулу о том что когда будет
запускасять энвой для этого сервиса то его надо посадить на порт 21000 ( я об этом ниже напишу) 



в четвертых надо постваить на хост где клиент прогу envoy. пробема в том что  для каждой версии консула
есть совместимость с версией энвоя. смотреть таблицу совместимости вот здесь 
  https://developer.hashicorp.com/consul/docs/connect/proxies/envoy#envoy-and-consul-client-agent
раздел "Envoy and Consul Client Agent"
подьбебка состоит в том что надо обратить внимание что там две сецкии по совместиости - энвой + консул агент. 
для него своя таблица и энвой+консул датаплейн. там другая таблца. смысл что значит енвой+консул датаплейн  я не понял.
ибо это как бутто значит случай кога мы ставим энвой на хосте где консул серевер а это вроде как неимеет смысла. хуй знает.
итак для косул агента 1.16 нужен энвой 1.26.4, 1.25.9, 1.24.10, 1.23.12
так вот пизда в том что даже для убунту 22.04 маккмиаальный энвой в apt репозитории это 1.8 тоесть пошел нахуй.
какой выход: нужно скачать на комп докер имадж с енвоем нужной версии. запустить контейеер. и экстрактировать 
из него бинарник энвоя и засунуть его в /usr/local/bin это пиздец. экстракт из контейнера делается вот так
      # docker cp    76533e32adc1:/usr/local/bin/envoy /usr/local/bin/envoy

но это еще не конец мучейний. 
нужно создать сервис для энвоя


# cat<<EOF > /etc/systemd/system/web-4-envoy.service 
[Unit]
Description="web-4 sidecar proxy service"
Requires=network-online.target
After=network-online.target


[Service]
ExecStart=/usr/local/bin/consul connect envoy -sidecar-for web-4 -admin-bind 127.0.0.1:19000
Restart=on-failure

[Install]
WantedBy=multi-user.target


здесь тоже охуенная засада порт :19000 это не порт который будет проксировать . нет нихуя. это некий 
административный порт энвоя. под которым можно на нем делать адмиинистративные работы. как я понял нам он нахуй ненужен
но он обязателен указать при запускке. важно понять что это нив коем случае на порт 21000. это отдельный порт 
для администрирования энвоя.


выше я писал что манифесте сервиса есть вот такое
"port": 21000, = эат настройка как я понимаю говорит консулу о том что когда будет
запускасять энвой для этого сервиса то его надо посадить на порт 21000

так вот запись про энвой в манифесте сервиса консула еще нихя этот энвой не запускает. энвой
запускается через системд юнит через строчку consul connect envoy -sidecar-for web-4 -admin-bind 127.0.0.1:19000
тоесть отдельная консольная команда консула запускает энвой. как я понимаю этот консул читает конфиг манифестов сервисов 
консула ( /etc/consul.d/web-4.json)и ищет там "web-4"  находт там порт 21000 и запускает энвой и сажает его на 21000 порт.

вот тольоко пробеващись столько шагов у нас будет успешно зареистрирован сервис "web-4" на консуле через энвой.

здесь я вижу огоромадую прблему навскидку в том что для каждого сервиса консула нужен свой systemd юнит энвоя. 
это какото пизец если честно. 

ноэто еще не конец пизец. потому что мы пок что поставили 1 сервис через энвой в консул. а надо еще поставит второй
и наладить между ними связь.


так тут я нашел пример сервиса из книжки. ставиим его
# wget https://github.com/consul-up/birdwatcher/releases/download/v1.0.1/frontend-linux-amd64
# mv frontend-linux-amd64 /usr/local/bin/frontend
# chmod +x /usr/local/bin/frontend

# cat<<EOF>/etc/systemd/system/example-frontend.service

[Unit]
Description="Frontend service"
# The service requires the VM's network
# to be configured, e.g., an IP address has been assigned.
Requires=network-online.target
After=network-online.target

[Service]
# ExecStart is the command to run.
ExecStart=/usr/local/bin/frontend

# Restart configures the restart policy. In this case, we
# want to restart the service if it fails.
Restart=on-failure

# Environment sets environment variables.
# We will set the frontend service to listen
# on port 6060.
Environment=BIND_ADDR=0.0.0.0:6060

# We set BACKEND_URL to http://localhost:7000 because
# that's the port we'll run our backend service on.
Environment=BACKEND_URL=http://localhost:7000

# The Install section configures this service to start
# automatically if the VM reboots.
[Install]
WantedBy=multi-user.target
EOF



# wget https://github.com/consul-up/birdwatcher/releases/download/v1.0.1/backend-linux-amd64
# chmod +x backend-linux-amd64
# mv backend-linux-amd64 /usr/local/bin/backend


# cat<<EOF >/etc/systemd/system/example-backend.service
[Unit]
Description="Backend service"
Requires=network-online.target
After=network-online.target

[Service]
ExecStart=/usr/local/bin/backend

Restart=on-failure

# We will set the backend service to listen
# on port 7000.
Environment=BIND_ADDR=0.0.0.0:7000

[Install]
WantedBy=multi-user.target
EOF

# systemctl daemon-reload
  systemctl enable example-frontend  example-backend
  systemctl start example-frontend example-backend
  systemctl status example-frontend example-backend


идем  в брайзер и тестим что сайт работает http://localhost:6060



но это только начало.

# cat<<EOF>/etc/consul.d/example-frontend.hcl
service {
   name = "frontend"

   # frontend runs on port 6060.
   port = 6060

   # The "connect" stanza configures service mesh
   # features.
   connect {
      sidecar_service {

         # frontend's proxy will listen on port 21100.
         port = 21100

         proxy {

                 # The "upstreams" stanza configures
                 # which ports the sidecar proxy will expose
                 # and what services they'll route to.
                  upstreams = [
                                {
                                   # Here you're configuring the sidecar proxy to
                                   # proxy port 6001 to the backend service.
                                   destination_name = "backend"
                                   local_bind_port = 6001
                                }
                  ]
         }
      }
  }
}
EOF

на счет апстрима и local_bind_port = 6001
что эта хрень значит. дело вот в чем.
наш frontend сервис будет обращаться к backend сервису. так вот этот бекенд сервис к коорому он будет 
обращаться нужно указать в upstream. причем порт 6001 это не тот порт который бекенд сервис реально слушает. нет.
это порт сайдкара на который мы будем обращаться чтобы он нас запроксировал чтобы мы попали на бекенд сервис.

(чем отличается сайдкар от прокси.  сайдкар это тот же самый прокси но сидит на том же хосте где и клиент. а простой прокси
сидит хер знает где обычно.)



на счет destination_name = "backend"
походу нужно указать имя в точности как будет указано в манифесте консула для бекенд сервиса.


получается сидит сервис фронтенд на хосте и слушает порт 6060.
также на хосте поднят прокси\сайдкар который слушает порт 21100 и если на 21100 приходит запрос
то он его проксирует на 6060
и если этот сервис обращается на порт 6001 то сайдкар проксирует запрос на сервис бекенд.



# consul validate /etc/consul*
Configuration is valid!




# cat<<EOF>/etc/consul.d/example-backend.hcl
service {
          name = "backend"

          # backend runs on port 7000.
          port = 7000

          meta {
                 version = "v1"
          }

          # The backend service doesn't call
          # any other services so it doesn't
          # need an "upstreams" stanza.
          #
          # The connect stanza is still required to
          # indicate that it needs a sidecar proxy.
          connect {
             sidecar_service {
                                # backend's proxy will listen on port 22000.
                                port = 22000
              }
          }
}
EOF

порт 7000 указывает что реально сервис сидит на хосте и слушает порт 7000
а порт 22000 указывает что сайдкар для этого сервиса долже роутить на него запросы если на сайдкар запрос
прилетел на порт 22000



# consul validate /etc/consul*
Configuration is valid!

# consul reload
Configuration reload triggered

# consul catalog  services
backend                 <================ 
backend-sidecar-proxy   <==========
consul
frontend                <============
frontend-sidecar-proxy  <============
nomad-client
web-2
web-3
web-4
web-4-sidecar-proxy


приколно что сайдкары показаны как отденые сервисы. а в веб морде их нет в разделе сервисы.
они типа хидден поэтому в веб модред невидны. 
также это докаызает что сайдкары по факту это отденые сервисы. тоесть когда мы в манифесте вставляем секцию сайдкар
мы как бутто на самом деле пропиваыем отдлеьный новый сервис ( я об  этом как раз читал на сайте)


у нас в веб морде косула для новых сервисов красный хелсчек потмоу что длякажодго сериса
нужно запустить его иниидвидуальный envoy.
и тут на помощь приходят шаблоны systemd которые уже более мнее реально помогут упостить и масшбтабировать этот процесс.


а именно вот мой конфиг для энвоя


# cat<<EOF > /etc/systemd/system/web-4-envoy.service 
[Unit]
Description="web-4 sidecar proxy service"
Requires=network-online.target
After=network-online.target


[Service]
ExecStart=/usr/local/bin/consul connect envoy -sidecar-for web-4 -admin-bind 127.0.0.1:19000
Restart=on-failure

[Install]
WantedBy=multi-user.target



вот здесь показан шаблон системд
https://gist.github.com/blake/96dd69b19f783223c029f63e5e511ee3

однако он недоратан сука. 
дело в том что там не указан параеметр -admin-bind 
а дело в том что без него envoy сука не стартует. точнее он тогда берет и делает его по умолчанию 127.0.0.1:19000
и залупа в том что если нам надо запутсти несоклько енвоев на хосте то тогда уже второй енвой не запуститься а ошлет нахуй
и скаже что порт 19000 занят. а именнно ошибка будет выглядеть так
     cannot bind '127.0.0.1:19000': Address already in use
так вот выход в том чтобы указать порт хитро -admin-bind 127.0.0.1:0
тогда системд будет сам автоматом искать тот порт который щас в системе свободен (подсмотрел вот тут https://github.com/envoyproxy/envoy/issues/1297) чувак предложил решение в форме конфига

 "admin": {                                       
   "access_log_path": "/dev/null",                
   "address": "tcp://127.0.0.1:0"                   
 } 

 однако в нашем случае мы запускаем энвой не сам по себе а через консул поэтому решение другое -admin-bind 127.0.0.1:0
 итого суммарный шаблон выглядит так

# cat<<EOF >/etc/systemd/system/envoy@.service
[Unit]
Description=Consul service mesh Envoy proxy for service %i
After=network.target consul.service
Requires=consul.service

[Service]
Type=simple
ExecStart=/usr/local/bin/consul connect envoy -sidecar-for=%i    -admin-bind 127.0.0.1:0
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF



# systemctl daemon-reload
  systemctl enable envoy@backend  envoy@frontend
  systemctl start  envoy@backend  envoy@frontend
  systemctl status envoy@backend  envoy@frontend

netstat покажет какйото ад

tcp        0      0 127.0.0.1:45055         0.0.0.0:*               LISTEN      26655/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         
tcp        0      0 0.0.0.0:21000           0.0.0.0:*               LISTEN      23836/envoy         


почемуто биндинг на один порт показыается несколько раз.
каждый инстанс зпапущенного енвоя жрет один порт на свою админку. один порт на то чтобы принимать запросы для сервиса консула
ради которого он запущен. итого два порта.

но в целом. теерперь идем в веб морду и у нас оба сервиса и frontend и backend зеленые .

вот этот трюк с щаблоном для systemd юнита для энвоя это крутой ход. на основе одного шаблона можно легко поднять
миллион энвоев на хосте. толкьо конечно по сути это идиотизм. это как бутто хапрокси запускает милион своих копий
для каждого порта который он проксирует. идиотизм.


создаем intention

идем на сервер коснула
# consul intention create -deny '*' '*'

идем в браузер вводим http://10.113.151.91:6060/ (10.113.151.91 lxd контйеенр где я развернул всю эту щарманку)
и вдим что все по прежнему работе. чт за хеня. ответ в том что в свойствавах systmd юнита для фронтенд сервиса у
нас стоит
Environment=BACKEND_URL=http://localhost:7000

тоесть наш фронтенд сервис плевал на сайдкары.он напряпую стучится на бекенд сервис. в обход сайдкаров
поэтому и работает. 

меняем на 
Environment=BACKEND_URL=http://localhost:6001

6001 этот тот порт который мы прписали в маниесте консула для сервиса фронтенд в разделе апстрим. 
тоесть это тот порт который является входящим для сайдкара коорый нас перенарпавить на бекенд сервис.

перезапускаем системд для фронтенд. идем в бразуер и видим что уже нащ сайт нерабоатет. теория 
совплаа с практивкой. 

идем на сервер консула и удаляем правило
# consul intention delete '*' '*'

идем в бразуер. сайт опять заработал


ссылка на то как создавать интешенсы. интешенсы - это херня которая разрешает или запрещает конект
от одного сервиса до другого сервиса в случае если они общаются через mesh.

ссылка где написано как создавать интешенсы = https://developer.hashicorp.com/consul/commands/intention

создаем правило которое запрещает все конекты через меш (это важно что только через меш запрешено а напрямую без проблем).
      # consul intention create -deny '*' '*'

создаем правило которое разрешает связь от сервиса frontend до сервиса backend через меш
      # consul intention create -allow frontend backend



====
>> вопросы


что я выяснил - что если  у нас два инстанса у сервиса. то под каждый нужен манифест. это понятно.
так вот в манифесте имя сервиса указается одинаковое в обоих слуаях а instance_id указыаетс разный.
пример

перый иснтанс
# cat example-backend.hcl 
service {
          id  = "back-7000"
          name = "backend"


второй инстанс
# cat example-backend2.hcl 
service {
          id   = "back-7001"
          name = "backend"


под ажый инстанас нужен свой инвой запускать. причем при заупуске эенвоя в параметере  -sidecar-for= надо указывать
не имя сервиса а его id!!!! 

в чем еще одна наебка. если проверка типа args и код возрвта 1 то для консула это значит что проверка пройдена
но с warning. и эту сука тогда считает что бекенд сервис работает. поэтому нужно в скрипте анализировать код возврата
и если он 1 то менять его на чтото кроме 0 и1  только ктогда окнсул считает что инстанс в состоянии fail
пример

 args  = [ "bash", "-c", "curl -s -I localhost:7001  | grep HTTP | grep 404; if [ $? -eq 1 ]; then  $(exit 5);fi" ]
посолку очти все скрипты вовзращают в резсултате ошибки 1 то это пиздец как это надо отселжитьва!



>> прикол. якобы вот так можно любо енвой поставить
export ENVOY_VERSION_STRING=x.y.z
curl -L https://func-e.io/install.sh | bash -s -- -b /usr/local/bin
взял отсюда = https://developer.hashicorp.com/consul/tutorials/developer-mesh/service-mesh-with-envoy-proxy?utm_source=docs


надо понять - если у меня ест сервис который дожен связываться с 5 другими сервисами. то мне что 5 энвоев поднимать.
или можно один а в нем пять портов ???

номад - что у них с оверлейными сетями. как докер с одного хоста может связаться с докером с другого хоста
как добавить токен для того чтобы номад мог на консуле зарегится?


