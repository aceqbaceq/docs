nomad

ставим номад на убунту
на основе этого https://www.linode.com/docs/guides/using-nomad-for-orchestration/

# sudo apt install wget gpg coreutils

# wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg

# echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list

# sudo apt update
# sudo apt install nomad

# nomad --version


далее юзаем вот это 
https://storiesfromtheherd.com/just-in-time-nomad-80f57cd403ca

для запуска номада на девеолперской машине (тестоввая инстляция)
# nomad agent -dev -bind 0.0.0.0 -log-level INFO

вот эту команду можно не запускать. ососбенно на вирт машине
# nomad ui
потому что по факту она просто нарпсто запускает бразуер с ссылкой 127.0.0.1:4646
а если у нас вирт машина не имеет граф режима то просто вылезет ошибка

Opening URL "http://127.0.0.1:4646"
Error opening URL: exec: "xdg-open": executable file not found in $PATH

нужно просто пробросить сокет 127.0.0.1:4646 наружу от вирт машины и запускать
уже с другго хоста просто в браузере.  для того чтобы пробрость 127.0.0.1:4646 наружу
из виртуальной машины то надо открыть файл iptables.txt и в нем найти секцию 

"| port forward
 | loopback
 | lo "



итак мы пробррсили 4646 наружу из вирт машины. тогда идем в бразуер
и набираем http://IP:4646
где IP это адрес виртуалки.
и мы попадем в веб мооду номада.


далее.
номад проодукт как я понял состоит из одного файла (бинарь)
и этот файл в себе содержит и cli и серверную часть и клиентскую часть(это не cli 
это другое). и это смешно . потому что это мегамонолит. щас мода разбивать монолит.
а тут что у к8 (фраза состоит из 5 бинарей)и что номад сосоттоит весь функционал
из одного бинаря. 

роль "сервер" за что она отвечает пока непоняно. как они пишут что эти типа аналог
мастер ноды у к8.

серверы оббразуют кластер (по ихнему регион). один из серверов явялется мастером
 а осталные пассивны. кластер стейт то "база"  в которой сохраняются всякие 
 там кластерные данные хранится у номада мастера в папке. у к8 переменные 
 кластера хранятся в etcd. а у номада стейт хранится в папке.  
далее написано чтото непнятное - написано что в эту папку данные записыаются по  алгоритму RAFT (как я помню это упрощенная веосия
 алгоритма PAXOS по которому мониторы в цефе приходят к конесунсу об изменении какойто переменной в кластере )
RAFT - Reliable, Replicated, Redundant, And Fault-Tolerant.
пока этот вопрос остатвляем про детали как рабтает RAFT. 
итак servver mode это такая хрень которая аналогична мастер ноде у к8.

есть client mode работы у номада это аналог воркер ноду у к8.

если к8 оперирует подами то номад помимо контейнеров можно еще оперировать виртуалками
как я понял.

еще раз у к8 это control plane сосотоящий из master nodes.
у номада это server nodes

у к8 это woker nodes
у номада это client nodes

то что мы будем запускать в номаде описывется в job file. он внутри распиыывается через 
язык HCL (hashicorp configuration language) (  у к8 это yaml)

у к8 это под. у номада это таск. 

task group это группа тасков

When defining a task, you specify: number of instances, task driver (e.g. docker vs Java vs ?), image, CPU resources (in MHz), and memory resources

task driver это некая хрень которая позволяет абстрагиррваться номаду от конкретики
таска (тоесть таск это либо контейнер либо виртуалка и между номадом и конейнером
стоит rask driver как слой абстрации )

если под к8 это кучка контейнеров то таск номада это контейнер

номаод таск группа это аналог пода у к8 . потому что под это кучка контйенеров.


далее берем job

job "2048-game" {
  datacenters = ["dc1"]  
  type = "service"
  group "game" {
    count = 1 # number of instances

    network {
      port "http" {
        static = 80
      }
    }
 
    task "2048" {
      driver = "docker"
 
      config {
        image = "alexwhen/docker-2048"

        ports = [
          "http"
        ]

      }

      resources {
        cpu    = 500 # 500 MHz
        memory = 256 # 256MB
      }
    }
  }
}


далее запускаем его
# nomad job run 2048-game.nomad

при этом у нас на хосте должен быть установлен docker

если все ок то в веб мооде будет написано что статус джоба  = running

если мы хотим юзать номад кластер в проде то нужно устанвливать нетолько номад
но и consul и vault это все хашикорп  проги

vault это прога для  работы  с secrets (секретами)

Consul is used as a service mesh — i.e. service discovery, DNS, and basic load-balancing (similar to Istio and Linkerd).  (пока тут мутное понимние)


consul это  видимо кластер или группа консул серверов и плюс его агенты устаолвенные
на nomad client машины.

консул это вроде как тоже один бинарник (щас все срут от этого копиятком) который может
работать либо в режиме клиента лиоб в режиме сервера


вот здесь вот есть картинка - https://storiesfromtheherd.com/just-in-time-nomad-80f57cd403ca
там нарсовано как увяывзаывается номад и консул.
а имено есть консул кластер. это отдельные машины.
есть набор машин с номад мастерами (они их зовут номад серверы) и есть ряд машин 
с номад воркерами (номад клиенты). на номад мастерах крутится контрол плейн номада.
на номал клиентах запускаются джобы(таски).  так вот консул клиенты (агенты консула) ставятся
на всех номал машинах тоесть и на номад мастерах и на номад клиентах.

как я понял в к8 его кластерные настройки(переменные) хранятся в etcd 
у номада это хранится в папке. папка хранится на мастер ноде. и реплицируется
на слейв ноды.

Consule - это service mesh. что это такое еще надо изучать

HCL - на нем пищутся файлов джобов. насколько я помню этот же HCL 
юзается в тераформе

то что у к8 обеспечивается чеерез kube-proxy (а что он делает?) у номада это делает консул
видимо через консул агенты

вот еще интересно - ищется аналогии между к8 и номадом

|Kubernetes                    |Nomad                                  |
|------------------------------|---------------------------------------|
|Control Plane (Master Node)   |Server                                 |
|Worker Node                   |Client                                 |
|Cluster                       |Region                                 |
|Kubelet                       |Client agent                           |
|Container Runtime             |Task Driver (not limited to containers)|
|Pod (smallest deployable unit)|Task Group                             |
|Container (in a Pod)          |Task (smallest deployable unit)        |
|Deployment manifest           |Job file                               |
|Kubernetes Deployment         |Nomad Job (for containers)             |
|Kubevirt (separate install)   |Nomad VM Task Driver                   |
|Service                       |Service                                |



номад типа может запускаь и контролировать - контейнеры, виртуалки, просто бинарные elf
файлы.

двигаем дальше.
вот здесь https://mykidong.medium.com/install-nomad-cluster-d9a40d2206f5
я читаю что к8 типа хороший для стейтлесс приложения и херовый для стейтфулл приложений.
а типа номад лучше подходит для стейтвфулл приложений.
говоится что консул сервер ноды  лучше ставит на туже виртуалку куда и номад сервер роли.
а консул клиенты(агенты) ставить на номад клиенты.

номад сервер роли прдлагается разворачитвать на 3-5 нодах. (макс 7 штук поддреживается)

чтобы установить номад кластер надо вначале иметь консул кластер.
итак ставим консул кластер
для начала поставим его прям руками, делаю по этой статье
https://computingforgeeks.com/how-to-install-consul-cluster-18-04-lts/?expand_article=1


# mkdir  ~/downloads
# cd ~/downloads
# wget https://releases.hashicorp.com/consul/1.16.0/consul_1.16.0_linux_amd64.zip
# sudo apt-get update && apt-get -y install unzip
# unzip ./consul_1.16.0_linux_amd64.zip
# chmod +x consul
# mv ./consul /usr/local/bin/






