lvm

---
как создать lvm раздел

вначале пересканируем доступные физ разделы на линуксе (см scsi.txt)

далее

$ sudo fdisk -l 

увидели доступные физ диски


теперь надо создать на диске таблицу разделов GPT
и один раздел на весь диск

# parted /dev/sde
mklabel gpt
mkpart
Partition name?  []?
File system type?  [ext2]? ext4
Start? 10M
End? 100%

(parted) set 1 lvm on
Number  Start   End     Size    File system  Name  Flags
 1      10.5MB  91.3GB  91.3GB  ext4               lvm


обращаю внимание что мы используем доп команду чтобы установить 
доп флаг lvm. (если все верно в столбце Flags должно быть написано lvm)

далее. создаем volume group

$ sudo vgcreate vg02 /dev/sde1
  Physical volume "/dev/sde1" successfully created
  Volume group "vg02" successfully created

а теперь на этом vg нарезаем столько lv и такого размера как нам надо

например создаем раздел lv 10GB
 
И ТУТ ВАЖНО УТОЧНИТЬ ! - что если мы планируем потом после форматирования получить 10GB чистого свободного места то нужно отрезать LVM кусок бОльшего размера на 10%!. иначе будет не 10Gb а 9.5Gb например. потом запорешься переделывать.

ДОБАВЬ 10% !!

$ sudo lvcreate vg02 -L 11G
  Logical volume "lvol0" created.

тут я останвоился на стадии создания ext4 раздела и что делать с
журналом.

форматируем раздела

$ sudo mkfs.ext4 /dev/mapper/vg02-lvol0

добавляем lv в /etc/fstab

/dev/mapper/vg02-lvol0          /var/lib/kubernetes-PV-folder/100G/00           ext4    errors=remount-ro,data=journal  0       2
/dev/mapper/vg02-lvol1          /var/lib/kubernetes-PV-folder/100G/01           ext4    errors=remount-ro,data=journal  0       2
/dev/mapper/vg02-lvol2          /var/lib/kubernetes-PV-folder/100G/02           ext4    errors=remount-ro,data=journal  0       2


дале надо пермишнсы поменять на смонтированном разделе чтобы владелец был mkadm.mkadm


---
как расширить раздел lv на LVM 

обращаю внимание на установку partition 

# pvcreate /dev/sde1

# vgdisplay

# vgextend vg01 /dev/sde1
  Volume group "vg01" successfully extended

# lvextend vg01/lvol0 -L 98G
  Size of logical volume vg01/lvol0 changed from 14.98 GiB (3835 extents) to 98.00 GiB (25088 extents).
  Logical volume lvol0 successfully resized.


~# resize2fs /dev/mapper/vg01-lvol0
resize2fs 1.42.13 (17-May-2015)
Filesystem at /dev/mapper/vg01-lvol0 is mounted on /var/lib/docker; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 7
The filesystem on /dev/mapper/vg01-lvol0 is now 25690112 (4k) blocks long.

--


как расширить pv_vg+lv

у нас есть диск sda
на нем есть sda3 который используется как pv в LVM
мы на сфере расшририли диск sda. теперь нам надо 
расширить sda3 и все что ниже (pv,vg,lv) для того 
чтобы в итоге на виртуалке раздел стал больше.

расширяем партицию через parted

# parted /dev/sda
(parted)> resizepart 3
(parted)> p
(parted)> q

расширяем pv

# pvresize /dev/sda3


проверяем

/# pvs
  PV         VG             Fmt  Attr PSize  PFree
  /dev/sda3  ubuntu-uefi-vg lvm2 a--  17.41g 10.62g
  
vg трогать непридется 

расшиярем lv

# lvextend ubuntu-uefi-vg/root -L 17000MB 

расшиярем уже линукс раздел (точку монтитвания)

# resize2fs /dev/ubuntu-uefi-vg/root

гоотово.

===========
lvm
thin 
thinpool

как создать thinpool

если у нас есть vgs на котором есть свободные блоки 
то на нем создаем thinpool вот так

# lvcreate -L 100M -T vg001/mythinpool

где vg001 = название vgs
mythinpool = название создаваемого пула


этот фин пул например нужен для lxd.

на этом фиинпуле уже можно создавать thin volumes
но для lxd это ненадо. он сам будет их создавать.

===
| ubuntu failed to connect to lvmetad
| failure

после перезагрузки линукс послал нахуй со словами

ubuntu failed to connect to lvmetad

как это лечить. идем в /etc/lvm/lvm.conf
находим там секцию general и вставляем туда волшебную фразу 

  use_lvmetad = 0



# cat /etc/lvm/lvm.conf | grep "global {" -A 10 
global {

  use_lvmetad = 0


после этого надо примонтировать эти диски на какойто live cd и пересоздать initrd
и тогда шарманка обратно заведется

==
|snapshot

создать снэпшот

# sudo lvcreate -s -n snap-erp-demo-2022-11-29 -L 49G ubuntu-vg/ubuntu-lv

на снэпшоте  будут лежать блоки оригиналььные с нашего массива. поэтому  при создании снэпшота ненужно 
отмонтировать точку монтровагия. ибо на нашем оригинальном диске будут лежть всегда самые последние актульные данные.

каждый раз когда мы хотим пистаь на наш оригиналный диск то lvm берет оригинальый блок и сует его на снэпшот раздел.
а на наш оригналрный диск сует новый блок. поэтому снэпшот удаляется мгновенно. при этом мы на оригинальном диске
имеет наши последние данные. а вот восстанввится до оригинала это уже процесс.  lvm надо брать блоки с снэпшот раздела
и обратно записывать на наш оригиналный диск. 

для этого надо отмонтроваться. 
и запустиь кманаду

# lvconvert --merge vg00/lvol1_snap

если у нас наш lv не отмонтрорван то линукс пощлеь нас нахуй фразой

lvcenvert Can't merge until origin volume is closed

проверяем отмонтрован ли у нас раздел через 

# df -h
# mount

у меня все было отмонтровано однако эта сука продолжала так писать.

пришлось преерегрузитться. далее вот что.  если у нас монтроуется наш раздел то он уже будет выглядеть как 
исходный (original) и в бекграунд режиме будет идти прозрачное сливание.
как определить что слитвание закончено.

# lvs vg-ssd-Heewu1aa/xfs-mysql-2
  LV          VG              Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  xfs-mysql-2 vg-ssd-Heewu1aa Owi-aos--- 230,00g             5,31   


  вот эта цифра 5,31 она будет уменьшаьбся
  пока не исчезнет это значит что снэпшот слит. 

  только после этого можно создавать новый снэпшот. а если попровбат создать +1 снэпшот
  во время того как он сливается то получим нахуй

  Snapshots of an origin that has a merging snapshot are not supported.

  а вот хорошая стьатья про эту ебалу с merge для lvm

  https://www.claudiokuenzler.com/blog/1070/lvm-restore-logical-volume-merging-snapshot-will-occur-on-next-activation
  ===
  | snapshot 

  еще раз как его сздать

  # lvcreate -s -n snap-mysql11-2023-08-14  -L 20G vg-ssd-Heewu1aa/xfs-mysql-2

  ==
  | lvm
  | snapshot
  | xfs

  снял снэпшот. и нихуя не могу его примонтровать

  # mount -t xfs -o ro /dev/vg01-hdd-Afgt56/snap-root2  /root/snapshot_mounted/
mount: /root/snapshot_mounted: wrong fs type, bad option, bad superblock on /dev/mapper/vg01--hdd--Afgt56-snap--root2, missing codepage or helper program, or other error.


окзывается
https://learn.redhat.com/t5/Platform-Linux/How-to-Mounting-an-XFS-LVM-snapshot/td-p/287


 dmesg command and you notice the following:
[86803.183360] XFS (dm-1): Filesystem has duplicate UUID 9e92f93c-1b03-4383-b753-ae4b449b6864 - can't mount
If you didn’t notice that, and checked /var/log/messages you would see something similar:
Feb 25 11:05:14 localhost kernel: XFS (dm-1): Filesystem has duplicate UUID 9e92f93c-1b03-4383-b753-ae4b449b6864 - can't mount

There are 2 things that you can do:
[1] Mount the filesystem without using its UUID
mount -o nouuid mount /dev/mapper/VG_DB-SNAP_MARIA /backups/maria/
or:
[2] Change the UUID of the snapshot LVM
xfs_repair -L /dev/mapper/VG_DB-SNAP_MARIA
xfs_admin -U $(uuidgen) /dev/mapper/VG_DB-SNAP_MARIA
mount /dev/mapper/VG_DB-SNAP_MARIA /backups/maria/

охуеть

=======

| LVM
| alignment


чтобы при добавлнии нового диска в LVM несоздавать gpt раздел (ибо это 
лишний труд) и при этом гарантировать alignment то можно делать вот так

  # pvcreate --dataalignment 8m /dev/sdb

делает то что LVM блоки начнут писатьнся на диске начиная с отступа 8МБ
(по дефолту этот отступ равен 1МБ)

а вот как проверить

 #  pvs -o +pe_start
  PV         VG Fmt  Attr PSize   PFree  1st PE 
  /dev/sda2  rl lvm2 a--  <31.00g     0    1.00m
  /dev/sdb      lvm2 ---   80.00g 80.00g   8.00m


========


